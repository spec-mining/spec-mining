ISSUE_LINK,PROBLEMATIC_API_EXIST,REASON,LIBRARY_NAME,API_NAME,ISSUE_DESCRIPTION,EXPECTED_VS_ACTUAL_BEHAVIOR,TRIGGER_CONDITIONS,REASON_FOR_DIFFICULTY_IN_DETECTION,ISSUE_TITLE,ISSUE_BODY,ANSWER_1,ANSWER_2,ANSWER_3,HUMAN_CLASSIFICATION,HUMAN_REASON
https://stackoverflow.com/questions/76876536,true,The issue does not involve a problematic API. The unexpected behavior is likely due to a misunderstanding of the feature names generated by the TfidfVectorizer.,scikit-learn,TfidfVectorizer,"The feature names generated by the TfidfVectorizer do not match the words in the input text. Some feature names are neither present in the input text nor in the tokens generated by the custom_tokenizer. Additionally, some feature names seem to be numeric values or arbitrary characters and words.",The expected behavior is for the feature names generated by the TfidfVectorizer to correspond accurately to the words in the input text.,The issue occurs when using the TfidfVectorizer with a custom tokenizer and input text that contains words not present in the feature names.,This issue might be challenging to detect because the TfidfVectorizer does not provide explicit error messages or warnings when the generated feature names do not match the input text.,Unexpected Feature Names in scikit-learn TfidfVectorizer,"I'm trying to use the TfidfVectorizer from scikit-learn to convert a list of words into a TF-IDF matrix. However, I'm encountering some unexpected behavior where the feature names generated by the vectorizer don't match the words in my input text.
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import brown
import re

txt = brown.words()
txt_str = "" "".join(txt)

def custom_tokenizer(text):
    tokens = re.findall(r'\b\w+\b', text.lower())
    return tokens

documents = [txt_str]

vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)
tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

print(feature_names)

I've noticed that some feature names in the output are neither present in my txt nor in the tokens generated by the custom_tokenizer. Additionally, some feature names seem to be numeric values or arbitrary characters and words.
I've tried various approaches, including using a custom tokenizer to ensure accurate tokenization, updating scikit-learn. It works on smaller texts as well as when the max_features is set to low enough value. However, for my project I do need all the features so I am trying to figure out what is going on.
Why I'm getting these unexpected feature names and how I can ensure that the feature names correspond accurately to the words in my input text?
some of the features are zwei' 'zworykin'.
","Have you some examples of unexpected feature names ?
From my side, I don't think there is any problem extracting features.
The words not present in feature names, are words with special characters like -  ' or $. WHen you extract the text with the function custom_tokenizer, you get two tokens for example polyether-type, gives ""polyether"" and ""type"" tokens
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import brown
import re

txt = brown.words()
txt_str = "" "".join(txt)

def custom_tokenizer(text):
    tokens = re.findall(r'\b\w+\b', text.lower())
    return tokens

documents = [txt_str]

vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)
tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

print(f""Nb features                     : { len(feature_names)}"")
print(f""Nb words in brown               : { len(txt) }"")

# nb uniq lowercase words in brown
mylist = list(dict.fromkeys(txt))
mylist = [ x.lower() for x in mylist]
print(f""Nb uniq lowercase words in brown: {len(mylist)}"")

# in li1 and not in li2
def Diff(li1, li2):
    set_dif = set(li1).difference(set(li2))
    temp3 = list(set_dif)
    return temp3
li3 = Diff(mylist,feature_names)

def info(text):
    custom_tokenizer(text)
    try:
        s=list(feature_names).index(text)
        print(f""The text {text} was found in features at position : {s}"")
    except:
        print(f""The text {text} was not found in features"")    
    

print(f""10 first words not in feature_names : \n { li3[0:10]} "")

info(""season"")
info(""mosquito"")

Nb features                     : 42432
Nb words in brown               : 1161192
Nb uniq lowercase words in brown: 56057
10 first words not in feature_names :
[""season's"", 'non-farm', ""chip-o's"",
'polyether-type', 'take-off', '1-0', 'spring-back', '$50',
'mosquito-plagued', 'anti-submarine']
The text season was found in features at position : 33478
The text mosquito was found in features at position : 24918
",,,false,
https://stackoverflow.com/questions/41949101,false,The behavior is not related to a problematic API. The issue is about the user's understanding of the scikit-learn LDA classifier and the linearity of the data.,,,,,,,scikit learn LDA giving unexpected results,"I am attempting to classify some data with the scikit learn LDA classifier.  I'm not entirely sure what to ""expect"" from it, but what I am getting is weird.  Seems like a good opportunity to learn about either a shortcoming of the technique, or a way in which I am applying it wrong.  I understand that no line could completely separate this data, but it seems that there are much ""better"" lines than the one it is finding.  I'm just using the default options.  Any thoughts on how to do this better?  I'm using LDA because it is linear in the size of my dataset.  Although I think a linear SVM has a similar complexity.  Perhaps it would be better for such data?  I will update when I have tested other possibilities.

The picture: (light blue is what my LDA classifier predicts will be dark blue)



The code:

import numpy as np
from numpy import array
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
import itertools

X = array([[ 0.23125754,  0.79170351],
       [ 0.78021491, -0.24999486],
       [ 0.00856446,  0.41452734],
       [ 0.66381753, -0.09872504],
       [-0.03178685,  0.04876317],
       [ 0.65574645, -0.68214948],
       [ 0.14290684,  0.38256002],
       [ 0.05156987,  0.11094875],
       [ 0.06843403,  0.19110019],
       [ 0.24070898, -0.07403764],
       [ 0.03184353,  0.4411446 ],
       [ 0.58708124, -0.38838008],
       [-0.00700369,  0.07540799],
       [-0.01907816,  0.07641038],
       [ 0.30778608,  0.30317186],
       [ 0.55774143, -0.38017325],
       [-0.00957214, -0.03303287],
       [ 0.8410637 ,  0.158594  ],
       [-0.00294113, -0.00380608],
       [ 0.26577841,  0.07833684],
       [-0.32249375,  0.49290502],
       [ 0.11313078,  0.35697211],
       [ 0.41153679, -0.4471876 ],
       [-0.00313315,  0.30065913],
       [ 0.14344143, -0.19127107],
       [ 0.04857767,  0.01339191],
       [ 0.5865007 ,  0.71209886],
       [ 0.08157439,  0.40909955],
       [ 0.72495202,  0.29583866],
       [-0.09391461,  0.17976605],
       [ 0.06149141,  0.79323099],
       [ 0.52208024, -0.2877661 ],
       [ 0.01992141, -0.00435266],
       [ 0.68492617, -0.46981335],
       [-0.00641231,  0.29699622],
       [ 0.2369677 ,  0.140319  ],
       [ 0.6602586 ,  0.11200433],
       [ 0.25311836, -0.03085372],
       [-0.0895014 ,  0.45147252],
       [-0.18485667,  0.43744524],
       [ 0.94636701,  0.16534406],
       [ 0.01887734, -0.07702135],
       [ 0.91586801,  0.17693792],
       [-0.18834833,  0.31944796],
       [ 0.20468328,  0.07099982],
       [-0.15506378,  0.94527383],
       [-0.14560083,  0.72027034],
       [-0.31037647,  0.81962815],
       [ 0.01719756, -0.01802322],
       [-0.08495304,  0.28148978],
       [ 0.01487427,  0.07632112],
       [ 0.65414479,  0.17391618],
       [ 0.00626276,  0.01200355],
       [ 0.43328095, -0.34016614],
       [ 0.05728525, -0.05233956],
       [ 0.61218382,  0.20922571],
       [-0.69803697,  2.16018536],
       [ 1.38616732, -1.86041621],
       [-1.21724616,  2.72682759],
       [-1.26584365,  1.80585403],
       [ 1.67900048, -2.36561699],
       [ 1.35537903, -1.60023078],
       [-0.77289615,  2.67040114],
       [ 1.62928969, -1.20851808],
       [-0.95174264,  2.51515935],
       [-1.61953649,  2.34420531],
       [ 1.38580104, -1.9908369 ],
       [ 1.53224512, -1.96537012]])

y = array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.])

classifier = LDA()
classifier.fit(X,y)

xx = np.array(list(itertools.product(np.linspace(-4,4,300), np.linspace(-4,4,300))))
yy = classifier.predict(xx)
b_colors = ['salmon' if yyy==0 else 'deepskyblue' for yyy in yy]
p_colors = ['r' if yyy==0 else 'b' for yyy in y]
plt.scatter(xx[:,0],xx[:,1],s=1,marker='o',edgecolor=b_colors,c=b_colors)
plt.scatter(X[:,0], X[:,1], marker='o', s=5, c=p_colors, edgecolor=p_colors)
plt.show()


UPDATE: Changing from using sklearn.discriminant_analysis.LinearDiscriminantAnalysis to sklearn.svm.LinearSVC also using the default options gives the following picture:



I think using the zero-one loss instead of the hinge loss would help, but sklearn.svm.LinearSVC doesn't seem to allow custom loss functions.

UPDATE: The loss function to sklearn.svm.LinearSVC approaches the zero-one loss as the parameter C goes to infinity.  Setting C = 1000 gives me what I was originally hoping for.  Not posting this as an answer, because the original question was about LDA.

picture:


","LDA models each class as a Gaussian, so the model for each class is determined by the class' estimated mean vector and covariance matrix.
Judging by the eye only, your blue and red classes have approximately the same mean and same covariance, which means the 2 Gaussians will 'sit' on top of each other, and the discrimination will be poor. Actually it also means that the separator (the blue-pink border) will be noisy, that is it will change a lot between random samples of your data.

Btw your data is clearly not linearly-separable, so every linear model will have a hard time discriminating the data.

If you must use a linear model, try using LDA with 3 components, such that the top-left blue blob is classified as '0', the bottom-right blue blob as '1', and the red as '2'. This way you will get a much better linear model. You can do it by preprocessing the blue class with a clustering algorithm with K=2 classes.
",,,false,
https://stackoverflow.com/questions/30066268,false,"The behavior is not related to a problematic API. The issue is about the user's desire to provide starting positions to the t-distributed Stochastic Neighbor Embedding (TSNE) algorithm in scikit-learn, which is not currently supported.",,,,,,,Provide Starting Positions to t-distributed Stochastic Neighbor Embedding (TSNE) in scikit-learn,"I've been looking at using scikit learns' TSNE method to visualize high dimensional data in 2D. However, I have some idea of where the starting positions should be in 2D space but I don't see any way of specifying this information. Any ideas how I might be able to provide the starting coordinates?

I see that there's an ""init"" parameter but it appears to only take ""random"" or ""pca"" as options. What I'm looking for is a way to specify exactly what the initial coordinates are.

Update

I'm trying to increase the distance between my data points (~6,500 points) but they are quite tightly clustered and overlapping. With default parameters, I get:

model = sklearn.manifold.TSNE(n_components=2, random_state=0)




A slight increase in n_iter and early_exaggeration didn't produce significantly different results as the points (within clusters) are still overlapping.

model = sklearn.manifold.TSNE(n_components=2, random_state=0, n_iter=10000, early_exaggeration=10)




However, increasing the early_exaggeration from 10 to 100 (which, according to the docs, should increase the distance between clusters) produced some unexpected results (I ran this twice and it was the same result):

model = sklearn.manifold.TSNE(n_components=2, random_state=0, n_iter=10000, early_exaggeration=100)




This link: https://beta.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm Provides an example (three quarters of the way down the page) for how to monkey patch the gradient_descent function to save coordinates/positions. 

Added Issue to scikit-learn
","It is currently not possible, but it would be a two-line change.
I think it would be a good addition, and we do support init=array for things like k-means. So PR welcome.
",,,false,
https://stackoverflow.com/questions/36865542,false,"The behavior is not related to a problematic API. The issue is about the user's unexpected results when using the Adjusted Mutual Information (AMI) index in scikit-learn, which has been fixed in the development version.",,,,,,,Adjusted Mutual Information (scikit-learn),"I have implemented a clustering algorithm for summarizing log files, and am currently testing it against ground-truth data with the Adjusted Rand index and the Adjusted Mutual Information index. 

Input to my algorithm is a list of log entries, and output is a list of integers (the cluster label that each item belongs to). The ground truth is similarly a list of integers where each integer represent the true cluster the item belongs to. For most of my test cases I receive normal/expected results, but one file is giving me unexpected output. I have enclosed the two lists, the ground-truth clustering as well as that of my algorithm's:

Ground truth list:
http://pastebin.com/9Y5TE6b7

Own clustering:
http://pastebin.com/hJz1M4sf

These two lists are fed into scikit-learn functions to get the ARI and AMI. The ARI score looks roughly correct, but AMI is above 1, which according to the documentation and definition of AMI should not be possible if I understand it correctly. This data set is highly unbalanced, but many of my other files are similarly balanced. I cannot figure this out. For reference, the scores I get for ARI and AMI is:

ARI: 0.99642743999922712

AMI: 1.0190170466324
","This has been fixed in the development version.
",,,false,
https://stackoverflow.com/questions/12358965,false,"The behavior is not related to a problematic API. The issue is about the user's unexpected results when using the RandomForestClassifier in scikit-learn, which can be attributed to the highly imbalanced dataset and the potential need for preprocessing or oversampling.",,,,,,,scikit-learn RandomForestClassifier produces &#39;unexpected&#39; results,"I'm trying to use sk-learn's RandomForestClassifier for a binary classification task (positive and negative examples). My training data contains 1.177.245 examples with 40 features, in SVM-light format (sparse vectors) which I load using sklearn.dataset's load_svmlight_file. It produces a sparse matrix of 'feature values' (1.177.245 * 40) and one array of 'target classes' (1s and 0s, 1.177.245 of them). I don't know whether this is worrysome, but the trainingdata has 3552 positives and the rest are all negative.

As the sk-learn's RFC doesn't accept sparse matrices, I convert the sparse matrix to a dense array (if I'm saying that right? Lots of 0s for absent features) using .toarray(). I print the matrix before and after converting to arrays and that seems to be going all right.

When I initiate the classifier and start fitting it to the data, it takes this long:

[Parallel(n_jobs=40)]: Done   1 out of  40 | elapsed: 24.7min remaining: 963.3min
[Parallel(n_jobs=40)]: Done  40 out of  40 | elapsed: 27.2min finished


(is that output right? Those 963 minutes take about 2 and a half...)

I then dump it using joblib.dump.
When I re-load it:

RandomForestClassifier: RandomForestClassifier(bootstrap=True, compute_importances=True,
        criterion=gini, max_depth=None, max_features=auto,
        min_density=0.1, min_samples_leaf=1, min_samples_split=1,
        n_estimators=1500, n_jobs=40, oob_score=False,
        random_state=&lt;mtrand.RandomState object at 0x2b2d076fa300&gt;,
        verbose=1)


And test it on real trainingdata (consisting out of 750.709 examples, exact same format as training data) I get ""unexpected"" results. To be exact; only one of the examples in the testingdata is classified as true. When I train on half the initial trainingdata and test on the other half, I get no positives at all.

Now I have no reason to believe anything is wrong with what's happening, it's just that I get weird results, and furthermore I think it's all done awfully quick. It's probably impossible to make a comparison, but training a RFClassifier on the same data using rt-rank (also with 1500 iterations, but with half the cores) takes over 12 hours...

Can anyone enlighten me whether I have any reason to believe something is not working the way it's supposed to? Could it be the ratio of positives to negatives in the training data? Cheers.
","Indeed this dataset is very very imbalanced. I would advise you to subsample the negative examples (e.g. pick n_positive_samples of them at random) or to oversample the positive example (the latter is more expensive and but might yield better models).

Also are you sure that all your features are numerical features (larger values means something in real life)? If some of them are categorical integer markers, those feature should be exploded as one-of-k boolean encodings instead as scikit-learn implementation of random forest s cannot directly deal with categorical data.
",,,false,
https://stackoverflow.com/questions/53215887,false,"The behavior is not related to a problematic API. The issue is about the user's unexpected result when creating a modification of the KNeighborsClassifier in scikit-learn, which can be attributed to a misunderstanding of the parameter grid search and how it interacts with the custom classifier.",,,,,,,unexpected result for inheritance in scikit-learn,"I am getting an unexpected result when trying to create a simple modification of the class KNeighborsClassifier:

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)

class my_Classifier(KNeighborsClassifier):  
  """"""My Nearest Neighbour classifier""""""
  def __init__(self, gamma=0):
    def my_dist(x, y):  # squared distance
      return np.sum((abs(x-y))**gamma)
    KNeighborsClassifier.__init__(self, n_neighbors=1, metric=my_dist)
    self.gamma = gamma
    print(gamma)
  def fit(self, X, y):
    KNeighborsClassifier.fit(self, X, y)
    return self
  def predict(self, X, y=None):
    return KNeighborsClassifier.predict(self, X)
  def score(self, X, y):
    return KNeighborsClassifier.score(self, X, y)


I have made the metric dependent on a parameter gamma&gt;=0.  If gamma=2, this is just the squared Euclidean metric, and if gamma=0, this is a useless metric (essentially a constant).  First it works as expected: for

knn = my_Classifier(gamma=2)
knn.fit(X_train, y_train)
knn.score(X_test,y_test)


the output is

2
0.9736842105263158


(the accuracy is good) and for

knn = my_Classifier(gamma=0)
knn.fit(X_train, y_train)
knn.score(X_test,y_test)


the output is

0
0.34210526315789475


(the accuracy is hopeless).  I am also printing the value of gamma used.

However, when I try

from sklearn.model_selection import GridSearchCV
param_grid = {'gamma': [0,2]}
grid_search = GridSearchCV(my_Classifier(), param_grid)
grid_search.fit(X_train, y_train)
grid_search.score(X_test, y_test)


the result is unexpected: 

0
0
0
0
0
0
0
0
0.34210526315789475


Why is the value gamma=0 used every time?  And gamma=2 (producing a much better result) is never tried.  I know I am making some silly mistake but can't see where.
","The value gamma = 0 is not used every time !

What you see when you get this:

0
0
0
0
0
0
0
0
0.34210526315789475


Is the score



To see the gamma use this:

grid_search.cv_results_ 

",,,false,
https://stackoverflow.com/questions/69916926,false,The issue does not involve a problematic API. The error encountered during the installation process is likely due to a configuration or setup problem rather than an issue with the conda API itself.,,,,,,,Unexpected error when trying to install conda environment from .yaml file,"I am trying to install a conda environment in WSL2 from a .yaml file, the instructions for the installation saying the first step is to run the following command:
conda env create -f devtools/conda-envs/ael-test.yaml

After the WSL2 terminal collects the package metadata and solves the environment, it encounters an error, which I don't really know how to solve. I run the command above from the base environment, from the directory where /devtools/conda-ens/ael-test.yaml is located.
Collecting package metadata (repodata.json): done
Solving environment: done
Preparing transaction: failed

# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

    Traceback (most recent call last):
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/exceptions.py"", line 1079, in __call__
        return func(*args, **kwargs)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/cli/main.py"", line 80, in do_call
        exit_code = getattr(module, func_name)(args, parser)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/cli/main_create.py"", line 141, in execute
        result[installer_type] = installer.install(prefix, pkg_specs, args, env)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/installers/conda.py"", line 59, in install
        unlink_link_transaction.execute()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 245, in execute
        self.verify()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 222, in verify
        self.prepare()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 211, in prepare
        grps = self._prepare(self.transaction_context, stp.target_prefix,
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 288, in _prepare
        packages_info_to_link = tuple(read_package_info(prec, pcrec)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 288, in &lt;genexpr&gt;
        packages_info_to_link = tuple(read_package_info(prec, pcrec)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/gateways/disk/read.py"", line 89, in read_package_info
        package_metadata = read_package_metadata(epd)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/gateways/disk/read.py"", line 144, in read_package_metadata
        data = json.loads(f.read())
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/__init__.py"", line 357, in loads
        return _default_decoder.decode(s)
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/decoder.py"", line 337, in decode
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/decoder.py"", line 355, in raw_decode
        raise JSONDecodeError(""Expecting value"", s, err.value) from None
    json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

`$ /home/gheorghe/anaconda3/bin/conda-env create -f devtools/conda-envs/ael-test.yaml`

  environment variables:
                 CIO_TEST=&lt;not set&gt;
  CONDA_AUTO_UPDATE_CONDA=false
        CONDA_DEFAULT_ENV=base
                CONDA_EXE=/home/gheorghe/anaconda3/bin/conda
             CONDA_PREFIX=/home/gheorghe/anaconda3
    CONDA_PROMPT_MODIFIER=(base)
         CONDA_PYTHON_EXE=/home/gheorghe/anaconda3/bin/python
               CONDA_ROOT=/home/gheorghe/anaconda3
              CONDA_SHLVL=1
           CURL_CA_BUNDLE=&lt;not set&gt;
                     PATH=/home/gheorghe/anaconda3/bin:/home/gheorghe/.vscode-server/bin/f4af3cb
                          f5a99787542e2a30fe1fd37cd644cc31f/bin:/home/gheorghe/anaconda3/bin:/ho
                          me/gheorghe/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbi
                          n:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Program Files
                          /WindowsApps/Microsoft.WindowsTerminal_1.11.2921.0_x64__8wekyb3d8bbwe:
                          /mnt/c/Program Files/copasi.org/COPASI 4.29.228/bin:/mnt/c/Windows/sys
                          tem32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/Syste
                          m32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Prog
                          ram Files (x86)/Intel/Intel(R) Management Engine
                          Components/DAL:/mnt/c/Program Files/Intel/Intel(R) Management Engine
                          Components/DAL:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA
                          NvDLISR:/mnt/c/Program Files (x86)/NVIDIA
                          Corporation/PhysX/Common:/mnt/c/Program Files/Mullvad VPN/resources:/m
                          nt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt
                          /c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/Ope
                          nSSH:/mnt/c/Program Files (x86)/Common Files/Propellerhead
                          Software/ReWire:/mnt/c/Program Files/Common Files/Propellerhead
                          Software/ReWire:/mnt/c/Program Files/NCBI/blast-2.12.0+/bin:/mnt/c/Use
                          rs/gheor/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/ghe
                          or/AppData/Local/Microsoft/WindowsApps:/snap/bin
       REQUESTS_CA_BUNDLE=&lt;not set&gt;
            SSL_CERT_FILE=&lt;not set&gt;

     active environment : base
    active env location : /home/gheorghe/anaconda3
            shell level : 1
       user config file : /home/gheorghe/.condarc
 populated config files : /home/gheorghe/.condarc
          conda version : 4.10.3
    conda-build version : 3.21.5
         python version : 3.8.11.final.0
       virtual packages : __linux=5.10.16.3=0
                          __glibc=2.31=0
                          __unix=0=0
                          __archspec=1=x86_64
       base environment : /home/gheorghe/anaconda3  (writable)
      conda av data dir : /home/gheorghe/anaconda3/etc/conda
  conda av metadata url : None
           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /home/gheorghe/anaconda3/pkgs
                          /home/gheorghe/.conda/pkgs
       envs directories : /home/gheorghe/anaconda3/envs
                          /home/gheorghe/.conda/envs
               platform : linux-64
             user-agent : conda/4.10.3 requests/2.26.0 CPython/3.8.11 Linux/5.10.16.3-microsoft-standard-WSL2 ubuntu/20.04.3 glibc/2.31
                UID:GID : 1000:1000
             netrc file : None
           offline mode : False


An unexpected error has occurred. Conda has prepared the above report.

If submitted, this report will be used by core maintainers to improve
future releases of conda.

Edit:
This is the .yaml file:
channels:
  - conda-forge
  - pytorch
dependencies:
  - python
  - pip

  - numpy
  - scipy
  - scikit-learn

  - qcelemental
  - openbabel

  - matplotlib
  - seaborn

  - mlflow

  - pytorch
  - torchvision

  - lark-parser
  - cython

  - pip:
    - torchani
    - ""git+https://github.com/RMeli/mdanalysis.git@develop#egg=MDAnalysis&amp;subdirectory=package""

  - black
  - flake8
  - mypy
  - isort

  - pytest
  - pytest-xdist
  - pytest-cov
  - codecov

","There is no problem in your environment, I managed to install it, getting
Successfully built MDAnalysis
Installing collected packages: msgpack, tqdm, mrcfile, mmtf-python, gsd, fasteners, biopython, GridDataFormats, torchani, MDAnalysis
Successfully installed GridDataFormats-1.0.1 MDAnalysis-2.6.0.dev0 biopython-1.81 fasteners-0.18 gsd-3.1.1 mmtf-python-1.1.3 mrcfile-1.4.3 msgpack-1.0.5 torchani-2.2.3 tqdm-4.66.1

done
#
# To activate this environment, use
#
#     $ conda activate ael

in the end.
The problem you encountering is probably related to https://github.com/conda/conda/issues/9590
which has a workaround
sudo rm -r ~/.condarc

or editing the ~/.condarc like https://github.com/conda/conda/issues/9590#issuecomment-1003211237
",,,false,
https://stackoverflow.com/questions/17794313,true,"The issue involves scikit-learn's SVM classification algorithm with the RBF kernel. The behavior observed is unexpected, with the entire area being classified as yellow except for the red and blue bits. This indicates a problem with the SVM algorithm or its parameters.",,,,,,,Unexpected results when using scikit-learn&#39;s SVM classification algorithm (RBF kernel),"Using the example on this page 
http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html,
I created my own graphs using some normally distributed data with a standard deviation of 10 instead of the iris data.

My graph turned out to be like this:


Notice how the RBF kernel graph is very different from the the one from the example. The entire area is classified to be yellow except the red and blue bits. In other words there are too many support vectors. I have tried changing C and degree but they didn't help. The code I used to produce this graph is shown below. 

Please note I need to use RBF kernel because polynomial kernels run significantly slower than RBF kernels.

import numpy as np
import pylab as pl
from sklearn import svm, datasets

FP_SIZE = 50
STD = 10

def gen(fp):

  data = []
  target = []

  fp_count = len(fp)

  # generate rssi reading for monitors / fingerprint points
  # using scikit-learn data structure
  for i in range(0, fp_count):
    for j in range(0,FP_SIZE):
      target.append(i)
      data.append(np.around(np.random.normal(fp[i],STD)))

  data = np.array(data)
  target = np.array(target)

  return data, target

fp = [[-30,-70],[-58,-30],[-60,-60]]

data, target = gen(fp)

# import some data to play with
# iris = datasets.load_iris()
X = data[:, :2]  # we only take the first two features. We could
                      # avoid this ugly slicing by using a two-dim dataset
Y = target

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, Y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)
lin_svc = svm.LinearSVC(C=C).fit(X, Y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# title for the plots
titles = ['SVC with linear kernel',
          'SVC with RBF kernel',
          'SVC with polynomial (degree 3) kernel',
          'LinearSVC (linear kernel)']


for i, clf in enumerate((svc, rbf_svc, poly_svc, lin_svc)):
    # Plot the decision boundary. For that, we will asign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    pl.subplot(2, 2, i + 1)
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    pl.contourf(xx, yy, Z, cmap=pl.cm.Paired)
    pl.axis('off')

    # Plot also the training points
    pl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)

    pl.title(titles[i])

pl.show()

","DId you use any other measure of correctness aside from what you get in the point.

Usually SVMs need to be run using a grid search, specially if you have  an RBF, C only will take care of the regularization, which will do little if your data is not sparse to begin with.

You need to run a grid search over gamma and C, they have a really good example of that here:

http://scikit-learn.org/0.13/auto_examples/grid_search_digits.html#example-grid-search-digits-py

Also, their library already takes care of the cross validation.

Remember that those examples are good for the toy datasets, the moment you enter with a new dataset, there is no reason to believe is going to behave anything like the one in the example.
",,,false,
https://stackoverflow.com/questions/59830510,true,"The issue involves the KNN algorithm in scikit-learn and the use of distance metrics, specifically mahalanobis and seuclidean. The error encountered during the grid search indicates a problem with specifying the 'V' parameter for these distance metrics.",,,,,,,Defining distance parameter (V) in knn crossval grid search (seuclidean/mahalanobis distance metrics),"I am trying to carry out a k-fold cross-validation grid search using the KNN algorithm using python sklearn, with parameters in the search being number of neighbors K and distance metric. I am including mahalanobis and seuclidean as distance metrics, and understand these have a parameter which needs to be specified, namely V or VI (covariance matrix of features or inverse of this). 

Below is my code: 

X_train, X_test, y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=10,stratify=y)

knn=KNeighborsClassifier()

grid_param={'n_neighbors':np.arange(1,51),'metric':['euclidean','minkowski','mahalanobis','seuclidean'],'metric_params':[{'V': np.cov(X_train)}]} 

knn_gscv=GridSearchCV(knn,grid_param,cv=5)

knn_gscv.fit(X_train,y_train) (*)


The (*) line throws this error when executed: 

TypeError: __init__() got an unexpected keyword argument 'V'

I have also tried VI instead of V but getting same error. 

I have come across potential solutions below but these don't help. 

https://github.com/scikit-learn/scikit-learn/issues/6915

Scikit-learn: How do we define a distance metric's parameter for grid search

Any help appreciated!

This is also my first question, so any feedback would be helpful also with this regard. 
","grid_params = [
    {'n_neighbors': np.arange(1, 51), 'metric': ['euclidean', 'minkowski']},
    {'n_neighbors': np.arange(1, 51), 'metric': ['mahalanobis', 'seuclidean'],
     'metric_params': [{'V': np.cov(X_train)}]}
]


The issue is that euclidean and minkowski metrics do not accepts V parameter. So you need to separate them. 
",,,false,
https://stackoverflow.com/questions/38966075,false,The issue does not involve a problematic API. It is a question about how to restrict Anaconda from upgrading a module being installed if it is a higher-level dependency.,,,,,,,How to restrict anaconda from upgrading the module being installed if its a higher level dependency,"I'm trying to use continuum io anaconda packing system to package python-2.7.10 with other dependent modules for our environment. I want to automate the pack distribution to simply be a single installation of python with the modules we require.

The issue I'm having is when I specify the modules under the build parameter in meta.yaml it will upgrade the version of python being installed despite the fact that it is python-2.7.10. This will cause an error in the build process.

Is there a way to pin the version of python being installed so that if there is a dependency it will hard fail, or use an earlier version of the package?

meta.yaml, ive tried not pinning the version of the modules as well.

package:
  name: python
  version: 2.7.10

source:
  fn: Python-2.7.10.tgz
  url: https://www.python.org/ftp/python/2.7.10/Python-2.7.10.tgz
  md5: d7547558fd673bd9d38e2108c6b42521

build:
  no_link: bin/python

requirements:
  build:
    - bzip2 [unix]
    - zlib [unix]
    - sqlite [unix]
    - readline [unix]
    - tk [unix]
    - openssl [unix]
    - system [linux]
    - ipython 5.0.0
    - numpy 1.11.1
    - cython 0.24.1
    - scipy 0.18.0
    - pandas 0.18.1
    - patsy 0.4.1
    - statsmodels 0.6.1
    - matplotlib 1.5.2
    - ggplot 0.9.4
    - scikit-learn 0.17.1
    - distribute 0.6.45
    - backports.ssl-match-hostname 3.5.0.1
    - certifi 14.05.14
    - nose_parameterized 0.5.0
    - pyparsing 2.1.4
    - python-dateutil 2.5.3
    - pytz 2016.6.1
    - pyzmq 15.3.0
    - simplejson 3.3.3
    - six 1.10.0
    - sympy 1.0
    - tornado 4.4.1
    - virtualenv 13.0.1
    - wsgiref 0.1.2
    - python-swiftclient 2.7.0
    #- python-ceilometerclient #issue
    #- python-heatclient #issue
    #- python-keystoneclient 1.6.0
    #- python-novaclient 2.26.0
    #- python-troveclient #issue
    - python-cinderclient 1.1.2
    - python-glanceclient 0.17.2
    - python-neutronclient 2.4.0
    - networkx 1.11
    - pysal 1.11.1
    - pyyaml 3.11
    - shapely 1.5.13
    - beautifulsoup4 4.4.1
    - nltk 3.2.1
    - requests 2.10.0
    - seaborn 0.5.0
    - h5py 2.6.0
    - xlrd 1.0.0
    - markupsafe 0.23
    - crypto 1.1.0
    - jinja2 2.8
    - openpyxl 2.3.2
    - jaro_winkler 1.0.2
    - bokeh 0.12.1
    - numexpr 2.6.1
    - pytables 3.2.3.1
    - pycurl 7.43.0
    - mgrs 1.1.0
    - psutil 4.3.0
    - biopython 1.67
    - enaml 0.9.8
    - mdp 3.5
    - bitarray 0.8.1
    - clusterpy 0.9.9
    - pyside 1.2.1
    - pyqt 4.11.4
    - parsedatetime 1.4
    - pymysql 0.6.7
    - pyodbc 3.0.10
    - tabulate 0.7.2

  run:
    - zlib [unix]
    - sqlite [unix]
    - readline [unix]
    - tk [unix]
    - openssl [unix]
    - system [linux]

test:
  commands:
    - python -V [unix]
    - 2to3 -h [unix]
    - python-config --help [unix]

about:
  home: http://www.python.org/
  summary: general purpose programming language
  license: PSF


The output with the error:

$ conda build .
Removing old build environment
BUILD START: python-2.7.10-0
    (actual version deferred until further download or env creation)
Using Anaconda Cloud api site https://api.anaconda.org

The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    geos-3.5.0                 |                0        16.9 MB  defaults
    libgcc-4.8.5               |                1         922 KB  r
    pixman-0.32.6              |                0         2.4 MB  defaults
    unixodbc-2.3.4             |                0         688 KB  defaults
    yaml-0.1.6                 |                0         246 KB  defaults
    curl-7.49.0                |                1         543 KB  defaults
    glib-2.43.0                |                2         7.4 MB  r
    hdf5-1.8.17                |                1         1.9 MB  defaults
    atom-0.3.10                |           py27_0         676 KB  defaults
    backports_abc-0.4          |           py27_0           5 KB  defaults
    beautifulsoup4-4.4.1       |           py27_0         116 KB  defaults
    bitarray-0.8.1             |           py27_0          89 KB  defaults
    et_xmlfile-1.0.1           |           py27_0          15 KB  defaults
    future-0.15.2              |           py27_0         616 KB  defaults
    jaro_winkler-1.0.2         |           py27_0          24 KB  auto
    jdcal-1.2                  |           py27_1           9 KB  defaults
    kiwisolver-0.1.3           |           py27_0         571 KB  defaults
    markupsafe-0.23            |           py27_2          31 KB  defaults
    mgrs-1.1.0                 |           py27_0          48 KB  auto
    mpmath-0.19                |           py27_1         873 KB  defaults
    nltk-3.2.1                 |           py27_0         1.7 MB  defaults
    parsedatetime-1.2          |           py27_0          39 KB  auto
    ply-3.8                    |           py27_0          71 KB  defaults
    psutil-4.3.0               |           py27_0         224 KB  defaults
    pycurl-7.43.0              |           py27_0         128 KB  defaults
    pymysql-0.7.6              |           py27_0         116 KB  defaults
    pyodbc-3.0.10              |           py27_0         146 KB  defaults
    pyyaml-3.11                |           py27_4         297 KB  defaults
    pyzmq-15.4.0               |           py27_0         705 KB  defaults
    requests-2.10.0            |           py27_0         611 KB  defaults
    shapely-1.5.16             |           py27_0         494 KB  defaults
    tabulate-0.7.2             |           py27_0          18 KB  auto
    wsgiref-0.1.2              |           py27_0          943 B  auto
    xlrd-1.0.0                 |           py27_0         181 KB  defaults
    biopython-1.67             |      np111py27_0         2.2 MB  defaults
    clusterpy-0.9.9            |           py27_1         101 KB  conda-forge
    cmd2-0.6.7                 |           py27_0          33 KB  auto
    h5py-2.6.0                 |      np111py27_2         2.4 MB  defaults
    jinja2-2.8                 |           py27_1         264 KB  defaults
    jsonschema-2.5.1           |           py27_0          55 KB  defaults
    mdp-3.5                    |           py27_0         477 KB  defaults
    networkx-1.11              |           py27_0         1.1 MB  defaults
    numexpr-2.6.1              |      np111py27_0         347 KB  defaults
    openpyxl-2.3.2             |           py27_0         248 KB  defaults
    rsa-3.4.2                  |           py27_0          50 KB  conda-forge
    singledispatch-3.4.0.3     |           py27_1          17 KB  r
    ssl_match_hostname-3.4.0.2 |           py27_1           6 KB  defaults
    cliff-1.10.1               |           py27_0          36 KB  gus
    crypto-1.1.0               |           py27_0           3 KB  auto
    pysal-1.11.1               |           py27_0        11.2 MB  defaults
    pytables-3.2.3.1           |      np111py27_0         3.4 MB  defaults
    tornado-4.4.1              |           py27_0         552 KB  defaults
    bokeh-0.12.1               |           py27_0         3.2 MB  defaults
    harfbuzz-0.9.35            |                6         1.1 MB  r
    ipython-5.1.0              |           py27_0         936 KB  defaults
    pyopenssl-16.0.0           |           py27_0          66 KB  defaults
    pango-1.36.8               |                3         796 KB  r
    qt-4.8.7                   |                4        32.7 MB  defaults
    python-neutronclient-2.4.0 |           py27_0         222 KB  gus
    shiboken-1.2.1             |           py27_0         883 KB  defaults
    enaml-0.9.8                |           py27_1         944 KB  defaults
    pyside-1.2.1               |           py27_1         5.7 MB  defaults
    seaborn-0.7.1              |           py27_0         272 KB  defaults
    ------------------------------------------------------------
                                           Total:       107.8 MB

The following NEW packages will be INSTALLED:

    atom:                         0.3.10-py27_0       defaults
    babel:                        2.3.3-py27_0        defaults
    backports:                    1.0-py27_0          defaults
    backports.ssl-match-hostname: 3.5.0.1-py27_0      getpantheon
    backports_abc:                0.4-py27_0          defaults
    beautifulsoup4:               4.4.1-py27_0        defaults
    biopython:                    1.67-np111py27_0    defaults
    bitarray:                     0.8.1-py27_0        defaults
    bokeh:                        0.12.1-py27_0       defaults
    brewer2mpl:                   1.4.1-py27_1        conda-forge
    bzip2:                        1.0.6-3             defaults
    cairo:                        1.12.18-6           defaults
    certifi:                      2016.2.28-py27_0    defaults
    cffi:                         1.6.0-py27_0        defaults
    cliff:                        1.10.1-py27_0       gus
    clusterpy:                    0.9.9-py27_1        conda-forge
    cmd2:                         0.6.7-py27_0        auto
    crypto:                       1.1.0-py27_0        auto
    cryptography:                 1.4-py27_0          defaults
    curl:                         7.49.0-1            defaults
    cycler:                       0.10.0-py27_0       defaults
    cython:                       0.24.1-py27_0       defaults
    decorator:                    4.0.10-py27_0       defaults
    distribute:                   0.6.45-py27_1       defaults
    enaml:                        0.9.8-py27_1        defaults
    enum34:                       1.1.6-py27_0        defaults
    et_xmlfile:                   1.0.1-py27_0        defaults
    fontconfig:                   2.11.1-6            defaults
    freetype:                     2.5.5-1             defaults
    functools32:                  3.2.3.2-py27_0      defaults
    future:                       0.15.2-py27_0       defaults
    futures:                      3.0.5-py27_0        defaults
    geos:                         3.5.0-0             defaults
    get_terminal_size:            1.0.0-py27_0        defaults
    ggplot:                       0.11.1-py27_1       conda-forge
    glib:                         2.43.0-2            r
    h5py:                         2.6.0-np111py27_2   defaults
    harfbuzz:                     0.9.35-6            r
    hdf5:                         1.8.17-1            defaults
    idna:                         2.1-py27_0          defaults
    ipaddress:                    1.0.16-py27_0       defaults
    ipython:                      5.1.0-py27_0        defaults
    ipython_genutils:             0.1.0-py27_0        defaults
    iso8601:                      0.1.11-py27_0       defaults
    jaro_winkler:                 1.0.2-py27_0        auto
    jdcal:                        1.2-py27_1          defaults
    jinja2:                       2.8-py27_1          defaults
    jsonpatch:                    1.3-py27_0          auto
    jsonpointer:                  1.2-py27_0          auto
    jsonschema:                   2.5.1-py27_0        defaults
    kiwisolver:                   0.1.3-py27_0        defaults
    libffi:                       3.2.1-0             defaults
    libgcc:                       4.8.5-1             r
    libgfortran:                  3.0.0-1             defaults
    libpng:                       1.6.22-0            defaults
    libsodium:                    1.0.10-0            defaults
    libxml2:                      2.9.2-0             defaults
    markupsafe:                   0.23-py27_2         defaults
    matplotlib:                   1.5.1-np111py27_0   defaults
    mdp:                          3.5-py27_0          defaults
    mgrs:                         1.1.0-py27_0        auto
    mkl:                          11.3.3-0            defaults
    mpmath:                       0.19-py27_1         defaults
    msgpack-python:               0.4.7-py27_0        defaults
    netaddr:                      0.7.18-py27_0       conda-forge
    netifaces:                    0.10.4-py27_0       conda-forge
    networkx:                     1.11-py27_0         defaults
    nltk:                         3.2.1-py27_0        defaults
    nose_parameterized:           0.5.0-py27_0        conda-forge
    numexpr:                      2.6.1-np111py27_0   defaults
    numpy:                        1.11.1-py27_0       defaults
    openpyxl:                     2.3.2-py27_0        defaults
    openssl:                      1.0.2h-1            defaults
    oslo.config:                  1.9.3-py27_0        gus
    oslo.i18n:                    1.5.0-py27_0        gus
    oslo.serialization:           1.4.0-py27_0        gus
    oslo.utils:                   1.4.0-py27_0        gus
    pandas:                       0.18.1-np111py27_0  defaults
    pango:                        1.36.8-3            r
    parsedatetime:                1.2-py27_0          auto
    path.py:                      8.2.1-py27_0        defaults
    pathlib2:                     2.1.0-py27_0        defaults
    patsy:                        0.4.1-py27_0        defaults
    pbr:                          0.11.0-py27_0       defaults
    pexpect:                      4.0.1-py27_0        defaults
    pickleshare:                  0.7.3-py27_0        defaults
    pip:                          8.1.2-py27_0        defaults
    pixman:                       0.32.6-0            defaults
    ply:                          3.8-py27_0          defaults
    prettytable:                  0.7.2-py27_0        conda-forge
    prompt_toolkit:               1.0.3-py27_0        defaults
    psutil:                       4.3.0-py27_0        defaults
    ptyprocess:                   0.5.1-py27_0        defaults
    pyasn1:                       0.1.9-py27_0        defaults
    pycairo:                      1.10.0-py27_0       defaults
    pycparser:                    2.14-py27_1         defaults
    pycurl:                       7.43.0-py27_0       defaults
    pygments:                     2.1.3-py27_0        defaults
    pymysql:                      0.7.6-py27_0        defaults
    pyodbc:                       3.0.10-py27_0       defaults
    pyopenssl:                    16.0.0-py27_0       defaults
    pyparsing:                    2.1.4-py27_0        defaults
    pyqt:                         4.11.4-py27_4       defaults
    pysal:                        1.11.1-py27_0       defaults
    pyside:                       1.2.1-py27_1        defaults
    pytables:                     3.2.3.1-np111py27_0 defaults
    python:                       2.7.12-1            defaults
    python-cinderclient:          1.1.2-py27_0        gus
    python-dateutil:              2.5.3-py27_0        defaults
    python-glanceclient:          0.17.2-py27_0       gus
    python-keystoneclient:        1.3.2-py27_0        gus
    python-neutronclient:         2.4.0-py27_0        gus
    python-swiftclient:           2.7.0-py27_0        chenghlee
    pytz:                         2016.6.1-py27_0     defaults
    pyyaml:                       3.11-py27_4         defaults
    pyzmq:                        15.4.0-py27_0       defaults
    qt:                           4.8.7-4             defaults
    readline:                     6.2-2               defaults
    requests:                     2.10.0-py27_0       defaults
    rsa:                          3.4.2-py27_0        conda-forge
    scikit-learn:                 0.17.1-np111py27_2  defaults
    scipy:                        0.18.0-np111py27_0  defaults
    seaborn:                      0.7.1-py27_0        defaults
    setuptools:                   25.1.6-py27_0       defaults
    shapely:                      1.5.16-py27_0       defaults
    shiboken:                     1.2.1-py27_0        defaults
    simplegeneric:                0.8.1-py27_1        defaults
    simplejson:                   3.8.2-py27_0        defaults
    singledispatch:               3.4.0.3-py27_1      r
    sip:                          4.18-py27_0         defaults
    six:                          1.10.0-py27_0       defaults
    sqlite:                       3.13.0-0            defaults
    ssl_match_hostname:           3.4.0.2-py27_1      defaults
    statsmodels:                  0.6.1-np111py27_1   defaults
    stevedore:                    1.3.0-py27_0        gus
    sympy:                        1.0-py27_0          defaults
    system:                       5.8-2               defaults
    tabulate:                     0.7.2-py27_0        auto
    tk:                           8.5.18-0            defaults
    tornado:                      4.4.1-py27_0        defaults
    traitlets:                    4.2.2-py27_0        defaults
    unixodbc:                     2.3.4-0             defaults
    virtualenv:                   13.0.1-py27_0       defaults
    warlock:                      1.3.0-py27_0        conda-forge
    wcwidth:                      0.1.7-py27_0        defaults
    wheel:                        0.29.0-py27_0       defaults
    wsgiref:                      0.1.2-py27_0        auto
    xlrd:                         1.0.0-py27_0        defaults
    yaml:                         0.1.6-0             defaults
    zeromq:                       4.1.4-0             defaults
    zlib:                         1.2.8-3             defaults

Source cache directory is: /opt/app/anaconda2/conda-bld/src_cache
Found source in cache: Python-2.7.10.tgz
Extracting download
BUILD START: python-2.7.10-0
python is installed as a build dependency. Removing.
An unexpected error has occurred, please consider sending the
following traceback to the conda GitHub issue tracker at:

    https://github.com/conda/conda-build/issues

Include the output of the command 'conda info' in your report.


Traceback (most recent call last):
  File ""/opt/app/anaconda2/bin/conda-build"", line 5, in &lt;module&gt;
    sys.exit(main())
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 152, in main
    args_func(args, p)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 415, in args_func
    args.func(args, p)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 358, in execute
    debug=args.debug)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/build.py"", line 561, in build
    assert not plan.nothing_to_do(actions), actions
AssertionError: defaultdict(&lt;type 'list'&gt;, {'op_order': ('RM_FETCHED', 'FETCH', 'RM_EXTRACTED', 'EXTRACT', 'UNLINK', 'LINK', 'SYMLINK_CONDA'), 'PREFIX': '/opt/app/anaconda2/envs/_build_placehold_placehold_placehold_placehold_placehold'})

","Unless there's a specific reason you need to compile python yourself, I think what you're actually going after is conda bundle (http://conda.pydata.org/docs/commands/conda-bundle.html).  Unfortunately we've removed it in conda 4.2 which will be coming out soon, intending to move it to conda-build.  Since that hasn't happened yet, and if it ends up actually being useful to people, we can add it back.



You could also try this using conda-build...

Remove the whole source block in your meta.yaml file. Also remove all of the build requirements that are also not run requirements.  Then in your build.sh file

conda install --yes --quiet \
    python=2.7.10 \
    ipython=5.0.0 \
    numpy=1.11.1 \
    cython=0.24.1 \
    scipy=0.18.0 \
    pandas=0.18.1 \
    patsy=0.4.1 \
    statsmodels=0.6.1 \
    matplotlib=1.5.2 \
    ggplot=0.9.4 \
    scikit-learn=0.17.1 \
    distribute=0.6.45 \
    backports.ssl-match-hostname=3.5.0.1 \
    certifi=14.05.14 \
    nose_parameterized=0.5.0 \
    pyparsing=2.1.4 \
    python-dateutil=2.5.3 \
    pytz=2016.6.1 \
    pyzmq=15.3.0 \
    simplejson=3.3.3 \
    six=1.10.0 \
    sympy=1.0 \
    tornado=4.4.1 \
    virtualenv=13.0.1 \
    wsgiref=0.1.2 \
    python-swiftclient=2.7.0 \
    python-cinderclient=1.1.2 \
    python-glanceclient=0.17.2 \
    python-neutronclient=2.4.0 \
    networkx=1.11 \
    pysal=1.11.1 \
    pyyaml=3.11 \
    shapely=1.5.13 \
    beautifulsoup4=4.4.1 \
    nltk=3.2.1 \
    requests=2.10.0 \
    seaborn=0.5.0 \
    h5py=2.6.0 \
    xlrd=1.0.0 \
    markupsafe=0.23 \
    crypto=1.1.0 \
    jinja2=2.8 \
    openpyxl=2.3.2 \
    jaro_winkler=1.0.2 \
    bokeh=0.12.1 \
    numexpr=2.6.1 \
    pytables=3.2.3.1 \
    pycurl=7.43.0 \
    mgrs=1.1.0 \
    psutil=4.3.0 \
    biopython=1.67 \
    enaml=0.9.8 \
    mdp=3.5 \
    bitarray=0.8.1 \
    clusterpy=0.9.9 \
    pyside=1.2.1 \
    pyqt=4.11.4 \
    parsedatetime=1.4 \
    pymysql=0.6.7 \
    pyodbc=3.0.10 \
    tabulate=0.7.2


The big difference: by listing all of those packages as build requirements, you're actually ensuring that they won't be in your final conda package.  Think of build requirements more like a compiler, or something that's necessary when you're building the package, but not when you're actually running it.
",,,false,
https://stackoverflow.com/questions/76309535,true,The issue involves a problematic API. The error message indicates that the predict() function of the RandomForestQuantileRegressor class is being called with an unexpected keyword argument 'quantiles'. This suggests that the API is not properly handling the 'quantiles' parameter in the predict() function.,quantile-forest,RandomForestQuantileRegressor.predict,"When attempting to use the 'quantiles' parameter in the predict() function of the RandomForestQuantileRegressor class from the quantile-forest library, a TypeError is raised, indicating that the 'quantiles' argument is unexpected.","The expected behavior is for the predict() function to accept the 'quantiles' parameter and return predictions at the specified quantiles. However, the API implementation does not support this functionality.",The issue is triggered when calling the predict() function of the RandomForestQuantileRegressor class from the quantile-forest library with the 'quantiles' parameter.,This issue might be challenging to detect during development and testing because the error is not raised during model training or initialization but rather when attempting to use the predict() function with the 'quantiles' parameter.,Quantile Forest error &quot;predict() got an unexpected keyword argument &#39;quantiles&#39;&quot;,"I continue to run into errors when run any form of quantile forest models with the prediction and quantile phases. I am following this example but with my own X and y. I have trained many a random forest and other derivations of tree models with this dataset, so I'm fairly certain it's not the input data issue.
https://github.com/zillow/quantile-forest
And have created an environment that supposedly follows all the installation requirements. I can provide my list of versions upon request. Time and time again, the RandomForestQuantileRegressor will work, but when I want to plot and see the quantiles, I get the error

**""TypeError: predict() got an unexpected keyword argument 'quantiles'""
**

Here is an example when I set up the environment:

Collecting quantile-forest
Downloading quantile_forest-1.1.2-cp310-cp310-macosx_10_9_x86_64.whl (188 kB)
 188.6/188.6 kB 2.7 MB/s eta 0:00:00
Requirement already satisfied: numpy&gt;=1.23 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from quantile-forest) (1.24.3)
Requirement already satisfied: scipy&gt;=1.4 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from quantile-forest) (1.10.1)
Requirement already satisfied: scikit-learn&gt;=1.0 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from quantile-forest) (1.2.2)
Requirement already satisfied: joblib&gt;=1.1.1 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from scikit-learn&gt;=1.0-&gt;quantile-forest) (1.2.0)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from scikit-learn&gt;=1.0-&gt;quantile-forest) (3.1.0)
Installing collected packages: quantile-forest
Successfully installed quantile-forest-1.1.2

This code works:
X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=0.5, random_state=0)

qrf = RandomForestQuantileRegressor(q=[0.05, 0.50, 0.95])
qrf.fit(X_train, y_train)

y_pred_5, y_pred_median, y_pred_95 = qrf.predict(X_test)
qrf.score(X_test, y_test)

But I cannot get any code that calls ""predict"" where I specify the quantiles to work without that type error.
","I notice that the code you've provided is an example from the sklearn-quantile package. Perhaps confusingly, both packages -- sklearn-quantile and quantile-forest -- provide a RandomForestQuantileRegressor class, but the packages have different ways of passing the quantiles to the class methods. As a result, the RandomForestQuantileRegressor classes from the two packages are not currently interchangeable.
In the code snippet you've provided, it's not clear what imports are being used, but it appears that you may be using the RandomForestQuantileRegressor class from the sklearn-quantile package. This class expects the quantiles to be passed to the initialization function instead of the predict function and would lead to the error you've presented. If this is correct, then you can fix this by importing the RandomForestQuantileRegressor class from the quantile-forest package and passing the quantiles to the predict function instead of the initialization function.
If the above is not helpful or you still are running into errors, you're welcome to create an issue in the quantile-forest repository here for additional troubleshooting.
",,,false,
https://stackoverflow.com/questions/40877501,true,The issue involves a problematic API. The error message indicates that there is an AttributeError when trying to update the scikit-learn package using conda. This suggests that there is an issue with the API's handling of the 'ssl_verify' attribute.,conda,conda.update,"When attempting to update the scikit-learn package using conda, an AttributeError is raised, indicating that the 'Extensions' object in the urllib3 package does not have the 'get_extension_for_class' attribute.","The expected behavior is for the conda.update function to successfully update the specified package. However, the API implementation encounters an AttributeError when trying to access the 'get_extension_for_class' attribute, resulting in an error.",The issue is triggered when calling the conda.update function to update the scikit-learn package.,"This issue might be challenging to detect during development and testing because the error is raised during the package update process, which may not be a common operation during regular development workflows.",Error using conda to update a package,"I launched on AWS EC2 an instance using ubuntu as AMI.

Then I ran a script to install the jupyter notebook and be able to access to it with the IP address and the specific port.
The code I wrote was this one :

sudo apt-get install git

git clone https://gist.github.com/rashmibanthia/5a1e4d7e313d6832f2ff nb

. nb/jupyter_notebook_ec2.sh 

cd;mkdir notebook;cd notebook

tmux new -s nb

jupyter notebook --certfile=~/certs/mycert.pem --keyfile ~/certs/mycert.key


Now I am in the notebook. Here comes my problem. When I try to import some packages and class, I get an error. For example with this line:

from sklearn.gaussian_process import GaussianProcessRegressor


I get this:


  ImportError                               Traceback (most recent call
  last)  in ()
  ----&gt; 1 from sklearn.gaussian_process import GaussianProcessRegressor
  
  ImportError: cannot import name 'GaussianProcessRegressor'


So I tried to update the scikit learn package with conda with conda update scikit-learn but I have another error:


  Fetching package metadata ...An unexpected error has occurred. Please
  consider posting the following information to the conda GitHub issue
  tracker at:

https://github.com/conda/conda/issues



Current conda install:

           platform : linux-64
      conda version : 4.2.13
   conda is private : False
  conda-env version : 4.2.13
conda-build version : 1.18.2
     python version : 3.5.2.final.0
   requests version : 2.12.1
   root environment : /home/ubuntu/anaconda3  (writable)
default environment : /home/ubuntu/anaconda3
   envs directories : /home/ubuntu/anaconda3/envs
      package cache : /home/ubuntu/anaconda3/pkgs
       channel URLs : https://repo.continuum.io/pkgs/free/linux-64
                      https://repo.continuum.io/pkgs/free/noarch
                      https://repo.continuum.io/pkgs/pro/linux-64
                      https://repo.continuum.io/pkgs/pro/noarch
        config file : None
       offline mode : False




$ /home/ubuntu/anaconda3/bin/conda update scikit-learn

Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/exceptions.py"", line 479, in conda_exception_handler
    return_value = func(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/cli/main.py"", line 145, in _main
    exit_code = args.func(args, p)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/cli/main_update.py"", line 65, in execute
    install(args, parser, 'update')
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/cli/install.py"", line 238, in install
    prefix=prefix)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/api.py"", line 24, in get_index
    index = fetch_index(channel_urls, use_cache=use_cache, unknown=unknown)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 300, in fetch_index
    repodatas = [(u, f.result()) for u, f in zip(urls, futures)]
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 300, in &lt;listcomp&gt;
    repodatas = [(u, f.result()) for u, f in zip(urls, futures)]
  File ""/home/ubuntu/anaconda3/lib/python3.5/concurrent/futures/_base.py"", line 405, in result
    return self.__get_result()
  File ""/home/ubuntu/anaconda3/lib/python3.5/concurrent/futures/_base.py"", line 357, in __get_result
    raise self._exception
  File ""/home/ubuntu/anaconda3/lib/python3.5/concurrent/futures/thread.py"", line 55, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 75, in func
    res = f(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 117, in fetch_repodata
    timeout=(6.1, 60))
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 501, in get
    return self.request('GET', url, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 488, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 609, in send
    r = adapter.send(request, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/adapters.py"", line 423, in send
    timeout=timeout
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 594, in urlopen
    chunked=chunked)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 350, in _make_request
    self._validate_conn(conn)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 835, in _validate_conn
    conn.connect()
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connection.py"", line 330, in connect
    cert = self.sock.getpeercert()
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 324, in getpeercert
    'subjectAltName': get_subj_alt_name(x509)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 171, in get_subj_alt_name
    ext = cert.extensions.get_extension_for_class(
AttributeError: 'Extensions' object has no attribute 'get_extension_for_class'

","As kalefranz comments here 'https://github.com/conda/conda/issues/3898' it might be because pyopenssl and cryptography are out of sync and this should fix it:

CONDA_SSL_VERIFY=false conda update pyopenssl

In windows to set ssl_verify the following command can be used:

conda config --set ssl_verify False

As it is said in Problems with updating anaconda and installing new packages
",,,false,
https://stackoverflow.com/questions/44729373,false,"The issue does not involve a problematic API. The problem mentioned in the issue is related to the regression results of a GaussianProcess model, which is more likely to be caused by model configuration or data-related factors rather than an issue with the API itself.",,,,,,,GaussianProcess regression results correct...up to a scale factor?,"I am running GaussianProcess regressions over some very noisy data. When I scatter plot predictions (which are, I know, predictions of means) vs actuals, I get a beautiful only slightly noisy y=x line.

Only one problem: the slope is completely wrong. Is there any way I can address this without building a second-stage linear regressor?

I regret I cannot share my data, but my model is fairly basic. X is a matrix with 10 columns, y a matrix with 1 column. I am using 1,000 examples to train and plot.

added: The below plot is plotting predicted versus actual. Given that I am using a nonlinear kernel, I find it strange that the GP regressor can find a relationship which is accurate up to a multiplier (slope).



kernel = (
    GP.kernels.RationalQuadratic(
        length_scale=.8,
        length_scale_bounds=(1e-3,1e3),
        alpha=.8,
        alpha_bounds=(1e-3,1e3),
        )
    + GP.kernels.WhiteKernel()
    )

gp = Pipeline( [
    ('scale',preproc.StandardScaler()),
    ('gp',GP.GaussianProcessRegressor(kernel=kernel)),
    ] )
gp.fit( X, y )


added: I'm a bit embarrassed, but I'm new to the GP world in particular and, really, regression as a ML problem in general. I had not plotted the model's performance over a test set, which revealed a strong overfit. Additionally, I've added an idiom to my code to deal with scikit-learn's default GP behavior, i.e., optimization makes me sad when I give it significant quantities of data, by ""pretraining"" on a small quantity of data, using the optimizer to find reasonable values for the kernel parameters, then ""training"" a much larger quantity of data. This allowed me to widen the parameter search and use multiple restarts on the optimizer, finding a much more generalizable model...which was almost all noise. Which was what I was expecting, really.

kernel = (
    GP.kernels.RationalQuadratic(
        length_scale=1,
        alpha=.5,
        )
    + GP.kernels.WhiteKernel(
        noise_level=1,
        )
    )*GP.kernels.ConstantKernel()

gp = Pipeline( [
    ('scale',preproc.StandardScaler()),
    ('gp',GP.GaussianProcessRegressor(
        kernel=kernel,
        n_restarts_optimizer=3,
        alpha=0,
        )),
    ] )
print(""pretraining model for target %s..."" % c)
x_pre = X_s.values[:500,:]
y_pre = y_s_scl[:500,:]
gp.fit( x_pre, y_pre )

gp = Pipeline( [
    ('scale',preproc.StandardScaler()),
    ('gp',GP.GaussianProcessRegressor(
        kernel=kernel,
        optimizer=None,
        alpha=0,
        )),
    ] )
print(""training model for target %s..."" % c)

","EDIT: Have you tried centering your data before doing the regression? (subtracting the mean of all the output values from each output). I know the Gp Toolbox in Matlab doesn't need the data to be centered, but I am not sure about the GP in sklearn. See:
https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia

OLD COMMENT:
Your initial values for the hyperparameters in the kernel function (i.e. length-scale and alpha) are very important. During the fit(), the hyperparameters are optimized and local maximum of hyperpareters can be found, which could in turn affect your result. Depending on the bounds you set for these hyperparameters, many local maximum can be found depending on the initial conditions. 
On the sklearn site it says:
""As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying n_restarts_optimizer.""
You may try using the RBF function as it is a very traditional kernel function for the GP. 
",,,false,
https://stackoverflow.com/questions/44527713,false,The issue does not involve a problematic API. The error message suggests that there is a conflict between the multiprocessing module in the directory where the script is being run and the built-in multiprocessing module. This is more likely a configuration or environment-related issue rather than a problem with the API itself.,,,,,,,Python script works from one folder but not on subfolders.,"Good evening everyone, 

I am trying to do some machine learning with python so I imported the module scikit-learn:

from sklearn.preprocessing import MinMaxScaler


And it gives me this strange error: 

Traceback (most recent call last):

  File ""&lt;ipython-input-22-b55a4eaccb0b&gt;"", line 1, in &lt;module&gt;
    from sklearn.preprocessing import MinMaxScaler

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\preprocessing\__init__.py"", line 6, in &lt;module&gt;
    from ._function_transformer import FunctionTransformer

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\preprocessing\_function_transformer.py"", line 2, in &lt;module&gt;
    from ..utils import check_array

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\utils\__init__.py"", line 18, in &lt;module&gt;
    from ..externals.joblib import cpu_count

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\externals\joblib\__init__.py"", line 128, in &lt;module&gt;
    from .parallel import Parallel

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 24, in &lt;module&gt;
    from ._multiprocessing_helpers import mp

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\externals\joblib\_multiprocessing_helpers.py"", line 24, in &lt;module&gt;
    _sem = mp.Semaphore()

AttributeError: module 'multiprocessing' has no attribute 'Semaphore'


The errors are copied from Anaconda (Spider) but they are present even by starting a python session from the command line. The folder where I run the script is: 

C:\Users\Francesco\Desktop\script_python


The very strange thing is that if I run the same script from the Desktop, just a folder up, it works!

C:\Users\Francesco\Desktop\ &lt;-- Here it works!!


I found out that the problem is relative to the multiprocessing module, here a snapshot of this strange behavior: 


","You probably have a module multiprocessing in the directory where you are running the script from.

Generally, in Python, the path where you run your script has precedence over the Python env, so it will try to pick that one first. This means that when sklearn imports multiprocessing, it uses your module, and not the built-in multiprocessing.

When this type of error appears, one way to identify the error is to run

python -c ""import multiprocessing; print(multiprocessing.__dir__)""


to double check which module is being used.
",,,false,
https://stackoverflow.com/questions/48263740,true,"The issue does not involve a problematic API. It is related to measuring memory usage and runtime, rather than an unexpected behavior or failure of an API.",,,,,,,strange memory consumption behaviour when running PCA in python,"At the moment I am trying to find the reason for a specific behavior in Python. 

First I want to describe my use case. The idea is a performance analysis on Python. Therefore I want to analyze the memory usage and runtime regarding the ""Principle Component Analysis""-algorithm. For that, I use scikit-learn (http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). 

To see, how the algorithm behaves for different datasets, I generated several randomly uniform distributed datasets. The strange thing is, there is an increase in memory usage and a faster runtime regarding two datasets.

Now I want to explain, how I measure the memory-usage and runtime shortly. 
Memory measurement (package: https://pypi.python.org/pypi/memory_profiler)


disable the garbage collection
Use the @profile decoration to find the increments of the function
Write the output into a log file
Use another python program to find the increments of the log files which belong to a specific dataset
sum up the increments - the result is the total memory allocation for the complete function
(I tested it with enabled garbage collection, too)


Time measurement (function: https://docs.python.org/2/library/timeit.html)


Use ""timeit.repeat()"" to get a list of many runtimes of the function
Write the list of those measurements to a CSV


After I had performed about 100 memory measurements and even more time measurements, I recognized the aforementioned behavior.
Hopefully, you can help me, find a reason for this behavior.

Here is the function, I measured:

@profile 
def pcaTrain(dataset):
 model = sklearn_pca.fit(dataset)
 model.variance = np.var(model.transform(dataset), axis=0, ddof=1)
 return model 


And here are some diagrams of the memory usage and the time measurements:

Memory measurements for dataset 4-17

Runtime measurements for dataset 4-17

Set up:


Python-version: 3.5.2
Memory: &gt; 100 GB


List of datasets:


dset04 -&gt; 1.000.000 rows per column (10 features x 1.000.000)
dset05 -&gt; 1.000.000 rows per column (12 features x 1.000.000)
dset06 -&gt; 1.000.000 rows per column (14 features x 1.000.000)
dset07 -&gt; 1.000.000 rows per column (16 features x 1.000.000)
dset08 -&gt; 1.000.000 rows per column (18 features x 1.000.000)
dset09 -&gt; 1.000.000 rows per column (20 features x 1.000.000)
dset10 -&gt; 1.000.000 rows per column (22 features x 1.000.000)
dset11 -&gt; 1.000.000 rows per column (24 features x 1.000.000)
dset12 -&gt; 1.000.000 rows per column (26 features x 1.000.000)
dset13 -&gt; 1.000.000 rows per column (28 features x 1.000.000)
dset14 -&gt; 1.000.000 rows per column (30 features x 1.000.000)
dset15 -&gt; 1.000.000 rows per column (35 features x 1.000.000)
dset16 -&gt; 1.000.000 rows per column (40 features x 1.000.000)
dset17 -&gt; 1.000.000 rows per column (45 features x 1.000.000)

","The important question is, how many of the features you are extractiong.
Since version 0.18 of Scikit-learn, the svd_solver flag of the PCA algorithm determines which algorithm to use.  The default behavior is to select the ""best"" choice, which is described in detail in the official documentation which you mentioned. 
Possibly, one of those selections hits your performance, depending on the number of components. Otherwise, I would suggest you mention this behavior in the official GitHub of scikit-learn, since this could be interesting to them as well.
",,,false,
https://stackoverflow.com/questions/53630329,true,The issue does not involve a problematic API. It is related to the behavior of the watershed algorithm and how to prevent the algorithm from adding a border around the image.,,,,,,,Watershed analysis of coins - wrong output,"For the counting of round objects in an image I want to use the watershed algorithm.
In order to learn how it works and how I can use it for my needs, I have searched some working examples in python (https://docs.opencv.org/3.1.0/d3/db4/tutorial_py_watershed.html ; http://scikit-image.org/docs/dev/auto_examples/segmentation/plot_label.html)

I finaly found a working solution, which works more or less out-of-the-box for my own purposes (How to define the markers for Watershed in OpenCV?)

With this code I get nice results, both with the example file as with my own images.
I do get a strange behavior though after the watershed analysis. For some reason, the watershed step also adds a border around the image. So next to the objects that are detected, also the whole edge of the image gets detected and colored.

My guess is that I should change a parameters in the code to stop this from happening, but so far I'm unable to find what I should do.

this is the code:

import cv2
import numpy as np
from scipy.ndimage import label
def segment_on_dt(a, img):
    border = cv2.dilate(img, None, iterations=3)
    border = border - cv2.erode(border, None)
    dt = cv2.distanceTransform(img, cv2.DIST_L2, 3)
    dt = ((dt - dt.min()) / (dt.max() - dt.min()) * 255).astype(np.uint8)
    _, dt = cv2.threshold(dt, 200, 255, cv2.THRESH_BINARY)
    lbl, ncc = label(dt)
    # Completing the markers now. 
    lbl[border == 255] = 255 
    lbl = lbl.astype(np.int32)
    cv2.watershed(a, lbl)
    lbl[lbl == -1] = 0
    lbl = lbl.astype(np.uint8)
    return 255 - lbl

# Load image file
img = cv2.imread('coins.jpg')
# Pre-processing.
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img_gray = cv2.GaussianBlur(img_gray,(5,5),0)  
width, height = img_gray.shape
_, img_bin = cv2.threshold(img_gray, 0,  255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
img_bin = cv2.morphologyEx(img_bin, cv2.MORPH_OPEN,np.ones((5, 5),     dtype=int))
result = segment_on_dt(img, img_bin)
result[result != 255] = 0
result = cv2.dilate(result, None)
img[result == 255] = (0, 0, 255)
cv2.imwrite('Img_output.png',img)


Running this code will give this result (at least on my pc)



The result for detecting the coins is good enough for my purposes, but I'm a bit puzzled about the image edge that is also detected. From what I see during debugging, the watershed adds this edge, but it is unclear to me why this happens.
","You can fix this by adding a background label using the tutorial provided by openCV. 
https://docs.opencv.org/3.1.0/d3/db4/tutorial_py_watershed.html

They added an extra step to insert sure background and sure foreground region to help the watershed algorithm to properly segment the coin regions.

*********** edit**************

After reading your code again. I found that your original code has no problem. 
Background label was set using the variable border.

You will probably get the same result by executing the code found in OpenCV tutorial. The problem is in the way your draw the results. Since this is a display problem, there are many ways we can tackle the problem. One of the many is to use the information of the sure-background

Here are the modification to the function segment_on_dt

def segment_on_dt(a, img):
    sure_background = cv2.dilate(img, None, iterations=3)
    border = sure_background - cv2.erode(sure_background, None)


    dt = cv2.distanceTransform(img, cv2.DIST_L2, 3)
    dt = ((dt - dt.min()) / (dt.max() - dt.min()) * 255).astype(np.uint8)
    _, dt = cv2.threshold(dt, 200, 255, cv2.THRESH_BINARY)
    lbl, ncc = label(dt)


    # Completing the markers now. 
    lbl[border == 255] = 255 


    lbl = lbl.astype(np.int32)
    cv2.watershed(a, lbl)
    lbl[lbl == -1] = 0
    # Only draw red line if its not in sure background
    lbl[sure_background == 0] = 255

    lbl = lbl.astype(np.uint8)
    cv2.imshow('lbl_2',lbl)

    return 255 - lbl


I have added a new condition for the red lines to be drawn. The line are only drawn if its not in sure background region.

Your final result should look like this.


",,,false,
https://stackoverflow.com/questions/28726548,false,The behavior is not related to a problematic API. It is likely due to the nature of the data and the parameters used in the k-means algorithm.,,,,,,,Text Documents Clustering - Non Uniform Clusters,"I have been trying to cluster a set of text documents. I have a sparse TFIDF matrix with around 10k documents (subset of a large dataset), and I try to run the scikit-learn k-means algorithm with different sizes of clusters (10,50,100). Rest all the parameters are default values.

I get a very strange behavior that no matter how many clusters I specify or even if I change the number of iterations, there would be 1 cluster in the lot which would contain most of the documents in itself and there will be many clusters which would have just 1 document in them. This is highly non-uniform behavior 

Does anyone know what kind of problem am I running into? 
","Here are the possible things that might be going ""wrong"":


Your k-means cluster initialization points are chosen as the same set of points in each run. I recommend using the 'random' for the init parameter of k-means http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. If that doesn't work then supply to k-means your own set of random initial cluster centers. Remember to initialize your random generator using its seed() method as the current date and time. https://docs.python.org/2/library/random.html uses current date-time as the default value. 
Your distance function, i.e. euclidean distance might be the culprit. This is less likely but it is always good to run k-means using cosine similarity especially when you are using it for document similarity. scikits doesn't have this functionality at present but you should look here: Is it possible to specify your own distance function using scikit-learn K-Means Clustering?


These two combined should give you good clusters.
","I noticed with the help of above answers and comments that there was a problem with outliers and noise in original space. For this, we should use a dimensionality reduction method which eliminates the unwanted noise in the data. I tried random projections first but it failed to work with text data, simply because the problem was still not solved.
Then using Truncated Singular Value Decomposition, I was able to get perfect uniform clusters. Hence, the Truncated SVD is the way to go with textual data in my opinion.
",,false,
https://stackoverflow.com/questions/62777025,true,The issue does not involve a problematic API. It is related to the behavior of the GridSearchCV class in scikit-learn and how it initializes and runs the estimator over the grid of hyperparameters.,,,,,,,GridSearchCV fails for own model class,"I'm trying to use a regression model I have implemented in combination with the GridSearchCV class of scikit-learn to optimize the hyper-parameters of my model. My modelclass is nicely build following the suggestions of the scikit-api:
class FOO(BaseEstimator, RegressorMixin):
def __init__(self,...)
    *** initialisation of all the parameters and hyperparameters (including the kernelfunction)***

def fit(self,X,y)
    *** implementation of fit: just takes input and performs fit of parameters.

def predict(self,X)
    *** implementation of predict: just takes input and calculates the result

The regression-class works as it should, but strangely enough, when I study the behavior of the hyperparameters, I tend to get inconsistencies. It appears one hyper-parameter is correctly applied by GridSearchCV, but the other one is clearly not.
So I am wondering, can someone explain to me how gridsearchCV is working (from the technical perspective)? How does it initialise the estimator, how does it run it over the grid?
My current assumption of the workings and required use of GridsearchCV is this:

Create a GridSearchCV instance  (CVmodel=GridSearchCV(MyRegressor,param_grid=Myparamgrid,...)
Fit the hyperparameter(s) via: CVmodel.fit(X,y). Which naively would work like this:

&gt; Loop over Parameter-values
&gt;          - create esimator instance with parameter value(and defaults for the other params)
&gt;          - estimator.fit
&gt;          - result[parameter-value]=estimator.predict

However, experience shows me this naive idea is quite wrong, as the hyper-parameter associated with the kernel-function of my regressor is not correctly initialized.
Can anyone provide some insight into what GridSearchCV is truly doing?
","After quite some digging I discovered, scikit-learn does not create new instances (as would be expected in OOP) but rather updates the properties of the object via the set_params method. In my case, this worked fine for the hyperparameter which is directly defined by the same keyword in the __ init __ method, however, it breaks down when the hyperparameter is a property of the static method set during the __ init __ method. Overriding the set_params method (which many tutorials advise against) to deal with this fixes the problem.
For those interested in more details, I wrote this all up in a tutorial myself.
",,,false,
https://stackoverflow.com/questions/63692895,false,"The behavior is not related to a problematic API. It is a specific requirement to build logistic regression models for each class, even if they predict the same class every time.",,,,,,,Is there a way to build a logistic regression model even if there is only one class?,"Is there a way to build a scikit-learn logistic regression model for only 1 class? Obviously this model would predict the same class every time, regardless of the input data. My models are currently using liblinear as the solver, I'm not sure if there's another solver that would allow for this?
I realize this is a very strange question for ML but I am building many hierarchical models and in my situation it is easier to have a model for every case even if it predicts the same class every time.
Background: I have a hierarchical prediction task where I'm trying to predict three parts of a 9 digit code (e.g. for a code = 001010424, part 1 = 001, part 2 = 01, part 3= 0424). To do this I'm building hierarchical models. Using the input data we first predict part 1, then using the highest confidence decision for part 1 we use the input data again in a model for part 2 that is specific to the part 1 code. So for example, if I run the part 1 model and get a prediction that part 1 = 001 I then go to the part 2 model for 001 which then (is trained on and) predicts part 2 given part 1 = 001. This hierarchical behavior is repeated for part 3.
","Scitkit learn needs samples of at least two classes.
  import numpy as np
  from sklearn.linear_model import LogisticRegression

  x = np.random.rand(5,2)
  y = np.ones(5).astype(int)
  model = LogisticRegression().fit(x, y)

This yields the error:
  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1

You are probably better off having your training algorithm check if there is only one y label, and if there is, just have your code memorize that.  It seems that such an implementation would be straightforward to implement and much easier to understand for anyone looking at the code later.
",,,false,
https://stackoverflow.com/questions/68155575,false,The behavior is not related to a problematic API. It is likely due to differences in the code structure and the specific operations performed in each case.,,,,,,,RandomForestClassifier takes less time to train when in Pipeline?,"I am experiencing strange behavior in Scikit-learn.
When I train a RandomForestClassifier standalone (not in a pipeline), the model takes 280 seconds to train, while the training process only takes 18 seconds in a pipeline.
Am I doing something wrong ?

","In 1st block, you have 6 lines of code. You are importing libraries, fitting your model, getting scores, and doing everything for the first time. Whereas in 2nd block you are importing the libraries like Scikit-learn which is already imported, so it won't take time. And, in 3rd block there is just one line of code. Another reason is that, In 1st block, you are getting some kind of warning which might be affecting the speed of running the code.
",,,false,
https://stackoverflow.com/questions/22054964,false,The issue does not involve a problematic API. It is a question about the differences between the scikit-learn LinearRegression and statsmodels OLS implementations.,,,,,,,OLS Regression: Scikit vs. Statsmodels?,"Short version: I was using the scikit LinearRegression on some data, but I'm used to p-values so put the data into the statsmodels OLS, and although the R^2 is about the same the variable coefficients are all different by large amounts. This concerns me since the most likely problem is that I've made an error somewhere and now I don't feel confident in either output (since likely I have made one model incorrectly but don't know which one).

Longer version: Because I don't know where the issue is, I don't know exactly which details to include, and including everything is probably too much. I am also not sure about including code or data. 

I am under the impression that scikit's LR and statsmodels OLS should both be doing OLS, and as far as I know OLS is OLS so the results should be the same.

For scikit's LR, the results are (statistically) the same whether or not I set normalize=True or =False, which I find somewhat strange.

For statsmodels OLS, I normalize the data using StandardScaler from sklearn. I add a column of ones so it includes an intercept (since scikit's output includes an intercept). More on that here: http://statsmodels.sourceforge.net/devel/examples/generated/example_ols.html (Adding this column did not change the variable coefficients to any notable degree and the intercept was very close to zero.) StandardScaler didn't like that my ints weren't floats, so I tried this: https://github.com/scikit-learn/scikit-learn/issues/1709
That makes the warning go away but the results are exactly the same.

Granted I'm using 5-folds cv for the sklearn approach (R^2 are consistent for both test and training data each time), and for statsmodels I just throw it all the data.

R^2 is about 0.41 for both sklearn and statsmodels (this is good for social science). This could be a good sign or just a coincidence.

The data is observations of avatars in WoW (from http://mmnet.iis.sinica.edu.tw/dl/wowah/) which I munged about to make it weekly with some different features. Originally this was a class project for a data science class.

Independent variables include number of observations in a week (int), character level (int), if in a guild (Boolean), when seen (Booleans on weekday day, weekday eve, weekday late, and the same three for weekend), a dummy for character class (at the time for the data collection, there were only 8 classes in WoW, so there are 7 dummy vars and the original string categorical variable is dropped), and others.

The dependent variable is how many levels each character gained during that week (int).

Interestingly, some of the relative order within like variables is maintained across statsmodels and sklearn. So, rank order of ""when seen"" is the same although the loadings are very different, and rank order for the character class dummies is the same although again the loadings are very different.

I think this question is similar to this one: Difference in Python statsmodels OLS and R's lm

I am good enough at Python and stats to make a go of it, but then not good enough to figure something like this out. I tried reading the sklearn docs and the statsmodels docs, but if the answer was there staring me in the face I did not understand it. 

I would love to know:


Which output might be accurate? (Granted they might both be if I missed a kwarg.)
If I made a mistake, what is it and how to fix it?
Could I have figured this out without asking here, and if so how?


I know this question has some rather vague bits (no code, no data, no output), but I am thinking it is more about the general processes of the two packages. Sure, one seems to be more stats and one seems to be more machine learning, but they're both OLS so I don't understand why the outputs aren't the same.

(I even tried some other OLS calls to triangulate, one gave a much lower R^2, one looped for five minutes and I killed it, and one crashed.)

Thanks!
","It sounds like you are not feeding the same matrix of regressors X to both procedures (but see below). Here's an example to show you which options you need to use for sklearn and statsmodels to produce identical results.

import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression

# Generate artificial data (2 regressors + constant)
nobs = 100 
X = np.random.random((nobs, 2)) 
X = sm.add_constant(X)
beta = [1, .1, .5] 
e = np.random.random(nobs)
y = np.dot(X, beta) + e 

# Fit regression model
sm.OLS(y, X).fit().params
&gt;&gt; array([ 1.4507724 ,  0.08612654,  0.60129898])

LinearRegression(fit_intercept=False).fit(X, y).coef_
&gt;&gt; array([ 1.4507724 ,  0.08612654,  0.60129898])


As a commenter suggested, even if you are giving both programs the same X, X may not have full column rank, and they sm/sk could be taking (different) actions under-the-hood to make the OLS computation go through (i.e. dropping different columns).

I recommend you use pandas and patsy to take care of this:

import pandas as pd
from patsy import dmatrices

dat = pd.read_csv('wow.csv')
y, X = dmatrices('levels ~ week + character + guild', data=dat)


Or, alternatively, the statsmodels formula interface:

import statsmodels.formula.api as smf
dat = pd.read_csv('wow.csv')
mod = smf.ols('levels ~ week + character + guild', data=dat).fit()


Edit: This example might be useful: http://statsmodels.sourceforge.net/devel/example_formulas.html
","If you use statsmodels, I would highly recommend using the statsmodels formula interface instead.  You will get the same old result from OLS using the statsmodels formula interface as you would from sklearn.linear_model.LinearRegression, or R, or SAS, or Excel.  

smod = smf.ols(formula ='y~ x', data=df)
result = smod.fit()
print(result.summary())


When in doubt, please 


try reading the source code
try a different language for benchmark, or 
try OLS from scratch, which is basic linear algebra. 

","i just wanted to add here, that in terms of sklearn, it does not use OLS method for linear regression under the hood. Since sklearn comes from the data-mining/machine-learning realm, they like to use Steepest Descent Gradient algorithm. This is a numerical method that is sensitive to initial conditions etc, while the OLS is an analytical closed form approach, so one should expect differences. So statsmodels comes from classical statistics field hence they would use OLS technique. So there are differences between the two linear regressions from the 2 different libraries
",false,
https://stackoverflow.com/questions/10407266,false,The issue does not involve a problematic API. It is a question about the differences between the Naive Bayes classifiers in NLTK and scikit-learn.,,,,,,,scikits learn and nltk: Naive Bayes classifier performance highly different,"I am comparing two Naive Bayes classifiers: one from NLTK and and one from scikit-learn. I'm dealing with a multi-class classification problem (3 classes: positive (1), negative (-1), and neutral (0)). 

Without performing any feature selection (that is, using all features available), and using a training dataset of 70,000 instances (noisy-labeled, with an instance distribution of  17% positive, 4%  negative and 78% neutral), I train two classifiers, the first one is a nltk.NaiveBayesClassifier, and the second one is a sklearn.naive_bayes.MultinomialNB (with fit_prior=True).

After training, I evaluated the classifiers on my test set of 30,000 instances and I get the following results:

**NLTK's NaiveBayes**
accuracy: 0.568740
class: 1
     precision: 0.331229
     recall: 0.331565
     F-Measure: 0.331355
class: -1
     precision: 0.079253 
     recall: 0.446331 
     F-Measure: 0.134596 
class: 0
     precision: 0.849842 
     recall: 0.628126 
     F-Measure: 0.722347 


**Scikit's MultinomialNB (with fit_prior=True)**
accuracy: 0.834670
class: 1
     precision: 0.400247
     recall: 0.125359
     F-Measure: 0.190917
class: -1
     precision: 0.330836
     recall: 0.012441
     F-Measure: 0.023939
class: 0
     precision: 0.852997
     recall: 0.973406
     F-Measure: 0.909191

**Scikit's MultinomialNB (with fit_prior=False)**
accuracy: 0.834680
class: 1
     precision: 0.400380
     recall: 0.125361
     F-Measure: 0.190934
class: -1
     precision: 0.330836
     recall: 0.012441
     F-Measure: 0.023939
class: 0
     precision: 0.852998
     recall: 0.973418
     F-Measure: 0.909197


I have noticed that while Scikit's classifier has better overall accuracy and precision, its recall is very low compared to the NLTK one, at least with my data. Taking into account that they might be (almost) the same classifiers, isn't this strange?
","Naive Bayes classifier usually means a Bayesian classfier over binary features that are assumed to be independent. This is what NLTK's Naive Bayes classifier implements. The corresponding scikit classifier is BernoulliNB classifier.

The restriction to boolean valued features is not actually necessary,
it is just the simplest to implement.
A naive Bayes classifier can be defined for (assumed) independent features from any parametric distribution.

MultinomialNB is for data with integer valued input features that are assumed to be multinomially distributed.

Sckit also has GaussianNB that for continuous valued features that are assumed to idependently Gaussian distributed.
",,,false,
https://stackoverflow.com/questions/13301986,true,The issue involves the predict_proba method in scikit-learn's LogisticRegression. The behavior of predict_proba is inconsistent when predicting individual instances versus multiple instances together.,,,,,,,predict_proba or decision_function as estimator &quot;confidence&quot;,"I'm using LogisticRegression as a model to train an estimator in scikit-learn. The features I use are (mostly) categorical; and so are the labels. Therefore, I use a DictVectorizer and a LabelEncoder, respectively, to encode the values properly. 

The training part is fairly straightforward, but I'm having problems with the test part. The simple thing to do is to use the ""predict"" method of the trained model and get the predicted label. However, for the processing I need to do afterwards, I need the probability for each possible label (class) for each particular instance. I decided to use the ""predict_proba"" method. However, I get different results for the same test instance, whether I use this method when the instance is by itself or accompanied by others. 

Next, is a code that reproduces the problem.

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import LabelEncoder


X_real = [{'head': u'n\xe3o', 'dep_rel': u'ADVL'}, 
          {'head': u'v\xe3o', 'dep_rel': u'ACC'}, 
          {'head': u'empresa', 'dep_rel': u'SUBJ'}, 
          {'head': u'era', 'dep_rel': u'ACC'}, 
          {'head': u't\xeam', 'dep_rel': u'ACC'}, 
          {'head': u'import\xe2ncia', 'dep_rel': u'PIV'}, 
          {'head': u'balan\xe7o', 'dep_rel': u'SUBJ'}, 
          {'head': u'ocupam', 'dep_rel': u'ACC'}, 
          {'head': u'acesso', 'dep_rel': u'PRED'}, 
          {'head': u'elas', 'dep_rel': u'SUBJ'}, 
          {'head': u'assinaram', 'dep_rel': u'ACC'}, 
          {'head': u'agredido', 'dep_rel': u'SUBJ'}, 
          {'head': u'pol\xedcia', 'dep_rel': u'ADVL'}, 
          {'head': u'se', 'dep_rel': u'ACC'}] 
y_real = [u'AM-NEG', u'A1', u'A0', u'A1', u'A1', u'A1', u'A0', u'A1', u'AM-ADV', u'A0', u'A1', u'A0', u'A2', u'A1']

feat_encoder =  DictVectorizer()
feat_encoder.fit(X_real)

label_encoder = LabelEncoder()
label_encoder.fit(y_real)

model = LogisticRegression()
model.fit(feat_encoder.transform(X_real), label_encoder.transform(y_real))

print ""Test 1...""
X_test1 = [{'head': u'governo', 'dep_rel': u'SUBJ'}]
X_test1_encoded = feat_encoder.transform(X_test1)
print ""Features Encoded""
print X_test1_encoded
print ""Shape""
print X_test1_encoded.shape
print ""decision_function:""
print model.decision_function(X_test1_encoded)
print ""predict_proba:""
print model.predict_proba(X_test1_encoded)

print ""Test 2...""
X_test2 = [{'head': u'governo', 'dep_rel': u'SUBJ'}, 
           {'head': u'atrav\xe9s', 'dep_rel': u'ADVL'}, 
           {'head': u'configuram', 'dep_rel': u'ACC'}]

X_test2_encoded = feat_encoder.transform(X_test2)
print ""Features Encoded""
print X_test2_encoded
print ""Shape""
print X_test2_encoded.shape
print ""decision_function:""
print model.decision_function(X_test2_encoded)
print ""predict_proba:""
print model.predict_proba(X_test2_encoded)


print ""Test 3...""
X_test3 = [{'head': u'governo', 'dep_rel': u'SUBJ'}, 
           {'head': u'atrav\xe9s', 'dep_rel': u'ADVL'}, 
           {'head': u'configuram', 'dep_rel': u'ACC'},
           {'head': u'configuram', 'dep_rel': u'ACC'},]

X_test3_encoded = feat_encoder.transform(X_test3)
print ""Features Encoded""
print X_test3_encoded
print ""Shape""
print X_test3_encoded.shape
print ""decision_function:""
print model.decision_function(X_test3_encoded)
print ""predict_proba:""
print model.predict_proba(X_test3_encoded)


Following is the output obtained:

Test 1...
Features Encoded
  (0, 4)    1.0
Shape
(1, 19)
decision_function:
[[ 0.55372615 -1.02949707 -1.75474347 -1.73324726 -1.75474347]]
predict_proba:
[[ 1.  1.  1.  1.  1.]]
Test 2...
Features Encoded
  (0, 4)    1.0
  (1, 1)    1.0
  (2, 0)    1.0
Shape
(3, 19)
decision_function:
[[ 0.55372615 -1.02949707 -1.75474347 -1.73324726 -1.75474347]
 [-1.07370197 -0.69103629 -0.89306092 -1.51402163 -0.89306092]
 [-1.55921001  1.11775556 -1.92080112 -1.90133404 -1.92080112]]
predict_proba:
[[ 0.59710757  0.19486904  0.26065002  0.32612646  0.26065002]
 [ 0.23950111  0.24715931  0.51348452  0.3916478   0.51348452]
 [ 0.16339132  0.55797165  0.22586546  0.28222574  0.22586546]]
Test 3...
Features Encoded
  (0, 4)    1.0
  (1, 1)    1.0
  (2, 0)    1.0
  (3, 0)    1.0
Shape
(4, 19)
decision_function:
[[ 0.55372615 -1.02949707 -1.75474347 -1.73324726 -1.75474347]
 [-1.07370197 -0.69103629 -0.89306092 -1.51402163 -0.89306092]
 [-1.55921001  1.11775556 -1.92080112 -1.90133404 -1.92080112]
 [-1.55921001  1.11775556 -1.92080112 -1.90133404 -1.92080112]]
predict_proba:
[[ 0.5132474   0.12507868  0.21262531  0.25434403  0.21262531]
 [ 0.20586462  0.15864173  0.4188751   0.30544372  0.4188751 ]
 [ 0.14044399  0.3581398   0.1842498   0.22010613  0.1842498 ]
 [ 0.14044399  0.3581398   0.1842498   0.22010613  0.1842498 ]]


As can be seen, the values obtained with ""predict_proba"" for the instance in ""X_test1"" change when that same instance is with others in X_test2. Also, ""X_test3"" just reproduces the ""X_test2"" and adds one more instance (that is equal to the last in ""X_test2""), but the probability values for all of them change. Why does this happen?
Also, I find it really strange that ALL the probabilities for ""X_test1"" are 1, shouldn't the sum of all be 1?

Now, if instead of using ""predict_proba"" I use ""decision_function"", I get the consistency in the values obtained that I need. The problem is that I get negative coefficients, and even some of the positives ones are greater than 1. 

So, what should I use? Why do the values of ""predict_proba"" change that way? Am I not understanding correctly what those values mean?

Thanks in advance for any help you could give me.

UPDATE

As suggested, I changed the code so as to also print the encoded ""X_test1"", ""X_test2"" and ""X_test3"", as well as their shapes. This doesn't appear to be the problem, as the encoding is consistant for the same instances between the test sets. 
",,,,false,
https://stackoverflow.com/questions/33937532,false,The issue does not involve a problematic API. It is a question about controlling the behavior of scikit-learn's decision tree to split on an attribute only once.,,,,,,,Use one attribute only once in scikit-learn decision tree in python,"I am using scikit-learn to create a decision tree, and its working like a charm. I would like to achieve one more thing: to make the tree to split on an attribute only once.
The reason behind this is because of my very strange dataset. I use a noisy dataset, and i am really interested in the noise as well. My class outcomes are binary let say [+,-]. I have a bunch of attributes with numbers mostly in the range of (0,1).
When scikit-learn creates the tree it splits on attributes multiple times, to make the tree ""better"". I understand that in this way the leaf nodes become more pure, but thats not the case i would like to achieve.
The thing i did was to define cutoffs for every attribute by counting the the information gain in different cutoffs, and choosing the maximum. In this way with ""leave-one-out"" and ""1/3-2/3"" cross validation techniques i get better results then the original tree.
The problem is that when i try to automatize this, i run into a problem  around the lower and upper bound e.g. around 0 and 1 because most of the elements will be under/upper that and  i get really high informational gain, cause one of the sets are pure, even if it only contains 1-2% of the full data.
All in all i would like to do something to make scikit-learn to only split on an attribute once.
If it cannot be done, do you guys have any advice how to generate those cutoffs in a nice way?
","To answer your question briefly, no, there is no built-in parameter to do this in sklearn. I tried to do the same a year ago, so I opened an issue requesting the addition of this feature.
sklearn builds nodes by randomly picking max_features features from the training dataset and searching for the cutoff that reduces the loss function the most. This exact same process is ran iteratively until some stopping criteria is met (max_depth, min_samples_leaf, etc.).
Hence, every feature always has the same probability of being picked, regardless of whether or not it has been used before.
If you're up for it, you can edit the source code of the classifier. In essence, all you need to do is drop the feature that minimizes the loss function after it has been chosen to build a node. That way, the algorithm will be unable to pick that feature again when taking a new sample of max_features features.
","I am not giving a method to directly deal with stopping the Classifier from using a feature multiple times. (Although you could do it by defining your own splitter and wiring it in, it is a lot of work.)

I would suggest making sure that you are balancing your classes in the first place, take a look at the class_weight parameter for details. That should help a lot in your issue. But if that does not work you can still enforce that there are no leafs having too small weight in them using the min_weight_fraction_leaf or similar parameters as suggested by maxymoo.
",,false,
https://stackoverflow.com/questions/28076232,false,The issue does not involve a problematic API. It is a comparison of the accuracy and convergence speed between Apache Spark and Scikit Learn for machine learning models.,,,,,,,Is Apache Spark less accurate than Scikit Learn?,"I've recently been trying to get to know Apache Spark as a replacement for Scikit Learn, however it seems to me that even in simple cases, Scikit converges to an accurate model far faster than Spark does.
For example I generated 1000 data points for a very simple linear function (z=x+y) with the following script:

from random import random

def func(in_vals):
    '''result = x (+y+z+w....)'''
    result = 0
    for v in in_vals:
        result += v
    return result

if __name__ == ""__main__"":
    entry_count = 1000
    dim_count = 2
    in_vals = [0]*dim_count
    with open(""data_yequalsx.csv"", ""w"") as out_file:
        for entry in range(entry_count):
            for i in range(dim_count):
                in_vals[i] = random()
            out_val = func(in_vals)
            out_file.write(','.join([str(x) for x in in_vals]))
            out_file.write("",%s\n"" % str(out_val))


I then ran the following Scikit script:

import sklearn
from sklearn import linear_model

import numpy as np

data = []
target = []
with open(""data_yequalsx.csv"") as inFile:
    for row in inFile:
        vals = row.split("","")
        data.append([float(x) for x in vals[:-1]])
        target.append(float(vals[-1]))

test_samples= len(data)/10

train_data = [0]*(len(data) - test_samples)
train_target = [0]*(len(data) - test_samples)
test_data = [0]*(test_samples)
test_target = [0]*(test_samples)
train_index = 0
test_index = 0
for j in range(len(data)):
    if j &gt;= test_samples:
        train_data[train_index] = data[j]
        train_target[train_index] = target[j]
        train_index += 1
    else:
        test_data[test_index] = data[j]
        test_target[test_index] = target[j]
        test_index += 1

model = linear_model.SGDRegressor(n_iter=100, learning_rate=""invscaling"", eta0=0.0001, power_t=0.5, penalty=""l2"", alpha=0.0001, loss=""squared_loss"")
model.fit(train_data, train_target)
print(model.coef_)
print(model.intercept_)

result = model.predict(test_data)
mse = np.mean((result - test_target) ** 2)
print(""Mean Squared Error = %s"" % str(mse))


And then this Spark script: (with spark-submit , no other arguments)

from pyspark.mllib.regression import LinearRegressionWithSGD, LabeledPoint
from pyspark import SparkContext

sc = SparkContext (appName=""mllib_simple_accuracy"")

raw_data = sc.textFile (""data_yequalsx.csv"", minPartitions=10) #MinPartitions doesnt guarantee that you get that many partitions, just that you wont have fewer than that many partitions
data = raw_data.map(lambda line: [float(x) for x in line.split ("","")]).map(lambda entry: LabeledPoint (entry[-1], entry[:-1])).zipWithIndex()
test_samples= data.count()/10

training_data = data.filter(lambda (entry, index): index &gt;= test_samples).map(lambda (lp,index): lp)
test_data = data.filter(lambda (entry, index): index &lt; test_samples).map(lambda (lp,index): lp)

model = LinearRegressionWithSGD.train(training_data, step=0.01, iterations=100, regType=""l2"", regParam=0.0001, intercept=True)
print(model._coeff)
print(model._intercept)

mse = (test_data.map(lambda lp: (lp.label - model.predict(lp.features))**2 ).reduce(lambda x,y: x+y))/test_samples;
print(""Mean Squared Error: %s"" % str(mse))

sc.stop ()


Strangely though, the error given by spark is an order of magnitude larger than that given by Scikit (0.185 and 0.045 respectively) despite the two models having a nearly identical setup (as far as I can tell)
I understand that this is using SGD with very few iterations and so the results may differ but I wouldn't have thought that it would be anywhere near such a large difference or such a large error, especially given the exceptionally simple data.



Is there something I'm misunderstanding in Spark? Is it not correctly configured? Surely I should be getting a smaller error than that?
",,,,false,
https://stackoverflow.com/questions/39550118,false,The issue does not involve a problematic API. It is a question about obtaining cross-validation accuracy for a logistic regression model in R.,,,,,,,Cross Validation function for logistic regression in R,"I Come from a predominantly python + scikit learn background, and I was wondering how would one obtain the cross validation accuracy for a logistic regression model in R? I was searching and surprised that there's no easy way to this. I'm looking for the equivalent:

import pandas as pd
from sklearn.cross_validation import cross_val_score
from sklearn.linear_model import LogisticRegression

## Assume pandas dataframe of dataset and target exist.

scores = cross_val_score(LogisticRegression(),dataset,target,cv=10)
print(scores)


For R: I have:

model = glm(df$Y~df$X,family=binomial')
summary(model) 


And now I'm stuck. Reason being, the deviance for my R model is 1900, implying its a bad fit, but the python one gives me 85% 10 fold cross validation accuracy.. which means its good. Seems a bit strange... So i wanted to run cross val in R to see if its the same result.

Any help is appreciated!
","R version using caret package:

library(caret)

# define training control
train_control &lt;- trainControl(method = ""cv"", number = 10)

# train the model on training set
model &lt;- train(target ~ .,
               data = train,
               trControl = train_control,
               method = ""glm"",
               family=binomial())

# print cv scores
summary(model)

","Below I took an answer from here and made a few changes. 

The changes I made were to make it a logit (logistic) model, add modeling and prediction, store the CV's results, and to make it a fully working example.

Also note that there are many packages and functions you could use, including cv.glm() from boot.

data(ChickWeight)

df                    &lt;- ChickWeight
df$Y                  &lt;- 0
df$Y[df$weight &gt; 100] &lt;- 1
df$X                  &lt;- df$Diet 

df     &lt;- df[sample(nrow(df)),]
folds  &lt;- cut(seq(1,nrow(df)),breaks=10,labels=FALSE)
result &lt;- list()

for(i in 1:10){
  testIndexes &lt;- which(folds==i,arr.ind=TRUE)
  testData    &lt;- df[testIndexes, ]
  trainData   &lt;- df[-testIndexes, ]
  model       &lt;- glm(Y~X,family=binomial,data=trainData)
  result[[i]] &lt;- predict(model, testData) 
}
result


You could add a line to calculate accuracy within the loop or just do it after the loop completes.
",,false,
https://stackoverflow.com/questions/49416716,true,"The issue involves a problematic API. The behavior of the OneVsRestClassifier in scikit-learn is affected by the LabelBinarizer, leading to different results when using one-hot encoding. The API is not behaving as expected in the multiclass example.",,,,,,,LabelBinarizer yields different result in multiclass example,"When executing the multiclass example in the scikit-learn tutorial


http://scikit-learn.org/stable/tutorial/basic/tutorial.html#multiclass-vs-multilabel-fitting


I came across a slight oddity.

&gt;&gt;&gt; import sklearn
&gt;&gt;&gt; sklearn.__version__
0.19.1

&gt;&gt;&gt; from sklearn.svm import SVC
&gt;&gt;&gt; from sklearn.multiclass import OneVsRestClassifier
&gt;&gt;&gt; from sklearn.preprocessing import LabelBinarizer

&gt;&gt;&gt; X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]
&gt;&gt;&gt; y = [0, 0, 1, 1, 2] # Three classes

&gt;&gt;&gt; clf = OneVsRestClassifier(estimator=SVC(random_state=0))
&gt;&gt;&gt; clf.fit(X, y).predict(X)
array([0, 0, 1, 1, 2])


This is all fine. Now with one-hot encoding:

&gt;&gt;&gt; y = LabelBinarizer().fit_transform(y)
&gt;&gt;&gt; y
array([[1, 0, 0],
       [1, 0, 0],
       [0, 1, 0],
       [0, 1, 0],
       [0, 0, 1]])


I would expect the label binarizer to only encode the target, but not having an influence on the classifier. However it yields a different result:

&gt;&gt;&gt; clf.fit(X, y).predict(X)
array([[1, 0, 0],
       [1, 0, 0],
       [0, 1, 0],
       [0, 0, 0],
       [0, 0, 0]])


Notebook on Google Colab (where the same code yields yet a different error, strangely):


https://drive.google.com/file/d/13dZ2aVbKTMgPOxj2SLsas2U2mOoKng2M/view?usp=sharing

","OneVsRestClassifier is applying  LabelBinarizer itself under the hood (the source code in sklearn/multiclass.py):

def fit(self, X, y):
  ...
  self.label_binarizer_ = LabelBinarizer(sparse_output=True)
  Y = self.label_binarizer_.fit_transform(y)
  Y = Y.tocsc()
  self.classes_ = self.label_binarizer_.classes_


So extra manual conversion is unnecessary. In fact, it's interpreting your one-hot encoded y as multi-label input. From the documentation:


  y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]
  
  Multi-class targets. An indicator matrix turns on multilabel
  classification.

",,,false,
https://stackoverflow.com/questions/59240556,true,"The issue involves a problematic API. The LatentDirichletAllocation function is not found in the decomposition module of scikit-learn, despite being documented. This indicates a potential problem with the installation or configuration of scikit-learn.",,,,,,,LatentDirichletAllocation was not installed in decomposition module of SciKit-Learn,"I got some strange problem at SciKit-Learn package. 
There is ""decomposition"" module inside SciKit-Learn package, which should contain LatentDirichletAllocation([]) function. See documentation here:
""https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition""

When I tried to import ""decomposition"" module: 
from sklearn import as decomposition
it gives error:

Traceback (most recent call last):
  File ""tf_1_day_scikit_dnn.py"", line 12, in &lt;module&gt;
    from sklearn import decomposition
  File ""/home/developer1/.local/lib/python3.6/site-packages/sklearn/decomposition/__init__.py"", line 19, in &lt;module&gt;
    from ._online_lda import LatentDirichletAllocation
ImportError: cannot import name 'LatentDirichletAllocation'


Command: 
ls -al ~/.local/lib/python3.6/site-packages/sklearn/decomposition shows:

drwxr-xr-x  4 developer1 developer1   4096 Dec  9 00:45 .
drwxr-xr-x 33 developer1 developer1   4096 Dec  9 00:45 ..
-rw-r--r--  1 developer1 developer1   5490 Dec  9 00:44 _base.py
-rw-r--r--  1 developer1 developer1    480 Dec  9 00:44 base.py
-rwxr-xr-x  1 developer1 developer1 179440 Dec  9 00:44 _cdnmf_fast.cpython-36m-x86_64-linux-gnu.so
-rwxr-xr-x  1 developer1 developer1 175344 Dec  3 00:09 cdnmf_fast.cpython-36m-x86_64-linux-gnu.so
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 cdnmf_fast.py
-rw-r--r--  1 developer1 developer1  54528 Dec  9 00:44 _dict_learning.py
-rw-r--r--  1 developer1 developer1    507 Dec  9 00:44 dict_learning.py
-rw-r--r--  1 developer1 developer1  12572 Dec  9 00:44 _factor_analysis.py
-rw-r--r--  1 developer1 developer1    513 Dec  9 00:44 factor_analysis.py
-rw-r--r--  1 developer1 developer1  20866 Dec  9 00:44 _fastica.py
-rw-r--r--  1 developer1 developer1    490 Dec  9 00:44 fastica_.py
-rw-r--r--  1 developer1 developer1  14076 Dec  9 00:44 _incremental_pca.py
-rw-r--r--  1 developer1 developer1    513 Dec  9 00:44 incremental_pca.py
-rw-r--r--  1 developer1 developer1   1401 Dec  9 00:44 __init__.py
-rw-r--r--  1 developer1 developer1  13597 Dec  9 00:44 _kernel_pca.py
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 kernel_pca.py
-rw-r--r--  1 developer1 developer1  47255 Dec  9 00:44 _nmf.py
-rw-r--r--  1 developer1 developer1    477 Dec  9 00:44 nmf.py
-rwxr-xr-x  1 developer1 developer1  62056 Dec  3 00:09 _online_lda.cpython-36m-x86_64-linux-gnu.so
-rwxr-xr-x  1 developer1 developer1  62064 Dec  9 00:44 _online_lda_fast.cpython-36m-x86_64-linux-gnu.so
-rw-r--r--  1 developer1 developer1    513 Dec  9 00:44 online_lda_fast.py
-rw-r--r--  1 developer1 developer1  30471 Dec  9 00:44 _online_lda.py
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 online_lda.py
-rw-r--r--  1 developer1 developer1  22807 Dec  9 00:44 _pca.py
-rw-r--r--  1 developer1 developer1    477 Dec  9 00:44 pca.py
drwxr-xr-x  2 developer1 developer1   4096 Dec  9 00:45 __pycache__
-rw-r--r--  1 developer1 developer1    855 Dec  9 00:44 setup.py
-rw-r--r--  1 developer1 developer1  13654 Dec  9 00:44 _sparse_pca.py
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 sparse_pca.py
drwxr-xr-x  3 developer1 developer1   4096 Dec  9 00:45 tests
-rw-r--r--  1 developer1 developer1   8346 Dec  9 00:44 _truncated_svd.py
-rw-r--r--  1 developer1 developer1    507 Dec  9 00:44 truncated_svd.py


Most of the functions are here but there are no traces of the ""LatentDirichletAllocation"" function. Yet I did see LatentDirichletAllocation class defined in the _online_lda.py file.

Command python3 -c ""import sklearn; sklearn.show_versions()"" prints all versions required:

System:
    python: 3.6.8 (default, Oct  7 2019, 12:59:55)  [GCC 8.3.0]
executable: /usr/bin/python3
   machine: Linux-4.15.0-65-generic-x86_64-with-Ubuntu-18.04-bionic

Python dependencies:
       pip: 9.0.1
setuptools: 42.0.2
   sklearn: 0.22
     numpy: 1.17.4
     scipy: 1.3.3
    Cython: None
    pandas: 0.25.3
matplotlib: 3.1.2
    joblib: 0.14.0

Built with OpenMP: True


So I assume all requirements were met here. (BTW SciKit-Learn worked fine till I tried to import ""decomposition"" module).

I installed the SciKit-Learn package as it was described here: 
""https://scikit-learn.org/stable/install.html""
using command pip3 install -U scikit-learn
I did reinstall that several times but got same result. 

What do I do wrong? Do I need to add missing ""LatentDirichletAllocation"" function into the ""decomposition"" module? If so - how? Should I install the whole package somehow different way? 

Thanks.
","I had the same problem today. I solved it by going back to the previous version 0.21 of scikit-learn:

pip3 install scikit-learn==0.21


EDIT: I think the answer from glemaitre (https://stackoverflow.com/a/59328446/10429267) shows a better solution.
","The validated ""answered"" is not the right fix since it only downgrades scikit-learn. You will not be able to benefit from new features and bug fixes.

It will be helpful instead to report and give feedback in the issue tracker: https://github.com/scikit-learn/scikit-learn/issues/15884

It will allow us to find the root of the problem and propose the proper fix which seems to affect several persons.

EDIT:
After some investigation, you need to remove the following files:


~/.local/lib/python3.6/site-packages/sklearn/decomposition/_online_lda.cpython-36m-x86_64-linux-gnu.so
~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/_hashing.cpython-36m-x86_64-linux-gnu.so
~/.local/lib/python3.6/site-packages/sklearn/datasets/_svmlight_format.cpython-36m-x86_64-linux-gnu.so


Apparently, the so files do not get removed when updating to scikit-learn. This might be due to an old pip version.
","Try:



For Python 3

pip3 uninstall scikit-learn
pip3 install -U scikit-learn==0.21.3




For Python 2

pip uninstall scikit-learn
pip install -U scikit-learn==0.20.4

",false,
https://stackoverflow.com/questions/33317896,false,The issue does not involve a problematic API. It is a question about the results obtained when applying scikit-learn's t-SNE implementation on word embeddings generated by word2vec.,,,,,,,scikit learn TSNE transform returns strange results when applied to word vectors,"I am getting weird results when applying scikit learn's manifold t-sne implementation on word embeddings generated by word2vec.
Normally, the distance and direction of (king-queen) should be the same as (man-woman) as stated in plenty of examples on the internet.

I am sure that my word2vec model is trained correctly as I am using the Google News data set and the query of most_similar(king-man+woman) also returns queen as it is supposed to.

The plotting of the transformed 2d-vectors is also done correctly. Therefore, it must be related to the t-sne transformation from scikit learn.

I am using the following python code for transformation:

vecs = np.concatenate(vecs)
np_vecs = np.array(vecs, dtype='float')
ts = TSNE(2)
reduced_vecs = ts.fit_transform(np_vecs)




I am I doing anything wrong or is the library configuration wrong?
","I would like to add to iceui2's answer. Original TSNE algorithm, is a non-linear dimension reduction technique, which tries to make sure that distribution of distances between neighbours is preserved in lower and higher dimension (neighbours in high dimension will be neighbours in lower dimension). 

However, your directions are likely to get distorted, as the manifold in which points used to lie has changed drastically.

To preserve directions, I recommend using PCA for dimension reduction and then plotting. 
",,,false,
https://stackoverflow.com/questions/21855410,false,The issue does not involve a problematic API. It is a question about the behavior of the DBSCAN algorithm in scikit-learn and the unexpected increase in the number of estimated clusters with the increase of the MinPts parameter.,,,,,,,Number of clusters increased with the increase of MinPts in scikit-learn DBSCAN,"I use DBSCAN implementation from scikit-learn library and I got strange results.
The number of estimated clusters increased with the increase of parameter MinPts (min_samples) and from my understanding of algorithm this should not happend. 

Here are my results:

Estimated number of clusters:34 eps=0.9 min_samples=13.0
Estimated number of clusters:35 eps=0.9 min_samples=12.0
Estimated number of clusters:42 eps=0.9 min_samples=11.0 &lt;- strange result here
Estimated number of clusters:37 eps=0.9 min_samples=10.0   
Estimated number of clusters:53 eps=0.9 min_samples=9.0
Estimated number of clusters:63 eps=0.9 min_samples=8.0


I use scikit-learn like this:

X = StandardScaler().fit_transform(X)
db = DBSCAN(eps=eps, min_samples=min_samples, algorithm='kd_tree').fit(X)


and X is an array that contains ~200k 12-dimensional points.

What can be the problem here?
",,,,false,
