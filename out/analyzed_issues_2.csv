ISSUE_LINK,ai_verdict,REASON,LIBRARY_NAME,API_NAME,ISSUE_DESCRIPTION,EXPECTED_VS_ACTUAL_BEHAVIOR,TRIGGER_CONDITIONS,REASON_FOR_DIFFICULTY_IN_DETECTION,ISSUE_TITLE,ISSUE_BODY,ANSWER_1,ANSWER_2,ANSWER_3,HUMAN_CLASSIFICATION,HUMAN_REASON
https://stackoverflow.com/questions/76876536,false,The issue does not involve a problematic API. It appears to be related to the discrepancy between the feature names generated by the TfidfVectorizer and the words in the input text. This is likely due to the tokenization process or the presence of special characters in the text.,,,,,,,Unexpected Feature Names in scikit-learn TfidfVectorizer,"I'm trying to use the TfidfVectorizer from scikit-learn to convert a list of words into a TF-IDF matrix. However, I'm encountering some unexpected behavior where the feature names generated by the vectorizer don't match the words in my input text.
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import brown
import re

txt = brown.words()
txt_str = "" "".join(txt)

def custom_tokenizer(text):
    tokens = re.findall(r'\b\w+\b', text.lower())
    return tokens

documents = [txt_str]

vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)
tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

print(feature_names)

I've noticed that some feature names in the output are neither present in my txt nor in the tokens generated by the custom_tokenizer. Additionally, some feature names seem to be numeric values or arbitrary characters and words.
I've tried various approaches, including using a custom tokenizer to ensure accurate tokenization, updating scikit-learn. It works on smaller texts as well as when the max_features is set to low enough value. However, for my project I do need all the features so I am trying to figure out what is going on.
Why I'm getting these unexpected feature names and how I can ensure that the feature names correspond accurately to the words in my input text?
some of the features are zwei' 'zworykin'.
","Have you some examples of unexpected feature names ?
From my side, I don't think there is any problem extracting features.
The words not present in feature names, are words with special characters like -  ' or $. WHen you extract the text with the function custom_tokenizer, you get two tokens for example polyether-type, gives ""polyether"" and ""type"" tokens
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import brown
import re

txt = brown.words()
txt_str = "" "".join(txt)

def custom_tokenizer(text):
    tokens = re.findall(r'\b\w+\b', text.lower())
    return tokens

documents = [txt_str]

vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)
tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

print(f""Nb features                     : { len(feature_names)}"")
print(f""Nb words in brown               : { len(txt) }"")

# nb uniq lowercase words in brown
mylist = list(dict.fromkeys(txt))
mylist = [ x.lower() for x in mylist]
print(f""Nb uniq lowercase words in brown: {len(mylist)}"")

# in li1 and not in li2
def Diff(li1, li2):
    set_dif = set(li1).difference(set(li2))
    temp3 = list(set_dif)
    return temp3
li3 = Diff(mylist,feature_names)

def info(text):
    custom_tokenizer(text)
    try:
        s=list(feature_names).index(text)
        print(f""The text {text} was found in features at position : {s}"")
    except:
        print(f""The text {text} was not found in features"")    
    

print(f""10 first words not in feature_names : \n { li3[0:10]} "")

info(""season"")
info(""mosquito"")

Nb features                     : 42432
Nb words in brown               : 1161192
Nb uniq lowercase words in brown: 56057
10 first words not in feature_names :
[""season's"", 'non-farm', ""chip-o's"",
'polyether-type', 'take-off', '1-0', 'spring-back', '$50',
'mosquito-plagued', 'anti-submarine']
The text season was found in features at position : 33478
The text mosquito was found in features at position : 24918
",,,false,
https://stackoverflow.com/questions/41949101,false,The behavior is not related to a problematic API. It appears to be a subjective evaluation of the results obtained from the scikit-learn LDA classifier. The user is questioning whether the results are as expected or if there are better ways to achieve the desired classification.,,,,,,,scikit learn LDA giving unexpected results,"I am attempting to classify some data with the scikit learn LDA classifier.  I'm not entirely sure what to ""expect"" from it, but what I am getting is weird.  Seems like a good opportunity to learn about either a shortcoming of the technique, or a way in which I am applying it wrong.  I understand that no line could completely separate this data, but it seems that there are much ""better"" lines than the one it is finding.  I'm just using the default options.  Any thoughts on how to do this better?  I'm using LDA because it is linear in the size of my dataset.  Although I think a linear SVM has a similar complexity.  Perhaps it would be better for such data?  I will update when I have tested other possibilities.

The picture: (light blue is what my LDA classifier predicts will be dark blue)



The code:

import numpy as np
from numpy import array
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
import itertools

X = array([[ 0.23125754,  0.79170351],
       [ 0.78021491, -0.24999486],
       [ 0.00856446,  0.41452734],
       [ 0.66381753, -0.09872504],
       [-0.03178685,  0.04876317],
       [ 0.65574645, -0.68214948],
       [ 0.14290684,  0.38256002],
       [ 0.05156987,  0.11094875],
       [ 0.06843403,  0.19110019],
       [ 0.24070898, -0.07403764],
       [ 0.03184353,  0.4411446 ],
       [ 0.58708124, -0.38838008],
       [-0.00700369,  0.07540799],
       [-0.01907816,  0.07641038],
       [ 0.30778608,  0.30317186],
       [ 0.55774143, -0.38017325],
       [-0.00957214, -0.03303287],
       [ 0.8410637 ,  0.158594  ],
       [-0.00294113, -0.00380608],
       [ 0.26577841,  0.07833684],
       [-0.32249375,  0.49290502],
       [ 0.11313078,  0.35697211],
       [ 0.41153679, -0.4471876 ],
       [-0.00313315,  0.30065913],
       [ 0.14344143, -0.19127107],
       [ 0.04857767,  0.01339191],
       [ 0.5865007 ,  0.71209886],
       [ 0.08157439,  0.40909955],
       [ 0.72495202,  0.29583866],
       [-0.09391461,  0.17976605],
       [ 0.06149141,  0.79323099],
       [ 0.52208024, -0.2877661 ],
       [ 0.01992141, -0.00435266],
       [ 0.68492617, -0.46981335],
       [-0.00641231,  0.29699622],
       [ 0.2369677 ,  0.140319  ],
       [ 0.6602586 ,  0.11200433],
       [ 0.25311836, -0.03085372],
       [-0.0895014 ,  0.45147252],
       [-0.18485667,  0.43744524],
       [ 0.94636701,  0.16534406],
       [ 0.01887734, -0.07702135],
       [ 0.91586801,  0.17693792],
       [-0.18834833,  0.31944796],
       [ 0.20468328,  0.07099982],
       [-0.15506378,  0.94527383],
       [-0.14560083,  0.72027034],
       [-0.31037647,  0.81962815],
       [ 0.01719756, -0.01802322],
       [-0.08495304,  0.28148978],
       [ 0.01487427,  0.07632112],
       [ 0.65414479,  0.17391618],
       [ 0.00626276,  0.01200355],
       [ 0.43328095, -0.34016614],
       [ 0.05728525, -0.05233956],
       [ 0.61218382,  0.20922571],
       [-0.69803697,  2.16018536],
       [ 1.38616732, -1.86041621],
       [-1.21724616,  2.72682759],
       [-1.26584365,  1.80585403],
       [ 1.67900048, -2.36561699],
       [ 1.35537903, -1.60023078],
       [-0.77289615,  2.67040114],
       [ 1.62928969, -1.20851808],
       [-0.95174264,  2.51515935],
       [-1.61953649,  2.34420531],
       [ 1.38580104, -1.9908369 ],
       [ 1.53224512, -1.96537012]])

y = array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.])

classifier = LDA()
classifier.fit(X,y)

xx = np.array(list(itertools.product(np.linspace(-4,4,300), np.linspace(-4,4,300))))
yy = classifier.predict(xx)
b_colors = ['salmon' if yyy==0 else 'deepskyblue' for yyy in yy]
p_colors = ['r' if yyy==0 else 'b' for yyy in y]
plt.scatter(xx[:,0],xx[:,1],s=1,marker='o',edgecolor=b_colors,c=b_colors)
plt.scatter(X[:,0], X[:,1], marker='o', s=5, c=p_colors, edgecolor=p_colors)
plt.show()


UPDATE: Changing from using sklearn.discriminant_analysis.LinearDiscriminantAnalysis to sklearn.svm.LinearSVC also using the default options gives the following picture:



I think using the zero-one loss instead of the hinge loss would help, but sklearn.svm.LinearSVC doesn't seem to allow custom loss functions.

UPDATE: The loss function to sklearn.svm.LinearSVC approaches the zero-one loss as the parameter C goes to infinity.  Setting C = 1000 gives me what I was originally hoping for.  Not posting this as an answer, because the original question was about LDA.

picture:


","LDA models each class as a Gaussian, so the model for each class is determined by the class' estimated mean vector and covariance matrix.
Judging by the eye only, your blue and red classes have approximately the same mean and same covariance, which means the 2 Gaussians will 'sit' on top of each other, and the discrimination will be poor. Actually it also means that the separator (the blue-pink border) will be noisy, that is it will change a lot between random samples of your data.

Btw your data is clearly not linearly-separable, so every linear model will have a hard time discriminating the data.

If you must use a linear model, try using LDA with 3 components, such that the top-left blue blob is classified as '0', the bottom-right blue blob as '1', and the red as '2'. This way you will get a much better linear model. You can do it by preprocessing the blue class with a clustering algorithm with K=2 classes.
",,,false,
https://stackoverflow.com/questions/30066268,false,"The issue does not involve a problematic API. The user is seeking a way to provide starting positions to the t-distributed Stochastic Neighbor Embedding (TSNE) algorithm in scikit-learn, which is not currently supported by the library.",,,,,,,Provide Starting Positions to t-distributed Stochastic Neighbor Embedding (TSNE) in scikit-learn,"I've been looking at using scikit learns' TSNE method to visualize high dimensional data in 2D. However, I have some idea of where the starting positions should be in 2D space but I don't see any way of specifying this information. Any ideas how I might be able to provide the starting coordinates?

I see that there's an ""init"" parameter but it appears to only take ""random"" or ""pca"" as options. What I'm looking for is a way to specify exactly what the initial coordinates are.

Update

I'm trying to increase the distance between my data points (~6,500 points) but they are quite tightly clustered and overlapping. With default parameters, I get:

model = sklearn.manifold.TSNE(n_components=2, random_state=0)




A slight increase in n_iter and early_exaggeration didn't produce significantly different results as the points (within clusters) are still overlapping.

model = sklearn.manifold.TSNE(n_components=2, random_state=0, n_iter=10000, early_exaggeration=10)




However, increasing the early_exaggeration from 10 to 100 (which, according to the docs, should increase the distance between clusters) produced some unexpected results (I ran this twice and it was the same result):

model = sklearn.manifold.TSNE(n_components=2, random_state=0, n_iter=10000, early_exaggeration=100)




This link: https://beta.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm Provides an example (three quarters of the way down the page) for how to monkey patch the gradient_descent function to save coordinates/positions. 

Added Issue to scikit-learn
","It is currently not possible, but it would be a two-line change.
I think it would be a good addition, and we do support init=array for things like k-means. So PR welcome.
",,,false,
https://stackoverflow.com/questions/36865542,false,"The issue does not involve a problematic API. The user is experiencing unexpected results with the Adjusted Mutual Information (AMI) index in scikit-learn, which could be due to specific characteristics of the data or the implementation of the algorithm.",,,,,,,Adjusted Mutual Information (scikit-learn),"I have implemented a clustering algorithm for summarizing log files, and am currently testing it against ground-truth data with the Adjusted Rand index and the Adjusted Mutual Information index. 

Input to my algorithm is a list of log entries, and output is a list of integers (the cluster label that each item belongs to). The ground truth is similarly a list of integers where each integer represent the true cluster the item belongs to. For most of my test cases I receive normal/expected results, but one file is giving me unexpected output. I have enclosed the two lists, the ground-truth clustering as well as that of my algorithm's:

Ground truth list:
http://pastebin.com/9Y5TE6b7

Own clustering:
http://pastebin.com/hJz1M4sf

These two lists are fed into scikit-learn functions to get the ARI and AMI. The ARI score looks roughly correct, but AMI is above 1, which according to the documentation and definition of AMI should not be possible if I understand it correctly. This data set is highly unbalanced, but many of my other files are similarly balanced. I cannot figure this out. For reference, the scores I get for ARI and AMI is:

ARI: 0.99642743999922712

AMI: 1.0190170466324
","This has been fixed in the development version.
",,,false,
https://stackoverflow.com/questions/12358965,false,"The issue does not involve a problematic API. The user is encountering unexpected results with the RandomForestClassifier in scikit-learn, which could be influenced by the imbalanced nature of the dataset or the specific characteristics of the features.",,,,,,,scikit-learn RandomForestClassifier produces &#39;unexpected&#39; results,"I'm trying to use sk-learn's RandomForestClassifier for a binary classification task (positive and negative examples). My training data contains 1.177.245 examples with 40 features, in SVM-light format (sparse vectors) which I load using sklearn.dataset's load_svmlight_file. It produces a sparse matrix of 'feature values' (1.177.245 * 40) and one array of 'target classes' (1s and 0s, 1.177.245 of them). I don't know whether this is worrysome, but the trainingdata has 3552 positives and the rest are all negative.

As the sk-learn's RFC doesn't accept sparse matrices, I convert the sparse matrix to a dense array (if I'm saying that right? Lots of 0s for absent features) using .toarray(). I print the matrix before and after converting to arrays and that seems to be going all right.

When I initiate the classifier and start fitting it to the data, it takes this long:

[Parallel(n_jobs=40)]: Done   1 out of  40 | elapsed: 24.7min remaining: 963.3min
[Parallel(n_jobs=40)]: Done  40 out of  40 | elapsed: 27.2min finished


(is that output right? Those 963 minutes take about 2 and a half...)

I then dump it using joblib.dump.
When I re-load it:

RandomForestClassifier: RandomForestClassifier(bootstrap=True, compute_importances=True,
        criterion=gini, max_depth=None, max_features=auto,
        min_density=0.1, min_samples_leaf=1, min_samples_split=1,
        n_estimators=1500, n_jobs=40, oob_score=False,
        random_state=&lt;mtrand.RandomState object at 0x2b2d076fa300&gt;,
        verbose=1)


And test it on real trainingdata (consisting out of 750.709 examples, exact same format as training data) I get ""unexpected"" results. To be exact; only one of the examples in the testingdata is classified as true. When I train on half the initial trainingdata and test on the other half, I get no positives at all.

Now I have no reason to believe anything is wrong with what's happening, it's just that I get weird results, and furthermore I think it's all done awfully quick. It's probably impossible to make a comparison, but training a RFClassifier on the same data using rt-rank (also with 1500 iterations, but with half the cores) takes over 12 hours...

Can anyone enlighten me whether I have any reason to believe something is not working the way it's supposed to? Could it be the ratio of positives to negatives in the training data? Cheers.
","Indeed this dataset is very very imbalanced. I would advise you to subsample the negative examples (e.g. pick n_positive_samples of them at random) or to oversample the positive example (the latter is more expensive and but might yield better models).

Also are you sure that all your features are numerical features (larger values means something in real life)? If some of them are categorical integer markers, those feature should be exploded as one-of-k boolean encodings instead as scikit-learn implementation of random forest s cannot directly deal with categorical data.
",,,false,
https://stackoverflow.com/questions/53215887,false,The issue does not involve a problematic API. The user is experiencing unexpected results when creating a modification of the KNeighborsClassifier class in scikit-learn. The issue is likely related to the implementation or usage of the custom class rather than a problem with the API itself.,,,,,,,unexpected result for inheritance in scikit-learn,"I am getting an unexpected result when trying to create a simple modification of the class KNeighborsClassifier:

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)

class my_Classifier(KNeighborsClassifier):  
  """"""My Nearest Neighbour classifier""""""
  def __init__(self, gamma=0):
    def my_dist(x, y):  # squared distance
      return np.sum((abs(x-y))**gamma)
    KNeighborsClassifier.__init__(self, n_neighbors=1, metric=my_dist)
    self.gamma = gamma
    print(gamma)
  def fit(self, X, y):
    KNeighborsClassifier.fit(self, X, y)
    return self
  def predict(self, X, y=None):
    return KNeighborsClassifier.predict(self, X)
  def score(self, X, y):
    return KNeighborsClassifier.score(self, X, y)


I have made the metric dependent on a parameter gamma&gt;=0.  If gamma=2, this is just the squared Euclidean metric, and if gamma=0, this is a useless metric (essentially a constant).  First it works as expected: for

knn = my_Classifier(gamma=2)
knn.fit(X_train, y_train)
knn.score(X_test,y_test)


the output is

2
0.9736842105263158


(the accuracy is good) and for

knn = my_Classifier(gamma=0)
knn.fit(X_train, y_train)
knn.score(X_test,y_test)


the output is

0
0.34210526315789475


(the accuracy is hopeless).  I am also printing the value of gamma used.

However, when I try

from sklearn.model_selection import GridSearchCV
param_grid = {'gamma': [0,2]}
grid_search = GridSearchCV(my_Classifier(), param_grid)
grid_search.fit(X_train, y_train)
grid_search.score(X_test, y_test)


the result is unexpected: 

0
0
0
0
0
0
0
0
0.34210526315789475


Why is the value gamma=0 used every time?  And gamma=2 (producing a much better result) is never tried.  I know I am making some silly mistake but can't see where.
","The value gamma = 0 is not used every time !

What you see when you get this:

0
0
0
0
0
0
0
0
0.34210526315789475


Is the score



To see the gamma use this:

grid_search.cv_results_ 

",,,false,
https://stackoverflow.com/questions/69916926,false,The issue does not involve a problematic API. It is related to an error encountered during the installation of a conda environment from a .yaml file. The error is likely caused by a configuration issue or an error in the .yaml file itself.,,,,,,,Unexpected error when trying to install conda environment from .yaml file,"I am trying to install a conda environment in WSL2 from a .yaml file, the instructions for the installation saying the first step is to run the following command:
conda env create -f devtools/conda-envs/ael-test.yaml

After the WSL2 terminal collects the package metadata and solves the environment, it encounters an error, which I don't really know how to solve. I run the command above from the base environment, from the directory where /devtools/conda-ens/ael-test.yaml is located.
Collecting package metadata (repodata.json): done
Solving environment: done
Preparing transaction: failed

# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

    Traceback (most recent call last):
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/exceptions.py"", line 1079, in __call__
        return func(*args, **kwargs)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/cli/main.py"", line 80, in do_call
        exit_code = getattr(module, func_name)(args, parser)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/cli/main_create.py"", line 141, in execute
        result[installer_type] = installer.install(prefix, pkg_specs, args, env)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/installers/conda.py"", line 59, in install
        unlink_link_transaction.execute()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 245, in execute
        self.verify()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 222, in verify
        self.prepare()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 211, in prepare
        grps = self._prepare(self.transaction_context, stp.target_prefix,
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 288, in _prepare
        packages_info_to_link = tuple(read_package_info(prec, pcrec)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 288, in &lt;genexpr&gt;
        packages_info_to_link = tuple(read_package_info(prec, pcrec)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/gateways/disk/read.py"", line 89, in read_package_info
        package_metadata = read_package_metadata(epd)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/gateways/disk/read.py"", line 144, in read_package_metadata
        data = json.loads(f.read())
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/__init__.py"", line 357, in loads
        return _default_decoder.decode(s)
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/decoder.py"", line 337, in decode
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/decoder.py"", line 355, in raw_decode
        raise JSONDecodeError(""Expecting value"", s, err.value) from None
    json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

`$ /home/gheorghe/anaconda3/bin/conda-env create -f devtools/conda-envs/ael-test.yaml`

  environment variables:
                 CIO_TEST=&lt;not set&gt;
  CONDA_AUTO_UPDATE_CONDA=false
        CONDA_DEFAULT_ENV=base
                CONDA_EXE=/home/gheorghe/anaconda3/bin/conda
             CONDA_PREFIX=/home/gheorghe/anaconda3
    CONDA_PROMPT_MODIFIER=(base)
         CONDA_PYTHON_EXE=/home/gheorghe/anaconda3/bin/python
               CONDA_ROOT=/home/gheorghe/anaconda3
              CONDA_SHLVL=1
           CURL_CA_BUNDLE=&lt;not set&gt;
                     PATH=/home/gheorghe/anaconda3/bin:/home/gheorghe/.vscode-server/bin/f4af3cb
                          f5a99787542e2a30fe1fd37cd644cc31f/bin:/home/gheorghe/anaconda3/bin:/ho
                          me/gheorghe/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbi
                          n:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Program Files
                          /WindowsApps/Microsoft.WindowsTerminal_1.11.2921.0_x64__8wekyb3d8bbwe:
                          /mnt/c/Program Files/copasi.org/COPASI 4.29.228/bin:/mnt/c/Windows/sys
                          tem32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/Syste
                          m32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Prog
                          ram Files (x86)/Intel/Intel(R) Management Engine
                          Components/DAL:/mnt/c/Program Files/Intel/Intel(R) Management Engine
                          Components/DAL:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA
                          NvDLISR:/mnt/c/Program Files (x86)/NVIDIA
                          Corporation/PhysX/Common:/mnt/c/Program Files/Mullvad VPN/resources:/m
                          nt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt
                          /c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/Ope
                          nSSH:/mnt/c/Program Files (x86)/Common Files/Propellerhead
                          Software/ReWire:/mnt/c/Program Files/Common Files/Propellerhead
                          Software/ReWire:/mnt/c/Program Files/NCBI/blast-2.12.0+/bin:/mnt/c/Use
                          rs/gheor/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/ghe
                          or/AppData/Local/Microsoft/WindowsApps:/snap/bin
       REQUESTS_CA_BUNDLE=&lt;not set&gt;
            SSL_CERT_FILE=&lt;not set&gt;

     active environment : base
    active env location : /home/gheorghe/anaconda3
            shell level : 1
       user config file : /home/gheorghe/.condarc
 populated config files : /home/gheorghe/.condarc
          conda version : 4.10.3
    conda-build version : 3.21.5
         python version : 3.8.11.final.0
       virtual packages : __linux=5.10.16.3=0
                          __glibc=2.31=0
                          __unix=0=0
                          __archspec=1=x86_64
       base environment : /home/gheorghe/anaconda3  (writable)
      conda av data dir : /home/gheorghe/anaconda3/etc/conda
  conda av metadata url : None
           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /home/gheorghe/anaconda3/pkgs
                          /home/gheorghe/.conda/pkgs
       envs directories : /home/gheorghe/anaconda3/envs
                          /home/gheorghe/.conda/envs
               platform : linux-64
             user-agent : conda/4.10.3 requests/2.26.0 CPython/3.8.11 Linux/5.10.16.3-microsoft-standard-WSL2 ubuntu/20.04.3 glibc/2.31
                UID:GID : 1000:1000
             netrc file : None
           offline mode : False


An unexpected error has occurred. Conda has prepared the above report.

If submitted, this report will be used by core maintainers to improve
future releases of conda.

Edit:
This is the .yaml file:
channels:
  - conda-forge
  - pytorch
dependencies:
  - python
  - pip

  - numpy
  - scipy
  - scikit-learn

  - qcelemental
  - openbabel

  - matplotlib
  - seaborn

  - mlflow

  - pytorch
  - torchvision

  - lark-parser
  - cython

  - pip:
    - torchani
    - ""git+https://github.com/RMeli/mdanalysis.git@develop#egg=MDAnalysis&amp;subdirectory=package""

  - black
  - flake8
  - mypy
  - isort

  - pytest
  - pytest-xdist
  - pytest-cov
  - codecov

","There is no problem in your environment, I managed to install it, getting
Successfully built MDAnalysis
Installing collected packages: msgpack, tqdm, mrcfile, mmtf-python, gsd, fasteners, biopython, GridDataFormats, torchani, MDAnalysis
Successfully installed GridDataFormats-1.0.1 MDAnalysis-2.6.0.dev0 biopython-1.81 fasteners-0.18 gsd-3.1.1 mmtf-python-1.1.3 mrcfile-1.4.3 msgpack-1.0.5 torchani-2.2.3 tqdm-4.66.1

done
#
# To activate this environment, use
#
#     $ conda activate ael

in the end.
The problem you encountering is probably related to https://github.com/conda/conda/issues/9590
which has a workaround
sudo rm -r ~/.condarc

or editing the ~/.condarc like https://github.com/conda/conda/issues/9590#issuecomment-1003211237
",,,false,
https://stackoverflow.com/questions/17794313,false,"The issue does not involve a problematic API. It is related to unexpected results when using scikit-learn's SVM classification algorithm with the RBF kernel. The issue is likely due to the specific dataset and parameters used, rather than a problem with the API itself.",,,,,,,Unexpected results when using scikit-learn&#39;s SVM classification algorithm (RBF kernel),"Using the example on this page 
http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html,
I created my own graphs using some normally distributed data with a standard deviation of 10 instead of the iris data.

My graph turned out to be like this:


Notice how the RBF kernel graph is very different from the the one from the example. The entire area is classified to be yellow except the red and blue bits. In other words there are too many support vectors. I have tried changing C and degree but they didn't help. The code I used to produce this graph is shown below. 

Please note I need to use RBF kernel because polynomial kernels run significantly slower than RBF kernels.

import numpy as np
import pylab as pl
from sklearn import svm, datasets

FP_SIZE = 50
STD = 10

def gen(fp):

  data = []
  target = []

  fp_count = len(fp)

  # generate rssi reading for monitors / fingerprint points
  # using scikit-learn data structure
  for i in range(0, fp_count):
    for j in range(0,FP_SIZE):
      target.append(i)
      data.append(np.around(np.random.normal(fp[i],STD)))

  data = np.array(data)
  target = np.array(target)

  return data, target

fp = [[-30,-70],[-58,-30],[-60,-60]]

data, target = gen(fp)

# import some data to play with
# iris = datasets.load_iris()
X = data[:, :2]  # we only take the first two features. We could
                      # avoid this ugly slicing by using a two-dim dataset
Y = target

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, Y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)
lin_svc = svm.LinearSVC(C=C).fit(X, Y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# title for the plots
titles = ['SVC with linear kernel',
          'SVC with RBF kernel',
          'SVC with polynomial (degree 3) kernel',
          'LinearSVC (linear kernel)']


for i, clf in enumerate((svc, rbf_svc, poly_svc, lin_svc)):
    # Plot the decision boundary. For that, we will asign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    pl.subplot(2, 2, i + 1)
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    pl.contourf(xx, yy, Z, cmap=pl.cm.Paired)
    pl.axis('off')

    # Plot also the training points
    pl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)

    pl.title(titles[i])

pl.show()

","DId you use any other measure of correctness aside from what you get in the point.

Usually SVMs need to be run using a grid search, specially if you have  an RBF, C only will take care of the regularization, which will do little if your data is not sparse to begin with.

You need to run a grid search over gamma and C, they have a really good example of that here:

http://scikit-learn.org/0.13/auto_examples/grid_search_digits.html#example-grid-search-digits-py

Also, their library already takes care of the cross validation.

Remember that those examples are good for the toy datasets, the moment you enter with a new dataset, there is no reason to believe is going to behave anything like the one in the example.
",,,false,
https://stackoverflow.com/questions/59830510,false,The issue does not involve a problematic API. It is related to a TypeError encountered when trying to carry out a k-fold cross-validation grid search using the KNN algorithm in scikit-learn. The error is likely due to incorrect usage or parameters passed to the KNeighborsClassifier class.,,,,,,,Defining distance parameter (V) in knn crossval grid search (seuclidean/mahalanobis distance metrics),"I am trying to carry out a k-fold cross-validation grid search using the KNN algorithm using python sklearn, with parameters in the search being number of neighbors K and distance metric. I am including mahalanobis and seuclidean as distance metrics, and understand these have a parameter which needs to be specified, namely V or VI (covariance matrix of features or inverse of this). 

Below is my code: 

X_train, X_test, y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=10,stratify=y)

knn=KNeighborsClassifier()

grid_param={'n_neighbors':np.arange(1,51),'metric':['euclidean','minkowski','mahalanobis','seuclidean'],'metric_params':[{'V': np.cov(X_train)}]} 

knn_gscv=GridSearchCV(knn,grid_param,cv=5)

knn_gscv.fit(X_train,y_train) (*)


The (*) line throws this error when executed: 

TypeError: __init__() got an unexpected keyword argument 'V'

I have also tried VI instead of V but getting same error. 

I have come across potential solutions below but these don't help. 

https://github.com/scikit-learn/scikit-learn/issues/6915

Scikit-learn: How do we define a distance metric's parameter for grid search

Any help appreciated!

This is also my first question, so any feedback would be helpful also with this regard. 
","grid_params = [
    {'n_neighbors': np.arange(1, 51), 'metric': ['euclidean', 'minkowski']},
    {'n_neighbors': np.arange(1, 51), 'metric': ['mahalanobis', 'seuclidean'],
     'metric_params': [{'V': np.cov(X_train)}]}
]


The issue is that euclidean and minkowski metrics do not accepts V parameter. So you need to separate them. 
",,,false,
https://stackoverflow.com/questions/38966075,false,The issue does not involve a problematic API. It is a question about how to restrict Anaconda from upgrading a module being installed if it is a higher-level dependency.,,,,,,,How to restrict anaconda from upgrading the module being installed if its a higher level dependency,"I'm trying to use continuum io anaconda packing system to package python-2.7.10 with other dependent modules for our environment. I want to automate the pack distribution to simply be a single installation of python with the modules we require.

The issue I'm having is when I specify the modules under the build parameter in meta.yaml it will upgrade the version of python being installed despite the fact that it is python-2.7.10. This will cause an error in the build process.

Is there a way to pin the version of python being installed so that if there is a dependency it will hard fail, or use an earlier version of the package?

meta.yaml, ive tried not pinning the version of the modules as well.

package:
  name: python
  version: 2.7.10

source:
  fn: Python-2.7.10.tgz
  url: https://www.python.org/ftp/python/2.7.10/Python-2.7.10.tgz
  md5: d7547558fd673bd9d38e2108c6b42521

build:
  no_link: bin/python

requirements:
  build:
    - bzip2 [unix]
    - zlib [unix]
    - sqlite [unix]
    - readline [unix]
    - tk [unix]
    - openssl [unix]
    - system [linux]
    - ipython 5.0.0
    - numpy 1.11.1
    - cython 0.24.1
    - scipy 0.18.0
    - pandas 0.18.1
    - patsy 0.4.1
    - statsmodels 0.6.1
    - matplotlib 1.5.2
    - ggplot 0.9.4
    - scikit-learn 0.17.1
    - distribute 0.6.45
    - backports.ssl-match-hostname 3.5.0.1
    - certifi 14.05.14
    - nose_parameterized 0.5.0
    - pyparsing 2.1.4
    - python-dateutil 2.5.3
    - pytz 2016.6.1
    - pyzmq 15.3.0
    - simplejson 3.3.3
    - six 1.10.0
    - sympy 1.0
    - tornado 4.4.1
    - virtualenv 13.0.1
    - wsgiref 0.1.2
    - python-swiftclient 2.7.0
    #- python-ceilometerclient #issue
    #- python-heatclient #issue
    #- python-keystoneclient 1.6.0
    #- python-novaclient 2.26.0
    #- python-troveclient #issue
    - python-cinderclient 1.1.2
    - python-glanceclient 0.17.2
    - python-neutronclient 2.4.0
    - networkx 1.11
    - pysal 1.11.1
    - pyyaml 3.11
    - shapely 1.5.13
    - beautifulsoup4 4.4.1
    - nltk 3.2.1
    - requests 2.10.0
    - seaborn 0.5.0
    - h5py 2.6.0
    - xlrd 1.0.0
    - markupsafe 0.23
    - crypto 1.1.0
    - jinja2 2.8
    - openpyxl 2.3.2
    - jaro_winkler 1.0.2
    - bokeh 0.12.1
    - numexpr 2.6.1
    - pytables 3.2.3.1
    - pycurl 7.43.0
    - mgrs 1.1.0
    - psutil 4.3.0
    - biopython 1.67
    - enaml 0.9.8
    - mdp 3.5
    - bitarray 0.8.1
    - clusterpy 0.9.9
    - pyside 1.2.1
    - pyqt 4.11.4
    - parsedatetime 1.4
    - pymysql 0.6.7
    - pyodbc 3.0.10
    - tabulate 0.7.2

  run:
    - zlib [unix]
    - sqlite [unix]
    - readline [unix]
    - tk [unix]
    - openssl [unix]
    - system [linux]

test:
  commands:
    - python -V [unix]
    - 2to3 -h [unix]
    - python-config --help [unix]

about:
  home: http://www.python.org/
  summary: general purpose programming language
  license: PSF


The output with the error:

$ conda build .
Removing old build environment
BUILD START: python-2.7.10-0
    (actual version deferred until further download or env creation)
Using Anaconda Cloud api site https://api.anaconda.org

The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    geos-3.5.0                 |                0        16.9 MB  defaults
    libgcc-4.8.5               |                1         922 KB  r
    pixman-0.32.6              |                0         2.4 MB  defaults
    unixodbc-2.3.4             |                0         688 KB  defaults
    yaml-0.1.6                 |                0         246 KB  defaults
    curl-7.49.0                |                1         543 KB  defaults
    glib-2.43.0                |                2         7.4 MB  r
    hdf5-1.8.17                |                1         1.9 MB  defaults
    atom-0.3.10                |           py27_0         676 KB  defaults
    backports_abc-0.4          |           py27_0           5 KB  defaults
    beautifulsoup4-4.4.1       |           py27_0         116 KB  defaults
    bitarray-0.8.1             |           py27_0          89 KB  defaults
    et_xmlfile-1.0.1           |           py27_0          15 KB  defaults
    future-0.15.2              |           py27_0         616 KB  defaults
    jaro_winkler-1.0.2         |           py27_0          24 KB  auto
    jdcal-1.2                  |           py27_1           9 KB  defaults
    kiwisolver-0.1.3           |           py27_0         571 KB  defaults
    markupsafe-0.23            |           py27_2          31 KB  defaults
    mgrs-1.1.0                 |           py27_0          48 KB  auto
    mpmath-0.19                |           py27_1         873 KB  defaults
    nltk-3.2.1                 |           py27_0         1.7 MB  defaults
    parsedatetime-1.2          |           py27_0          39 KB  auto
    ply-3.8                    |           py27_0          71 KB  defaults
    psutil-4.3.0               |           py27_0         224 KB  defaults
    pycurl-7.43.0              |           py27_0         128 KB  defaults
    pymysql-0.7.6              |           py27_0         116 KB  defaults
    pyodbc-3.0.10              |           py27_0         146 KB  defaults
    pyyaml-3.11                |           py27_4         297 KB  defaults
    pyzmq-15.4.0               |           py27_0         705 KB  defaults
    requests-2.10.0            |           py27_0         611 KB  defaults
    shapely-1.5.16             |           py27_0         494 KB  defaults
    tabulate-0.7.2             |           py27_0          18 KB  auto
    wsgiref-0.1.2              |           py27_0          943 B  auto
    xlrd-1.0.0                 |           py27_0         181 KB  defaults
    biopython-1.67             |      np111py27_0         2.2 MB  defaults
    clusterpy-0.9.9            |           py27_1         101 KB  conda-forge
    cmd2-0.6.7                 |           py27_0          33 KB  auto
    h5py-2.6.0                 |      np111py27_2         2.4 MB  defaults
    jinja2-2.8                 |           py27_1         264 KB  defaults
    jsonschema-2.5.1           |           py27_0          55 KB  defaults
    mdp-3.5                    |           py27_0         477 KB  defaults
    networkx-1.11              |           py27_0         1.1 MB  defaults
    numexpr-2.6.1              |      np111py27_0         347 KB  defaults
    openpyxl-2.3.2             |           py27_0         248 KB  defaults
    rsa-3.4.2                  |           py27_0          50 KB  conda-forge
    singledispatch-3.4.0.3     |           py27_1          17 KB  r
    ssl_match_hostname-3.4.0.2 |           py27_1           6 KB  defaults
    cliff-1.10.1               |           py27_0          36 KB  gus
    crypto-1.1.0               |           py27_0           3 KB  auto
    pysal-1.11.1               |           py27_0        11.2 MB  defaults
    pytables-3.2.3.1           |      np111py27_0         3.4 MB  defaults
    tornado-4.4.1              |           py27_0         552 KB  defaults
    bokeh-0.12.1               |           py27_0         3.2 MB  defaults
    harfbuzz-0.9.35            |                6         1.1 MB  r
    ipython-5.1.0              |           py27_0         936 KB  defaults
    pyopenssl-16.0.0           |           py27_0          66 KB  defaults
    pango-1.36.8               |                3         796 KB  r
    qt-4.8.7                   |                4        32.7 MB  defaults
    python-neutronclient-2.4.0 |           py27_0         222 KB  gus
    shiboken-1.2.1             |           py27_0         883 KB  defaults
    enaml-0.9.8                |           py27_1         944 KB  defaults
    pyside-1.2.1               |           py27_1         5.7 MB  defaults
    seaborn-0.7.1              |           py27_0         272 KB  defaults
    ------------------------------------------------------------
                                           Total:       107.8 MB

The following NEW packages will be INSTALLED:

    atom:                         0.3.10-py27_0       defaults
    babel:                        2.3.3-py27_0        defaults
    backports:                    1.0-py27_0          defaults
    backports.ssl-match-hostname: 3.5.0.1-py27_0      getpantheon
    backports_abc:                0.4-py27_0          defaults
    beautifulsoup4:               4.4.1-py27_0        defaults
    biopython:                    1.67-np111py27_0    defaults
    bitarray:                     0.8.1-py27_0        defaults
    bokeh:                        0.12.1-py27_0       defaults
    brewer2mpl:                   1.4.1-py27_1        conda-forge
    bzip2:                        1.0.6-3             defaults
    cairo:                        1.12.18-6           defaults
    certifi:                      2016.2.28-py27_0    defaults
    cffi:                         1.6.0-py27_0        defaults
    cliff:                        1.10.1-py27_0       gus
    clusterpy:                    0.9.9-py27_1        conda-forge
    cmd2:                         0.6.7-py27_0        auto
    crypto:                       1.1.0-py27_0        auto
    cryptography:                 1.4-py27_0          defaults
    curl:                         7.49.0-1            defaults
    cycler:                       0.10.0-py27_0       defaults
    cython:                       0.24.1-py27_0       defaults
    decorator:                    4.0.10-py27_0       defaults
    distribute:                   0.6.45-py27_1       defaults
    enaml:                        0.9.8-py27_1        defaults
    enum34:                       1.1.6-py27_0        defaults
    et_xmlfile:                   1.0.1-py27_0        defaults
    fontconfig:                   2.11.1-6            defaults
    freetype:                     2.5.5-1             defaults
    functools32:                  3.2.3.2-py27_0      defaults
    future:                       0.15.2-py27_0       defaults
    futures:                      3.0.5-py27_0        defaults
    geos:                         3.5.0-0             defaults
    get_terminal_size:            1.0.0-py27_0        defaults
    ggplot:                       0.11.1-py27_1       conda-forge
    glib:                         2.43.0-2            r
    h5py:                         2.6.0-np111py27_2   defaults
    harfbuzz:                     0.9.35-6            r
    hdf5:                         1.8.17-1            defaults
    idna:                         2.1-py27_0          defaults
    ipaddress:                    1.0.16-py27_0       defaults
    ipython:                      5.1.0-py27_0        defaults
    ipython_genutils:             0.1.0-py27_0        defaults
    iso8601:                      0.1.11-py27_0       defaults
    jaro_winkler:                 1.0.2-py27_0        auto
    jdcal:                        1.2-py27_1          defaults
    jinja2:                       2.8-py27_1          defaults
    jsonpatch:                    1.3-py27_0          auto
    jsonpointer:                  1.2-py27_0          auto
    jsonschema:                   2.5.1-py27_0        defaults
    kiwisolver:                   0.1.3-py27_0        defaults
    libffi:                       3.2.1-0             defaults
    libgcc:                       4.8.5-1             r
    libgfortran:                  3.0.0-1             defaults
    libpng:                       1.6.22-0            defaults
    libsodium:                    1.0.10-0            defaults
    libxml2:                      2.9.2-0             defaults
    markupsafe:                   0.23-py27_2         defaults
    matplotlib:                   1.5.1-np111py27_0   defaults
    mdp:                          3.5-py27_0          defaults
    mgrs:                         1.1.0-py27_0        auto
    mkl:                          11.3.3-0            defaults
    mpmath:                       0.19-py27_1         defaults
    msgpack-python:               0.4.7-py27_0        defaults
    netaddr:                      0.7.18-py27_0       conda-forge
    netifaces:                    0.10.4-py27_0       conda-forge
    networkx:                     1.11-py27_0         defaults
    nltk:                         3.2.1-py27_0        defaults
    nose_parameterized:           0.5.0-py27_0        conda-forge
    numexpr:                      2.6.1-np111py27_0   defaults
    numpy:                        1.11.1-py27_0       defaults
    openpyxl:                     2.3.2-py27_0        defaults
    openssl:                      1.0.2h-1            defaults
    oslo.config:                  1.9.3-py27_0        gus
    oslo.i18n:                    1.5.0-py27_0        gus
    oslo.serialization:           1.4.0-py27_0        gus
    oslo.utils:                   1.4.0-py27_0        gus
    pandas:                       0.18.1-np111py27_0  defaults
    pango:                        1.36.8-3            r
    parsedatetime:                1.2-py27_0          auto
    path.py:                      8.2.1-py27_0        defaults
    pathlib2:                     2.1.0-py27_0        defaults
    patsy:                        0.4.1-py27_0        defaults
    pbr:                          0.11.0-py27_0       defaults
    pexpect:                      4.0.1-py27_0        defaults
    pickleshare:                  0.7.3-py27_0        defaults
    pip:                          8.1.2-py27_0        defaults
    pixman:                       0.32.6-0            defaults
    ply:                          3.8-py27_0          defaults
    prettytable:                  0.7.2-py27_0        conda-forge
    prompt_toolkit:               1.0.3-py27_0        defaults
    psutil:                       4.3.0-py27_0        defaults
    ptyprocess:                   0.5.1-py27_0        defaults
    pyasn1:                       0.1.9-py27_0        defaults
    pycairo:                      1.10.0-py27_0       defaults
    pycparser:                    2.14-py27_1         defaults
    pycurl:                       7.43.0-py27_0       defaults
    pygments:                     2.1.3-py27_0        defaults
    pymysql:                      0.7.6-py27_0        defaults
    pyodbc:                       3.0.10-py27_0       defaults
    pyopenssl:                    16.0.0-py27_0       defaults
    pyparsing:                    2.1.4-py27_0        defaults
    pyqt:                         4.11.4-py27_4       defaults
    pysal:                        1.11.1-py27_0       defaults
    pyside:                       1.2.1-py27_1        defaults
    pytables:                     3.2.3.1-np111py27_0 defaults
    python:                       2.7.12-1            defaults
    python-cinderclient:          1.1.2-py27_0        gus
    python-dateutil:              2.5.3-py27_0        defaults
    python-glanceclient:          0.17.2-py27_0       gus
    python-keystoneclient:        1.3.2-py27_0        gus
    python-neutronclient:         2.4.0-py27_0        gus
    python-swiftclient:           2.7.0-py27_0        chenghlee
    pytz:                         2016.6.1-py27_0     defaults
    pyyaml:                       3.11-py27_4         defaults
    pyzmq:                        15.4.0-py27_0       defaults
    qt:                           4.8.7-4             defaults
    readline:                     6.2-2               defaults
    requests:                     2.10.0-py27_0       defaults
    rsa:                          3.4.2-py27_0        conda-forge
    scikit-learn:                 0.17.1-np111py27_2  defaults
    scipy:                        0.18.0-np111py27_0  defaults
    seaborn:                      0.7.1-py27_0        defaults
    setuptools:                   25.1.6-py27_0       defaults
    shapely:                      1.5.16-py27_0       defaults
    shiboken:                     1.2.1-py27_0        defaults
    simplegeneric:                0.8.1-py27_1        defaults
    simplejson:                   3.8.2-py27_0        defaults
    singledispatch:               3.4.0.3-py27_1      r
    sip:                          4.18-py27_0         defaults
    six:                          1.10.0-py27_0       defaults
    sqlite:                       3.13.0-0            defaults
    ssl_match_hostname:           3.4.0.2-py27_1      defaults
    statsmodels:                  0.6.1-np111py27_1   defaults
    stevedore:                    1.3.0-py27_0        gus
    sympy:                        1.0-py27_0          defaults
    system:                       5.8-2               defaults
    tabulate:                     0.7.2-py27_0        auto
    tk:                           8.5.18-0            defaults
    tornado:                      4.4.1-py27_0        defaults
    traitlets:                    4.2.2-py27_0        defaults
    unixodbc:                     2.3.4-0             defaults
    virtualenv:                   13.0.1-py27_0       defaults
    warlock:                      1.3.0-py27_0        conda-forge
    wcwidth:                      0.1.7-py27_0        defaults
    wheel:                        0.29.0-py27_0       defaults
    wsgiref:                      0.1.2-py27_0        auto
    xlrd:                         1.0.0-py27_0        defaults
    yaml:                         0.1.6-0             defaults
    zeromq:                       4.1.4-0             defaults
    zlib:                         1.2.8-3             defaults

Source cache directory is: /opt/app/anaconda2/conda-bld/src_cache
Found source in cache: Python-2.7.10.tgz
Extracting download
BUILD START: python-2.7.10-0
python is installed as a build dependency. Removing.
An unexpected error has occurred, please consider sending the
following traceback to the conda GitHub issue tracker at:

    https://github.com/conda/conda-build/issues

Include the output of the command 'conda info' in your report.


Traceback (most recent call last):
  File ""/opt/app/anaconda2/bin/conda-build"", line 5, in &lt;module&gt;
    sys.exit(main())
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 152, in main
    args_func(args, p)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 415, in args_func
    args.func(args, p)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 358, in execute
    debug=args.debug)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/build.py"", line 561, in build
    assert not plan.nothing_to_do(actions), actions
AssertionError: defaultdict(&lt;type 'list'&gt;, {'op_order': ('RM_FETCHED', 'FETCH', 'RM_EXTRACTED', 'EXTRACT', 'UNLINK', 'LINK', 'SYMLINK_CONDA'), 'PREFIX': '/opt/app/anaconda2/envs/_build_placehold_placehold_placehold_placehold_placehold'})

","Unless there's a specific reason you need to compile python yourself, I think what you're actually going after is conda bundle (http://conda.pydata.org/docs/commands/conda-bundle.html).  Unfortunately we've removed it in conda 4.2 which will be coming out soon, intending to move it to conda-build.  Since that hasn't happened yet, and if it ends up actually being useful to people, we can add it back.



You could also try this using conda-build...

Remove the whole source block in your meta.yaml file. Also remove all of the build requirements that are also not run requirements.  Then in your build.sh file

conda install --yes --quiet \
    python=2.7.10 \
    ipython=5.0.0 \
    numpy=1.11.1 \
    cython=0.24.1 \
    scipy=0.18.0 \
    pandas=0.18.1 \
    patsy=0.4.1 \
    statsmodels=0.6.1 \
    matplotlib=1.5.2 \
    ggplot=0.9.4 \
    scikit-learn=0.17.1 \
    distribute=0.6.45 \
    backports.ssl-match-hostname=3.5.0.1 \
    certifi=14.05.14 \
    nose_parameterized=0.5.0 \
    pyparsing=2.1.4 \
    python-dateutil=2.5.3 \
    pytz=2016.6.1 \
    pyzmq=15.3.0 \
    simplejson=3.3.3 \
    six=1.10.0 \
    sympy=1.0 \
    tornado=4.4.1 \
    virtualenv=13.0.1 \
    wsgiref=0.1.2 \
    python-swiftclient=2.7.0 \
    python-cinderclient=1.1.2 \
    python-glanceclient=0.17.2 \
    python-neutronclient=2.4.0 \
    networkx=1.11 \
    pysal=1.11.1 \
    pyyaml=3.11 \
    shapely=1.5.13 \
    beautifulsoup4=4.4.1 \
    nltk=3.2.1 \
    requests=2.10.0 \
    seaborn=0.5.0 \
    h5py=2.6.0 \
    xlrd=1.0.0 \
    markupsafe=0.23 \
    crypto=1.1.0 \
    jinja2=2.8 \
    openpyxl=2.3.2 \
    jaro_winkler=1.0.2 \
    bokeh=0.12.1 \
    numexpr=2.6.1 \
    pytables=3.2.3.1 \
    pycurl=7.43.0 \
    mgrs=1.1.0 \
    psutil=4.3.0 \
    biopython=1.67 \
    enaml=0.9.8 \
    mdp=3.5 \
    bitarray=0.8.1 \
    clusterpy=0.9.9 \
    pyside=1.2.1 \
    pyqt=4.11.4 \
    parsedatetime=1.4 \
    pymysql=0.6.7 \
    pyodbc=3.0.10 \
    tabulate=0.7.2


The big difference: by listing all of those packages as build requirements, you're actually ensuring that they won't be in your final conda package.  Think of build requirements more like a compiler, or something that's necessary when you're building the package, but not when you're actually running it.
",,,false,
https://stackoverflow.com/questions/76309535,false,The issue does not involve a problematic API. It is related to a confusion between two different packages (sklearn-quantile and quantile-forest) that provide similar classes with different ways of passing quantiles. The error is caused by using the wrong package's class and passing the quantiles incorrectly.,,,,,,,Quantile Forest error &quot;predict() got an unexpected keyword argument &#39;quantiles&#39;&quot;,"I continue to run into errors when run any form of quantile forest models with the prediction and quantile phases. I am following this example but with my own X and y. I have trained many a random forest and other derivations of tree models with this dataset, so I'm fairly certain it's not the input data issue.
https://github.com/zillow/quantile-forest
And have created an environment that supposedly follows all the installation requirements. I can provide my list of versions upon request. Time and time again, the RandomForestQuantileRegressor will work, but when I want to plot and see the quantiles, I get the error

**""TypeError: predict() got an unexpected keyword argument 'quantiles'""
**

Here is an example when I set up the environment:

Collecting quantile-forest
Downloading quantile_forest-1.1.2-cp310-cp310-macosx_10_9_x86_64.whl (188 kB)
 188.6/188.6 kB 2.7 MB/s eta 0:00:00
Requirement already satisfied: numpy&gt;=1.23 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from quantile-forest) (1.24.3)
Requirement already satisfied: scipy&gt;=1.4 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from quantile-forest) (1.10.1)
Requirement already satisfied: scikit-learn&gt;=1.0 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from quantile-forest) (1.2.2)
Requirement already satisfied: joblib&gt;=1.1.1 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from scikit-learn&gt;=1.0-&gt;quantile-forest) (1.2.0)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from scikit-learn&gt;=1.0-&gt;quantile-forest) (3.1.0)
Installing collected packages: quantile-forest
Successfully installed quantile-forest-1.1.2

This code works:
X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=0.5, random_state=0)

qrf = RandomForestQuantileRegressor(q=[0.05, 0.50, 0.95])
qrf.fit(X_train, y_train)

y_pred_5, y_pred_median, y_pred_95 = qrf.predict(X_test)
qrf.score(X_test, y_test)

But I cannot get any code that calls ""predict"" where I specify the quantiles to work without that type error.
","I notice that the code you've provided is an example from the sklearn-quantile package. Perhaps confusingly, both packages -- sklearn-quantile and quantile-forest -- provide a RandomForestQuantileRegressor class, but the packages have different ways of passing the quantiles to the class methods. As a result, the RandomForestQuantileRegressor classes from the two packages are not currently interchangeable.
In the code snippet you've provided, it's not clear what imports are being used, but it appears that you may be using the RandomForestQuantileRegressor class from the sklearn-quantile package. This class expects the quantiles to be passed to the initialization function instead of the predict function and would lead to the error you've presented. If this is correct, then you can fix this by importing the RandomForestQuantileRegressor class from the quantile-forest package and passing the quantiles to the predict function instead of the initialization function.
If the above is not helpful or you still are running into errors, you're welcome to create an issue in the quantile-forest repository here for additional troubleshooting.
",,,false,
https://stackoverflow.com/questions/40877501,false,"The issue does not involve a problematic API. It is related to an error in updating a package using conda, specifically with the 'ssl_verify' setting and the 'AttributeError' related to the 'Extensions' object. The issue is likely due to a mismatch or conflict between the pyopenssl and cryptography packages.",,,,,,,Error using conda to update a package,"I launched on AWS EC2 an instance using ubuntu as AMI.

Then I ran a script to install the jupyter notebook and be able to access to it with the IP address and the specific port.
The code I wrote was this one :

sudo apt-get install git

git clone https://gist.github.com/rashmibanthia/5a1e4d7e313d6832f2ff nb

. nb/jupyter_notebook_ec2.sh 

cd;mkdir notebook;cd notebook

tmux new -s nb

jupyter notebook --certfile=~/certs/mycert.pem --keyfile ~/certs/mycert.key


Now I am in the notebook. Here comes my problem. When I try to import some packages and class, I get an error. For example with this line:

from sklearn.gaussian_process import GaussianProcessRegressor


I get this:


  ImportError                               Traceback (most recent call
  last)  in ()
  ----&gt; 1 from sklearn.gaussian_process import GaussianProcessRegressor
  
  ImportError: cannot import name 'GaussianProcessRegressor'


So I tried to update the scikit learn package with conda with conda update scikit-learn but I have another error:


  Fetching package metadata ...An unexpected error has occurred. Please
  consider posting the following information to the conda GitHub issue
  tracker at:

https://github.com/conda/conda/issues



Current conda install:

           platform : linux-64
      conda version : 4.2.13
   conda is private : False
  conda-env version : 4.2.13
conda-build version : 1.18.2
     python version : 3.5.2.final.0
   requests version : 2.12.1
   root environment : /home/ubuntu/anaconda3  (writable)
default environment : /home/ubuntu/anaconda3
   envs directories : /home/ubuntu/anaconda3/envs
      package cache : /home/ubuntu/anaconda3/pkgs
       channel URLs : https://repo.continuum.io/pkgs/free/linux-64
                      https://repo.continuum.io/pkgs/free/noarch
                      https://repo.continuum.io/pkgs/pro/linux-64
                      https://repo.continuum.io/pkgs/pro/noarch
        config file : None
       offline mode : False




$ /home/ubuntu/anaconda3/bin/conda update scikit-learn

Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/exceptions.py"", line 479, in conda_exception_handler
    return_value = func(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/cli/main.py"", line 145, in _main
    exit_code = args.func(args, p)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/cli/main_update.py"", line 65, in execute
    install(args, parser, 'update')
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/cli/install.py"", line 238, in install
    prefix=prefix)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/api.py"", line 24, in get_index
    index = fetch_index(channel_urls, use_cache=use_cache, unknown=unknown)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 300, in fetch_index
    repodatas = [(u, f.result()) for u, f in zip(urls, futures)]
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 300, in &lt;listcomp&gt;
    repodatas = [(u, f.result()) for u, f in zip(urls, futures)]
  File ""/home/ubuntu/anaconda3/lib/python3.5/concurrent/futures/_base.py"", line 405, in result
    return self.__get_result()
  File ""/home/ubuntu/anaconda3/lib/python3.5/concurrent/futures/_base.py"", line 357, in __get_result
    raise self._exception
  File ""/home/ubuntu/anaconda3/lib/python3.5/concurrent/futures/thread.py"", line 55, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 75, in func
    res = f(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 117, in fetch_repodata
    timeout=(6.1, 60))
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 501, in get
    return self.request('GET', url, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 488, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 609, in send
    r = adapter.send(request, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/adapters.py"", line 423, in send
    timeout=timeout
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 594, in urlopen
    chunked=chunked)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 350, in _make_request
    self._validate_conn(conn)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 835, in _validate_conn
    conn.connect()
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connection.py"", line 330, in connect
    cert = self.sock.getpeercert()
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 324, in getpeercert
    'subjectAltName': get_subj_alt_name(x509)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 171, in get_subj_alt_name
    ext = cert.extensions.get_extension_for_class(
AttributeError: 'Extensions' object has no attribute 'get_extension_for_class'

","As kalefranz comments here 'https://github.com/conda/conda/issues/3898' it might be because pyopenssl and cryptography are out of sync and this should fix it:

CONDA_SSL_VERIFY=false conda update pyopenssl

In windows to set ssl_verify the following command can be used:

conda config --set ssl_verify False

As it is said in Problems with updating anaconda and installing new packages
",,,false,
https://stackoverflow.com/questions/44729373,false,The issue does not involve a problematic API. It is related to the incorrect interpretation of the results of GaussianProcess regression. The slope discrepancy is likely due to the data not being centered or the initial values for the hyperparameters in the kernel function not being optimal.,,,,,,,GaussianProcess regression results correct...up to a scale factor?,"I am running GaussianProcess regressions over some very noisy data. When I scatter plot predictions (which are, I know, predictions of means) vs actuals, I get a beautiful only slightly noisy y=x line.

Only one problem: the slope is completely wrong. Is there any way I can address this without building a second-stage linear regressor?

I regret I cannot share my data, but my model is fairly basic. X is a matrix with 10 columns, y a matrix with 1 column. I am using 1,000 examples to train and plot.

added: The below plot is plotting predicted versus actual. Given that I am using a nonlinear kernel, I find it strange that the GP regressor can find a relationship which is accurate up to a multiplier (slope).



kernel = (
    GP.kernels.RationalQuadratic(
        length_scale=.8,
        length_scale_bounds=(1e-3,1e3),
        alpha=.8,
        alpha_bounds=(1e-3,1e3),
        )
    + GP.kernels.WhiteKernel()
    )

gp = Pipeline( [
    ('scale',preproc.StandardScaler()),
    ('gp',GP.GaussianProcessRegressor(kernel=kernel)),
    ] )
gp.fit( X, y )


added: I'm a bit embarrassed, but I'm new to the GP world in particular and, really, regression as a ML problem in general. I had not plotted the model's performance over a test set, which revealed a strong overfit. Additionally, I've added an idiom to my code to deal with scikit-learn's default GP behavior, i.e., optimization makes me sad when I give it significant quantities of data, by ""pretraining"" on a small quantity of data, using the optimizer to find reasonable values for the kernel parameters, then ""training"" a much larger quantity of data. This allowed me to widen the parameter search and use multiple restarts on the optimizer, finding a much more generalizable model...which was almost all noise. Which was what I was expecting, really.

kernel = (
    GP.kernels.RationalQuadratic(
        length_scale=1,
        alpha=.5,
        )
    + GP.kernels.WhiteKernel(
        noise_level=1,
        )
    )*GP.kernels.ConstantKernel()

gp = Pipeline( [
    ('scale',preproc.StandardScaler()),
    ('gp',GP.GaussianProcessRegressor(
        kernel=kernel,
        n_restarts_optimizer=3,
        alpha=0,
        )),
    ] )
print(""pretraining model for target %s..."" % c)
x_pre = X_s.values[:500,:]
y_pre = y_s_scl[:500,:]
gp.fit( x_pre, y_pre )

gp = Pipeline( [
    ('scale',preproc.StandardScaler()),
    ('gp',GP.GaussianProcessRegressor(
        kernel=kernel,
        optimizer=None,
        alpha=0,
        )),
    ] )
print(""training model for target %s..."" % c)

","EDIT: Have you tried centering your data before doing the regression? (subtracting the mean of all the output values from each output). I know the Gp Toolbox in Matlab doesn't need the data to be centered, but I am not sure about the GP in sklearn. See:
https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia

OLD COMMENT:
Your initial values for the hyperparameters in the kernel function (i.e. length-scale and alpha) are very important. During the fit(), the hyperparameters are optimized and local maximum of hyperpareters can be found, which could in turn affect your result. Depending on the bounds you set for these hyperparameters, many local maximum can be found depending on the initial conditions. 
On the sklearn site it says:
""As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying n_restarts_optimizer.""
You may try using the RBF function as it is a very traditional kernel function for the GP. 
",,,false,
https://stackoverflow.com/questions/44527713,false,"The issue does not involve a problematic API. It is related to a module import error with scikit-learn's MinMaxScaler. The error is caused by the presence of a module named 'multiprocessing' in the same directory as the script, which is conflicting with the built-in 'multiprocessing' module.",,,,,,,Python script works from one folder but not on subfolders.,"Good evening everyone, 

I am trying to do some machine learning with python so I imported the module scikit-learn:

from sklearn.preprocessing import MinMaxScaler


And it gives me this strange error: 

Traceback (most recent call last):

  File ""&lt;ipython-input-22-b55a4eaccb0b&gt;"", line 1, in &lt;module&gt;
    from sklearn.preprocessing import MinMaxScaler

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\preprocessing\__init__.py"", line 6, in &lt;module&gt;
    from ._function_transformer import FunctionTransformer

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\preprocessing\_function_transformer.py"", line 2, in &lt;module&gt;
    from ..utils import check_array

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\utils\__init__.py"", line 18, in &lt;module&gt;
    from ..externals.joblib import cpu_count

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\externals\joblib\__init__.py"", line 128, in &lt;module&gt;
    from .parallel import Parallel

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 24, in &lt;module&gt;
    from ._multiprocessing_helpers import mp

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\externals\joblib\_multiprocessing_helpers.py"", line 24, in &lt;module&gt;
    _sem = mp.Semaphore()

AttributeError: module 'multiprocessing' has no attribute 'Semaphore'


The errors are copied from Anaconda (Spider) but they are present even by starting a python session from the command line. The folder where I run the script is: 

C:\Users\Francesco\Desktop\script_python


The very strange thing is that if I run the same script from the Desktop, just a folder up, it works!

C:\Users\Francesco\Desktop\ &lt;-- Here it works!!


I found out that the problem is relative to the multiprocessing module, here a snapshot of this strange behavior: 


","You probably have a module multiprocessing in the directory where you are running the script from.

Generally, in Python, the path where you run your script has precedence over the Python env, so it will try to pick that one first. This means that when sklearn imports multiprocessing, it uses your module, and not the built-in multiprocessing.

When this type of error appears, one way to identify the error is to run

python -c ""import multiprocessing; print(multiprocessing.__dir__)""


to double check which module is being used.
",,,false,
https://stackoverflow.com/questions/48263740,true,The issue involves the behavior of the PCA algorithm in scikit-learn under specific runtime conditions.,scikit-learn,sklearn.decomposition.PCA,The issue arises when applying the Principle Component Analysis (PCA) algorithm to different datasets. There is an increase in memory usage and a faster runtime for certain datasets.,"The expected behavior of PCA is to perform dimensionality reduction on the input dataset, but the issue indicates that the memory usage and runtime vary for different datasets.",The issue is triggered when running the PCA algorithm on specific datasets with varying characteristics.,This issue might be challenging to detect without detailed analysis and comparison of memory usage and runtime for different datasets.,strange memory consumption behaviour when running PCA in python,"At the moment I am trying to find the reason for a specific behavior in Python. 

First I want to describe my use case. The idea is a performance analysis on Python. Therefore I want to analyze the memory usage and runtime regarding the ""Principle Component Analysis""-algorithm. For that, I use scikit-learn (http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). 

To see, how the algorithm behaves for different datasets, I generated several randomly uniform distributed datasets. The strange thing is, there is an increase in memory usage and a faster runtime regarding two datasets.

Now I want to explain, how I measure the memory-usage and runtime shortly. 
Memory measurement (package: https://pypi.python.org/pypi/memory_profiler)


disable the garbage collection
Use the @profile decoration to find the increments of the function
Write the output into a log file
Use another python program to find the increments of the log files which belong to a specific dataset
sum up the increments - the result is the total memory allocation for the complete function
(I tested it with enabled garbage collection, too)


Time measurement (function: https://docs.python.org/2/library/timeit.html)


Use ""timeit.repeat()"" to get a list of many runtimes of the function
Write the list of those measurements to a CSV


After I had performed about 100 memory measurements and even more time measurements, I recognized the aforementioned behavior.
Hopefully, you can help me, find a reason for this behavior.

Here is the function, I measured:

@profile 
def pcaTrain(dataset):
 model = sklearn_pca.fit(dataset)
 model.variance = np.var(model.transform(dataset), axis=0, ddof=1)
 return model 


And here are some diagrams of the memory usage and the time measurements:

Memory measurements for dataset 4-17

Runtime measurements for dataset 4-17

Set up:


Python-version: 3.5.2
Memory: &gt; 100 GB


List of datasets:


dset04 -&gt; 1.000.000 rows per column (10 features x 1.000.000)
dset05 -&gt; 1.000.000 rows per column (12 features x 1.000.000)
dset06 -&gt; 1.000.000 rows per column (14 features x 1.000.000)
dset07 -&gt; 1.000.000 rows per column (16 features x 1.000.000)
dset08 -&gt; 1.000.000 rows per column (18 features x 1.000.000)
dset09 -&gt; 1.000.000 rows per column (20 features x 1.000.000)
dset10 -&gt; 1.000.000 rows per column (22 features x 1.000.000)
dset11 -&gt; 1.000.000 rows per column (24 features x 1.000.000)
dset12 -&gt; 1.000.000 rows per column (26 features x 1.000.000)
dset13 -&gt; 1.000.000 rows per column (28 features x 1.000.000)
dset14 -&gt; 1.000.000 rows per column (30 features x 1.000.000)
dset15 -&gt; 1.000.000 rows per column (35 features x 1.000.000)
dset16 -&gt; 1.000.000 rows per column (40 features x 1.000.000)
dset17 -&gt; 1.000.000 rows per column (45 features x 1.000.000)

","The important question is, how many of the features you are extractiong.
Since version 0.18 of Scikit-learn, the svd_solver flag of the PCA algorithm determines which algorithm to use.  The default behavior is to select the ""best"" choice, which is described in detail in the official documentation which you mentioned. 
Possibly, one of those selections hits your performance, depending on the number of components. Otherwise, I would suggest you mention this behavior in the official GitHub of scikit-learn, since this could be interesting to them as well.
",,,false,
https://stackoverflow.com/questions/53630329,true,The issue involves the behavior of the watershed algorithm in OpenCV under specific runtime conditions.,OpenCV,cv2.watershed,"The issue occurs after applying the watershed algorithm to an image. It results in the detection and coloring of the whole edge of the image, in addition to the desired objects.","The expected behavior of the watershed algorithm is to segment and label objects in an image, but the issue leads to the inclusion of the image edge as well.",The issue is triggered when applying the watershed algorithm to an image with specific characteristics or when certain parameters are not properly set.,"This issue might be challenging to detect without a thorough understanding of the watershed algorithm and its parameters, as well as careful examination of the resulting image.",Watershed analysis of coins - wrong output,"For the counting of round objects in an image I want to use the watershed algorithm.
In order to learn how it works and how I can use it for my needs, I have searched some working examples in python (https://docs.opencv.org/3.1.0/d3/db4/tutorial_py_watershed.html ; http://scikit-image.org/docs/dev/auto_examples/segmentation/plot_label.html)

I finaly found a working solution, which works more or less out-of-the-box for my own purposes (How to define the markers for Watershed in OpenCV?)

With this code I get nice results, both with the example file as with my own images.
I do get a strange behavior though after the watershed analysis. For some reason, the watershed step also adds a border around the image. So next to the objects that are detected, also the whole edge of the image gets detected and colored.

My guess is that I should change a parameters in the code to stop this from happening, but so far I'm unable to find what I should do.

this is the code:

import cv2
import numpy as np
from scipy.ndimage import label
def segment_on_dt(a, img):
    border = cv2.dilate(img, None, iterations=3)
    border = border - cv2.erode(border, None)
    dt = cv2.distanceTransform(img, cv2.DIST_L2, 3)
    dt = ((dt - dt.min()) / (dt.max() - dt.min()) * 255).astype(np.uint8)
    _, dt = cv2.threshold(dt, 200, 255, cv2.THRESH_BINARY)
    lbl, ncc = label(dt)
    # Completing the markers now. 
    lbl[border == 255] = 255 
    lbl = lbl.astype(np.int32)
    cv2.watershed(a, lbl)
    lbl[lbl == -1] = 0
    lbl = lbl.astype(np.uint8)
    return 255 - lbl

# Load image file
img = cv2.imread('coins.jpg')
# Pre-processing.
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img_gray = cv2.GaussianBlur(img_gray,(5,5),0)  
width, height = img_gray.shape
_, img_bin = cv2.threshold(img_gray, 0,  255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
img_bin = cv2.morphologyEx(img_bin, cv2.MORPH_OPEN,np.ones((5, 5),     dtype=int))
result = segment_on_dt(img, img_bin)
result[result != 255] = 0
result = cv2.dilate(result, None)
img[result == 255] = (0, 0, 255)
cv2.imwrite('Img_output.png',img)


Running this code will give this result (at least on my pc)



The result for detecting the coins is good enough for my purposes, but I'm a bit puzzled about the image edge that is also detected. From what I see during debugging, the watershed adds this edge, but it is unclear to me why this happens.
","You can fix this by adding a background label using the tutorial provided by openCV. 
https://docs.opencv.org/3.1.0/d3/db4/tutorial_py_watershed.html

They added an extra step to insert sure background and sure foreground region to help the watershed algorithm to properly segment the coin regions.

*********** edit**************

After reading your code again. I found that your original code has no problem. 
Background label was set using the variable border.

You will probably get the same result by executing the code found in OpenCV tutorial. The problem is in the way your draw the results. Since this is a display problem, there are many ways we can tackle the problem. One of the many is to use the information of the sure-background

Here are the modification to the function segment_on_dt

def segment_on_dt(a, img):
    sure_background = cv2.dilate(img, None, iterations=3)
    border = sure_background - cv2.erode(sure_background, None)


    dt = cv2.distanceTransform(img, cv2.DIST_L2, 3)
    dt = ((dt - dt.min()) / (dt.max() - dt.min()) * 255).astype(np.uint8)
    _, dt = cv2.threshold(dt, 200, 255, cv2.THRESH_BINARY)
    lbl, ncc = label(dt)


    # Completing the markers now. 
    lbl[border == 255] = 255 


    lbl = lbl.astype(np.int32)
    cv2.watershed(a, lbl)
    lbl[lbl == -1] = 0
    # Only draw red line if its not in sure background
    lbl[sure_background == 0] = 255

    lbl = lbl.astype(np.uint8)
    cv2.imshow('lbl_2',lbl)

    return 255 - lbl


I have added a new condition for the red lines to be drawn. The line are only drawn if its not in sure background region.

Your final result should look like this.


",,,false,
https://stackoverflow.com/questions/28726548,true,The issue involves the clustering behavior of the k-means algorithm in scikit-learn under specific runtime conditions.,scikit-learn,sklearn.cluster.KMeans,The issue is related to the non-uniform behavior of the k-means clustering algorithm when applied to a sparse TFIDF matrix of text documents.,"The expected behavior of k-means clustering is to partition the documents into uniform clusters, but the issue results in clusters with highly non-uniform distribution of documents.",The issue is triggered when running the k-means algorithm on a sparse TFIDF matrix with specific characteristics or when certain parameters are not properly set.,"This issue might be challenging to detect without a thorough analysis of the input data, the k-means algorithm, and the resulting clusters.",Text Documents Clustering - Non Uniform Clusters,"I have been trying to cluster a set of text documents. I have a sparse TFIDF matrix with around 10k documents (subset of a large dataset), and I try to run the scikit-learn k-means algorithm with different sizes of clusters (10,50,100). Rest all the parameters are default values.

I get a very strange behavior that no matter how many clusters I specify or even if I change the number of iterations, there would be 1 cluster in the lot which would contain most of the documents in itself and there will be many clusters which would have just 1 document in them. This is highly non-uniform behavior 

Does anyone know what kind of problem am I running into? 
","Here are the possible things that might be going ""wrong"":


Your k-means cluster initialization points are chosen as the same set of points in each run. I recommend using the 'random' for the init parameter of k-means http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. If that doesn't work then supply to k-means your own set of random initial cluster centers. Remember to initialize your random generator using its seed() method as the current date and time. https://docs.python.org/2/library/random.html uses current date-time as the default value. 
Your distance function, i.e. euclidean distance might be the culprit. This is less likely but it is always good to run k-means using cosine similarity especially when you are using it for document similarity. scikits doesn't have this functionality at present but you should look here: Is it possible to specify your own distance function using scikit-learn K-Means Clustering?


These two combined should give you good clusters.
","I noticed with the help of above answers and comments that there was a problem with outliers and noise in original space. For this, we should use a dimensionality reduction method which eliminates the unwanted noise in the data. I tried random projections first but it failed to work with text data, simply because the problem was still not solved.
Then using Truncated Singular Value Decomposition, I was able to get perfect uniform clusters. Hence, the Truncated SVD is the way to go with textual data in my opinion.
",,false,
https://stackoverflow.com/questions/62777025,true,The issue involves the behavior of the GridSearchCV class in scikit-learn when used with a custom regression model.,scikit-learn,sklearn.model_selection.GridSearchCV,"The issue is related to inconsistencies in the application of hyperparameters by GridSearchCV for a custom regression model. One hyperparameter is correctly applied, but another one is not.","The expected behavior of GridSearchCV is to explore the hyperparameter space and apply the specified hyperparameters to the custom regression model, but the issue indicates that one hyperparameter is not properly initialized.",The issue is triggered when using GridSearchCV with a custom regression model and specific hyperparameters or when there are inconsistencies in the initialization of hyperparameters.,This issue might be challenging to detect without a deep understanding of the GridSearchCV implementation and the interactions between the hyperparameters and the custom regression model.,GridSearchCV fails for own model class,"I'm trying to use a regression model I have implemented in combination with the GridSearchCV class of scikit-learn to optimize the hyper-parameters of my model. My modelclass is nicely build following the suggestions of the scikit-api:
class FOO(BaseEstimator, RegressorMixin):
def __init__(self,...)
    *** initialisation of all the parameters and hyperparameters (including the kernelfunction)***

def fit(self,X,y)
    *** implementation of fit: just takes input and performs fit of parameters.

def predict(self,X)
    *** implementation of predict: just takes input and calculates the result

The regression-class works as it should, but strangely enough, when I study the behavior of the hyperparameters, I tend to get inconsistencies. It appears one hyper-parameter is correctly applied by GridSearchCV, but the other one is clearly not.
So I am wondering, can someone explain to me how gridsearchCV is working (from the technical perspective)? How does it initialise the estimator, how does it run it over the grid?
My current assumption of the workings and required use of GridsearchCV is this:

Create a GridSearchCV instance  (CVmodel=GridSearchCV(MyRegressor,param_grid=Myparamgrid,...)
Fit the hyperparameter(s) via: CVmodel.fit(X,y). Which naively would work like this:

&gt; Loop over Parameter-values
&gt;          - create esimator instance with parameter value(and defaults for the other params)
&gt;          - estimator.fit
&gt;          - result[parameter-value]=estimator.predict

However, experience shows me this naive idea is quite wrong, as the hyper-parameter associated with the kernel-function of my regressor is not correctly initialized.
Can anyone provide some insight into what GridSearchCV is truly doing?
","After quite some digging I discovered, scikit-learn does not create new instances (as would be expected in OOP) but rather updates the properties of the object via the set_params method. In my case, this worked fine for the hyperparameter which is directly defined by the same keyword in the __ init __ method, however, it breaks down when the hyperparameter is a property of the static method set during the __ init __ method. Overriding the set_params method (which many tutorials advise against) to deal with this fixes the problem.
For those interested in more details, I wrote this all up in a tutorial myself.
",,,false,
https://stackoverflow.com/questions/63692895,false,The behavior observed with RandomForestClassifier in a pipeline is likely due to other factors and not directly related to the API itself.,,,,,,,Is there a way to build a logistic regression model even if there is only one class?,"Is there a way to build a scikit-learn logistic regression model for only 1 class? Obviously this model would predict the same class every time, regardless of the input data. My models are currently using liblinear as the solver, I'm not sure if there's another solver that would allow for this?
I realize this is a very strange question for ML but I am building many hierarchical models and in my situation it is easier to have a model for every case even if it predicts the same class every time.
Background: I have a hierarchical prediction task where I'm trying to predict three parts of a 9 digit code (e.g. for a code = 001010424, part 1 = 001, part 2 = 01, part 3= 0424). To do this I'm building hierarchical models. Using the input data we first predict part 1, then using the highest confidence decision for part 1 we use the input data again in a model for part 2 that is specific to the part 1 code. So for example, if I run the part 1 model and get a prediction that part 1 = 001 I then go to the part 2 model for 001 which then (is trained on and) predicts part 2 given part 1 = 001. This hierarchical behavior is repeated for part 3.
","Scitkit learn needs samples of at least two classes.
  import numpy as np
  from sklearn.linear_model import LogisticRegression

  x = np.random.rand(5,2)
  y = np.ones(5).astype(int)
  model = LogisticRegression().fit(x, y)

This yields the error:
  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1

You are probably better off having your training algorithm check if there is only one y label, and if there is, just have your code memorize that.  It seems that such an implementation would be straightforward to implement and much easier to understand for anyone looking at the code later.
",,,false,
https://stackoverflow.com/questions/68155575,false,The behavior of the logistic regression model in scikit-learn when there is only one class is expected and not considered as involving a problematic API.,,,,,,,RandomForestClassifier takes less time to train when in Pipeline?,"I am experiencing strange behavior in Scikit-learn.
When I train a RandomForestClassifier standalone (not in a pipeline), the model takes 280 seconds to train, while the training process only takes 18 seconds in a pipeline.
Am I doing something wrong ?

","In 1st block, you have 6 lines of code. You are importing libraries, fitting your model, getting scores, and doing everything for the first time. Whereas in 2nd block you are importing the libraries like Scikit-learn which is already imported, so it won't take time. And, in 3rd block there is just one line of code. Another reason is that, In 1st block, you are getting some kind of warning which might be affecting the speed of running the code.
",,,false,
https://stackoverflow.com/questions/22054964,false,The issue does not involve a problematic API. It is a question about the difference in results between scikit-learn's LinearRegression and statsmodels' OLS implementations.,,,,,,,OLS Regression: Scikit vs. Statsmodels?,"Short version: I was using the scikit LinearRegression on some data, but I'm used to p-values so put the data into the statsmodels OLS, and although the R^2 is about the same the variable coefficients are all different by large amounts. This concerns me since the most likely problem is that I've made an error somewhere and now I don't feel confident in either output (since likely I have made one model incorrectly but don't know which one).

Longer version: Because I don't know where the issue is, I don't know exactly which details to include, and including everything is probably too much. I am also not sure about including code or data. 

I am under the impression that scikit's LR and statsmodels OLS should both be doing OLS, and as far as I know OLS is OLS so the results should be the same.

For scikit's LR, the results are (statistically) the same whether or not I set normalize=True or =False, which I find somewhat strange.

For statsmodels OLS, I normalize the data using StandardScaler from sklearn. I add a column of ones so it includes an intercept (since scikit's output includes an intercept). More on that here: http://statsmodels.sourceforge.net/devel/examples/generated/example_ols.html (Adding this column did not change the variable coefficients to any notable degree and the intercept was very close to zero.) StandardScaler didn't like that my ints weren't floats, so I tried this: https://github.com/scikit-learn/scikit-learn/issues/1709
That makes the warning go away but the results are exactly the same.

Granted I'm using 5-folds cv for the sklearn approach (R^2 are consistent for both test and training data each time), and for statsmodels I just throw it all the data.

R^2 is about 0.41 for both sklearn and statsmodels (this is good for social science). This could be a good sign or just a coincidence.

The data is observations of avatars in WoW (from http://mmnet.iis.sinica.edu.tw/dl/wowah/) which I munged about to make it weekly with some different features. Originally this was a class project for a data science class.

Independent variables include number of observations in a week (int), character level (int), if in a guild (Boolean), when seen (Booleans on weekday day, weekday eve, weekday late, and the same three for weekend), a dummy for character class (at the time for the data collection, there were only 8 classes in WoW, so there are 7 dummy vars and the original string categorical variable is dropped), and others.

The dependent variable is how many levels each character gained during that week (int).

Interestingly, some of the relative order within like variables is maintained across statsmodels and sklearn. So, rank order of ""when seen"" is the same although the loadings are very different, and rank order for the character class dummies is the same although again the loadings are very different.

I think this question is similar to this one: Difference in Python statsmodels OLS and R's lm

I am good enough at Python and stats to make a go of it, but then not good enough to figure something like this out. I tried reading the sklearn docs and the statsmodels docs, but if the answer was there staring me in the face I did not understand it. 

I would love to know:


Which output might be accurate? (Granted they might both be if I missed a kwarg.)
If I made a mistake, what is it and how to fix it?
Could I have figured this out without asking here, and if so how?


I know this question has some rather vague bits (no code, no data, no output), but I am thinking it is more about the general processes of the two packages. Sure, one seems to be more stats and one seems to be more machine learning, but they're both OLS so I don't understand why the outputs aren't the same.

(I even tried some other OLS calls to triangulate, one gave a much lower R^2, one looped for five minutes and I killed it, and one crashed.)

Thanks!
","It sounds like you are not feeding the same matrix of regressors X to both procedures (but see below). Here's an example to show you which options you need to use for sklearn and statsmodels to produce identical results.

import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression

# Generate artificial data (2 regressors + constant)
nobs = 100 
X = np.random.random((nobs, 2)) 
X = sm.add_constant(X)
beta = [1, .1, .5] 
e = np.random.random(nobs)
y = np.dot(X, beta) + e 

# Fit regression model
sm.OLS(y, X).fit().params
&gt;&gt; array([ 1.4507724 ,  0.08612654,  0.60129898])

LinearRegression(fit_intercept=False).fit(X, y).coef_
&gt;&gt; array([ 1.4507724 ,  0.08612654,  0.60129898])


As a commenter suggested, even if you are giving both programs the same X, X may not have full column rank, and they sm/sk could be taking (different) actions under-the-hood to make the OLS computation go through (i.e. dropping different columns).

I recommend you use pandas and patsy to take care of this:

import pandas as pd
from patsy import dmatrices

dat = pd.read_csv('wow.csv')
y, X = dmatrices('levels ~ week + character + guild', data=dat)


Or, alternatively, the statsmodels formula interface:

import statsmodels.formula.api as smf
dat = pd.read_csv('wow.csv')
mod = smf.ols('levels ~ week + character + guild', data=dat).fit()


Edit: This example might be useful: http://statsmodels.sourceforge.net/devel/example_formulas.html
","If you use statsmodels, I would highly recommend using the statsmodels formula interface instead.  You will get the same old result from OLS using the statsmodels formula interface as you would from sklearn.linear_model.LinearRegression, or R, or SAS, or Excel.  

smod = smf.ols(formula ='y~ x', data=df)
result = smod.fit()
print(result.summary())


When in doubt, please 


try reading the source code
try a different language for benchmark, or 
try OLS from scratch, which is basic linear algebra. 

","i just wanted to add here, that in terms of sklearn, it does not use OLS method for linear regression under the hood. Since sklearn comes from the data-mining/machine-learning realm, they like to use Steepest Descent Gradient algorithm. This is a numerical method that is sensitive to initial conditions etc, while the OLS is an analytical closed form approach, so one should expect differences. So statsmodels comes from classical statistics field hence they would use OLS technique. So there are differences between the two linear regressions from the 2 different libraries
",false,
https://stackoverflow.com/questions/10407266,false,The issue does not involve a problematic API. It is a question about the difference in performance between Naive Bayes classifiers implemented in NLTK and scikit-learn.,,,,,,,scikits learn and nltk: Naive Bayes classifier performance highly different,"I am comparing two Naive Bayes classifiers: one from NLTK and and one from scikit-learn. I'm dealing with a multi-class classification problem (3 classes: positive (1), negative (-1), and neutral (0)). 

Without performing any feature selection (that is, using all features available), and using a training dataset of 70,000 instances (noisy-labeled, with an instance distribution of  17% positive, 4%  negative and 78% neutral), I train two classifiers, the first one is a nltk.NaiveBayesClassifier, and the second one is a sklearn.naive_bayes.MultinomialNB (with fit_prior=True).

After training, I evaluated the classifiers on my test set of 30,000 instances and I get the following results:

**NLTK's NaiveBayes**
accuracy: 0.568740
class: 1
     precision: 0.331229
     recall: 0.331565
     F-Measure: 0.331355
class: -1
     precision: 0.079253 
     recall: 0.446331 
     F-Measure: 0.134596 
class: 0
     precision: 0.849842 
     recall: 0.628126 
     F-Measure: 0.722347 


**Scikit's MultinomialNB (with fit_prior=True)**
accuracy: 0.834670
class: 1
     precision: 0.400247
     recall: 0.125359
     F-Measure: 0.190917
class: -1
     precision: 0.330836
     recall: 0.012441
     F-Measure: 0.023939
class: 0
     precision: 0.852997
     recall: 0.973406
     F-Measure: 0.909191

**Scikit's MultinomialNB (with fit_prior=False)**
accuracy: 0.834680
class: 1
     precision: 0.400380
     recall: 0.125361
     F-Measure: 0.190934
class: -1
     precision: 0.330836
     recall: 0.012441
     F-Measure: 0.023939
class: 0
     precision: 0.852998
     recall: 0.973418
     F-Measure: 0.909197


I have noticed that while Scikit's classifier has better overall accuracy and precision, its recall is very low compared to the NLTK one, at least with my data. Taking into account that they might be (almost) the same classifiers, isn't this strange?
","Naive Bayes classifier usually means a Bayesian classfier over binary features that are assumed to be independent. This is what NLTK's Naive Bayes classifier implements. The corresponding scikit classifier is BernoulliNB classifier.

The restriction to boolean valued features is not actually necessary,
it is just the simplest to implement.
A naive Bayes classifier can be defined for (assumed) independent features from any parametric distribution.

MultinomialNB is for data with integer valued input features that are assumed to be multinomially distributed.

Sckit also has GaussianNB that for continuous valued features that are assumed to idependently Gaussian distributed.
",,,false,
https://stackoverflow.com/questions/13301986,false,"The issue does not involve a problematic API. It is a question about the difference in results between the ""predict_proba"" and ""decision_function"" methods in scikit-learn's LogisticRegression.",,,,,,,predict_proba or decision_function as estimator &quot;confidence&quot;,"I'm using LogisticRegression as a model to train an estimator in scikit-learn. The features I use are (mostly) categorical; and so are the labels. Therefore, I use a DictVectorizer and a LabelEncoder, respectively, to encode the values properly. 

The training part is fairly straightforward, but I'm having problems with the test part. The simple thing to do is to use the ""predict"" method of the trained model and get the predicted label. However, for the processing I need to do afterwards, I need the probability for each possible label (class) for each particular instance. I decided to use the ""predict_proba"" method. However, I get different results for the same test instance, whether I use this method when the instance is by itself or accompanied by others. 

Next, is a code that reproduces the problem.

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import LabelEncoder


X_real = [{'head': u'n\xe3o', 'dep_rel': u'ADVL'}, 
          {'head': u'v\xe3o', 'dep_rel': u'ACC'}, 
          {'head': u'empresa', 'dep_rel': u'SUBJ'}, 
          {'head': u'era', 'dep_rel': u'ACC'}, 
          {'head': u't\xeam', 'dep_rel': u'ACC'}, 
          {'head': u'import\xe2ncia', 'dep_rel': u'PIV'}, 
          {'head': u'balan\xe7o', 'dep_rel': u'SUBJ'}, 
          {'head': u'ocupam', 'dep_rel': u'ACC'}, 
          {'head': u'acesso', 'dep_rel': u'PRED'}, 
          {'head': u'elas', 'dep_rel': u'SUBJ'}, 
          {'head': u'assinaram', 'dep_rel': u'ACC'}, 
          {'head': u'agredido', 'dep_rel': u'SUBJ'}, 
          {'head': u'pol\xedcia', 'dep_rel': u'ADVL'}, 
          {'head': u'se', 'dep_rel': u'ACC'}] 
y_real = [u'AM-NEG', u'A1', u'A0', u'A1', u'A1', u'A1', u'A0', u'A1', u'AM-ADV', u'A0', u'A1', u'A0', u'A2', u'A1']

feat_encoder =  DictVectorizer()
feat_encoder.fit(X_real)

label_encoder = LabelEncoder()
label_encoder.fit(y_real)

model = LogisticRegression()
model.fit(feat_encoder.transform(X_real), label_encoder.transform(y_real))

print ""Test 1...""
X_test1 = [{'head': u'governo', 'dep_rel': u'SUBJ'}]
X_test1_encoded = feat_encoder.transform(X_test1)
print ""Features Encoded""
print X_test1_encoded
print ""Shape""
print X_test1_encoded.shape
print ""decision_function:""
print model.decision_function(X_test1_encoded)
print ""predict_proba:""
print model.predict_proba(X_test1_encoded)

print ""Test 2...""
X_test2 = [{'head': u'governo', 'dep_rel': u'SUBJ'}, 
           {'head': u'atrav\xe9s', 'dep_rel': u'ADVL'}, 
           {'head': u'configuram', 'dep_rel': u'ACC'}]

X_test2_encoded = feat_encoder.transform(X_test2)
print ""Features Encoded""
print X_test2_encoded
print ""Shape""
print X_test2_encoded.shape
print ""decision_function:""
print model.decision_function(X_test2_encoded)
print ""predict_proba:""
print model.predict_proba(X_test2_encoded)


print ""Test 3...""
X_test3 = [{'head': u'governo', 'dep_rel': u'SUBJ'}, 
           {'head': u'atrav\xe9s', 'dep_rel': u'ADVL'}, 
           {'head': u'configuram', 'dep_rel': u'ACC'},
           {'head': u'configuram', 'dep_rel': u'ACC'},]

X_test3_encoded = feat_encoder.transform(X_test3)
print ""Features Encoded""
print X_test3_encoded
print ""Shape""
print X_test3_encoded.shape
print ""decision_function:""
print model.decision_function(X_test3_encoded)
print ""predict_proba:""
print model.predict_proba(X_test3_encoded)


Following is the output obtained:

Test 1...
Features Encoded
  (0, 4)    1.0
Shape
(1, 19)
decision_function:
[[ 0.55372615 -1.02949707 -1.75474347 -1.73324726 -1.75474347]]
predict_proba:
[[ 1.  1.  1.  1.  1.]]
Test 2...
Features Encoded
  (0, 4)    1.0
  (1, 1)    1.0
  (2, 0)    1.0
Shape
(3, 19)
decision_function:
[[ 0.55372615 -1.02949707 -1.75474347 -1.73324726 -1.75474347]
 [-1.07370197 -0.69103629 -0.89306092 -1.51402163 -0.89306092]
 [-1.55921001  1.11775556 -1.92080112 -1.90133404 -1.92080112]]
predict_proba:
[[ 0.59710757  0.19486904  0.26065002  0.32612646  0.26065002]
 [ 0.23950111  0.24715931  0.51348452  0.3916478   0.51348452]
 [ 0.16339132  0.55797165  0.22586546  0.28222574  0.22586546]]
Test 3...
Features Encoded
  (0, 4)    1.0
  (1, 1)    1.0
  (2, 0)    1.0
  (3, 0)    1.0
Shape
(4, 19)
decision_function:
[[ 0.55372615 -1.02949707 -1.75474347 -1.73324726 -1.75474347]
 [-1.07370197 -0.69103629 -0.89306092 -1.51402163 -0.89306092]
 [-1.55921001  1.11775556 -1.92080112 -1.90133404 -1.92080112]
 [-1.55921001  1.11775556 -1.92080112 -1.90133404 -1.92080112]]
predict_proba:
[[ 0.5132474   0.12507868  0.21262531  0.25434403  0.21262531]
 [ 0.20586462  0.15864173  0.4188751   0.30544372  0.4188751 ]
 [ 0.14044399  0.3581398   0.1842498   0.22010613  0.1842498 ]
 [ 0.14044399  0.3581398   0.1842498   0.22010613  0.1842498 ]]


As can be seen, the values obtained with ""predict_proba"" for the instance in ""X_test1"" change when that same instance is with others in X_test2. Also, ""X_test3"" just reproduces the ""X_test2"" and adds one more instance (that is equal to the last in ""X_test2""), but the probability values for all of them change. Why does this happen?
Also, I find it really strange that ALL the probabilities for ""X_test1"" are 1, shouldn't the sum of all be 1?

Now, if instead of using ""predict_proba"" I use ""decision_function"", I get the consistency in the values obtained that I need. The problem is that I get negative coefficients, and even some of the positives ones are greater than 1. 

So, what should I use? Why do the values of ""predict_proba"" change that way? Am I not understanding correctly what those values mean?

Thanks in advance for any help you could give me.

UPDATE

As suggested, I changed the code so as to also print the encoded ""X_test1"", ""X_test2"" and ""X_test3"", as well as their shapes. This doesn't appear to be the problem, as the encoding is consistant for the same instances between the test sets. 
",,,,false,
https://stackoverflow.com/questions/33937532,false,The issue does not involve a problematic API. It is a question about modifying the behavior of scikit-learn's decision tree to split on an attribute only once.,,,,,,,Use one attribute only once in scikit-learn decision tree in python,"I am using scikit-learn to create a decision tree, and its working like a charm. I would like to achieve one more thing: to make the tree to split on an attribute only once.
The reason behind this is because of my very strange dataset. I use a noisy dataset, and i am really interested in the noise as well. My class outcomes are binary let say [+,-]. I have a bunch of attributes with numbers mostly in the range of (0,1).
When scikit-learn creates the tree it splits on attributes multiple times, to make the tree ""better"". I understand that in this way the leaf nodes become more pure, but thats not the case i would like to achieve.
The thing i did was to define cutoffs for every attribute by counting the the information gain in different cutoffs, and choosing the maximum. In this way with ""leave-one-out"" and ""1/3-2/3"" cross validation techniques i get better results then the original tree.
The problem is that when i try to automatize this, i run into a problem  around the lower and upper bound e.g. around 0 and 1 because most of the elements will be under/upper that and  i get really high informational gain, cause one of the sets are pure, even if it only contains 1-2% of the full data.
All in all i would like to do something to make scikit-learn to only split on an attribute once.
If it cannot be done, do you guys have any advice how to generate those cutoffs in a nice way?
","To answer your question briefly, no, there is no built-in parameter to do this in sklearn. I tried to do the same a year ago, so I opened an issue requesting the addition of this feature.
sklearn builds nodes by randomly picking max_features features from the training dataset and searching for the cutoff that reduces the loss function the most. This exact same process is ran iteratively until some stopping criteria is met (max_depth, min_samples_leaf, etc.).
Hence, every feature always has the same probability of being picked, regardless of whether or not it has been used before.
If you're up for it, you can edit the source code of the classifier. In essence, all you need to do is drop the feature that minimizes the loss function after it has been chosen to build a node. That way, the algorithm will be unable to pick that feature again when taking a new sample of max_features features.
","I am not giving a method to directly deal with stopping the Classifier from using a feature multiple times. (Although you could do it by defining your own splitter and wiring it in, it is a lot of work.)

I would suggest making sure that you are balancing your classes in the first place, take a look at the class_weight parameter for details. That should help a lot in your issue. But if that does not work you can still enforce that there are no leafs having too small weight in them using the min_weight_fraction_leaf or similar parameters as suggested by maxymoo.
",,false,
https://stackoverflow.com/questions/28076232,false,The issue does not involve a problematic API. It is a comparison between Apache Spark and Scikit Learn regarding their convergence speed and accuracy in a specific use case.,,,,,,,Is Apache Spark less accurate than Scikit Learn?,"I've recently been trying to get to know Apache Spark as a replacement for Scikit Learn, however it seems to me that even in simple cases, Scikit converges to an accurate model far faster than Spark does.
For example I generated 1000 data points for a very simple linear function (z=x+y) with the following script:

from random import random

def func(in_vals):
    '''result = x (+y+z+w....)'''
    result = 0
    for v in in_vals:
        result += v
    return result

if __name__ == ""__main__"":
    entry_count = 1000
    dim_count = 2
    in_vals = [0]*dim_count
    with open(""data_yequalsx.csv"", ""w"") as out_file:
        for entry in range(entry_count):
            for i in range(dim_count):
                in_vals[i] = random()
            out_val = func(in_vals)
            out_file.write(','.join([str(x) for x in in_vals]))
            out_file.write("",%s\n"" % str(out_val))


I then ran the following Scikit script:

import sklearn
from sklearn import linear_model

import numpy as np

data = []
target = []
with open(""data_yequalsx.csv"") as inFile:
    for row in inFile:
        vals = row.split("","")
        data.append([float(x) for x in vals[:-1]])
        target.append(float(vals[-1]))

test_samples= len(data)/10

train_data = [0]*(len(data) - test_samples)
train_target = [0]*(len(data) - test_samples)
test_data = [0]*(test_samples)
test_target = [0]*(test_samples)
train_index = 0
test_index = 0
for j in range(len(data)):
    if j &gt;= test_samples:
        train_data[train_index] = data[j]
        train_target[train_index] = target[j]
        train_index += 1
    else:
        test_data[test_index] = data[j]
        test_target[test_index] = target[j]
        test_index += 1

model = linear_model.SGDRegressor(n_iter=100, learning_rate=""invscaling"", eta0=0.0001, power_t=0.5, penalty=""l2"", alpha=0.0001, loss=""squared_loss"")
model.fit(train_data, train_target)
print(model.coef_)
print(model.intercept_)

result = model.predict(test_data)
mse = np.mean((result - test_target) ** 2)
print(""Mean Squared Error = %s"" % str(mse))


And then this Spark script: (with spark-submit , no other arguments)

from pyspark.mllib.regression import LinearRegressionWithSGD, LabeledPoint
from pyspark import SparkContext

sc = SparkContext (appName=""mllib_simple_accuracy"")

raw_data = sc.textFile (""data_yequalsx.csv"", minPartitions=10) #MinPartitions doesnt guarantee that you get that many partitions, just that you wont have fewer than that many partitions
data = raw_data.map(lambda line: [float(x) for x in line.split ("","")]).map(lambda entry: LabeledPoint (entry[-1], entry[:-1])).zipWithIndex()
test_samples= data.count()/10

training_data = data.filter(lambda (entry, index): index &gt;= test_samples).map(lambda (lp,index): lp)
test_data = data.filter(lambda (entry, index): index &lt; test_samples).map(lambda (lp,index): lp)

model = LinearRegressionWithSGD.train(training_data, step=0.01, iterations=100, regType=""l2"", regParam=0.0001, intercept=True)
print(model._coeff)
print(model._intercept)

mse = (test_data.map(lambda lp: (lp.label - model.predict(lp.features))**2 ).reduce(lambda x,y: x+y))/test_samples;
print(""Mean Squared Error: %s"" % str(mse))

sc.stop ()


Strangely though, the error given by spark is an order of magnitude larger than that given by Scikit (0.185 and 0.045 respectively) despite the two models having a nearly identical setup (as far as I can tell)
I understand that this is using SGD with very few iterations and so the results may differ but I wouldn't have thought that it would be anywhere near such a large difference or such a large error, especially given the exceptionally simple data.



Is there something I'm misunderstanding in Spark? Is it not correctly configured? Surely I should be getting a smaller error than that?
",,,,false,
https://stackoverflow.com/questions/39550118,false,The issue does not involve a problematic API. It is a question about obtaining cross-validation accuracy for a logistic regression model in R.,,,,,,,Cross Validation function for logistic regression in R,"I Come from a predominantly python + scikit learn background, and I was wondering how would one obtain the cross validation accuracy for a logistic regression model in R? I was searching and surprised that there's no easy way to this. I'm looking for the equivalent:

import pandas as pd
from sklearn.cross_validation import cross_val_score
from sklearn.linear_model import LogisticRegression

## Assume pandas dataframe of dataset and target exist.

scores = cross_val_score(LogisticRegression(),dataset,target,cv=10)
print(scores)


For R: I have:

model = glm(df$Y~df$X,family=binomial')
summary(model) 


And now I'm stuck. Reason being, the deviance for my R model is 1900, implying its a bad fit, but the python one gives me 85% 10 fold cross validation accuracy.. which means its good. Seems a bit strange... So i wanted to run cross val in R to see if its the same result.

Any help is appreciated!
","R version using caret package:

library(caret)

# define training control
train_control &lt;- trainControl(method = ""cv"", number = 10)

# train the model on training set
model &lt;- train(target ~ .,
               data = train,
               trControl = train_control,
               method = ""glm"",
               family=binomial())

# print cv scores
summary(model)

","Below I took an answer from here and made a few changes. 

The changes I made were to make it a logit (logistic) model, add modeling and prediction, store the CV's results, and to make it a fully working example.

Also note that there are many packages and functions you could use, including cv.glm() from boot.

data(ChickWeight)

df                    &lt;- ChickWeight
df$Y                  &lt;- 0
df$Y[df$weight &gt; 100] &lt;- 1
df$X                  &lt;- df$Diet 

df     &lt;- df[sample(nrow(df)),]
folds  &lt;- cut(seq(1,nrow(df)),breaks=10,labels=FALSE)
result &lt;- list()

for(i in 1:10){
  testIndexes &lt;- which(folds==i,arr.ind=TRUE)
  testData    &lt;- df[testIndexes, ]
  trainData   &lt;- df[-testIndexes, ]
  model       &lt;- glm(Y~X,family=binomial,data=trainData)
  result[[i]] &lt;- predict(model, testData) 
}
result


You could add a line to calculate accuracy within the loop or just do it after the loop completes.
",,false,
https://stackoverflow.com/questions/49416716,true,The issue involves a problematic API. The behavior of the OneVsRestClassifier in scikit-learn with LabelBinarizer leads to unexpected results when predicting using one-hot encoded labels.,,,,,,,LabelBinarizer yields different result in multiclass example,"When executing the multiclass example in the scikit-learn tutorial


http://scikit-learn.org/stable/tutorial/basic/tutorial.html#multiclass-vs-multilabel-fitting


I came across a slight oddity.

&gt;&gt;&gt; import sklearn
&gt;&gt;&gt; sklearn.__version__
0.19.1

&gt;&gt;&gt; from sklearn.svm import SVC
&gt;&gt;&gt; from sklearn.multiclass import OneVsRestClassifier
&gt;&gt;&gt; from sklearn.preprocessing import LabelBinarizer

&gt;&gt;&gt; X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]
&gt;&gt;&gt; y = [0, 0, 1, 1, 2] # Three classes

&gt;&gt;&gt; clf = OneVsRestClassifier(estimator=SVC(random_state=0))
&gt;&gt;&gt; clf.fit(X, y).predict(X)
array([0, 0, 1, 1, 2])


This is all fine. Now with one-hot encoding:

&gt;&gt;&gt; y = LabelBinarizer().fit_transform(y)
&gt;&gt;&gt; y
array([[1, 0, 0],
       [1, 0, 0],
       [0, 1, 0],
       [0, 1, 0],
       [0, 0, 1]])


I would expect the label binarizer to only encode the target, but not having an influence on the classifier. However it yields a different result:

&gt;&gt;&gt; clf.fit(X, y).predict(X)
array([[1, 0, 0],
       [1, 0, 0],
       [0, 1, 0],
       [0, 0, 0],
       [0, 0, 0]])


Notebook on Google Colab (where the same code yields yet a different error, strangely):


https://drive.google.com/file/d/13dZ2aVbKTMgPOxj2SLsas2U2mOoKng2M/view?usp=sharing

","OneVsRestClassifier is applying  LabelBinarizer itself under the hood (the source code in sklearn/multiclass.py):

def fit(self, X, y):
  ...
  self.label_binarizer_ = LabelBinarizer(sparse_output=True)
  Y = self.label_binarizer_.fit_transform(y)
  Y = Y.tocsc()
  self.classes_ = self.label_binarizer_.classes_


So extra manual conversion is unnecessary. In fact, it's interpreting your one-hot encoded y as multi-label input. From the documentation:


  y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]
  
  Multi-class targets. An indicator matrix turns on multilabel
  classification.

",,,false,
https://stackoverflow.com/questions/59240556,false,The issue does not involve a problematic API. It is a problem with the installation or configuration of the LatentDirichletAllocation function in the decomposition module of scikit-learn.,,,,,,,LatentDirichletAllocation was not installed in decomposition module of SciKit-Learn,"I got some strange problem at SciKit-Learn package. 
There is ""decomposition"" module inside SciKit-Learn package, which should contain LatentDirichletAllocation([]) function. See documentation here:
""https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition""

When I tried to import ""decomposition"" module: 
from sklearn import as decomposition
it gives error:

Traceback (most recent call last):
  File ""tf_1_day_scikit_dnn.py"", line 12, in &lt;module&gt;
    from sklearn import decomposition
  File ""/home/developer1/.local/lib/python3.6/site-packages/sklearn/decomposition/__init__.py"", line 19, in &lt;module&gt;
    from ._online_lda import LatentDirichletAllocation
ImportError: cannot import name 'LatentDirichletAllocation'


Command: 
ls -al ~/.local/lib/python3.6/site-packages/sklearn/decomposition shows:

drwxr-xr-x  4 developer1 developer1   4096 Dec  9 00:45 .
drwxr-xr-x 33 developer1 developer1   4096 Dec  9 00:45 ..
-rw-r--r--  1 developer1 developer1   5490 Dec  9 00:44 _base.py
-rw-r--r--  1 developer1 developer1    480 Dec  9 00:44 base.py
-rwxr-xr-x  1 developer1 developer1 179440 Dec  9 00:44 _cdnmf_fast.cpython-36m-x86_64-linux-gnu.so
-rwxr-xr-x  1 developer1 developer1 175344 Dec  3 00:09 cdnmf_fast.cpython-36m-x86_64-linux-gnu.so
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 cdnmf_fast.py
-rw-r--r--  1 developer1 developer1  54528 Dec  9 00:44 _dict_learning.py
-rw-r--r--  1 developer1 developer1    507 Dec  9 00:44 dict_learning.py
-rw-r--r--  1 developer1 developer1  12572 Dec  9 00:44 _factor_analysis.py
-rw-r--r--  1 developer1 developer1    513 Dec  9 00:44 factor_analysis.py
-rw-r--r--  1 developer1 developer1  20866 Dec  9 00:44 _fastica.py
-rw-r--r--  1 developer1 developer1    490 Dec  9 00:44 fastica_.py
-rw-r--r--  1 developer1 developer1  14076 Dec  9 00:44 _incremental_pca.py
-rw-r--r--  1 developer1 developer1    513 Dec  9 00:44 incremental_pca.py
-rw-r--r--  1 developer1 developer1   1401 Dec  9 00:44 __init__.py
-rw-r--r--  1 developer1 developer1  13597 Dec  9 00:44 _kernel_pca.py
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 kernel_pca.py
-rw-r--r--  1 developer1 developer1  47255 Dec  9 00:44 _nmf.py
-rw-r--r--  1 developer1 developer1    477 Dec  9 00:44 nmf.py
-rwxr-xr-x  1 developer1 developer1  62056 Dec  3 00:09 _online_lda.cpython-36m-x86_64-linux-gnu.so
-rwxr-xr-x  1 developer1 developer1  62064 Dec  9 00:44 _online_lda_fast.cpython-36m-x86_64-linux-gnu.so
-rw-r--r--  1 developer1 developer1    513 Dec  9 00:44 online_lda_fast.py
-rw-r--r--  1 developer1 developer1  30471 Dec  9 00:44 _online_lda.py
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 online_lda.py
-rw-r--r--  1 developer1 developer1  22807 Dec  9 00:44 _pca.py
-rw-r--r--  1 developer1 developer1    477 Dec  9 00:44 pca.py
drwxr-xr-x  2 developer1 developer1   4096 Dec  9 00:45 __pycache__
-rw-r--r--  1 developer1 developer1    855 Dec  9 00:44 setup.py
-rw-r--r--  1 developer1 developer1  13654 Dec  9 00:44 _sparse_pca.py
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 sparse_pca.py
drwxr-xr-x  3 developer1 developer1   4096 Dec  9 00:45 tests
-rw-r--r--  1 developer1 developer1   8346 Dec  9 00:44 _truncated_svd.py
-rw-r--r--  1 developer1 developer1    507 Dec  9 00:44 truncated_svd.py


Most of the functions are here but there are no traces of the ""LatentDirichletAllocation"" function. Yet I did see LatentDirichletAllocation class defined in the _online_lda.py file.

Command python3 -c ""import sklearn; sklearn.show_versions()"" prints all versions required:

System:
    python: 3.6.8 (default, Oct  7 2019, 12:59:55)  [GCC 8.3.0]
executable: /usr/bin/python3
   machine: Linux-4.15.0-65-generic-x86_64-with-Ubuntu-18.04-bionic

Python dependencies:
       pip: 9.0.1
setuptools: 42.0.2
   sklearn: 0.22
     numpy: 1.17.4
     scipy: 1.3.3
    Cython: None
    pandas: 0.25.3
matplotlib: 3.1.2
    joblib: 0.14.0

Built with OpenMP: True


So I assume all requirements were met here. (BTW SciKit-Learn worked fine till I tried to import ""decomposition"" module).

I installed the SciKit-Learn package as it was described here: 
""https://scikit-learn.org/stable/install.html""
using command pip3 install -U scikit-learn
I did reinstall that several times but got same result. 

What do I do wrong? Do I need to add missing ""LatentDirichletAllocation"" function into the ""decomposition"" module? If so - how? Should I install the whole package somehow different way? 

Thanks.
","I had the same problem today. I solved it by going back to the previous version 0.21 of scikit-learn:

pip3 install scikit-learn==0.21


EDIT: I think the answer from glemaitre (https://stackoverflow.com/a/59328446/10429267) shows a better solution.
","The validated ""answered"" is not the right fix since it only downgrades scikit-learn. You will not be able to benefit from new features and bug fixes.

It will be helpful instead to report and give feedback in the issue tracker: https://github.com/scikit-learn/scikit-learn/issues/15884

It will allow us to find the root of the problem and propose the proper fix which seems to affect several persons.

EDIT:
After some investigation, you need to remove the following files:


~/.local/lib/python3.6/site-packages/sklearn/decomposition/_online_lda.cpython-36m-x86_64-linux-gnu.so
~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/_hashing.cpython-36m-x86_64-linux-gnu.so
~/.local/lib/python3.6/site-packages/sklearn/datasets/_svmlight_format.cpython-36m-x86_64-linux-gnu.so


Apparently, the so files do not get removed when updating to scikit-learn. This might be due to an old pip version.
","Try:



For Python 3

pip3 uninstall scikit-learn
pip3 install -U scikit-learn==0.21.3




For Python 2

pip uninstall scikit-learn
pip install -U scikit-learn==0.20.4

",false,
https://stackoverflow.com/questions/33317896,false,The issue does not involve a problematic API. It is a question about the behavior of scikit-learn's t-SNE implementation when applied to word embeddings.,,,,,,,scikit learn TSNE transform returns strange results when applied to word vectors,"I am getting weird results when applying scikit learn's manifold t-sne implementation on word embeddings generated by word2vec.
Normally, the distance and direction of (king-queen) should be the same as (man-woman) as stated in plenty of examples on the internet.

I am sure that my word2vec model is trained correctly as I am using the Google News data set and the query of most_similar(king-man+woman) also returns queen as it is supposed to.

The plotting of the transformed 2d-vectors is also done correctly. Therefore, it must be related to the t-sne transformation from scikit learn.

I am using the following python code for transformation:

vecs = np.concatenate(vecs)
np_vecs = np.array(vecs, dtype='float')
ts = TSNE(2)
reduced_vecs = ts.fit_transform(np_vecs)




I am I doing anything wrong or is the library configuration wrong?
","I would like to add to iceui2's answer. Original TSNE algorithm, is a non-linear dimension reduction technique, which tries to make sure that distribution of distances between neighbours is preserved in lower and higher dimension (neighbours in high dimension will be neighbours in lower dimension). 

However, your directions are likely to get distorted, as the manifold in which points used to lie has changed drastically.

To preserve directions, I recommend using PCA for dimension reduction and then plotting. 
",,,false,
https://stackoverflow.com/questions/21855410,true,"The issue involves a problematic API. The number of estimated clusters in scikit-learn's DBSCAN implementation increases with the increase of the MinPts parameter, which is unexpected behavior.",,,,,,,Number of clusters increased with the increase of MinPts in scikit-learn DBSCAN,"I use DBSCAN implementation from scikit-learn library and I got strange results.
The number of estimated clusters increased with the increase of parameter MinPts (min_samples) and from my understanding of algorithm this should not happend. 

Here are my results:

Estimated number of clusters:34 eps=0.9 min_samples=13.0
Estimated number of clusters:35 eps=0.9 min_samples=12.0
Estimated number of clusters:42 eps=0.9 min_samples=11.0 &lt;- strange result here
Estimated number of clusters:37 eps=0.9 min_samples=10.0   
Estimated number of clusters:53 eps=0.9 min_samples=9.0
Estimated number of clusters:63 eps=0.9 min_samples=8.0


I use scikit-learn like this:

X = StandardScaler().fit_transform(X)
db = DBSCAN(eps=eps, min_samples=min_samples, algorithm='kd_tree').fit(X)


and X is an array that contains ~200k 12-dimensional points.

What can be the problem here?
",,,,false,
https://stackoverflow.com/questions/60673168,false,The issue does not involve a problematic API. It is a question about the behavior of a linear regression learning curve and does not indicate any unexpected behavior or failure of an API.,,,,,,,Weird linear regression learning curve,"I'm trying to build a prediction model for apartments price. I use python scikit-learn toolset. I'm using a dataset having total floor area and location of the apartment, which I have converted to dummy features. So the dataset looks like this:

Then I build a learning curve to see how the model is doing. 
I build the learning curve this way:

from matplotlib import pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import learning_curve

model = LinearRegression()
training_sizes, training_scores, validation_scores = learning_curve(
    estimator = model,
    X = X_train,
    y = y_train,
    train_sizes = np.linspace(5, len(X_train) * 0.8, dtype = int), 
    cv = 5
)
line1, line2 = plt.plot(
    training_sizes, training_scores.mean(axis = 1), 'g', 
    training_sizes, validation_scores.mean(axis = 1), 'r')
plt.legend((line1, line2), ('Training', 'Cross-validation'))


The picture I see is somewhat confusing:

Anomalies I see here are:


Huge error on cross-validation set
Error not steadily decreasing on training examples number growth. 


Is it normal? 

Learning curve of training set only is also not so smooth but at least the error isn't that huge:


Also I tried to add to add polynomial features of 2nd degree. But this didn't make the model perform any different. And because I have a lot of categorical features (total 106) it takes quite long even for 2nd degree polynomial. So I didn't try for higher degrees. 

Also I tried to build a model using as simple cost function and gradient descent as possible using Octave. The result with weird error was same.  

Update:
Thanks to tolik I made several amendments:

Data preparation:
Categorical data are independent. So I can't combine them into one feature.
Features were scaled using StandardScaler(). Thank you for that.

Feature extraction:
After features transformation with PCA I found out one new feature has explained variance ratio over 99%. Though it's strange I used only this one. That also allowed to increase polynomial degree though it didn't increase performance.

Model selection:
I tried several different models but none seem to perform better than LinearRegression. Interesting thing - all models perform worse on full data set. Probably it's because I sorted by price and higher prices are rather outliers. So when I start training sets on 1000 samples and go to the maximum, I get this picture (for nearly all models):

","My explanation have 3 steps: The data preparation, feature extraction, and model selection.

Data preparation:


In this dataset there are lots of Categorical and Ordinal values. If
the column has several non related categories it's ok to one-hot it.
but if the column has categories with order like
""bad"",""normal"",""good"" you can convert it to numerical as
{Good:1,Normal:0.5,Bad:0}.
Value ranges: the value ranges for each feature differs from the other, therefore the best thing to do is to normalise each feature along itself between 0:1.


Feature Extraction:


Your goal is to maximise the score so I guess you don't care about which feature is more important. Use PCA (has an implementation in scikit-learn library) , this algorithm convert your feature vectors into different features that each of them is a linear combination of the other features.  These new features are ordered by their explained variance. The first features describes the data better than the last one. You select the first features that their explained_variance_ sums to 99%. Now you have weigh less features.


Model Selection: 
You don't really know what is a good model, because No Free Lunch Theory but in this problem the best results that don't use deep learning , use these: XGBoost-Regressor , Random-Forest-Regressor ,Ada-Boost.

The most important thing is the Data Preparation!!!
",,,false,
https://stackoverflow.com/questions/53218341,true,"The issue involves an API related to handling imbalanced datasets. The user is using the resample function from scikit-learn to upsample the minor classes in their dataset. However, when only upsampling the training data and leaving the original data for testing, the accuracy of the minority classes becomes zero. This behavior is due to leaking the test data into the training data, resulting in biased predictions. The issue can be resolved by performing the upsampling within the training data and ensuring a proper split between the training and test sets.",,,,,,,Up Sampling imbalanced dataset&#39;s minor classes,"i am using scikit-learn to classify my data, at the moment i am running a simple DecisionTree classifier.
I have three classes with a big imbalanced problem. The classes are 0,1 and 2. The minor classes are 1 and 2.

To give you an idea about the number of samples of the classes:

0 = 25.000 samples
1 = 15/20 less or more
2 = 15/20 less or more


so minor classes are about 0.06% of the dataset.
The approach that i am following to solve the imbalance problem is the UPSAMPLING of the minor classes. Code:

from sklearn.utils import resample,
resample(data, replace=True, n_samples=len_major_class, random_state=1234)


Now comes the problem. I did two tests:


If I upsample the minor classes and then divide my dataset in two groups one for training and one for testing... the accuracy is:



             precision    recall  f1-score   support

          0       1.00      1.00      1.00     20570
          1       1.00      1.00      1.00     20533
          2       1.00      1.00      1.00     20439

avg / total       1.00      1.00      1.00     61542



very good result.


If I ONLY upsample the training data and leave the original data for testing, the result is:



             precision    recall  f1-score   support

          0       1.00      1.00      1.00     20570
          1       0.00      0.00      0.00        15
          2       0.00      0.00      0.00        16

avg / total       1.00      1.00      1.00     20601



as you can see the global accuracy is high, but the accuracy of the class 1 and 2 is zero.

I am creating the classifier in this way:

DecisionTreeClassifier(max_depth=20, max_features=0.4, random_state=1234, criterion='entropy')


I also have tried adding the class_weight with balanced value, but it makes no difference.

I should only upsample the training data, why am i getting this strange problem?
","The fact that you obtain that behavior is quite normal when you do the re-sampling before the splitting; you are inducing a bias in your data.

If you oversample the data and then split, the minority samples in the test won't be anymore independent from the samples in the training set because they are generated together. In your case they are exact copies of the samples in the training set. Your accuracy is 100% because the classifier is classifying samples that have already been seen in the training.

Since your problem is strongly umbalanced I would suggest to use an ensemble of classifiers to handle it. 1) Split your dataset in training set and test set. Given the size of the dataset you can sample 1-2 samples from the minority class for test and leave the other for training. 2) From the training you generate N datasets containing all the remaining samples of the minority class and under-samples from the majority class (i would say 2*number of samples in the minority class). 3) For each one of the dataset obtained you train a model. 4) Use the test set to obtain the prediction; the final prediction will be the results of a majority vote of all the predictions of the classifiers. 

To have robust metrics perform different iterations with different initial splitting test/training. 
","You should not split the dataset after upsampling. You can do the upsampling within the training data.

Basically, you are leaking the test data into the training data. 
","I have a function that resamples the dataset for each class to have the same amount of instance.
from sklearn.utils import resample
import pandas as pd

def make_resample(_df, column):

  dfs_r = {}
  dfs_c = {}
  bigger = 0
  ignore = """"
  for c in _df[column].unique():
    dfs_c[c] = _df[df[column] == c]
    if dfs_c[c].shape[0] &gt; bigger:
      bigger = dfs_c[c].shape[0]
      ignore = c

  for c in dfs_c:
    if c == ignore:
      continue
    dfs_r[c] = resample(dfs_c[c], 
                        replace=True,
                        n_samples=bigger - dfs_c[c].shape[0],
                        random_state=0)
  return pd.concat([dfs_r[c] for c in dfs_r] + [_df])

",false,
https://stackoverflow.com/questions/46046360,false,"The issue does not involve a problematic API. It is a comparison of processing time between different libraries (Theano and scikit-learn) on different hardware (CPU and GPU). The observed performance difference is likely due to various factors, such as the efficiency of CPU and GPU utilization, memory access patterns, and the specific task being performed. It is not indicative of an API exhibiting unexpected failures or unpredictable behaviors.",,,,,,,Why is sklearn faster on CPU than Theano on GPU?,"I've compared processing time with theano(CPU), theano(GPU) and Scikit-learn(CPU) using Python.
But, I got strange result.
Here look at the graph that I plot.

Processing Time Comparison:



you can see the result of scikit-learn that is faster than theano(GPU).
The program that I checked its elapsed time is to compute euclidean distance matrix from a matrix which have n * 40 elements.

Here is the part of code.

points = T.fmatrix(""points"")
edm = T.zeros_like(points)

def get_point_to_points_euclidean_distances(point_id):
    euclideans = (T.sqrt((T.sqr(points- points[point_id, : ])).sum(axis=1)))

    return euclideans

def get_EDM_CPU(points):
    EDM = np.zeros((points.shape[0], points.shape[0])).astype(np.float32)
    for row in range(points.shape[0]):
        EDM[row, :] = np.sqrt(np.sum((points - points[row, :])**2, axis=1))

    return EDM

def get_sk(points):
    EDM = sk.pairwise_distances(a, metric='l2')

    return EDM

seq = T.arange(T.shape(points)[0])
(result, _) = theano.scan(fn = get_point_to_points_euclidean_distances, \
outputs_info = None , \
sequences = seq)

get_EDM_GPU = theano.function(inputs = [points], outputs = result, allow_input_downcast = True)


I thought that the reason why GPU is slower than sci-kit learn is probably transfer time. So I did profiling GPU with nvprof command. then I got this.

==27105== NVPROF is profiling process 27105, command: python ./EDM_test.py
Using gpu device 0: GeForce GTX 580 (CNMeM is disabled, cuDNN not available)
data shape :  (10000, 40)
get_EDM_GPU elapsed time :  1.84863090515 (s)
get_EDM_CPU elapsed time :  8.09937691689 (s)
get_EDM_sk elapsed time :  1.10968112946 (s)
ratio :  4.38128395145
==27105== Profiling application: python ./EDM_test.py
==27105== Warning: Found 9 invalid records in the result.
==27105== Warning: This could be because device ran out of memory when profiling.
==27105== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 71.34%  1.28028s      9998  128.05us  127.65us  128.78us  kernel_reduce_01_node_316e2e1cbfbe8cfb8e4a101f329ffeec_0(int, int, float const *, int, int, float*, int)
 19.95%  357.97ms      9997  35.807us  35.068us  36.948us  kernel_Sub_node_bc41b3f8f12c93d29f2c4360ad445d80_0_2(unsigned int, int, int, float const *, int, int, float const *, int, int, float*, int, int)
  7.32%  131.38ms         2  65.690ms  1.2480us  131.38ms  [CUDA memcpy DtoH]
  1.25%  22.456ms      9996  2.2460us  2.1140us  2.8420us  kernel_Sqrt_node_23508f8f49d12f3e8369d543f5620c15_0_Ccontiguous(unsigned int, float const *, float*)
  0.12%  2.1847ms         1  2.1847ms  2.1847ms  2.1847ms  [CUDA memset]
  0.01%  259.73us         5  51.946us     640ns  250.36us  [CUDA memcpy HtoD]
  0.00%  17.086us         1  17.086us  17.086us  17.086us  kernel_reduce_ccontig_node_97496c4d3cf9a06dc4082cc141f918d2_0(unsigned int, float const *, float*)
  0.00%  2.0090us         1  2.0090us  2.0090us  2.0090us  void copy_kernel&lt;float, int=0&gt;(cublasCopyParams&lt;float&gt;)


The transfer [CUDA memcpy DtoH] was performed twice { 1.248 [us],  131.38 [ms] }

The transfer [CUDA memcpy HtoD] was performed 5x { min: 640 [ns], max: 250.36 [us] }  

The transfer time is about 131.639 ms (131.88 ms + 259.73 us).
but the gap between GPU and scikit-learn is about 700ms (1.8 s - 1.1 s) So, the gap is over the transfer time.

does it compute only upper triangular matrix from symmetric matrix?

what makes scikit-learn so fast?
","What makes scikit-learn ( on pure CPU-side ) so fast?

My initial candidates would be a mix of:  


highly efficient use of available CPU-cores' L1-/ L2- sizes within the fastest [ns]-distances  
smart numpy vectorised execution being friendly to CPU cache-lines  
dataset so small, it can completely remain non-evicted from cache ( test to scale the dataset-under-review way above the L2-/L3-cache sizes to see the DDRx-memory-cost effects on the observed performance ( details are in the URL below ) )  
might enjoy even better timing on numpy, if avoiding .astype() conversions ( test it )


Facts on the GPU-side


auto-generated GPU-kernels do not have much chance to get ultimate levels of global memory latency-masking, compared to manually tweaked kernel-designs, tailor fit to respective GPU-silicon-architecture / latencies observed in-vivo  
data-structures larger than just a few KB remain paying GPU-SM/GDDR-MEM distances of ~ large hundreds of [ns], nearly [us] -v/s- compared to small units ~ small tens of [ns] at CPU/L1/L2/L3/DDRx ) ref. timing details in &gt;&gt;&gt; https://stackoverflow.com/a/33065382  
not being able to enjoy much of the GPU/SMX power, due to this task's obvious low-reuse of data points and dataset size beyond the GPU/SM-silicon limits, that causes and must cause GPU/SM-register capacity spillovers in any kind of GPU-kernel design attempts and tweaking  
the global task is not having a minimum reasonable amount of asynchronous, isolated ( non-communicating islands ) mathematically-dense, yet SMX-local, GPU-kernel processing steps ( there is not much to compute so as to adjust for the add-on overheads and expensive SMX/GDDR memory costs )  


GPU-s can lovely exhibit it's best-performance, if sufficiently enough densely-convoluted re-processing operations take place -- like in large-scale/high-resolution image-processing -- on [m,n,o]-convolution-kernel matrices so small, so as that all these m*n*o constant values can reside local to SM, inside an available set of SMX-SM_registers and if the GPU-kernel-launchers are optimally tweaked by the 3D-tblock/grid processing-layout geometries, so that the global memory access latencies are at its best-masked performance, having all the GPU-threads enforced within the hardware WARP-aligned SMx:WarpScheduler RoundRobin thread-scheduling capabilites ( the first swap from Round-Robin into Greedy-WarpSchedule mode loses the whole battle in case of divergent execution-paths in GPU-kernel-code ).
",,,false,
https://stackoverflow.com/questions/39366120,true,"The issue involves the numpy cosine_similarity function from scikit-learn. When comparing big vectors and using different methods to compute cosine similarity, the results differ depending on the size of the vectors. This behavior is due to floating-point precision limitations, and the equality operator should not be used to compare numpy arrays. Instead, the np.isclose function should be used for numerical comparison. This ensures that small differences in floating-point values are considered as equal.",,,,,,,Numpy Cosine Similarity difference over big collections,"I need to use the Scikit-learn sklearn.metric.pairwise.cosine_similarity over big matrixes. 
For some optimizations i need to compute only some rows of the matrixes, and so i tried different methods.

I found that in some cases the results were different depending on the size of the vectors, and I saw this strange behaviour on this test case (big vectors, transpose and estimate cosine):

from sklearn.metrics.pairwise import cosine_similarity
from scipy import spatial
import numpy as np
from scipy.sparse import csc_matrix

size=200
a=np.array([[1,0,1,0]]*size)
sparse_a=csc_matrix(a.T)
#standard cosine similarity between the whole transposed matrix, take only the first row
res1=cosine_similarity(a.T,a.T)[0]
#take the row obtained by the multiplication of the first row of the transposed matrix with transposed matrix itself (optimized for the first row calculus only)
res2=cosine_similarity([a.T[0]],a.T)[0]
#sparse matrix implementation with the transposed, which should be faster
res3=cosine_similarity(sparse_a,sparse_a)[0]
print(""res1: "",res1)
print(""res2: "",res2)
print(""res3: "",res3)
print(""res1 vs res2: "",res1==res2)
print(""res1 vs res3: "",res1==res3)
print(""res2 vs res3: "", res2==res3)


If ""size"" is set to 200 I got this result, that is ok:

res1:  [ 1.  0.  1.  0.]
res2:  [ 1.  0.  1.  0.]
res3:  [ 1.  0.  1.  0.]
res1 vs res2:  [ True  True  True  True]
res1 vs res3:  [ True  True  True  True]
res2 vs res3:  [ True  True  True  True]


But if ""size"" is set to 2000 or more, some strange things happen:

res1:  [ 1.  0.  1.  0.]
res2:  [ 1.  0.  1.  0.]
res3:  [ 1.  0.  1.  0.]
res1 vs res2:  [False  True False  True]
res1 vs res3:  [False  True False  True]
res2 vs res3:  [ True  True  True  True]


Does anybody know what am I missing?

Thanks in advance
","In order to compare numpy.array you have to use np.isclose instead of equality operator. Try:

from sklearn.metrics.pairwise import cosine_similarity
from scipy import spatial
import numpy as np
from scipy.sparse import csc_matrix

size=2000
a=np.array([[1,0,1,0]]*size)
sparse_a=csc_matrix(a.T)
#standard cosine similarity between the whole transposed matrix, take only the first row
res1=cosine_similarity(a.T,a.T)[0]
#take the row obtained by the multiplication of the first row of the transposed matrix with transposed matrix itself (optimized for the first     row calculus only)
res2=cosine_similarity([a.T[0]],a.T)[0]
#sparse matrix implementation with the transposed, which should befaster
res3=cosine_similarity(sparse_a,sparse_a)[0]
print(""res1: "",res1)
print(""res2: "",res2)
print(""res3: "",res3)
print(""res1 vs res2: "", np.isclose(res1, res2))
print(""res1 vs res3: "", np.isclose(res1, res3))
print(""res2 vs res3: "", np.isclose(res2, res2))


The results are:

res1:  [ 1.  0.  1.  0.]
res2:  [ 1.  0.  1.  0.]
res3:  [ 1.  0.  1.  0.]
res1 vs res2:  [ True  True  True  True]
res1 vs res3:  [ True  True  True  True]
res2 vs res3:  [ True  True  True  True]


as expected.
",,,false,
https://stackoverflow.com/questions/73491673,false,The issue does not involve a problematic API. It is related to data preprocessing and clustering techniques rather than an API exhibiting unexpected failures or unpredictable behaviors.,,,,,,,Strange results when scaling data using scikit learn,"I have an input dataset that has 4 time series with 288 values for 80 days. So the actual shape is (80,4,288). I would like to cluster differnt days. I have 80 days and all of them have 4 time series: outside temperature, solar radiation, electrical demand, electricity prices. What I want is to group similar days with regard to these 4 time series combined into clusters. Days belonging to the same cluster should have similar time series.
Before clustering the days using k-means or Ward's method, I would like to scale them using scikit learn. For this I have to transform the data into a 2 dimensional shape array with the shape (80, 4*288) = (80, 1152), as the Standard Scaler of scikit learn does not accept 3-dimensional input. The Standard Scaler just standardizes features by removing the mean and scaling to unit variance.
Now I scale this data using sckit learn's standard scaler:
import numpy as np
from sklearn.preprocessing import StandardScaler
import pandas as pd

data_Unscaled = pd.read_csv(""C:/Users/User1/Desktop/data_Unscaled.csv"", sep="";"")
scaler = StandardScaler()
data_Scaled = scaler.fit_transform(data_Unscaled)
np.savetxt(""C:/Users/User1/Desktop/data_Scaled.csv"", data_Scaled, delimiter="";"")

When I now compare the unscaled and scaled data e.g. for the first day (1 row) and the 4th time series (columns 864 - 1152 in the csv file), the results look quite strange as you can see in the following figure:

As far as I see it, they are not in line with each other. For example in the timeslots between 111 and 201 the unscaled data does not change at all whereas the scaled data fluctuates. I can't explain that. Do you have any idea why this is happening and why they don't seem to be in line?
Here is the unscaled input data with shape (80,1152): https://filetransfer.io/data-package/CfbGV9Uk#link
and here the scaled output of the scaling with shape (80,1152): https://filetransfer.io/data-package/23dmFFCb#link
","You have two issues here: scaling and clustering. As the question title refers to scaling, I'll handle that one in detail. The clustering issue is probably better suited for CrossValidated.
You don't say it, but it seems natural that all temperatures, be it on day 1 or day 80, are measured on a same scale. The same holds for the other three variables. So, for the purpose of scaling you essentially have four time series.
StandardScaler, like basically everything in sklearn, expects your observations to be organised in rows and variables in columns. It treats each column separately, deducting its mean from all the values in the column and dividing the resulting values by their standard deviation.
I reckon from your data that the first 288 entries in each row correspond to one variable, the next 288 to the second one etc. You need to reshape these data to form 288*80=23040 rows and 4 columns, one for each variable.
You apply StandardScaler on that array and reformat the data into the original shape, with 80 rows and 4*288=1152 columns. The code below should do the trick:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

data_Unscaled = pd.read_csv(""C:/Users/User1/Desktop/data_Unscaled.csv"", sep="";"", header=None)

X = data_Unscaled.to_numpy()
X_narrow = np.array([X[:, i*288:(i+1)*288].ravel() for i in range(4)]).T
scaler = StandardScaler()
X_narrow_scaled = scaler.fit_transform(X_narrow)
X_scaled = np.array([X_narrow_scaled[i*288:(i+1)*288, :].T.ravel() for i in range(80)])

# Plot the original data:
i=3
j=0
plt.plot(X[j, i*288:(i+1)*288])
plt.title('TimeSeries_Unscaled')
plt.show()

# plot the scaled data:
plt.plot(X_scaled[j, i*288:(i+1)*288])
plt.title('TimeSeries_Scaled')
plt.show()

resulting in the following graphs:


The line
X_narrow = np.array([X[:, i*288:(i+1)*288].ravel() for i in range(4)]).T

uses list comprehension to generate the four columns of the long, narrow array X_narrow. Basically, it is just a shorthand for a for-loop over your four variables. It takes the first 288 columns of X, flattens them into a vector, which it then puts into the first column of X_narrow. Then it does the same for the next 288 columns, X[:, 288:576], and then for the third and the fourth block of the 288 observed values per day. This way, each column in X_narrow contains a long time series, spanning 80 days (and 288 observations per day), of exactly one of your variables (outside temperature, solar radiation, electrical demand, electricity prices).
Now, you might try to cluster X_scaled using K-means, but I doubt it will work. You have just 80 points in a 1152-dimensional space, so the curse of dimensionality will almost certainly kick in. You'll most probably need to perform some kind of dimensionality reduction, but, as I noted above, that's a different question.
",,,false,
https://stackoverflow.com/questions/59140556,false,"The issue does not involve a problematic API. It is a question about using past label columns as features in machine learning models, which does not indicate any unexpected behavior or failure of an API.",,,,,,,Can / should I use past (e.g. monthly) label columns from a database as features in an ML prediction (no time-series!)?,"The question: Is it normal / usual / professional to use the past of the labels as features?
I could not find anything reliable on this, although it is a basic question.
Edited: Please mind, this is not a time-series question, I have deleted the time-series tag now and I changed the question. This question is about features that change regularly over time, yes! But we do not create a time-series from this, as there are many other features as well which are not like the label and are also important features in the model. Now please think of using past labels as normal features without a time-series approach.
I try to predict a certain month of data that is available monthly, thus a time-series, but I am not using it as a time-series, it is just monthly avaiable data of various different features.
It is a classification model, and now I want to predict a label column of a selected month of that time-series. The previous months before the selected label month are now the point of the question.
I do not want to just drop the past months of the label just because they are ""almost"" a label (or in other words: they were just the label columns of the preceding models in time). I know the past of the label, why not considering it as features as well?
My predictions are of course much better when adding the past labels of the time-series of labels to the features. This is logical as the labels usually do not change so much from one month to the other and thus can be predicted very well if you have fed the data with the past of the label. It would be strange not to use such ""past labels"" as features, as any simple time-series regression would then be better than the ml model.
Example: Let's say I predict the IQ test result of a person, and I use her past IQ test results as features in addition to other normal ""non-label"" features like age, education aso. I use the first 11 months of ""past labels"" of a year as features in addition to my normal ""non-label"" features. I predict the label of the 12th month.
Predicting the label of the 12th month works much better if you add the past of the labels to the features - obviously. This is because the historical labels, if there are any, are of course better indicators of the final outcome than normal columns like age and education.
Possibly related p.s.:
p.s.1: In auto-regressive models, the past of the dependent variable can well be used as independent variable, see: https://de.wikipedia.org/wiki/Regressionsanalyse
p.s.2: In ML you can perhaps just try any features and take what gives you the best results, a bit like &gt;Good question, try them [feature selection methods] all and see what works best&lt; in https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/ &gt;If the features are relevant to the outcome, the model will figure out how to use them. Or most models will.&lt; The same is said in Does the feature selection matter for learning algorithm with regularization?
p.s.3: Also probably relevant is the problem of multicollinearity: https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/ though multicollinearity is said to be no issue for the prediction: &gt;Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you dont need to understand the role of each independent variable, you dont need to reduce severe multicollinearity.
","It is perfectly possible and also good practice to include past label columns as features, though it depends on your question: do you want to explain the label only with other features (on purpose), or do you want to consider other and your past label columns to get the next label predicted, as a sort of adding a time-series character to the model without using a time-series?
The sequence in time is not even important, as long as all of such monthly columns are shifted in time consistently by the same time when going over to the predicting set. The model does not care if it is just January and February of the same column type, for the model, every feature is isolated.
Example: You can perfectly run a random forest model on various features, including their past label columns that repeat the same column type again and again, only representing different months. Any month's column can be dealt with as an independent new feature in the ml model, the only importance is to shift all of those monthly columns by the exactly same period to reach a consistent predicting set. In other words, obviously you should avoid replacing January with March column when you go from a training set January-June to a predicting set February-July, instead you must replace January with February of course.
Update 202301: model name is ""walk-forward""
This model setup is called ""walk-forward"", see Why isnt out-of-time validation more ubiquitous? --&gt; option 3 almost at the bottom of the page.
I got this from a comment at Splitting Time Series Data into Train/Test/Validation Sets.
In the following, it shows only training and testing set. It writes ""validation set"", but it is known that this gets mixed up all over the place, see What is the Difference Between Test and Validation Datasets?, and it must be meant as the testing set in the default understanding of it.

Thus, with the right wording, it is:

This should be the best model for labels that become features in time.
validation set in a ""walk-forward"" model?
As you can see in the model, no validation set is needed since the test data must be biased ""forward"" in time, that is the whole idea of predicting the ""step forward in time"", and any validation set would have to be in that same biased artificial future - which is already the past at the time of training, but the model does not know this.
The validation happens by default, without a needed dataset split, during the walk-forward, when the model learns again and again to predict the future and the output metrics can be put against each other. As the model is to predict the time-biased future, there is no need to prove that or how the artificial future is biased and sort of ""overtrained by time"". It is the aim of the model to have the validation in the artificial future and predict the real future as a last step only.
But then, why not still having a validation set on top of this, at least if it is just a small k-fold validation? It could play a role if the testing set has a few strong changes that happen in small time windows but which are still important to be predicted, or at least hinted at, but should also not be overtrained within each training step. The validation set would hit some of these time windows and might show whether the model can handle them well enough. Any other method than k-fold would shrink the power of the model too much. The more you take away from the testing set during training, the less it can predict the future.
Wrap up:
Try it out, and in doubt, leave the validation aside and judge upon the model by checking its metrics over time, during the ""walk-forward"". This model is not like the others.
Thus, in the end, you can, but you do not have to, split a k-fold validation from the testing set. That would look like:

After predicting a lot of known futures, the very last step in time is then the prediction of the unknown future.
This also answers Does the training+testing set have to be different from the predicting set (so that you need to apply a time-shift to ALL columns)?.
",,,false,
https://stackoverflow.com/questions/42265434,false,"The issue does not involve a problematic API. It is related to the interpretation and significance testing of results using scikit-learn's permutation test, but there is no indication of unexpected behavior or failure of an API.",,,,,,,significant test using scikit-learn&#39;s permutation test results in the same p-value for all classifiers,"I'm trying to find out the significance of the results using scikit-learn's permutation test as in:

score, permutation_scores, pvalue = permutation_test_score(clf.best_estimator_, X_train, Y_train, cv=10, n_jobs=10, n_permutations=100, scoring='accuracy')


where the clf.best_estimator is the result of cross-validation.

I use it for several classifiers (several independent clf.best_estimator_) but the p-values for all of them is the same 0.00990099009901.

I have no idea why this happens. The strange thing is that this is the same number that is reported in the linked code in scikit-learn user guide.
","I asked the same question in scikit-learn's issues and the answer was: for most of the good classifiers if the random classifier is better than the trained classifier in 1 test out of 100, this magic number would be the result.

so there's nothing wrong with this magic number.
",,,false,
https://stackoverflow.com/questions/39865557,false,"The issue does not involve a problematic API. It is a question about converting an array into a specific format for scikit-learn, but there is no indication of unexpected behavior or failure of an API.",,,,,,,sentiment analysis joint list,"im doing the sentiment analysis with scikit-learn python, now I'm using the nltk to do the words lemmatization in order to increase processing speed, for example:

I get the following arrays after nltk processing:

array([ ['Really', 'a', 'terrible', 'course', u'lecture', u'be', 'so', 'boring', 'i', u'contemplate', 'suicide', 'on', 'numerous', u'occasion', 'and', 'the', 'tutes', u'go', 'for', 'two', u'hour', 'and', u'be', 'completely'], ['Management', 'accounting', u'require', 'sufficient', 'practice', 'to', 'get', 'a', 'hang', 'of', 'Made', 'easier', 'with', 'a', 'great', 'lecturer']], dtype=object)


but the scklearn require the array is 

array([ 'Really a terrible course  lectures were so boring i contemplated suicide on numerous occasions and the tutes went for two hours and were completely ', 'Management accounting requires sufficient practice to get a hang of  Made easier with a great lecturer '],dtype=object)


so what is the best way to convert this array into the right form? I try to use joint list but the result is strange
","You would do:

second_array = [' '.join(each) for each in first_array]


Alternatively you can tell sklearn.CountVectorizer to just use your tokens:

vect = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)
X = vect.fit_transform(first_array)

",,,false,
https://stackoverflow.com/questions/23914472,false,"The issue does not involve a problematic API. It is related to the calculation and interpretation of the F1 score in scikit-learn, but there is no indication of unexpected behavior or failure of an API.",,,,,,,strange F1 score result using scikit learn,"I am doing some classification and I was looking at the f1 score and noticed something strange.

When I do: 

""f1:"" + str(f1_score(y_test_bin, target, average=""weighted""))


I get :

f1:0.444444444444


When I do:

print ""f1:"" + str(f1_score(y_test_bin, target,pos_label=0, average=""weighted""))


I get:

f1:0.823529411765


Which is stange since I set the average to be 'weighted'. This should give me the weighted average of those two scores. Which is independent of the ""True label""

I can also see this in the classification report:

         precision    recall  f1-score   support

      0       0.76      0.90      0.82        39
      1       0.60      0.35      0.44        17

avg / total       0.71      0.73      0.71        56


In the classification report I get the weighted average, but not when I use the f1 score function. Why is this ?
","I was struggling with this problem as well and found a solution after reading eickenberg's answer on this thread which is definitely worth a read for the background on this.

In short, sklearn automatically overrides the averaging to take the positive class score when it interprets the data as binary. It does this automatically or when you specify a pos_label. The solution then is to redefine the pos_label as None.

print ""f1:"" + str(f1_score(y_test_bin, target, pos_label=None, average=""weighted""))


Hope this helps!
",,,false,
https://stackoverflow.com/questions/65856432,false,"The issue does not involve a problematic API. The error in the AdaBoostClassifier is likely due to the specific data or configuration being used, rather than an issue with the API itself.",,,,,,,Using Perceptron sklearn.ensemble.AdaBoostClassifier() gives an error,"I have the problem of using perceptrons for AdaBoost classifier.
The training and testing data from here
should be turned to 0 and 1 in the last column (""Poker Hand""), (it is from 1 to 9 inclusively originally), then both Decision Tree Classifier and AdaBoost Classifier with the total of 15 weak perceptron classifiers should be implemented in the data. I try to use scikit-learn libraries, but while my Decision Tree Classifier provides good results, AdaBoost Classifier throws error:
ValueError: BaseClassifier in AdaBoostClassifier ensemble is worse than random, ensemble can not be fit.
Here, the crucial parts of the code.
import pandas as pd
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import Perceptron
from sklearn import metrics

if __name__ == ""__main__"":
   
    data_train = pd.read_csv(""poker-hand-testing.data"",header=None)
    data_test = pd.read_csv(""poker-hand-training-true.data"",header=None)
    

    for value in range(0, len(data_train)):
        if data_train[10][value] != 0:
            data_train[10][value] = 1
    
    for value in range(0, len(data_test)):
        if data_test[10][value] != 0:
            data_test[10][value] = 1

    col=['Suit of card #1','Rank of card #1',
     'Suit of card #2','Rank of card #2',
     'Suit of card #3','Rank of card #3',
     'Suit of card #4','Rank of card #4',
     'Suit of card #5','Rank of card #5',
     'Poker Hand']
    
    data_train.columns=col
    data_test.columns=col
    
    y_train=data_train['Poker Hand']
    y_test=data_test['Poker Hand']
    
    x_train=data_train.drop('Poker Hand',axis=1)
    x_test=data_test.drop('Poker Hand',axis=1)
    
#The problematic part
    classifier = AdaBoostClassifier(base_estimator=Perceptron(), n_estimators=15, algorithm='SAMME')
    classifier = classifier.fit(x_train, y_train)
    y_pred = classifier.predict(x_test)
    
    print(""Accuracy of AdaBoost:"", metrics.accuracy_score(y_test, y_pred))

The strange thing is that this error occurs only once per 9-10 times when I don't change values to binary ones, while binary values almost always gives an error. Also, changing Perceptron() to SGDClassifier(loss=""perceptron"", eta0=1, learning_rate=""constant"", penalty=None) also throws such errors.
My questions are:

What is the solution with the possibility of using scikit-learn library?

Is there any way to handle such an exception? For example, if it gives error, execute it again until desired results?

Are there any other alternatives where I can use both Decision Tree and AdaBoost with Perceptron, if it is couldn't be solved in scikit-learn library?


","The problematic part could be solved by try-catch block. For example,
#The problematic part solution
AdaBoost_accuracy = 0

while AdaBoost_accuracy == 0:
    try:
        classifier = AdaBoostClassifier(base_estimator=Perceptron(), n_estimators=15, algorithm='SAMME')
        classifier = classifier.fit(x_train, y_train)
        y_pred = classifier.predict(x_test)
        AdaBoost_accuracy = metrics.accuracy_score(y_test, y_pred)
    except:
        print(""Let me reclassify AdaBoost again"")

print(""Accuracy of AdaBoost:"", AdaBoost_accuracy)

",,,false,
https://stackoverflow.com/questions/37907775,false,"The issue does not involve a problematic API. The difference in accuracy when using TfidfTransformer is likely due to the specific data or the impact of TF-IDF weighting on the classification task, rather than an issue with the API itself.",,,,,,,Scikitklearns TfidfTransformer makes my pipeline predict just one label,"I have a pandas dataframe containing texts and labels, and I'm trying to predict the labels using scikit-learn's CountVectorizer, TfidfTransformer and MultinomialNB. Here's what the dataframe looks like:

                                                text party
0  Herr lderspresident! Att vara talman i Sverig...     S
1  Herr lderspresident! rade ledamter av Sveri...     M
2  Herr lderspresident! Som fretrdare fr Alli...     M
3  Val av andre vice talman Herr lderspresident!...    SD
4  Herr lderspresident! Vnsterpartiet vill utny...     V


When I construct a pipeline with the three estimators mentioned above, I only get a ~35% accuracy in my predictions, but when I remove the TfidfTransformer the accuracy is bumped up to a more reasonable ~75% accuracy.

text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()), # problematic row
                     ('clf', MultinomialNB()),
                   ])

text_clf = text_clf.fit(df.text.values, df.party.values)

test = df.sample(500, random_state=42)
docs_test = test.text.values
predicted = text_clf.predict(docs_test)
np.mean(predicted == test.party.values)
# Out: either 0.35 or 0.75 depending on whether I comment out the problematic row above


When I get 0.35 and inspect predicted I see that it almost exclusively contains one label ('S'). This is the most common label in the original dataset, but that shouldn't impact the predictions, right? Any ideas on why I get these strange results?

EDIT: Link to data where anforandetext and parti are the relevant columns.
","The reason that you are getting so much difference is because of smoothing. If you checkout the documentation of MultinomialNB class, checkout the alpha parameter. The default value for that is 1.0. This means that it implements Plus One smoothing by default. Plus one smoothing is a very common technique used with relative frequency estimates to account for unseen data. In Plus One smoothing, we add 1 to all raw counts to account for unseen terms and the sparsity of the document-term matrix.

However, when you end up using TF-IDF weights, the numbers that you get are very small and mostly between 0 - 1. To illustrate, if I use your data and only convert it into TF-IDF weights, this is the small snapshot of the TF-IDF weights that I obtain.

  (0, 80914)    0.0698184481033
  (0, 80552)    0.0304609466459
  (0, 80288)    0.0301759343786
  (0, 80224)    0.103630302925
  (0, 80204)    0.0437500703747
  (0, 80192)    0.0808649191625


You can see that these are really small numbers and adding 1 to them for smoothing will have a drastic effect on the calculations that Multinomial Naive Bayes makes. By adding 1 to these numbers, you completely change their scale for classification and hence your estimates mess up. I am assuming, you have a good idea about how Multinomial Naive Bayes works. If not, then definitely see this video. The video and my answer will be sufficient to understand what is going wrong over here. 

You should either use a small value of alpha in TF-IDF case or you should build TF-IDF weights after doing smoothing on the raw counts. Also on a secondary note, please use cross-validation to get any accuracy estimates. By testing the model on a sample of the training data, your accuracy numbers will be extremely biased. I would recommend using cross-validation or a separate hold-out set to evaluate your model.

Hope that helps.
",,,false,
https://stackoverflow.com/questions/35232804,false,"The issue does not involve a problematic API. The discrepancy in the number of classes between classifier.classes_, supp, and np.unique(y_test) is likely due to the specific data or the classifier's internal representation of classes, rather than an issue with the API itself.",,,,,,,Scikit-learn: precision_recall_fscore_support returns strange results,"I am doing some text minining/classification and attempt to evaluate performance with the precision_recall_fscore_support function from the sklearn.metrics module. I am not sure how I can create a really small example reproducing the problem, but maybe somebody can help even so because it is something obvious I am missing. 

The aforementioned function returns among other things the support for each class. The documentation states


  support: int (if average is not None) or array of int, shape = [n_unique_labels] :
  The number of occurrences of each label in y_true.


But in my case, the number of classes for which support is returned is not the same as the number of different classes in the testing data.     

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)

classifier = svm.SVC(kernel=""linear"")
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
prec, rec, fbeta, supp = precision_recall_fscore_support(y_test, y_pred)

print(len(classifier.classes_)) # prints 18
print(len(supp))                # prints 19
print(len(np.unique(y_test)))   # prints 18


How can this be? How can there be support for a class which is not in the data?
","I am not sure what the problem is, but in my case there seems to be a mismatch between the classes learned by the classifier and the ones occurring in the test data. One can force the the function to compute the performance measures for the right classes by explicitly naming them.

prec, rec, fbeta, supp = precision_recall_fscore_support(y_test, y_pred, labels=classifier.classes_)

",,,false,
https://stackoverflow.com/questions/54166529,false,"The issue does not involve a problematic API. The unexpected prediction results are likely due to the specific data or the nature of the relationship between the features and targets, rather than an issue with the API itself.",,,,,,,How to fix strange prediction results in scikit-learn,"I have a simple example in scikit-learn for prediction. Here is my data file (data.csv):

first second third target1 target2
 800   400    240    400     25
 400   200    120    200     50
 200   100    60     100     100
 100   50     30     50      200
 50    25     15     25      400


The features include : first ,second and third 

The targets include : target1 and target2

Now I want to provide new values for the features and predict target1 and target2. As you can see, there is a trend between the values of features and the values of the target1 and target2:

By doubling the features values the value of target1 also doubles while the value of target2 cuts in half 

For example if I provide the values : 1600 , 800 and 480 for the first, second and the third features, I expect to get 800 and 12.5 for the target1 and target2 respectively. Here is the code:

import pandas as pd
from sklearn.model_selection import train_test_split
from collections import *
from sklearn.linear_model import LinearRegression

features = pd.read_csv('data.csv')

features.head()
features_name = ['first' , 'second' , 'third']
target_name = ['target1','target2']

X = features[features_name]
y = features[target_name]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.155, random_state = 42)

linear_regression_model = LinearRegression()
linear_regression_model.fit(X_train,y_train)

new_data  = OrderedDict([('first',1600) ,('second',800),('third',480) ])
new_data = pd.Series(new_data).values.reshape(1,-1)
ss = linear_regression_model.predict(new_data)

print (ss)


Here is the output:

[[ 800.         -284.58549223]]


As you can see the predicted value for the target1 is exactly what is expected while the predicted value for the target2 (-284.58) is far away from what is expected (12.5)

Does anybody know what is wrong here and how I can fix this issue? Thanks in advance for your help. 
","Edit: there is no linear correlation between your inputs (any of them, they are essentially all the same) and your target2. It seems to be an exponential decay. Try transforming your feature to another feature (1/exp(x)) and then you can do a linear regression on that feature

Edit2: my mistake, it is simply 1/x

Also.. (original post below)
I'm not an expert, but it appears that your input features are not only highly correlated, they are actually perfectly correlated (linear correlation that is) - this generally makes regression breaks.
The intuitive reason is that because they are correlated, there exists an infinite amount of possible coefficients, where they counter one another, and the prediction would still be exactly the same (imagine in your case first-second is exactly the same as 2first-3second or 0first-1second etc).
I also think this causes the normal equation of linear regression to generate a matrix that cannot be inverted..
",,,false,
https://stackoverflow.com/questions/57484399,false,"The issue does not involve a problematic API. It is related to a DLL load error and a potential deletion of the mkl_rt.dll file, which is not directly related to the behavior of the scikit-learn or scipy libraries.",,,,,,,Issue importing scikit-learn: module &#39;scipy&#39; has no attribute &#39;_lib&#39;,"I'm new to Python and am using Anaconda on Windows 10 to learn how to implement machine learning. Running this code on Spyder:

import sklearn as skl


Originally got me this:

Traceback (most recent call last):

  File ""&lt;ipython-input-1-7135d3f24347&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/julia/.spyder-py3/temp.py', wdir='C:/Users/julia/.spyder-py3')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/julia/.spyder-py3/temp.py"", line 3, in &lt;module&gt;
    from sklearn.family import Model

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\__init__.py"", line 76, in &lt;module&gt;
    from .base import clone

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\base.py"", line 16, in &lt;module&gt;
    from .utils import _IS_32BIT

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\__init__.py"", line 20, in &lt;module&gt;
    from .validation import (as_float_array,

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py"", line 21, in &lt;module&gt;
    from .fixes import _object_dtype_isnan

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\fixes.py"", line 289, in &lt;module&gt;
    from scipy.sparse.linalg import lsqr as sparse_lsqr

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\__init__.py"", line 114, in &lt;module&gt;
    from .isolve import *

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\isolve\__init__.py"", line 6, in &lt;module&gt;
    from .iterative import *

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\isolve\iterative.py"", line 10, in &lt;module&gt;
    from . import _iterative

ImportError: DLL load failed: The specified module could not be found.


I then went to the command line and did

pip uninstall scipy
pip install scipy

pip uninstall scikit-learn
pip install scikit-learn


and got no errors when doing so, with scipy 1.3.1 (along with numpy 1.17.0) and scikit-learn 0.21.3 being installed according to the command line.

However, now when I try to import sklearn I get a different error:

 File ""&lt;ipython-input-2-7135d3f24347&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/julia/.spyder-py3/temp.py', wdir='C:/Users/julia/.spyder-py3')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/julia/.spyder-py3/temp.py"", line 3, in &lt;module&gt;
    from sklearn.family import Model

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\__init__.py"", line 76, in &lt;module&gt;
    from .base import clone

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\base.py"", line 16, in &lt;module&gt;
    from .utils import _IS_32BIT

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\__init__.py"", line 20, in &lt;module&gt;
    from .validation import (as_float_array,

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py"", line 21, in &lt;module&gt;
    from .fixes import _object_dtype_isnan

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\fixes.py"", line 289, in &lt;module&gt;
    from scipy.sparse.linalg import lsqr as sparse_lsqr

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\__init__.py"", line 113, in &lt;module&gt;
    from .isolve import *

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\isolve\__init__.py"", line 6, in &lt;module&gt;
    from .iterative import *

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\isolve\iterative.py"", line 136, in &lt;module&gt;
    def bicg(A, b, x0=None, tol=1e-5, maxiter=None, M=None, callback=None, atol=None):

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\_lib\_threadsafety.py"", line 59, in decorator
    return lock.decorate(func)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\_lib\_threadsafety.py"", line 47, in decorate
    return scipy._lib.decorator.decorate(func, caller)

AttributeError: module 'scipy' has no attribute '_lib'


Any suggestions? I've uninstalled and reinstalled Anaconda and I'm still getting the same issue.

EDIT: When I do 

conda list --show-channel-urls


I get

# packages in environment at C:\ProgramData\Anaconda3:
#
# Name                    Version                   Build  Channel
_ipyw_jlab_nb_ext_conf    0.1.0                    py37_0    defaults
alabaster                 0.7.12                   py37_0    defaults
anaconda-client           1.7.2                    py37_0    defaults
anaconda-navigator        1.9.7                    py37_0    defaults
asn1crypto                0.24.0                   py37_0    defaults
astroid                   2.2.5                    py37_0    defaults
attrs                     19.1.0                   py37_1    defaults
babel                     2.7.0                      py_0    defaults
backcall                  0.1.0                    py37_0    defaults
backports                 1.0                        py_2    defaults
backports.functools_lru_cache 1.5                        py_2    defaults
backports.tempfile        1.0                        py_1    defaults
backports.weakref         1.0.post1                  py_1    defaults
beautifulsoup4            4.7.1                    py37_1    defaults
blas                      1.0                         mkl    defaults
bleach                    3.1.0                    py37_0    defaults
bzip2                     1.0.8                he774522_0    defaults
ca-certificates           2019.5.15                     1    defaults
certifi                   2019.6.16                py37_1    defaults
cffi                      1.12.3           py37h7a1dbc1_0    defaults
chardet                   3.0.4                 py37_1003    defaults
click                     7.0                      py37_0    defaults
cloudpickle               1.2.1                      py_0    defaults
clyent                    1.2.2                    py37_1    defaults
colorama                  0.4.1                    py37_0    defaults
conda                     4.7.11                   py37_0    defaults
conda-build               3.18.8                   py37_0    defaults
conda-env                 2.6.0                         1    defaults
conda-package-handling    1.3.11                   py37_0    defaults
conda-verify              3.4.2                      py_1    defaults
console_shortcut          0.1.1                         3    defaults
cryptography              2.7              py37h7a1dbc1_0    defaults
decorator                 4.4.0                    py37_1    defaults
defusedxml                0.6.0                      py_0    defaults
docutils                  0.15.1                   py37_0    defaults
entrypoints               0.3                      py37_0    defaults
filelock                  3.0.12                     py_0    defaults
freetype                  2.9.1                ha9979f8_1    defaults
future                    0.17.1                   py37_0    defaults
glob2                     0.7                        py_0    defaults
icc_rt                    2019.0.0             h0cc432a_1    defaults
icu                       58.2                 ha66f8fd_1    defaults
idna                      2.8                      py37_0    defaults
imagesize                 1.1.0                    py37_0    defaults
intel-openmp              2019.4                      245    defaults
ipykernel                 5.1.1            py37h39e3cac_0    defaults
ipython                   7.7.0            py37h39e3cac_0    defaults
ipython_genutils          0.2.0                    py37_0    defaults
ipywidgets                7.5.1                      py_0    defaults
isort                     4.3.21                   py37_0    defaults
jedi                      0.13.3                   py37_0    defaults
jinja2                    2.10.1                   py37_0    defaults
joblib                    0.13.2                   py37_0    defaults
jpeg                      9b                   hb83a4c4_2    defaults
json5                     0.8.5                      py_0    defaults
jsonschema                3.0.1                    py37_0    defaults
jupyter_client            5.3.1                      py_0    defaults
jupyter_core              4.5.0                      py_0    defaults
jupyterlab                1.0.2            py37hf63ae98_0    defaults
jupyterlab_server         1.0.0                      py_1    defaults
keyring                   18.0.0                   py37_0    defaults
lazy-object-proxy         1.4.1            py37he774522_0    defaults
libarchive                3.3.3                h0643e63_5    defaults
libiconv                  1.15                 h1df5818_7    defaults
liblief                   0.9.0                ha925a31_2    defaults
libpng                    1.6.37               h2a8f88b_0    defaults
libsodium                 1.0.16               h9d3ae62_0    defaults
libtiff                   4.0.10               hb898794_2    defaults
libxml2                   2.9.9                h464c3ec_0    defaults
lz4-c                     1.8.1.2              h2fa13f4_0    defaults
lzo                       2.10                 h6df0209_2    defaults
m2w64-gcc-libgfortran     5.3.0                         6    defaults
m2w64-gcc-libs            5.3.0                         7    defaults
m2w64-gcc-libs-core       5.3.0                         7    defaults
m2w64-gmp                 6.1.0                         2    defaults
m2w64-libwinpthread-git   5.0.0.4634.697f757               2    defaults
markupsafe                1.1.1            py37he774522_0    defaults
mccabe                    0.6.1                    py37_1    defaults
menuinst                  1.4.16           py37he774522_0    defaults
mistune                   0.8.4            py37he774522_0    defaults
mkl                       2019.4                      245    defaults
mkl-service               2.0.2            py37he774522_0    defaults
mkl_fft                   1.0.12           py37h14836fe_0    defaults
mkl_random                1.0.2            py37h343c172_0    defaults
msys2-conda-epoch         20160418                      1    defaults
navigator-updater         0.2.1                    py37_0    defaults
nbconvert                 5.5.0                      py_0    defaults
nbformat                  4.4.0                    py37_0    defaults
notebook                  6.0.0                    py37_0    defaults
numpy                     1.17.0                   pypi_0    pypi
numpy-base                1.16.4           py37hc3f5095_0    defaults
numpydoc                  0.9.1                      py_0    defaults
olefile                   0.46                     py37_0    defaults
openssl                   1.1.1c               he774522_1    defaults
packaging                 19.0                     py37_0    defaults
pandas                    0.25.0           py37ha925a31_0    defaults
pandoc                    2.2.3.2                       0    defaults
pandocfilters             1.4.2                    py37_1    defaults
parso                     0.5.0                      py_0    defaults
pickleshare               0.7.5                    py37_0    defaults
pillow                    6.1.0            py37hdc69c19_0    defaults
pip                       19.2.2                   pypi_0    pypi
pkginfo                   1.5.0.1                  py37_0    defaults
powershell_shortcut       0.0.1                         2    defaults
prometheus_client         0.7.1                      py_0    defaults
prompt_toolkit            2.0.9                    py37_0    defaults
psutil                    5.6.3            py37he774522_0    defaults
py-lief                   0.9.0            py37ha925a31_2    defaults
pycodestyle               2.5.0                    py37_0    defaults
pycosat                   0.6.3            py37hfa6e2cd_0    defaults
pycparser                 2.19                     py37_0    defaults
pyflakes                  2.1.1                    py37_0    defaults
pygments                  2.4.2                      py_0    defaults
pylint                    2.3.1                    py37_0    defaults
pyopenssl                 19.0.0                   py37_0    defaults
pyparsing                 2.4.0                      py_0    defaults
pyqt                      5.9.2            py37h6538335_2    defaults
pyrsistent                0.14.11          py37he774522_0    defaults
pysocks                   1.7.0                    py37_0    defaults
python                    3.7.3                h8c8aaf0_1    defaults
python-dateutil           2.8.0                    py37_0    defaults
python-libarchive-c       2.8                     py37_13    defaults
pytz                      2019.1                     py_0    defaults
pywin32                   223              py37hfa6e2cd_1    defaults
pywinpty                  0.5.5                 py37_1000    defaults
pyyaml                    5.1.1            py37he774522_0    defaults
pyzmq                     18.0.0           py37ha925a31_0    defaults
qt                        5.9.7            vc14h73c81de_0    defaults
qtawesome                 0.5.7                    py37_1    defaults
qtconsole                 4.5.2                      py_0    defaults
qtpy                      1.8.0                      py_0    defaults
requests                  2.22.0                   py37_0    defaults
rope                      0.14.0                     py_0    defaults
ruamel_yaml               0.15.46          py37hfa6e2cd_0    defaults
scikit-learn              0.21.3                   pypi_0    pypi
scipy                     1.3.0                    pypi_0    pypi
send2trash                1.5.0                    py37_0    defaults
setuptools                41.0.1                   py37_0    defaults
sip                       4.19.8           py37h6538335_0    defaults
six                       1.12.0                   py37_0    defaults
snowballstemmer           1.9.0                      py_0    defaults
soupsieve                 1.9.2                    py37_0    defaults
sphinx                    2.1.2                      py_0    defaults
sphinxcontrib-applehelp   1.0.1                      py_0    defaults
sphinxcontrib-devhelp     1.0.1                      py_0    defaults
sphinxcontrib-htmlhelp    1.0.2                      py_0    defaults
sphinxcontrib-jsmath      1.0.1                      py_0    defaults
sphinxcontrib-qthelp      1.0.2                      py_0    defaults
sphinxcontrib-serializinghtml 1.1.3                      py_0    defaults
spyder                    3.3.6                    py37_0    defaults
spyder-kernels            0.5.1                    py37_0    defaults
sqlite                    3.29.0               he774522_0    defaults
terminado                 0.8.2                    py37_0    defaults
testpath                  0.4.2                    py37_0    defaults
tk                        8.6.8                hfa6e2cd_0    defaults
tornado                   6.0.3            py37he774522_0    defaults
tqdm                      4.32.1                     py_0    defaults
traitlets                 4.3.2                    py37_0    defaults
urllib3                   1.24.2                   py37_0    defaults
vc                        14.1                 h0510ff6_4    defaults
vs2015_runtime            14.15.26706          h3a45250_4    defaults
wcwidth                   0.1.7                    py37_0    defaults
webencodings              0.5.1                    py37_1    defaults
wheel                     0.33.4                   py37_0    defaults
widgetsnbextension        3.5.0                    py37_0    defaults
win_inet_pton             1.1.0                    py37_0    defaults
wincertstore              0.2                      py37_0    defaults
winpty                    0.4.3                         4    defaults
wrapt                     1.11.2           py37he774522_0    defaults
xz                        5.2.4                h2fa13f4_4    defaults
yaml                      0.1.7                hc54c509_2    defaults
zeromq                    4.3.1                h33f27b4_3    defaults
zlib                      1.2.11               h62dcd97_3    defaults
zstd                      1.3.7                h508b16e_0    defaults


with the version of scipy not matching up with the version that pip installed. Not sure how significant it is but it seemed strange to me.

EDIT 2:
Doing pip list returns

Package                       Version
----------------------------- ---------
-cipy                         1.3.0
alabaster                     0.7.12
anaconda-client               1.7.2
anaconda-navigator            1.9.7
asn1crypto                    0.24.0
astroid                       2.2.5
attrs                         19.1.0
Babel                         2.7.0
backcall                      0.1.0
backports.functools-lru-cache 1.5
backports.tempfile            1.0
backports.weakref             1.0.post1
beautifulsoup4                4.7.1
bleach                        3.1.0
certifi                       2019.6.16
cffi                          1.12.3
chardet                       3.0.4
Click                         7.0
cloudpickle                   1.2.1
clyent                        1.2.2
colorama                      0.4.1
conda                         4.7.11
conda-build                   3.18.8
conda-package-handling        1.3.11
conda-verify                  3.4.2
cryptography                  2.7
decorator                     4.4.0
defusedxml                    0.6.0
docutils                      0.15.1
entrypoints                   0.3
filelock                      3.0.12
future                        0.17.1
glob2                         0.7
idna                          2.8
imagesize                     1.1.0
ipykernel                     5.1.1
ipython                       7.7.0
ipython-genutils              0.2.0
ipywidgets                    7.5.1
isort                         4.3.21
jedi                          0.13.3
Jinja2                        2.10.1
joblib                        0.13.2
json5                         0.8.5
jsonschema                    3.0.1
jupyter-client                5.3.1
jupyter-core                  4.5.0
jupyterlab                    1.0.2
jupyterlab-server             1.0.0
keyring                       18.0.0
lazy-object-proxy             1.4.1
libarchive-c                  2.8
MarkupSafe                    1.1.1
mccabe                        0.6.1
menuinst                      1.4.16
mistune                       0.8.4
mkl-fft                       1.0.12
mkl-random                    1.0.2
mkl-service                   2.0.2
navigator-updater             0.2.1
nbconvert                     5.5.0
nbformat                      4.4.0
notebook                      6.0.0
numpy                         1.17.0
numpydoc                      0.9.1
olefile                       0.46
packaging                     19.0
pandas                        0.25.0
pandocfilters                 1.4.2
parso                         0.5.0
pickleshare                   0.7.5
Pillow                        6.1.0
pio                           0.0.3
pip                           19.2.2
pkginfo                       1.5.0.1
prometheus-client             0.7.1
prompt-toolkit                2.0.9
psutil                        5.6.3
pycodestyle                   2.5.0
pycosat                       0.6.3
pycparser                     2.19
pyflakes                      2.1.1
Pygments                      2.4.2
pylint                        2.3.1
pyOpenSSL                     19.0.0
pyparsing                     2.4.0
pyrsistent                    0.14.11
PySocks                       1.7.0
python-dateutil               2.8.0
pytz                          2019.1
pywin32                       223
pywinpty                      0.5.5
PyYAML                        5.1.1
pyzmq                         18.0.0
QtAwesome                     0.5.7
qtconsole                     4.5.2
QtPy                          1.8.0
requests                      2.22.0
rope                          0.14.0
ruamel-yaml                   0.15.46
scikit-learn                  0.21.3
scipy                         1.3.1
Send2Trash                    1.5.0
setuptools                    41.0.1
six                           1.12.0
snowballstemmer               1.9.0
soupsieve                     1.9.2
Sphinx                        2.1.2
sphinxcontrib-applehelp       1.0.1
sphinxcontrib-devhelp         1.0.1
sphinxcontrib-htmlhelp        1.0.2
sphinxcontrib-jsmath          1.0.1
sphinxcontrib-qthelp          1.0.2
sphinxcontrib-serializinghtml 1.1.3
spyder                        3.3.6
spyder-kernels                0.5.1
terminado                     0.8.2
testpath                      0.4.2
tornado                       6.0.3
tqdm                          4.32.1
traitlets                     4.3.2
urllib3                       1.24.2
wcwidth                       0.1.7
webencodings                  0.5.1
wheel                         0.33.4
widgetsnbextension            3.5.0
win-inet-pton                 1.1.0
wincertstore                  0.2
wrapt                         1.11.2


pip list says scipy is version 1.3.1, while conda list says it's version 1.3.0. Again, not sure how relevant it is, but seems strange

EDIT 3: I got this error after putting the following lines (suggested by @Brennan) in my command prompt then running the file

pip uninstall scikit-learn
pip uninstall scipy
conda uninstall scikit-learn
conda uninstall scipy

conda update --all
conda install scipy
conda install scikit-learn


This is the new error I get when trying to import sklearn:

Traceback (most recent call last):

  File ""&lt;ipython-input-15-7135d3f24347&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/julia/.spyder-py3/temp.py', wdir='C:/Users/julia/.spyder-py3')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/julia/.spyder-py3/temp.py"", line 2, in &lt;module&gt;
    import sklearn as skl

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\__init__.py"", line 76, in &lt;module&gt;
    from .base import clone

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\base.py"", line 13, in &lt;module&gt;
    import numpy as np

  File ""C:\ProgramData\Anaconda3\lib\site-packages\numpy\__init__.py"", line 140, in &lt;module&gt;
    from . import _distributor_init

  File ""C:\ProgramData\Anaconda3\lib\site-packages\numpy\_distributor_init.py"", line 34, in &lt;module&gt;
    from . import _mklinit

ImportError: DLL load failed: The specified module could not be found.


A possible cause of this might be me deleting the mkl_rt.dll file from my Anaconda/Library/bin after encountering the error described here: https://github.com/ContinuumIO/anaconda-issues/issues/10182

This puts me in a predicament, because reinstalling Anaconda to repair this will get me the same ""ordinal 242 could not be located"" error that I faced earlier, but not repairing it will continue the issue with sklearn...

FINAL EDIT: Solved by installing old version of Anaconda. Will mark as solved when I am able to (2 days)
","I encountered the same error after letting my PC sit for 4 days unattended. Restarting the kernel solved it.
This probably won't work for everyone, but it might save someone a little agony.
","I ended up fixing this by uninstalling my current version of Anaconda and installing a version from a few months ago. I didn't get the ""ordinal 242"" error nor the issues with scikit-learn.
","had a similar problem on google collaboratory, simply uninstalling and reinstalling the scipy module alone solved it for me

and the simple fact that collab ai suggested this move emboldened me
",false,
https://stackoverflow.com/questions/73491958,true,"The issue involves a problematic API, specifically PyCaret, which exhibits unexpected behavior during the setup() process under certain conditions.",PyCaret,setup(),"During the setup() process, PyCaret encounters multiple errors, including a ValueError related to the random_state parameter and an AttributeError related to the fill_value_categorical attribute of the Simple_Imputer object.","The expected behavior of PyCaret's setup() function is to preprocess the data without errors. However, in this case, the API fails at various steps, indicating a problem with the setup process.",The issue is triggered when using PyCaret version 2.2.2 in a specific runtime environment (new desktop with Jupyter) and encountering errors related to random_state and Simple_Imputer.,"This issue might be challenging to detect during development and testing due to the specific combination of PyCaret version, runtime environment, and dependencies that lead to the unexpected behavior.",Installation always stuck on PyCaret 2.2.2 + Package problems,"I'm stuck on an issue that I can't seem to solve. I was fine using PyCaret on my other PC and had recently got a new desktop.
I was working on one dataset on my old PC and had no problems with setup() and PyCaret preprocessed my data without any issues. When I worked on my the same dataset with my new desktop and Jupyter newly installed, I noticed I ran into an ValueError: Setting a random_state has no effect since shuffle is False. You should leave random_state to its default (None), or set shuffle=True. I thought it was strange but went on to set fold_shuffle=True to get through this.
Next, I encountered AttributeError: 'Simple_Imputer' object has no attribute 'fill_value_categorical'. It seems I'm getting failures at every step of setup(). I went through the forums and found a thread where at the bottom of it, @eddygeek mentioned that PyCaret was set up to fail if the sklearn version is wrong. This got me looking into the packages I have that may meet dependencies between packages.
I noticed the following issues:

I get several errors:
ERROR: Command errored out with exit status 1: C:\Users\%%USER%%\anaconda3\python.exe'
Ignoring numpy: markers 'python_version &gt;= ""3.8"" and platform_system == ""AIX""' don't match your environment
ERROR: Could not find a version that satisfies the requirement scikit-learn==0.23.2
Screenshot of more errors attached

Jupyter Notebook fails to launch because of Pandas Profiling Import Error: cannot import name 'soft_unicode' from 'markupsafe'. I got around this by installing markupsafe===2.0.1 but this leads to incompatibility warning by pandas-profiling 3.2.0 saying it needs markupsafe 2.1.1

PyCaret keeps getting installed as 2.2.2 version. I think that's why it keeps looking for scikit-learn 0.23.2 when the latest PyCaret 2.3.10 works with scikit-learn &gt;=1.0. I've tried uninstalling and reinstalling PyCaret several times but it's still the same.


What I've done
I'm on Python 3.9.12 that was installed together with Anaconda3. My PyCaret was installed with pip install pycaret[full] --user on Anaconda Prompt.
In my pip list, I have:

scikit-learn 1.1.2
markupsafe 2.1.1
pandas-profiling 3.2.0
pycaret 2.2.2

I've added C:\Users\%%USER%%\AppData\Roaming\Python\Python39\Scripts to PATH
I'm really at my wits end so I hope I can get some advice on this. Thank you.
","You are using a very old version of pycaret which does not work in Python 3.9. Please install the latest version in a fresh (conda) environment. Make sure it is a new environment in order to avoid any package issues.
# This installs the pre-release 3.0.0 release which has reduced dependencies.
pip install --pre pycaret

","I've encountered the very same issues and solved as follows.
According to the documentation, there are a few problems with your setup:

PyCaret is not yet compatible with sklearn&gt;=0.23.2

PyCaret is tested and supported on the following 64-bit systems:
Python 3.6  3.8
Python 3.9 for Ubuntu only


So if you're using python 3.9 on Windows, I'd start with that.
I went into a rabbit hole of downgrading the packages and getting one error after another.
Long story short, the setup that finally worked was:
sklearn 0.23.1
scipy 1.5.2

Both installed on a virtual conda environment but at the end I had to run:
pip3 install pycaret[full]

Notice pip3 intead of pip because I was getting permission errors.
",,false,
https://stackoverflow.com/questions/44490337,true,"The issue involves conflicting imports between the Sklearn and Pymesh libraries, resulting in unexpected behavior during import statements.",Sklearn and Pymesh,import statements,"When importing Sklearn and Pymesh in different orders, one of the libraries fails to import correctly. The issue is likely caused by conflicting dependencies or naming conflicts between the two libraries.","The expected behavior is for both Sklearn and Pymesh to import successfully without conflicts. However, in this case, importing one library before the other leads to import errors.","The issue is triggered when importing Sklearn and Pymesh in a specific order, potentially due to conflicting dependencies or naming conflicts.","This issue might be challenging to detect during development and testing, as it depends on the specific order of import statements and potential conflicts between the libraries.",Import module works in Jupyter notebook but not in IDLE,"I dont understand what Im doing wrong. I re-installed my windows last week; after that I got python (3.6), and all the libraries I need, but when I try to import them in the IDLE returns an error (ModuleNotFoundError: No module named), and when I check in the command window with pip list or conda list, the package is already there. The strange thing is when I tried to import them from a Jupyter notebook worked without any problem. 

Im working with a laptop Windows 7, 64 bits. No previous version of python was installed before. I am having issues with scikit-learn, pandas-datareader and beautifulsoup4. I tried to install the packages with pip, conda and the wheel file. Nothing seems to work.  (since Im in my laptop, I only download miniconda, and Im installing the modules I need).

Any suggestions? The safety of my laptop is in your hands. :D 
","The reason is that your pip/conda installed library paths are not accessible by python IDLE.    You have to add those library paths to your environment variable(PATH). To do this open my computer &gt; properties &gt; advanced system settings &gt; system.

Under environment variables look for PATH and at the end add the location of installed libraries. Refer this for more details on how to add locations in path variable. Once you do these you will be able to import the libraries. In order to know what locations python searches for libraries you can use 

import sys 
print sys.path


This will give you a list of locations where python searches for libraries. Once you edit the PATH variable those locations will be reflected here.

Refer this also in order to know how to add python library path.

Note: The tutorial is a reference on how to edit PATH variable. I encourage you to find the location of installed libraries and follow the steps to edit the same.
",,,false,
https://stackoverflow.com/questions/37165344,false,The issue does not involve a problematic API but rather a question about retrieving specific values from a StratifiedKFold object in scikit-learn.,,,,,,,python - sklearn 0.17 and 0.18 - how to retrieve only the first value of an StratifiedKFold object?,"I have a python problem.
I use scikit-learn 0.17
Someone give me an example code that I have to run.
The code was in python 2, and scikit-learn 0.18, the dev version.
I transformed the code in python 3 without issue.
But he used the function :

sklearn.model_selection.StratifiedKFold(n_folds=3, shuffle=False, random_state=None)


In scikit-learn 0.17, I have function that looks like this one : 

sklearn.cross_validation.StratifiedKFold(y, n_folds=3, shuffle=False, random_state=None)


As you can see, they seem very similar : there is only the ""y"" that change.

So I have two solution : update to 0.18 (I'm using windows, and I can't use linux) I tried this one, but I don't succeed to compile sources.

The second solution is to use the StratifiedKFold of 0.17.
I succeed this solution. (I don't know if it doing the things it suppose to do, however x) ).

So, there is my problem (finally) :

I have to adapt this two line : 

skf = StratifiedKFold(n_folds=5)
train_idx, test_idx = skf.split(patch_arr, labels).next()


for the moment, I wrote this (cover your eyes):

skf = StratifiedKFold(labels, n_folds=5)
for train_idx, test_idx in skf:
    break 


And the program run, but my code is awful. Is there any other method to retrieve only the first couple of (train_idx, test_idx) ?
I tried with skf[0] and skf.next(), but none of this works.

(for your information, train_idx and test_idx are two arrays)

Have you got any idea ?
I find strange that I can't use patch_arr in my solution too, but if I can avoid to spend more hours trying to update scikit-image, I will be very happy :)
","The following should suffice.

train_idx, test_idx = next(iter(skf))

",,,false,
https://stackoverflow.com/questions/5941234,false,The issue does not involve a problematic API but rather a question about profiling SQL queries in SQLAlchemy.,,,,,,,Profiling SQL query,"I'm using sqlalchemy (expression language, not full ORM) with MySQL and experiencing some unexpected slowness.  Particularly, the time spent performing a select query in by sqlalchemy is ten times greater than the time spent performing the same query from the mysql command line.

Output from cprofile:

ncalls  tottime  percall  cumtime  percall filename:lineno(function)
100  206.703    2.067  206.703    2.067 {method 'query' of '_mysql.connection' objects}


MySQL time:  0.26 seconds

The consensus seems to be that there is some overhead using sqlalchemy, but not nearly this much.  Any suggestions as to what could cause behavior like this?

The queries are generally of the form:

SELECT fieldnames.minage, fieldnames.maxage, fieldnames.race,    
fieldnames.sex, sum( pop.population ) AS pop, pop.zip5
FROM pop
INNER JOIN fieldnames ON fieldnames.fieldname = pop.fieldname_id
WHERE fieldnames.race IN (""White alone"")
AND fieldnames.sex IN (""Female"")
AND fieldnames.maxage &gt;=101
AND fieldnames.minage &lt;=107
GROUP BY fieldnames.minage, fieldnames.maxage

","One possible reason for slowness - Does sql alchemy use prepared statements? If yes, then a reason why you may be experiencing a difference in performance is because the mysql optimizer has different information when creating the two query plans.

When you run the query from the command-line, the mysql optimizer has the complete query with all where clause values filled in (as you showed above3), thus can optimize explicitly for these values.

When you run from sql alchemy, the mysql optimizer may only see this (perhaps fieldnames.race and fieldnames.sex are parameterized as well):

SELECT fieldnames.minage, fieldnames.maxage, fieldnames.race,    
fieldnames.sex, sum( pop.population ) AS pop, pop.zip5
FROM pop
INNER JOIN fieldnames ON fieldnames.fieldname = pop.fieldname_id
WHERE fieldnames.race IN (""White alone"")
AND fieldnames.sex IN (""Female"")
AND fieldnames.maxage &gt;= ?
AND fieldnames.minage &lt;= ?
GROUP BY fieldnames.minage, fieldnames.maxage


Thus the optimizer has to make a guess on what values you might use then optimize around that. Unfortunately, it may make a bad guess, and thus in a worst case create a query plan that makes the query run significantly slower than you expect.
",,,false,
https://stackoverflow.com/questions/28901050,true,"The issue involves a problematic API, specifically SQLAlchemy, aiopg, and psycopg2, which exhibit unexpected behavior related to default values in PostgreSQL.","SQLAlchemy, aiopg, and psycopg2",Table definition and insert statement,"When defining a table with default values and executing an insert statement, the default value for a column is not applied, resulting in a violation of the not-null constraint.","The expected behavior is for the default value to be applied automatically when inserting a row without specifying a value for the column. However, in this case, the default value is not applied, leading to a violation of the not-null constraint.","The issue is triggered when using SQLAlchemy 0.9.8, PostgreSQL 9.3.5, psycopg2 2.5.4, and aiopg 0.5.1, specifically when defining a table with a default value and executing an insert statement without providing a value for the column.","This issue might be challenging to detect during development and testing, as it involves the interaction between multiple libraries (SQLAlchemy, aiopg, and psycopg2) and the specific versions used.",Default value doesn&#39;t work in SQLAlchemy + PostgreSQL + aiopg + psycopg2,"I've found an unexpected behavior in SQLAlchemy. I'm using the following versions:


SQLAlchemy (0.9.8)
PostgreSQL (9.3.5)
psycopg2 (2.5.4)
aiopg (0.5.1)


This is the table definition for the example:

import asyncio
from aiopg.sa import create_engine
from sqlalchemy import (
  MetaData,
  Column,
  Integer,
  Table,
  String,
)
metadata = MetaData()

users = Table('users', metadata,
  Column('id_user', Integer, primary_key=True, nullable=False),
  Column('name', String(20), unique=True),
  Column('age', Integer, nullable=False, default=0),
)


Now if I try to execute a simple insert to the table just populating the id_user and name, the column age should be auto-generated right? Lets see...

@asyncio.coroutine
def go():
  engine = yield from create_engine('postgresql://USER@localhost/DB')
  data = {'id_user':1, 'name':'Jimmy' }
  stmt = users.insert(values=data, inline=False)
  with (yield from engine) as conn:
    result = yield from conn.execute(stmt)


loop = asyncio.get_event_loop()
loop.run_until_complete(go())


This is the resulting statement with the corresponding error:

INSERT INTO users (id_user, name, age) VALUES (1, 'Jimmy', null);

psycopg2.IntegrityError: null value in column ""age"" violates not-null constraint


I didn't provide the age column, so where is that age = null value coming from? I was expecting something like this:

INSERT INTO users (id_user, name) VALUES (1, 'Jimmy');


Or if the default flag actually works should be:

INSERT INTO users (id_user, name, Age) VALUES (1, 'Jimmy', 0);


Could you put some light on this?
","This issue has been confirmed has an aiopg bug. Seems like at the moment it's ignoring the default argument on data manipulation. 

I've fixed the issue using server_default instead:

users = Table('users', metadata,
          Column('id_user', Integer, primary_key=True, nullable=False),
          Column('name', String(20), unique=True),
          Column('age', Integer, nullable=False, server_default='0'))

","I think you need to use inline=True in your insert. This turns off 'pre-execution'.
Docs are a bit cryptic on what exactly this 'pre-execution' entails, but they mentions default parameters:

    :param inline:
      if True, SQL defaults present on :class:`.Column` objects via
      the ``default`` keyword will be compiled 'inline' into the statement
      and not pre-executed.  This means that their values will not
      be available in the dictionary returned from
      :meth:`.ResultProxy.last_updated_params`.


This piece of docstring is from Update class, but they have a shared behavior with Insert. 

Besides, that's the only way they test it: 
https://github.com/zzzeek/sqlalchemy/blob/rel_0_9/test/sql/test_insert.py#L385
",,false,
https://stackoverflow.com/questions/17959320,false,"The issue does not involve a problematic API. It is related to the behavior of Flask-Login and Flask-Principal libraries, and the unexpected behavior is likely due to a misconfiguration or misuse of these libraries.",,,,,,,Flask-Login &amp; Flask-Principle authenticated user drops to flask_login.AnonymousUserMixin,"I'm getting two problems


My authenticated user constantly drops to flask_login.AnonymousUserMixin
I get unexpected signalling using Flask-Login &amp; Flask-Principal


Trying to get /projects/10 URL which is protected with
@admin_permission.require(http_exception=403)

This is my console output:

127.0.0.1 - - [30/Jul/2013 16:22:58] ""GET /projects/10 HTTP/1.1"" 302 -
127.0.0.1 - - [30/Jul/2013 16:22:58] ""GET /login HTTP/1.1"" 200 -


Getting on login form (All good so far). Typing valid login &amp; password and getting crazy signalling and behavior is not what I expect:

127.0.0.1 - - [30/Jul/2013 16:24:06] ""POST /login HTTP/1.1"" 302 -
&lt;Employee('103','Dmitry Semenov')&gt;
&lt;Identity id=""103"" auth_type=""None"" provides=set([Need(method='role', value='manager'), Need(method='id', value=103L), Need(method='role', value='admin')])&gt;
&lt;flask_login.AnonymousUserMixin object at 0x03258790&gt;
&lt;Identity id=""103"" auth_type=""None"" provides=set([])&gt;
127.0.0.1 - - [30/Jul/2013 16:24:06] ""GET /projects/10 HTTP/1.1"" 302 -
&lt;flask_login.AnonymousUserMixin object at 0x03342AF0&gt;
&lt;Identity id=""103"" auth_type=""None"" provides=set([])&gt;
127.0.0.1 - - [30/Jul/2013 16:24:06] ""GET /login HTTP/1.1"" 200 -
&lt;flask_login.AnonymousUserMixin object at 0x03342E90&gt;
&lt;Identity id=""103"" auth_type=""None"" provides=set([])&gt;


As you see I got current_user pointing to valid Employee instance (class) and identity id=103, but then immediately it becamse flask_login.AnonymousUserMixin for some reason and then the auth system passes that user and don't allow me to open /projects/10 URL.

Any Ideas what is wrong? And Why I'm getting that many signals - according to the code they should happen only on successfull login. What do I miss?

Source code:

# flask-principal
principals = Principal()
normal_role = RoleNeed('normal')
normal_permission = Permission(normal_role)
admin_permission  = Permission(RoleNeed('admin'))
principals._init_app(app)

login_manager    = LoginManager()
login_manager.init_app(app)

@login_manager.user_loader
def load_user(userid):
    return mysqlsess.query(Employee).get(userid)


@app.route(""/"")
@app.route(""/dashboard"")
def vDashboard():
    return render_template('dashboard.html')

@app.route('/projects')
def vPojects():
    return ""Projects""


@app.route('/projects/&lt;ID&gt;')
@admin_permission.require(http_exception=403)
def vProject(ID):
    return current_user.roles[1]

# somewhere to login    
@app.route('/login', methods=['GET', 'POST'])
def login():
    # A hypothetical login form that uses Flask-WTF
    form = LoginForm()

    # Validate form input
    if form.validate_on_submit():
        # Retrieve the user from the hypothetical datastore
        user = mysqlsess.query(Employee).get(form.email.data)

        # Compare passwords (use password hashing production)
        if form.password.data == str(user.ID):
            # Keep the user info in the session using Flask-Login
            login_user(user)
            # Tell Flask-Principal the identity changed
            identity_changed.send(app,
                                  identity=Identity(user.ID))
            return redirect(session['redirected_from'] or '/')
        else:
            return abort(401)

    return render_template('login.html', form=form)


# somewhere to logout
@app.route(""/logout"")
def logout():
    logout_user()

    for key in ['identity.name', 'identity.auth_type', 'redirected_from']:
        try:
            del session[key]
        except:
            pass
    return Response('&lt;p&gt;Logged out&lt;/p&gt;')


# handle login failed
@app.errorhandler(401)
def page_not_found(e):
    return Response('&lt;p&gt;Login failed&lt;/p&gt;')


@app.errorhandler(403)
def page_not_found(e):
    session['redirected_from'] = request.url
    return redirect(url_for('login'))


@identity_loaded.connect_via(app)
def on_identity_loaded(sender, identity):
        identity.user = current_user
        print identity.user


        if hasattr(current_user, 'ID'):
            identity.provides.add(UserNeed(current_user.ID))

        if hasattr(current_user, 'roles'):
            for role in current_user.roles:
                identity.provides.add(RoleNeed(role))

        print identity


class LoginForm(Form):
    email = TextField()
    password = PasswordField()

if __name__ == ""__main__"":
    app.run()


And my Employee SQLAlchemy class

class Employee(Base):

__tablename__  = ""Employees""

# Properties
ID           = Column(BigInteger,   primary_key=True)
name         = Column(VARCHAR(255), nullable=False)
created      = Column(DateTime,     nullable=False, default=datetime.now())
updated      = Column(DateTime)
deleted      = Column(DateTime)
branchID     = Column(BigInteger,   ForeignKey('Branches.ID'),    nullable=False)
departmentID = Column(BigInteger,   ForeignKey('Departments.ID'), nullable=False)
utilization  = Column(SmallInteger, nullable=False, default=1)
statusID     = Column(Enum('active', 'fired', 'vacation'), default='active')
birthday     = Column(Date)

# Relationships
Branch       = relationship(""Branch"")
Department   = relationship(""Department"")
ProjectStat  = relationship(""ProjectStat"",  lazy=""dynamic"")

roles        = [""admin"", ""manager""]

# Methods
def zzz(self):
    session = object_session(self)

    stats = self.ProjectStat.filter(and_(ProjectStat.metricID=='hb', ProjectStat.metricValue&gt;=6)).all()
    for s in stats:
        print s.metricValue

# Constructor
def __init__(self, ID, name):
    self.ID   = ID
    self.name = name

# Friendly Print
def __repr__(self):
    return ""&lt;Employee('%s','%s')&gt;"" % (self.ID, self.name)

def is_active(self):
    return True

def get_id(self):
    return unicode(self.ID)

def is_authenticated(self):
    return True

def is_anonymous(self):
    return False

","You need to instantiate principle after login.

This is a repeat question, see here Flask Login and Principal - current_user is Anonymous even though I'm logged in
",,,false,
https://stackoverflow.com/questions/48462859,true,"The issue involves an unexpected behavior in SQLAlchemy when selecting from SQLite tables with datetimes stored as NUMERIC data type. The API in question is SQLAlchemy's reflection mechanism, which fails to handle the conversion of NUMERIC values to Python objects correctly.",,,,,,,"SQLAlchemy selects give different results on SQLite table, raw sql versus selectable","While reading SQLite tables using pandas and dask, I came across some unexpected behavior of SQLAlchemy when selecting from SQLite tables with datetimes (ISO formatted strings) stored as NUMERIC data type. An SQLAlchemy raw SQL query works fine, but a query using a selectable constructed from reflection fails. The two queries appear to be equivalent.

I have pasted an example below, along with the traceback. Can someone explain what is wrong with the third query in the example?

Set up table with NUMERIC datetime:

import sqlalchemy as sa
from sqlalchemy import text

connString = ""sqlite:///c:\\temp\\test.db""
engine = sa.create_engine(connString)
conn = engine.connect()
conn.execute(""create table testtable (uid INTEGER Primary Key, datetime NUMERIC)"")
conn.execute(""insert into testtable values (1, '2017-08-03 01:11:31')"")
print(conn.execute('PRAGMA table_info(testtable)').fetchall())
# [(0, 'uid', 'INTEGER', 0, None, 1), (1, 'datetime', 'NUMERIC', 0, None, 0)]


Query with raw SQL works:

resultList1 = conn.execute(""SELECT testtable.uid, testtable.datetime \nFROM testtable"").fetchall()
print(resultList1)
# [(1, '2017-08-03 01:11:31')]


Query with this selectable works:

resultList2 = conn.execute(sa.sql.select(columns=[text('uid'),text('datetime')]).select_from(text('testtable'))).fetchall() 
print(resultList2)
# [(1, '2017-08-03 01:11:31')]


Query with this selectable fails:

m = sa.MetaData()
table = sa.Table('testtable', m, autoload=True, autoload_with=engine)
selectble = sa.sql.select(table.columns).select_from(table)
print(selectble.compile().string)
#  note: same raw sql query as above
# ""SELECT testtable.uid, testtable.datetime \nFROM testtable""

resultList3 = conn.execute(sa.sql.select(table.columns).select_from(table)).fetchall()
# SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively...
print(resultList3)

conn.close()


The error:

Traceback (most recent call last):

  File ""&lt;ipython-input-20-188c84a35d95&gt;"", line 1, in &lt;module&gt;
    print(resultList3)

  File ""c:\program files\python36\lib\site-packages\sqlalchemy\engine\result.py"", line 156, in __repr__
    return repr(sql_util._repr_row(self))

  File ""c:\program files\python36\lib\site-packages\sqlalchemy\sql\util.py"", line 329, in __repr__
    "", "".join(trunc(value) for value in self.row),

TypeError: must be real number, not str

","SQLite has a very different type system from most SQL databases: it uses dynamic typing, and after conversion the typename you give a column determines its affinity, such as NUMERIC:


  A column with NUMERIC affinity may contain values using all five storage classes. When text data is inserted into a NUMERIC column, the storage class of the text is converted to INTEGER or REAL (in order of preference) if such conversion is lossless and reversible. For conversions between TEXT and REAL storage classes, SQLite considers the conversion to be lossless and reversible if the first 15 significant decimal digits of the number are preserved. If the lossless conversion of TEXT to INTEGER or REAL is not possible then the value is stored using the TEXT storage class. No attempt is made to convert NULL or BLOB values.


Since you've inserted values for which a (lossless) conversion to INTEGER or REAL1 is not possible, your values use the TEXT storage class, and SQLAlchemy/pysqlite is unhappy since it on the other hand expected values that it can convert to float, which fails.

The typing system causes other similar issues, such as when reflecting the resulting table from a CREATE TABLE ... AS against a SELECT from a table using DATETIME typename, which is converted to NUMERIC affinity.

A shorter code example that demonstrates the issue:

In [2]: foo = Table('foo', metadata, Column('bar', NUMERIC))

In [3]: foo.create(engine)
CREATE TABLE foo (
        bar NUMERIC
)

In [4]: engine.execute(""insert into foo values ('not really a number, no')"")
Out[4]: &lt;sqlalchemy.engine.result.ResultProxy at 0x7fbcd7ee8f98&gt;

In [5]: foo.select().execute().fetchall()
Out[5]: ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
  ...
~/Work/SO/lib/python3.6/site-packages/sqlalchemy/sql/util.py in __repr__(self)
    327         trunc = self.trunc
    328         return ""(%s%s)"" % (
--&gt; 329             "", "".join(trunc(value) for value in self.row),
    330             "","" if len(self.row) == 1 else """"
    331         )

TypeError: must be real number, not str


1 Probably the reason why the sqlite+pysqlite dialect does not support Decimal natively  neither does SQLite
",,,false,
https://stackoverflow.com/questions/62170836,false,"The issue does not involve a problematic API. It is related to the behavior of SQLAlchemy's query and relationship mechanisms, and the unexpected behavior is likely due to a misunderstanding of how these mechanisms work together.",,,,,,,How do I fix this Elastic Beanstalk error when deploying a Flask app: Python 3.7.0 was not found on your system,"I am trying to deploy my Flask app using the EB CLI following this official AWS tutorial. I receive the error 'Create environment operation is complete, but with errors'. Observing the environment logs, the server seems to be lacking the correct Python version.

Below are the EB environment details, my PipFile, the EB CLI output and the error in the deploy logs.

How can I fix this problem?

Environment details:

 Application name: server_eb
  Region: eu-west-2
  Deployed Version: XXX
  Environment ID: XXX
  Platform: arn:aws:elasticbeanstalk:eu-west-2::platform/Python 3.7 running on 64bit Amazon Linux 2/3.0.1
  Tier: WebServer-Standard-1.0
  CNAME: UNKNOWN
  Updated: 2020-06-02 16:53:10.321000+00:00


PipFile:

[[source]]
name = ""pypi""
url = ""https://pypi.org/simple""
verify_ssl = true

[dev-packages]

[packages]
flask = ""~=1.1.1""
python-dotenv = ""~=0.13.0""
psycopg2 = ""~=2.8.5""
flask-sqlalchemy = ""~=2.4.1""
flask-migrate = ""~=2.5.3""
flask-dance = {extras = [""sqla""],version = ""~=3.0.0""}
flask-login = ""~=0.4.1""
blinker = ""~=1.4""
flask-session = ""==0.3.1""
redis = ""==3.4.1""

[scripts]
migrate=""flask db upgrade""
start=""flask run --cert=cert.pem --key=key.pem""

[requires]
python_version = ""3.7.0""


CLI output:

2020-06-02 16:54:55    ERROR   [Instance: XXX] Command failed on instance. An unexpected error has occurred [ErrorCode: 0000000001].
2020-06-02 16:54:55    INFO    Command execution completed on all instances. Summary: [Successful: 0, Failed: 1].
2020-06-02 16:55:57    ERROR   Create environment operation is complete, but with errors. For more information, see troubleshooting documentation.

ERROR: ServiceError - Create environment operation is complete, but with errors. For more information, see troubleshooting documentation.


Logs: 

2020/06/02 16:54:55.030996 [ERROR] An error occurred during execution of command [app-deploy] - [SetUpPythonEnvironment]. Stop running the command. Error: fail to install Gunicorn with error Command /bin/sh -c python3 -m pipenv install gunicorn --skip-lock failed with error exit status 1. Stderr:Warning: the environment variable LANG is not set!
We recommend setting this in ~/.profile (or equivalent) for proper expected behavior.
Warning: Python 3.7.0 was not found on your system
You can specify specific versions of Python with:
  $ pipenv --python path/to/python

","Based on the comments, the issue was that the EB uses Python 3.7.6, while the OP's dependencies required version 3.7.0. 

The solution was to change the dependency to Python 3.7.6.

The current python version running on EB can be found here:


  64bit Amazon Linux 2 v3.0.1 running Python 3.7 - Python 3.7.6

",,,false,
https://stackoverflow.com/questions/66656355,false,"The issue does not involve a problematic API. It is related to the behavior of SQLAlchemy's query and relationship mechanisms, and the unexpected behavior is likely due to a misunderstanding of how these mechanisms work together.",,,,,,,How to access only filtered objects when filtering on joined tables in sqlalchemy,"I am using sqlalchemy for the first time and I have an unexpected behavior in reading results of a query.
Let's say these are my database models:
class Parent(Base):
    __table__ = ""parents""
    id = Column(Integer, primary_key=True)
    children = relationship(""Child"", back_populates=""parent"")

class Child(Base):
    __table__ = ""children""
    id = Column(Integer, primary_key=True)
    parent_id = Column(Integer, ForeignKey(""parents.id""))
    did_homework = Column(Boolean, default=False)
    parent = relationship(""Parent"", back_populates=""children"")

When I query the database like this:
parents = session.query(Parent).join(Parent.children).filter(Child.did_homework == True).all()
good_children = parents[0].children

I expected all good_children to have did_homework set to True.
But it seems the query returns all parents which have at least one Child with did_homework set to True and then when I access parents[0].children I get all children of that parent, not just the ones that satisfy my criteria.
I have tried querying like this:
result = (
        db.query(Parent, Child)
        .filter(Parent.id == Child.parent_id)
        .filter(Child.did_homework == True)
        .all()
    )

which gives the expected tuples as the result (every Child in the tuple has did_homework == True), but every Parent in the tuple still has all children, not just the ones that did their homework.
Question:
Is there a way of writing a query that would assign only the children which are not filtered out to the parent they belong to?
So that when I write: good_children = parents[0].children I could be sure all objects in good_children have did_homework set to True.
","After posting this I found the same question here with an answer that didn't work for me, but with a small modification I managed to do it:
parents = (
        db.query(Parent)
        .join(Child)
        .filter(Child.did_homework == True)
        .options(contains_eager(""children""))
        .all()
    )

",,,false,
https://stackoverflow.com/questions/53726041,true,"The issue involves an unexpected behavior in SQLAlchemy's commit operation. The API in question is SQLAlchemy's commit method, which fails to persist the update to the database.",,,,,,,SQLAlchemy not committing update,"I am get some unexpected behavior when trying to commit an update with SQLalchemy..

I have an object like so..

updatedINfo = {'id': 1, 'type': 'NewData'}
# The ID is an id from the table, the 'type' is a column from that table
# The 'NewData' is what needs to replace whatever is currently stored in
# 'type' column.


Because I want this function to be reusble and not have to hardcode the column names, I figured I could do something like this;

# Find the event first
the_event = db.query(EventsPending).filter(EventsPending.id == updatedInfo.get('id')).one_or_none()

# If it returns None, something went wrong
if not the_event:
    return {'Error': 'Something went wrong..'}

# Take the id out of the JSON post
del updatedInfo['id']

# Turn our db query into a dict
event_dict = the_event.as_dict()

# Set the keys for cross checking
for key in event_dict.keys():
    if key in updatedInfo.keys():
        the_event.__dict__[key] = updatedInfo.get(key)
        print(the_event.type)
        # THIS PRINTS THE UPDATED TYPE
        print(the_event.as_dict())
        # THIS ALSO PRINTS THE UPDATED TYPE

       db.commit()
       print(the_event.type)
       # HOWEVER THIS PRINTS THE OLD TYPE AFTER THE COMMIT
       # The commit does not make it to the database
       # I see no errors, and logging shows no useful info


Obviously I can work around this by coding in the different types but really seems ugly and inefficient. Any help is appreciated.
","For anyone running into this.. You need

setattr(the_event, key, data)
# the_event being the sqlalchemy object
# key being the column name
# data being the new value


Hope this helps someone.
",,,false,
https://stackoverflow.com/questions/76536770,true,"The issue involves the behavior of SQLAlchemy Sessions and PostgreSQL transactions, specifically in relation to committing changes. It is not a problem with a specific API exhibiting unexpected failures or behaviors.",SQLAlchemy,N/A,"The issue revolves around the behavior of SQLAlchemy Sessions and PostgreSQL transactions, where committing changes is necessary for subsequent queries to see the updated data. Without committing the changes, the queries do not see the modifications made within the transaction.","The expected behavior is for the queries to see the changes made within the transaction, but without committing the changes, the queries retrieve the original data.","The issue is triggered when changes are made within a transaction but not committed, and subsequent queries are executed within the same session without the changes being visible.",This issue might be challenging to detect for users who are unfamiliar with the specific behavior of SQLAlchemy Sessions and PostgreSQL transactions or those who assume that changes made within a transaction are immediately visible to subsequent queries.,SQLAlchemy: Understanding Select statements within transactions,"I'm trying to understand the behavior of SQLAlchemy Sessions and their relations to PostgreSQL transactions, as they don't seem to behave quite the same. I've created a simple program to insert, update, and subsequently update a row:
engine = sa.create_engine(f""postgresql+psycopg2://{pg_user}:@{pg_host}:{pg_port}/{pg_db}"", echo=True)
Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)
    name = Column(String)

Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine, expire_on_commit=True)
session = Session()

# Add a user
u1 = User(id=1, name=""u1"")
session.add(u1)
session.commit()

# Modify user's name
session.query(User).filter_by(id=1).update({""name"": ""u2""})
session.commit()

# Get user's name
user = session.query(User).filter_by(id=1).one()
print(user.name) # prints ""u2"" as expected

However, if I now remove the session.commit() statement between the modification and select, I get an unexpected result:
# ...

# Modify user's name
session.query(User).filter_by(id=1).update({""name"": ""u2""})

# Get user's name
user = session.query(User).filter_by(id=1).one()
print(user.name) # prints ""u1"" -- NOT expected!

Presumably this is because the session is querying the database, in which the user's name has not been updated (because the transaction has not yet been committed). However, I would have thought that because I'm using the same session object to query the user, it should recognize the changes that have occurred on the User object within the transaction.
What am I not understanding here?
","PostgreSQL uses ""Read Committed"" isolation level. This means that the each query being executed sees data committed earlier.
So, when you are making changes without ""session.commit()"", the changes are not made in the database, so the queries does not see those changes. Once you have committed the changes, the changes are made in database and upcoming queries within same session will see updated data.
To avoid ""session.commit()"" after every query, you can set ""autocommit"" to true.
",,,false,
https://stackoverflow.com/questions/77024615,true,"The issue involves the behavior of Redshift transactions and session management, specifically in relation to querying and processing large datasets. It is not a problem with a specific API exhibiting unexpected failures or behaviors.",SQLAlchemy,N/A,"The issue relates to querying and processing large datasets in Redshift, where the results may be inconsistent or contain duplicate entries. It is caused by the queries being executed in separate transactions or sessions, leading to different versions of the tables and unexpected results.","The expected behavior is to retrieve consistent and non-duplicate results from the initial query, but due to separate transactions or sessions, the results are inconsistent and may contain duplicates.","The issue is triggered when queries are executed in separate transactions or sessions, causing inconsistencies and duplicate entries in the results.",This issue might be challenging to detect for users who are not familiar with the behavior of Redshift transactions and session management or those who assume that queries will always retrieve consistent and non-duplicate results.,Redshift Sqlalchemy transactions and session management,"Database - Redshift
SQLAlchemy version - 1.4.2
I'm trying to create a re-usable query using the SQLAlchemy ORM. What I do is something like this.
def get_page(query, page_size, page_number):
    return query.limit(page_size).offset((page_number - 1) * page_size).all()

def my_query(self):
    with Session(engine) as session:
        query = session.query(my_model).order_by(
            my_model.id
        )
        return query

query = my_query()

while True:
    page_number = 1
    results = get_page(query, 100, page_number)
    if not results:
        break
    for result in results:
        print(result)
        
    page_number+=1

I'm experiencing unexpected behavior using this method on very large data sets. Where it will miss a bunch of entries, or will repeat some of the entries it has already done.  (entry A could appear on page 1 and 4 for some reason)
I'm assuming my problem is that every time i get a page and call .all() it is performing a brand new query so if new data is put onto the table it will now effect my results.
This is a watered down example of my real world use case.
The real scenarios query is a left join on two tables.
Table X (a join table) and table Y (The data to act on)
I join table Y to table X and find all of Y that does not have a corresponding X entry.
I iterate through my results using the method above and after 500 entries are processed i create 500 join entries on table X that reference the processed table Y entry.(keeping track of what data has been processed this way)
What I want to have happen is have all the results from my query -&gt; Stream them by chunks of 100, process them, and be able to perform bulk inserts onto the join table without effecting my initial query results or committing my query session(resulting in me requerying and ending up with different results). (meaning if the data changes it is not changed in my results of the initial query)
Tell me what is wrong with my method?
","For this to be happening these different queries would need to be in different transactions (xid).  Can you confirm this is what is happening?  (I say this b/c if they happen in the same transaction you should get the same version of the tables.  And it sounds like from your description that this isn't just result ordering differences.)
So the first solution would be to have everything happen in a single transaction (if possible).  The best way to run this would be with a cursor to store the results between fetches.  See: https://docs.aws.amazon.com/redshift/latest/dg/declare.html and https://docs.aws.amazon.com/redshift/latest/dg/fetch.html
If it isn't possible to be in a single transaction, then can it be in a single session?  This way you could keep the results in a temp table and read from there.
If these queries absolutely need to be made across separate connections then you will need to set up a perm table to hold results.  This has the disadvantage of needing to be cleaned up when the process ends.  You will want to add some per iteration random chars to the table name so that no iteration collisions can happen on the table name.
I'd start by examining the system tables for these queries on Redshift and note the pid and xid of each.  This will give you more specific information on how SQLAlchemy is interacting with Redshift and what changes are needed.
",,,false,
https://stackoverflow.com/questions/14719507,false,The issue does not involve a problematic API. It is a question about testing queries in SQLAlchemy and does not indicate any unexpected failures or behaviors related to specific API usage.,,,,,,,Unit tests for Query in SQLAlchemy,"How does one go about testing queries in SQLAlchemy? For example suppose we have this models.py

from sqlalchemy import (
        Column,
        Integer,
        String,
)
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class Panel(Base):
    __tablename__ = 'Panels'

    id = Column(Integer, primary_key=True)
    category = Column(Integer, nullable=False)
    platform = Column(String, nullable=False)
    region = Column(String, nullable=False)

    def __init__(self, category, platform, region):
        self.category = category
        self.platform = platform
        self.region = region


    def __repr__(self):
        return (
            ""&lt;Panel('{self.category}', '{self.platform}', ""
            ""'{self.region}')&gt;"".format(self=self)
        )


and this tests.py

import unittest

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from models import Base, Panel


class TestQuery(unittest.TestCase):

    engine = create_engine('sqlite:///:memory:')
    Session = sessionmaker(bind=engine)
    session = Session()

    def setUp(self):
        Base.metadata.create_all(self.engine)
        self.session.add(Panel(1, 'ion torrent', 'start'))
        self.session.commit()

    def tearDown(self):
        Base.metadata.drop_all(self.engine)

    def test_query_panel(self):
        expected = [Panel(1, 'ion torrent', 'start')]
        result = self.session.query(Panel).all()
        self.assertEqual(result, expected)


When we try running the test, it fails, even though the two Panels look identical.

$ nosetests
F
======================================================================
FAIL: test_query_panel (tests.TestQuery)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/clasher/tmp/tests.py"", line 31, in test_query_panel
    self.assertEqual(result, expected)
AssertionError: Lists differ: [&lt;Panel('1', 'ion torrent', 's... != [&lt;Panel('1', 'ion torrent', 's...

First differing element 0:
&lt;Panel('1', 'ion torrent', 'start')&gt;
&lt;Panel('1', 'ion torrent', 'start')&gt;

  [&lt;Panel('1', 'ion torrent', 'start')&gt;, &lt;Panel('2', 'ion torrent', 'end')&gt;]

----------------------------------------------------------------------
Ran 1 test in 0.063s

FAILED (failures=1)


One solution I've found is to make a query for every single instance I expect to find in the query:

class TestQuery(unittest.TestCase):

    ...

    def test_query_panel(self):
        expected = [
            (1, 'ion torrent', 'start'),
            (2, 'ion torrent', 'end')
        ]
        successful = True
        # Check to make sure every expected item is in the query
        try:
            for category, platform, region in expected:
                self.session.query(Panel).filter_by(
                        category=category, platform=platform,
                        region=region).one()
        except (NoResultFound, MultipleResultsFound):
            successful = False
        self.assertTrue(successful)
        # Check to make sure no unexpected items are in the query
        self.assertEqual(self.session.query(Panel).count(),
                         len(expected))


This strikes me as pretty ugly, though, and I'm not even getting to the point where I have a complex filtered query that I'm trying to test. Is there a more elegant solution, or do I always have to manually make a bunch of individual queries?
","your original test is on the right track, you just have to do one of two things: either make sure that two Panel objects of the same primary key identity compare as True:
import unittest

from sqlalchemy import create_engine
from sqlalchemy.orm import Session

from database.models import Base

class Panel(Base):
    # ...

    def __eq__(self, other):
        return isinstance(other, Panel) and other.id == self.id

or you can organize your test such that you make sure you're checking against the same Panel instance (because here we take advantage of the identity map):
class TestQuery(unittest.TestCase):
    def setUp(self):
        self.engine = create_engine('sqlite:///:memory:')
        self.session = Session(self.engine)
        Base.metadata.create_all(self.engine)
        self.panel = Panel(1, 'ion torrent', 'start')
        self.session.add(self.panel)
        self.session.commit()

    def tearDown(self):
        Base.metadata.drop_all(self.engine)

    def test_query_panel(self):
        expected = [self.panel]
        result = self.session.query(Panel).all()
        self.assertEqual(result, expected)

as far as the engine/session setup/teardown, I'd go for a pattern where you use a single engine for all tests, and assuming your schema is fixed, a single schema for all tests, then you make sure the data you work with is performed within a transaction that can be rolled back.   The Session can be made to work this way, such that calling commit() doesn't actually commit the ""real"" transaction, by wrapping the whole test within an explicit Transaction.  The example at https://docs.sqlalchemy.org/en/latest/orm/session_transaction.html#joining-a-session-into-an-external-transaction-such-as-for-test-suites illustrates this usage.    Having a "":memory:"" engine on every test fixture will take up a lot of memory and not really scale out to other databases besides SQLite.
",,,false,
https://stackoverflow.com/questions/70104873,true,The issue involves accessing relationships with async SQLAlchemy and encountering a specific error related to greenlet_spawn. It is not a problem with a specific API exhibiting unexpected failures or behaviors.,SQLAlchemy,N/A,"The issue pertains to accessing relationships in async SQLAlchemy and encountering a 'greenlet_spawn has not been called' error. It is caused by attempting to call await_() in an unexpected place, possibly due to IO being attempted without the necessary setup.",The expected behavior is to be able to access the relationship attributes without encountering the 'greenlet_spawn has not been called' error.,"The issue is triggered when attempting to access relationship attributes in async SQLAlchemy without the necessary setup for greenlet_spawn, leading to the 'greenlet_spawn has not been called' error.",This issue might be challenging to detect for users who are not familiar with the specific requirements and limitations of async SQLAlchemy or those who are not aware of the necessary setup for greenlet_spawn.,how to access relationships with async sqlalchemy?,"import asyncio

from sqlalchemy import Column
from sqlalchemy import DateTime
from sqlalchemy import ForeignKey
from sqlalchemy import func
from sqlalchemy import Integer
from sqlalchemy import String
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.ext.asyncio import create_async_engine
from sqlalchemy.future import select
from sqlalchemy.orm import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.orm import selectinload
from sqlalchemy.orm import sessionmaker

engine = create_async_engine(
        ""postgresql+asyncpg://user:pass@localhost/db"",
        echo=True,
    )


# expire_on_commit=False will prevent attributes from being expired
# after commit.
async_session = sessionmaker(
    engine, expire_on_commit=False, class_=AsyncSession
)


Base = declarative_base()

class A(Base):
    __tablename__ = ""a""

    id = Column(Integer, primary_key=True)
    name = Column(String, unique=True)
    data = Column(String)
    create_date = Column(DateTime, server_default=func.now())
    bs = relationship(""B"")

    # required in order to access columns with server defaults
    # or SQL expression defaults, subsequent to a flush, without
    # triggering an expired load
    __mapper_args__ = {""eager_defaults"": True}


class B(Base):
    __tablename__ = ""b""
    id = Column(Integer, primary_key=True)
    a_id = Column(ForeignKey(""a.id""))
    data = Column(String)
    
    
       

async with engine.begin() as conn:
    await conn.run_sync(Base.metadata.drop_all)
    await conn.run_sync(Base.metadata.create_all)


async with async_session() as session:
    async with session.begin():
        session.add_all(
            [
                A(bs=[B(), B()], data=""a1""),
                A(bs=[B()], data=""a2""),
            ]
        )


async with async_session() as session:
    result = await session.execute(select(A).order_by(A.id))
    a1 = result.scalars().first()

    # no issue: 
    print(a1.name, a1.data)

    # throws error:
    print(a1.bs)
    

Trying to access a1.bs  gives this error:
     59     current = greenlet.getcurrent()
     60     if not isinstance(current, _AsyncIoGreenlet):
---&gt; 61         raise exc.MissingGreenlet(
     62             ""greenlet_spawn has not been called; can't call await_() here. ""
     63             ""Was IO attempted in an unexpected place?""

MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/14/xd2s)



","This is how:
from sqlalchemy.orm import selectinload

async with async_session() as session:
    result = await session.execute(select(A).order_by(A.id)
                                            .options(selectinload(A.bs)))
    a = result.scalars().first()

    print(a.bs)


key is using the selectinload method to prevent implicit IO
UPDATE
There are a few alternatives to selectinload like joinedload, lazyload. I am still trying to understand the differences.
","muons answer is correct if you want eager loading (which is better).
But if for some reason you already have loaded your model and later want to load a relationship, there is a way starting with SQLAlchemy 2.0.4:
Using session.refresh, you can tell it to load a1.bs:
await session.refresh(a1, attribute_names=[""bs""])
print(a1.bs)  # This works

From the docs:

New in version 2.0.4: Added support for AsyncSession.refresh() and the underlying Session.refresh() method to force lazy-loaded relationships to load, if they are named explicitly in the Session.refresh.attribute_names parameter.

","There is an additional way of accessing the relationship attributes in SQLAlchemy v2, for the case where you have already loaded your model and later want to load a relationship.
As the documentation says, you can use AsyncAttrs: ""when added to a specific class or more generally to the Declarative Base superclass, provides an accessor AsyncAttrs.awaitable_attrs which delivers any attribute as an awaitable"". So you can create a Declarative Base class like this:
class Base(AsyncAttrs, DeclarativeBase):
    pass

And then use awaitable_attrs to access the relationship that must be previously loaded:
a1_bs = await a1.awaitable_attrs.bs
print(a1_bs)

",false,
https://stackoverflow.com/questions/40681371,false,"The issue does not involve a problematic API. The provided information is incomplete, and there is no clear indication of unexpected failures or behaviors related to specific API usage.",,,,,,,Bug in SQLAlchemy Rollback after DB Exception?,,,,,false,
https://stackoverflow.com/questions/68195361,true,The issue involves a problematic API related to the asynchronous usage of SQLAlchemy. The error message indicates a problem with lazyloading and the usage of Greenlet. This suggests that the issue stems from a specific behavior of the SQLAlchemy API when used in an asynchronous context.,SQLAlchemy,relationship,"The issue occurs when attempting to access the ""tags"" attribute of a User object asynchronously using the ""await"" keyword. This triggers a lazyload operation, which is not compatible with the Greenlet-based asynchronous execution.","The expected behavior is to retrieve the tags associated with a User object asynchronously. However, the lazyload operation fails due to the incompatibility between Greenlet and the asynchronous context.","The issue is triggered when trying to access the ""tags"" attribute of a User object asynchronously using the ""await"" keyword.",This issue might be challenging to detect for users who are not familiar with the specific requirements and limitations of SQLAlchemy's asynchronous usage or those who assume that lazyloading works seamlessly in an asynchronous context.,How to properly handle many to many in async sqlalchemy?,"I was trying to implement many to many relationship between tables.
When I use backpopulates all tags for a specific user must be in the tags field.
The tables are successfully created.
Users and tags are added.
Link table too.
import asyncio
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.util import await_only, greenlet_spawn

from sqlalchemy import Column, Table, ForeignKey
from sqlalchemy.orm import declarative_base, relationship
from sqlalchemy.dialects.postgresql import VARCHAR, INTEGER

Base = declarative_base()

user_tag = Table('user_tag', Base.metadata,
                 Column('user_id', INTEGER, ForeignKey('users.id')),
                 Column('tag_id', INTEGER, ForeignKey('tags.id'))
                 )


class User(Base):
    __tablename__ = 'users'
    id = Column(INTEGER, primary_key=True)
    name = Column(VARCHAR(32), nullable=False, unique=True)
    tags = relationship(""Tag"",
                        secondary=user_tag,
                        back_populates=""users"")


class Tag(Base):
    __tablename__ = 'tags'
    id = Column(INTEGER, primary_key=True)
    tag = Column(VARCHAR(255), nullable=False, unique=True)
    users = relationship(""User"",
                         secondary=user_tag,
                         back_populates=""tags"")


async def main():
    engine = create_async_engine(
        ""postgresql+asyncpg://postgres:pgs12345@localhost/test"",
        echo=False,
    )

    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)
        await conn.run_sync(Base.metadata.create_all)

    users = [User(name=""p1""), User(name=""p2""), User(name=""p3"")]
    tags = [Tag(tag=""tag1""), Tag(tag=""tag2""), Tag(tag=""tag3"")]

    async with AsyncSession(engine) as session:
        async with session.begin():
            session.add_all(users)
            session.add_all(tags)

        for user in users:
            await session.refresh(user)
        for tag in tags:
            await session.refresh(tag)

        for user in users:
            for i in range(3, user.id - 1, -1):
                await session.execute(user_tag.insert().values(user_id=user.id, tag_id=i))
        await session.commit()

        for user in users:
            await session.refresh(user)
        for tag in tags:
            await session.refresh(tag)

        tags = await greenlet_spawn(users[0].tags)
        print(tags)


loop = asyncio.get_event_loop()
loop.run_until_complete(main())

When I run the program, it crashes with:
 File ""C:\Sources\asyncSQLAl test\main.py"", line 48, in &lt;module&gt;
    loop.run_until_complete(main())
  File ""C:\Users\Stanislav\AppData\Local\Programs\Python\Python39\lib\asyncio\base_events.py"", line 
642, in run_until_complete
    return future.result()
  File ""C:\Sources\asyncSQLAl test\main.py"", line 41, in main
    tags = await greenlet_spawn(await users[0].tags)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\attributes.py"", line 480, in __get__
    return self.impl.get(state, dict_)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\attributes.py"", line 931, in get
    value = self.callable_(state, passive)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\strategies.py"", line 879, in _load_for_state
    return self._emit_lazyload(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\strategies.py"", line 1036, 
in _emit_lazyload
    result = session.execute(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1689, in 
execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1582, in 
_execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\sql\lambdas.py"", line 481, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1451, in 
_execute_clauseelement
    ret = self._execute_context(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1813, in 
_execute_context
    self._handle_dbapi_exception(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1998, in 
_handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1770, in 
_execute_context
    self.dialect.do_execute(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 717, in do_execute
    cursor.execute(statement, parameters)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py"", line 449, in execute
    self._adapt_connection.await_(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\util\_concurrency_py3k.py"", line 60, in await_only
    raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)      
sys:1: RuntimeWarning: coroutine 'AsyncAdapt_asyncpg_cursor._prepare_and_execute' was never awaited

I don't quite understand how greenlet_spawn works here and where it should be used in this example.
For example, same program, but in sync style
from sqlalchemy import Column, Table, ForeignKey
from sqlalchemy.orm import declarative_base, relationship, sessionmaker
from sqlalchemy import create_engine
from sqlalchemy.dialects.postgresql import VARCHAR, INTEGER

Base = declarative_base()

user_tag = Table('user_tag', Base.metadata,
                 Column('user_id', INTEGER, ForeignKey('users.id')),
                 Column('tag_id', INTEGER, ForeignKey('tags.id'))
                 )


class User(Base):
    __tablename__ = 'users'
    id = Column(INTEGER, primary_key=True)
    name = Column(VARCHAR(32), nullable=False, unique=True)
    tags = relationship(""Tag"",
                        secondary=user_tag,
                        back_populates=""users"")


class Tag(Base):
    __tablename__ = 'tags'
    id = Column(INTEGER, primary_key=True)
    tag = Column(VARCHAR(255), nullable=False, unique=True)
    users = relationship(""User"",
                         secondary=user_tag,
                         back_populates=""tags"")
    
    def __str__(self):
        return self.tag


def main():
    engine = create_engine(
        ""postgresql+psycopg2://postgres:pgs12345@localhost/test"",
        echo=False,
    )

    Base.metadata.drop_all(engine)
    Base.metadata.create_all(engine)

    Session = sessionmaker(bind=engine)
    session = Session()

    users = [User(name=""p1""), User(name=""p2""), User(name=""p3"")]
    tags = [Tag(tag=""tag1""), Tag(tag=""tag2""), Tag(tag=""tag3"")]

    with session.begin():
        session.add_all(users)
        session.add_all(tags)

    for user in users:
        for i in range(3, user.id - 1, -1):
            session.execute(user_tag.insert().values(
                user_id=user.id, tag_id=i))
    session.commit()

    for tag in users[0].tags:
        print(tag, end="" "")

main()

Gives me:
tag1 tag2 tag3 

","I've been stuck on this today too and I've narrowed it down to the fact that a lazyload is attempted, which GreenLet is not happy about. I wasn't sure whether this was just my lack of skill but I've found this article that details some of the common errors:
https://matt.sh/sqlalchemy-the-async-ening, where it's mentioned that this very issue will occur in this way. Furthermore, the docs go into detail about needing to avoid lazyloading: https://docs.sqlalchemy.org/en/14/orm/extensions/asyncio.html.
My solution at the moment is to effectively prefetch the Child relation upon the initial query of the Parent object and then manipulate it from there. Whether this is a true bug, in the sense that it should work in async when it already works in sync or simply a limitation of the async method, I've no idea.
Edit 06/08/21, here is how I am prefetching relationships:
import sqlalchemy as sa
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload
from . import models

async def get_parent_prefetch_children(db: AsyncSession, parent_id: int) -&gt; models.Parent:
    result = await db.execute(
        sa.select(models.Parent).where(models.Parent.id == parent_id).options(
            selectinload(models.Parent.children)
        )
    )
    return result.scalar()

In your case, you call users[0].tags, which makes a lazyload and fails. In order for you to avoid this, you must refetch users with their tags eagerly loaded.
","From their official doc for relationship, you can use lazy=""joined"" to ask the SQLAlchemy do prefetch for you on session.refresh(...) or session.execute(select(...)). This will do joined style eagerly load at the time you query for the object. Note that this may introduce performance issue to your application since sometime you are not going to access the foreign'd objects while the database will still do joined style query for you.
tags = relationship(""Tag"",
    secondary=user_tag,
    back_populates=""users"",
    lazy=""joined"")

The results and SQL stmt looks like:
&gt;&gt;&gt;tags = users[0].tags
&gt;&gt;&gt;print(tags)
[&lt;__main__.Tag object at 0x10bc68610&gt;, &lt;__main__.Tag object at 0x10bc41d00&gt;, &lt;__main__.Tag object at 0x10bc12b80&gt;]
&gt;&gt;&gt;from sqlalchemy.future import select
&gt;&gt;&gt;print(select(User))
FROM users LEFT OUTER JOIN (user_tag AS user_tag_1 JOIN tags AS tags_1 ON tags_1.id = user_tag_1.tag_id) ON users.id = user_tag_1.user_id

",,false,
https://stackoverflow.com/questions/67897872,false,The issue does not involve a problematic API. It appears to be a misunderstanding of how to use SQLAlchemy in an asynchronous context and the difference between synchronous and asynchronous connectors.,,,,,,,Exception has occurred: MissingGreenlet whe making connection query in sql alchemy 1.4,"I'm trying to do a simple query with SQLAlchemy 1.4.17 from within pytest
def test_first():
    engine = create_engine(settings.SQLALCHEMY_DATABASE_URI)
    result = engine.execute(text(""SELECT email FROM user""))

but am getting this error
Exception has occurred: MissingGreenlet
greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)
  File ""/Users/mattc/Development/inference/server/inference_server/app/tests/test_01_user.py"", line 27, in test_first
    result = engine.execute(text(""SELECT email FROM user""))

and do not know why? Any suggestions?
","You are trying to use an async connector package in the same way as a synchronous connector
&gt;&gt;&gt; import sqlalchemy as sa
&gt;&gt;&gt; engine = sa.create_engine('postgresql+asyncpg:///')
&gt;&gt;&gt; res = engine.execute(sa.text('SELECT 1'))
&lt;stdin&gt;:1: RemovedIn20Warning: The Engine.execute() method is considered legacy as of the 1.x series of SQLAlchemy and will be removed in 2.0. All statement execution in SQLAlchemy 2.0 is performed by the Connection.execute() method of Connection, or in the ORM by the Session.execute() method of Session. (Background on SQLAlchemy 2.0 at: http://sqlalche.me/e/b8d9)
Traceback (most recent call last):
...
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)

You need to either use a synchronous connector, for example psycopg2, pg8000, or write async code:
import sqlalchemy as sa

import asyncio

from sqlalchemy.ext.asyncio import create_async_engine


async def async_main():
    engine = create_async_engine(
        ""postgresql+asyncpg:///test"", echo=True,
    )

    async with engine.connect() as conn:

        # select a Result, which will be delivered with buffered
        # results
        result = await conn.execute(sa.text('select email from users'))

        print(result.fetchall())
    await engine.dispose()


asyncio.run(async_main())

",,,false,
https://stackoverflow.com/questions/71116549,true,"The issue involves an error related to the PostgreSQL database connection using SQLAlchemy. The error message indicates an OperationalError with psycopg2, which is the PostgreSQL adapter for Python.",SQLAlchemy,psycopg2,The error message 'sqlalchemy.exc.OperationalError: (psycopg2.OperationalError)' suggests that there is a problem with the connection to the PostgreSQL database.,"The expected behavior is for the SQLAlchemy connection to establish a successful connection to the PostgreSQL database. However, the actual behavior is an OperationalError, indicating a failure in the connection process.",The issue is triggered when attempting to connect to the PostgreSQL database using SQLAlchemy with the provided connection URL.,"This issue might be challenging to detect during development and testing because it depends on the specific configuration and environment setup, such as the availability of the PostgreSQL database and the correctness of the connection URL.",sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) with PostgreSQL,"I searched for this error a lot, but I only find some with more information behind that like ""FATAL: ..."". Mine has none. It only says
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) 

I have a postgres database inside a docker container that has set his port to the standard 5432.
I've created the container with the following command:
docker run --name some-postgres -e POSTGRES_PASSWORD=mysecretpassword -p 5432:5432 -d postgres

It is clean so no database created. The API should automatically create them.
I'm using Pycharm IDE, maybe it has something todo with that.
Traceback (most recent call last):
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\protocols\http\h11_impl.py"", line 373, in run_asgi
    result = await app(self.scope, self.receive, self.send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\proxy_headers.py"", line 75, in __call__
    return await self.app(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\debug.py"", line 96, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\debug.py"", line 93, in __call__
    await self.app(scope, receive, inner_send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\applications.py"", line 208, in __call__
    await super().__call__(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\applications.py"", line 112, in __call__
    await self.middleware_stack(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\middleware\errors.py"", line 181, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\middleware\errors.py"", line 159, in __call__
    await self.app(scope, receive, _send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\exceptions.py"", line 82, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\exceptions.py"", line 71, in __call__
    await self.app(scope, receive, sender)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 580, in __call__
    await route.handle(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 241, in handle
    await self.app(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 52, in app
    response = await func(request)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\routing.py"", line 226, in app
    raw_response = await run_endpoint_function(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\routing.py"", line 159, in run_endpoint_function
    return await dependant.call(**values)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\app\routers\v1\users.py"", line 31, in create_user
    session.commit()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1428, in commit
    self._transaction.commit(_to_root=self.future)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 829, in commit
    self._prepare_impl()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 808, in _prepare_impl
    self.session.flush()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3339, in flush
    self._flush(objects)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3479, in _flush
    transaction.rollback(_capture_exception=True)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3439, in _flush
    flush_context.execute()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 209, in save_obj
    for (
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 370, in _organize_states_for_save
    for state, dict_, mapper, connection in _connections_for_states(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 1709, in _connections_for_states
    connection = uowtransaction.transaction.connection(base_mapper)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 626, in connection
    return self._connection_for_bind(bind, execution_options)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 735, in _connection_for_bind
    conn = self._parent._connection_for_bind(bind, execution_options)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 747, in _connection_for_bind
    conn = bind.connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\future\engine.py"", line 419, in connect
    return super(Engine, self).connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3194, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 96, in __init__
    else engine.raw_connection()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3273, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3243, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 2097, in _handle_dbapi_exception_noconnection
    util.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3240, in _wrap_pool_connect
    return fn()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 476, in checkout
    rec = pool._do_get()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\impl.py"", line 146, in _do_get
    self._dec_overflow()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\impl.py"", line 143, in _do_get
    return self._create_connection()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 256, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 371, in __init__
    self.__connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 666, in __connect
    pool.logger.debug(""Error on connect(): %s"", e)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\create.py"", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\psycopg2\__init__.py"", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) 
(Background on this error at: https://sqlalche.me/e/14/e3q8)

This is my error that i get. My code looks like this:
main.py:
import os
import uvicorn
if __name__ == '__main__':
    port = int(os.getenv(""PORT""))
    uvicorn.run(""main:app"", host='0.0.0.0', port=port, reload=True, debug=True, workers=3)

app/main.py:
import os
from fastapi import FastAPI
from .database import engine
from .routers import v1
engine.init_db()
port = int(os.getenv(""PORT"")) #Port is 8000
app = FastAPI()

app.include_router(v1.router, prefix=""/v1"")

app/database/engine.py: (referenced in the file above)
import os

from fastapi.security import HTTPBearer
from sqlmodel import create_engine, SQLModel, Session
DATABASE_URL = ""postgresql+psycopg2://postgres:mysecretpassword@localhost:5432""
engine = create_engine(DATABASE_URL, echo=True)
token_auth_scheme = HTTPBearer()


async def init_db():
    async with engine.begin() as conn:
        # await conn.run_sync(SQLModel.metadata.drop_all)
        await conn.run_sync(SQLModel.metadata.create_all)


async def get_session():
    session = Session(engine)
    try:
        yield session
    finally:
        session.close()

The route ""users"" inside routers/v1/users.py:
from typing import Optional

from fastapi import APIRouter, Depends, HTTPException, Query, Path, Response, status
from pydantic import ValidationError
from sqlalchemy.exc import IntegrityError
from sqlalchemy.sql.functions import concat
from sqlalchemy import func
from sqlmodel import Session, select, col
from starlette import status

from app.database import models
from app.database.authentication import VerifyToken
from app.database.engine import get_session, token_auth_scheme

router = APIRouter()


@router.post("""", status_code=status.HTTP_201_CREATED, response_model=models.UserRead,
             response_model_exclude_none=True, name=""Create User"", tags=[""users""])
async def create_user(user_data: models.UserCreate,
                      session: Session = Depends(get_session)):

    try:
        new_user = models.User(**dict(user_data))
        session.add(new_user)
        session.commit()
        session.refresh(new_user)

        return new_user
    except IntegrityError:
        session.rollback()
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT, detail=""IntegrityError"")
    except ValidationError:
        session.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, detail=""ValidationError"")

The models.User:
class UserBase(SQLModel):
    id: str
    username: Optional[str]
    country_code: Optional[str]
    phone: Optional[str]

    class Config:
        allow_population_by_field_name = True

class User(UserBase, table=True):
    __tablename__ = 'users'
    id: str = Field(primary_key=True)
    username: Optional[str] = Field(sa_column=Column('username', VARCHAR(length=50), unique=True, default=None))
    phone: Optional[str] = Field(sa_column=Column('phone', VARCHAR(length=20), unique=True, default=None))

I hope that this is everything you guys need to find something. If you need something else contact me.
Best regards
Colin
EDIT:
After I changed the link from `postgresql+psycopg2` to `postgresql+asyncpg` I get a new error:

ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\protocols\http\h11_impl.py"", line 373, in run_asgi
    result = await app(self.scope, self.receive, self.send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\proxy_headers.py"", line 75, in __call__
    return await self.app(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\debug.py"", line 96, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\debug.py"", line 93, in __call__
    await self.app(scope, receive, inner_send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\applications.py"", line 208, in __call__
    await super().__call__(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\applications.py"", line 112, in __call__
    await self.middleware_stack(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\middleware\errors.py"", line 181, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\middleware\errors.py"", line 159, in __call__
    await self.app(scope, receive, _send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\exceptions.py"", line 82, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\exceptions.py"", line 71, in __call__
    await self.app(scope, receive, sender)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 580, in __call__
    await route.handle(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 241, in handle
    await self.app(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 52, in app
    response = await func(request)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\routing.py"", line 226, in app
    raw_response = await run_endpoint_function(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\routing.py"", line 159, in run_endpoint_function
    return await dependant.call(**values)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\app\routers\v1\users.py"", line 26, in create_user
    session.commit()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1428, in commit
    self._transaction.commit(_to_root=self.future)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 829, in commit
    self._prepare_impl()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 808, in _prepare_impl
    self.session.flush()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3339, in flush
    self._flush(objects)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3479, in _flush
    transaction.rollback(_capture_exception=True)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3439, in _flush
    flush_context.execute()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 209, in save_obj
    for (
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 370, in _organize_states_for_save
    for state, dict_, mapper, connection in _connections_for_states(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 1709, in _connections_for_states
    connection = uowtransaction.transaction.connection(base_mapper)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 626, in connection
    return self._connection_for_bind(bind, execution_options)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 735, in _connection_for_bind
    conn = self._parent._connection_for_bind(bind, execution_options)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 747, in _connection_for_bind
    conn = bind.connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\future\engine.py"", line 419, in connect
    return super(Engine, self).connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3194, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 96, in __init__
    else engine.raw_connection()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3273, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3240, in _wrap_pool_connect
    return fn()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 476, in checkout
    rec = pool._do_get()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\impl.py"", line 146, in _do_get
    self._dec_overflow()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\impl.py"", line 143, in _do_get
    return self._create_connection()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 256, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 371, in __init__
    self.__connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 666, in __connect
    pool.logger.debug(""Error on connect(): %s"", e)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\create.py"", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py"", line 748, in connect
    await_only(self.asyncpg.connect(*arg, **kw)),
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\_concurrency_py3k.py"", line 61, in await_only
    raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/14/xd2s)

EDIT 2:
If I use my local postgres database in docker i get this error. If I use my extern database from Heroku it works perfectly fine!
EDIT 3:
So apparently it won't work with my local docker postgres database. Now I am using my production database over heroku (so externally) and everything works with psycopg2. I didn't found the error why it won't let me use my local db but whatever.
","Even though I can see that you are using default port, maybe this can help to somebody with the same problem.
For me the problem was in explicit port definition - as I'm running two different postgres DB and (both from containers), one of them I set to listen to 5433 port - and this gives me exact same problem, so instead
SQLALCHEMY_DATABASE_URL = ""postgresql://postgres:password@localhost/fastapi""

I just put:
SQLALCHEMY_DATABASE_URL = ""postgresql://postgres:password@localhost:5433/fastapi""

Problem solved right away.
",,,false,
https://stackoverflow.com/questions/46977257,false,The issue does not involve a problematic API. No specific API or library is mentioned in the issue description.,,,,,,,SQLAlchemy returns unexpected results with limit/offset,,,,,false,
https://stackoverflow.com/questions/54836672,false,The issue does not involve a problematic API. No specific API or library is mentioned in the issue description.,,,,,,,"Using a Celery worker to interact with a SQLAlchemy DB, including knowing the user from the request",,,,,false,
https://stackoverflow.com/questions/29222426,true,The issue involves a problematic API in SQLAlchemy related to comparing dates in a Flask-SQLAlchemy query.,SQLAlchemy,filter,"The issue arises when comparing dates in a query using SQLAlchemy's filter function. The code attempts to filter records based on a date range, but the comparison does not work as expected.","The expected behavior is for the query to return records that fall within the specified date range. However, the query does not return any results regardless of the provided dates.",The issue is triggered when using the filter function to compare dates in a Flask-SQLAlchemy query.,"This issue might be challenging to detect because the code appears to be correct syntactically, but the comparison does not produce the expected results.",Comparing Dates in Flask-SQLAlchemy,"I've been trying to compare dates in a query given to SQLALchemy as follows:  

start = time.strptime(start, ""%d%m%y"")
end = time.strptime(end, ""%d%m%y"")
list_ideas = Idea.query.filter(time &gt;= start, time &lt;= end).all()


However, this does not return results regardless of dates given (where Idea.time = db.Column(db.DateTime, default=db.func.now())). I've searched through some other answers regarding this topic and from what I have gathered, I am not making the same mistakes.

In addition, changing the query to Idea.query.filter(time &gt;= start, time &lt;= end, deleted=False).all() gives the error:  


  TypeError: filter() got an unexpected keyword argument 'deleted'


Any pointers would be appreciated.

--
EDIT: I noticed that I was using import time, which may have caused the error. However, after changing it to from time import strptime, I now experience the error:  


  NameError: global name 'time' is not defined

","Try to use the func.DATE() function:
from sqlalchemy import func

start = time.strptime(start, ""%d%m%y"")
end = time.strptime(end, ""%d%m%y"")
list_ideas = Idea.query.filter(func.DATE(time) &gt;= start, func.DATE(time) &lt;= end).all()

refer to similar issue.
",,,false,
https://stackoverflow.com/questions/68212540,true,The issue involves a problematic API in marshmallow-sqlalchemy related to using it with async code.,marshmallow-sqlalchemy,dump,The issue occurs when attempting to serialize a SQLAlchemy response using marshmallow-sqlalchemy in an async context. The code raises a MissingGreenlet error.,"The expected behavior is for marshmallow-sqlalchemy to serialize the SQLAlchemy response into a JSON representation. However, the code encounters a MissingGreenlet error.",The issue is triggered when calling the dump function from marshmallow-sqlalchemy to serialize a SQLAlchemy response in an async context.,"This issue might be challenging to detect because it involves the interaction between marshmallow-sqlalchemy, SQLAlchemy, and async code, which requires a deeper understanding of these libraries and their integration.",How to use marshmallow-sqlalchemy with async code?,"I'm trying to use marshmallow-sqlalchemy with aiohttp and I have followed their docs with the basic example and I'm getting an error.
I have this schema:
from marshmallow_sqlalchemy import SQLAlchemyAutoSchema

from db.customer import Customer

class CustomerSchema(SQLAlchemyAutoSchema):
    class Meta:
        model = Customer
        include_relationships = True
        load_instance = True


And then the following code for the query:
from sqlalchemy import select
from db import db_conn
from db.customer import Customer
from queries.schema import CustomerSchema

customer_schema = CustomerSchema()

async def get_all_users():
    async with db_conn.get_async_sa_session() as session:
        statement = select(Customer)
        results = await session.execute(statement)
        _ = (results.scalars().all())
        print(_)
        response = customer_schema.dump(_, many=True)
        print(response)

For the first print statement I'm getting
[&lt;db.customer.Customer object at 0x10a183340&gt;, &lt;db.customer.Customer object at 0x10a183940&gt;, &lt;db.customer.Customer object at 0x10b0cd9d0&gt;]

But then it fails with
File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/util/_concurrency_py3k.py"", line 60, in await_only
    raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)

So how can I use marshmallow-sqlalchemy to serialize the SqlAlchemy reponse?
Another options (packages, etc) or a generic custom solutions are OK too.
For the time being I'm using this:
statement = select(Customer)
results = await session.execute(statement)
_ = (results.scalars().all())
response = {}
for result in _:
    value = {k: (v if not isinstance(v, sqlalchemy.orm.state.InstanceState) else '_') for k, v in result.__dict__.items()}
    response[f'customer {value[""id""]}'] = value
return response

Full traceback:

Traceback (most recent call last):
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/aiohttp/web_protocol.py"", line 422, in _handle_request
    resp = await self._request_handler(request)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/aiohttp/web_app.py"", line 499, in _handle
    resp = await handler(request)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/aiohttp/web_urldispatcher.py"", line 948, in _iter
    resp = await method()
  File ""/Users/ruslan/OneDrive/Home/Dev/projects/code/education/other/cft/views/user.py"", line 24, in get
    await get_all_users()
  File ""/Users/ruslan/OneDrive/Home/Dev/projects/code/education/other/cft/queries/user.py"", line 18, in get_all_users
    response = customer_schema.dump(_, many=True)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 547, in dump
    result = self._serialize(processed_obj, many=many)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 509, in _serialize
    return [
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 510, in &lt;listcomp&gt;
    self._serialize(d, many=False)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 515, in _serialize
    value = field_obj.serialize(attr_name, obj, accessor=self.get_attribute)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/fields.py"", line 310, in serialize
    value = self.get_value(obj, attr, accessor=accessor)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow_sqlalchemy/fields.py"", line 27, in get_value
    return super(fields.List, self).get_value(obj, attr, accessor=accessor)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/fields.py"", line 239, in get_value
    return accessor_func(obj, check_key, default)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 472, in get_attribute
    return get_value(obj, attr, default)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/utils.py"", line 239, in get_value
    return _get_value_for_key(obj, key, default)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/utils.py"", line 253, in _get_value_for_key
    return getattr(obj, key, default)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/attributes.py"", line 480, in __get__
    return self.impl.get(state, dict_)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/attributes.py"", line 931, in get
    value = self.callable_(state, passive)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/strategies.py"", line 879, in _load_for_state
    return self._emit_lazyload(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/strategies.py"", line 1036, in _emit_lazyload
    result = session.execute(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 1689, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1582, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/sql/lambdas.py"", line 481, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1451, in _execute_clauseelement
    ret = self._execute_context(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1813, in _execute_context
    self._handle_dbapi_exception(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1998, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
    raise exception
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1770, in _execute_context
    self.dialect.do_execute(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/default.py"", line 717, in do_execute
    cursor.execute(statement, parameters)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py"", line 449, in execute
    self._adapt_connection.await_(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/util/_concurrency_py3k.py"", line 60, in await_only
    raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)

","The problem in this case is that the Marshmallow schema is configured to load related models (include_relationships=True).  Since the initial query doesn't load them automatically, the schema triggers a query to fetch them, and this causes the error.
The simplest solution, demonstrated in the docs, is to eagerly load the related objects with their ""parent"":
async def get_all_users():
    async with db_conn.get_async_sa_session() as session:

        # Let's assume a Customer has a 1 to many relationship with an Order model
        statement = select(Customer).options(orm.selectinload(Customer.orders))

        results = await session.execute(statement)
        _ = (results.scalars().all())
        print(_)
        response = customer_schema.dump(_, many=True)
        print(response)

There is more discussion in the Preventing Implicit IO when Using AsyncSession section of the docs.
",,,false,
https://stackoverflow.com/questions/78010668,true,The issue involves a problematic API related to connecting FastAPI with MySQL using Docker. The error logs indicate a failure to connect to the MySQL server.,SQLAlchemy,create_engine,The error occurs when trying to establish a connection to the MySQL server using SQLAlchemy's create_engine function.,"The expected behavior is for the create_engine function to successfully connect to the MySQL server. However, it fails with the error message 'Can't connect to MySQL server on 'mysql' (255)'",The issue is triggered when the FastAPI application attempts to establish a connection to the MySQL server specified in the .env file.,"This issue might be challenging to detect during development and testing, especially for users who are new to Docker and FastAPI, as it involves configuring the connection to the MySQL server within the Docker environment.",Docker fastapi unable to connect to mysql,"I'm new to docker and fastapi and I'm trying to connect fastapi with mysql service but it always say that it cannot connect to mysql in the logs. What am I missing here?
.env file
API_PATH='api'
API_VERSION='v1'
FASTAPI_CONTAINER_NAME='app_fastapi'
FASTAPI_PORT=8000
REDIS_CONTAINER_NAME='app_redis'
REDIS_PORT=8001
MYSQL_CONTAINER_NAME='app_mysql'
MYSQL_DATABASE_NAME='fastapi_db'
MYSQL_ROOT_PASSWORD='p@ssw0rD'
MYSQL_USER='fastapi_mysql_admin'
MYSQL_USER_PASSWORD='p@ssw0rD'
MYSQL_HOST='mysql'
MYSQL_PORT=3306

docker-compose file
version: ""3.9""  # Specify a compatible Docker Compose version

services:
  fastapi:
    build:
      context: ./fastapi  # Build your FastAPI app
      dockerfile: Dockerfile  # Use your app's Dockerfile
    container_name: ${FASTAPI_CONTAINER_NAME}
    ports:
      - ""${FASTAPI_PORT}:${FASTAPI_PORT}""  # Expose port 8000 for the FastAPI app
    volumes:
      - .:/app
    env_file:
      - .env
    depends_on:
      mysql:  # Ensure mysql starts first
        condition: service_healthy
      redis: # Ensure redis starts first
        condition: service_healthy
    restart: on-failure
    tty: true
    networks:
      - backend

  mysql:
    build:
      context: ./mysql  # Build from the current directory
      dockerfile: Dockerfile  # Use the specific Dockerfile
    container_name: ${MYSQL_CONTAINER_NAME}
    ports:
      - ""${MYSQL_PORT}:${MYSQL_PORT}""  # Expose port 3306 for database access
    environment:
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_PASSWORD=${MYSQL_USER_PASSWORD}
      - MYSQL_DATABASE=${MYSQL_DATABASE_NAME}
    healthcheck:
      test: [""CMD"", ""mysqladmin"", ""ping"", ""-h"", ""localhost"", ""-p${MYSQL_USER_PASSWORD}""]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 7s
    tty: true
    restart: on-failure
    networks:
      - backend
  
  redis:
    build:
      context: ./redis  # Build from the current directory
      dockerfile: Dockerfile  # Use the specific Dockerfile
    container_name: ${REDIS_CONTAINER_NAME}
    ports:
      - ""${REDIS_PORT}:${REDIS_PORT}""  # Expose Redis port
    healthcheck:
      test: [""CMD"", ""redis-cli"", ""ping""]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 7s
    tty: true
    restart: on-failure
    networks:
      - backend

networks:
  backend:

database.py
import sqlalchemy
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.exc import OperationalError
from sqlalchemy.orm import sessionmaker
from os import environ as env

user = env['MYSQL_USER']
password = env['MYSQL_USER_PASSWORD']
host = env['MYSQL_HOST']
port = env['MYSQL_PORT']
database = env['MYSQL_DATABASE_NAME']

DATABASE_URL = ""mysql+pymysql://{}@{}:{}/{}"".format(
    user,
    host,
    port,
    database
)

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

main.py
from fastapi import FastAPI
from os import environ as env
from app.modules.todo import router as TodoRouter
app = FastAPI()
apiPath = '/' + env['API_PATH'] + '/' + env['API_VERSION']
# routers
app.include_router(TodoRouter.router, prefix=apiPath + '/todo', tags=['Todo'])

todo/router.py
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from app.database import get_db
from app.modules.todo.data_models import TodoRequestModel

router = APIRouter()

@router.post('/')
def create(todo: TodoRequestModel, db: Session = Depends(get_db)):
    return TodoController.create(todo, db)

On docker start


Docker container log when /post endpoint is hit
- INFO:     172.31.0.1:37760 - ""POST /api/v1/todo/ HTTP/1.1"" 500 Internal Server Error
- ERROR:    Exception in ASGI application
- Traceback (most recent call last):
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 796, in _connect
-     self._get_server_information()
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 994, in _get_server_information
-     self.server_charset = charset_by_id(lang).name
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/charset.py"", line 34, in by_id
-     return self._by_id[id]
- KeyError: 255
- 
- During handling of the above exception, another exception occurred:
- 
- Traceback (most recent call last):
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3250, in _wrap_pool_connect
-     return fn()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 310, in connect
-     return _ConnectionFairy._checkout(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 868, in _checkout
-     fairy = _ConnectionRecord.checkout(pool)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 476, in checkout
-     rec = pool._do_get()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 145, in _do_get
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 143, in _do_get
-     return self._create_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 256, in _create_connection
-     return _ConnectionRecord(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 371, in __init__
-     self.__connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 665, in __connect
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 661, in __connect
-     self.dbapi_connection = connection = pool._invoke_creator(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/create.py"", line 590, in connect
-     return dialect.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py"", line 597, in connect
-     return self.dbapi.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/__init__.py"", line 88, in Connect
-     return Connection(*args, **kwargs)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 634, in __init__
-     self._connect()
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 817, in _connect
-     raise OperationalError(
- pymysql.err.OperationalError: (2003, ""Can't connect to MySQL server on 'mysql' (255)"")
- 
- The above exception was the direct cause of the following exception:
- 
- Traceback (most recent call last):
-   File ""/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py"", line 404, in run_asgi
-     result = await app(  # type: ignore[func-returns-value]
-   File ""/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py"", line 84, in __call__
-     return await self.app(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/applications.py"", line 1054, in __call__
-     await super().__call__(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/applications.py"", line 123, in __call__
-     await self.middleware_stack(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 186, in __call__
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 164, in __call__
-     await self.app(scope, receive, _send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py"", line 62, in __call__
-     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 64, in wrapped_app
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
-     await app(scope, receive, sender)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 762, in __call__
-     await self.middleware_stack(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 782, in app
-     await route.handle(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 297, in handle
-     await self.app(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 77, in app
-     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 64, in wrapped_app
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
-     await app(scope, receive, sender)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 72, in app
-     response = await func(request)
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 299, in app
-     raise e
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 294, in app
-     raw_response = await run_endpoint_function(
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 193, in run_endpoint_function
-     return await run_in_threadpool(dependant.call, **values)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/concurrency.py"", line 40, in run_in_threadpool
-     return await anyio.to_thread.run_sync(func, *args)
-   File ""/usr/local/lib/python3.10/site-packages/anyio/to_thread.py"", line 56, in run_sync
-     return await get_async_backend().run_sync_in_worker_thread(
-   File ""/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 2134, in run_sync_in_worker_thread
-     return await future
-   File ""/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 851, in run
-     result = context.run(func, *args)
-   File ""/app/fastapi/app/modules/todo/router.py"", line 23, in create
-     return TodoController.create(todo, db)
-   File ""/app/fastapi/app/modules/todo/controller.py"", line 17, in create
-     return TodoService.create(student, db)
-   File ""/app/fastapi/app/modules/todo/repository.py"", line 62, in create
-     db.commit()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 1431, in commit
-     self._transaction.commit(_to_root=self.future)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 829, in commit
-     self._prepare_impl()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 808, in _prepare_impl
-     self.session.flush()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3363, in flush
-     self._flush(objects)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3502, in _flush
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3463, in _flush
-     flush_context.execute()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
-     rec.execute(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
-     util.preloaded.orm_persistence.save_obj(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 211, in save_obj
-     for (
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 372, in _organize_states_for_save
-     for state, dict_, mapper, connection in _connections_for_states(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 1711, in _connections_for_states
-     connection = uowtransaction.transaction.connection(base_mapper)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 626, in connection
-     return self._connection_for_bind(bind, execution_options)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 735, in _connection_for_bind
-     conn = self._parent._connection_for_bind(bind, execution_options)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 747, in _connection_for_bind
-     conn = bind.connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3204, in connect
-     return self._connection_cls(self, close_with_result=close_with_result)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 96, in __init__
-     else engine.raw_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3283, in raw_connection
-     return self._wrap_pool_connect(self.pool.connect, _connection)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3253, in _wrap_pool_connect
-     Connection._handle_dbapi_exception_noconnection(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 2100, in _handle_dbapi_exception_noconnection
-     util.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3250, in _wrap_pool_connect
-     return fn()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 310, in connect
-     return _ConnectionFairy._checkout(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 868, in _checkout
-     fairy = _ConnectionRecord.checkout(pool)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 476, in checkout
-     rec = pool._do_get()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 145, in _do_get
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 143, in _do_get
-     return self._create_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 256, in _create_connection
-     return _ConnectionRecord(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 371, in __init__
-     self.__connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 665, in __connect
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 661, in __connect
-     self.dbapi_connection = connection = pool._invoke_creator(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/create.py"", line 590, in connect
-     return dialect.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py"", line 597, in connect
-     return self.dbapi.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/__init__.py"", line 88, in Connect
-     return Connection(*args, **kwargs)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 634, in __init__
-     self._connect()
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 817, in _connect
-     raise OperationalError(
- sqlalchemy.exc.OperationalError: (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'mysql' (255)"")
- (Background on this error at: https://sqlalche.me/e/14/e3q8)

I've tried adding a password in the database connection string but gives me a different error like the following
DATABASE_URL = ""mysql+pymysql://{}:{}@{}:{}/{}"".format(
        user,
        password,
        host,
        port,
        database
    )

new log
- INFO:     172.31.0.1:52082 - ""POST /api/v1/todo/ HTTP/1.1"" 500 Internal Server Error
- ERROR:    Exception in ASGI application
- Traceback (most recent call last):
-   File ""/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py"", line 404, in run_asgi
-     result = await app(  # type: ignore[func-returns-value]
-   File ""/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py"", line 84, in __call__
-     return await self.app(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/applications.py"", line 1054, in __call__
-     await super().__call__(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/applications.py"", line 123, in __call__
-     await self.middleware_stack(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 186, in __call__
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 164, in __call__
-     await self.app(scope, receive, _send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py"", line 62, in __call__
-     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 64, in wrapped_app
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
-     await app(scope, receive, sender)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 762, in __call__
-     await self.middleware_stack(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 782, in app
-     await route.handle(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 297, in handle
-     await self.app(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 77, in app
-     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 64, in wrapped_app
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
-     await app(scope, receive, sender)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 72, in app
-     response = await func(request)
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 299, in app
-     raise e
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 294, in app
-     raw_response = await run_endpoint_function(
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 193, in run_endpoint_function
-     return await run_in_threadpool(dependant.call, **values)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/concurrency.py"", line 40, in run_in_threadpool
-     return await anyio.to_thread.run_sync(func, *args)
-   File ""/usr/local/lib/python3.10/site-packages/anyio/to_thread.py"", line 56, in run_sync
-     return await get_async_backend().run_sync_in_worker_thread(
-   File ""/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 2134, in run_sync_in_worker_thread
-     return await future
-   File ""/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 851, in run
-     result = context.run(func, *args)
-   File ""/app/fastapi/app/modules/todo/router.py"", line 23, in create
-     return TodoController.create(todo, db)
-   File ""/app/fastapi/app/modules/todo/controller.py"", line 17, in create
-     return TodoService.create(student, db)
-   File ""/app/fastapi/app/modules/todo/repository.py"", line 62, in create
-     db.commit()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 1431, in commit
-     self._transaction.commit(_to_root=self.future)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 829, in commit
-     self._prepare_impl()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 808, in _prepare_impl
-     self.session.flush()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3363, in flush
-     self._flush(objects)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3502, in _flush
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3463, in _flush
-     flush_context.execute()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
-     rec.execute(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
-     util.preloaded.orm_persistence.save_obj(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 211, in save_obj
-     for (
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 372, in _organize_states_for_save
-     for state, dict_, mapper, connection in _connections_for_states(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 1711, in _connections_for_states
-     connection = uowtransaction.transaction.connection(base_mapper)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 626, in connection
-     return self._connection_for_bind(bind, execution_options)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 735, in _connection_for_bind
-     conn = self._parent._connection_for_bind(bind, execution_options)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 747, in _connection_for_bind
-     conn = bind.connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3204, in connect
-     return self._connection_cls(self, close_with_result=close_with_result)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 96, in __init__
-     else engine.raw_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3283, in raw_connection
-     return self._wrap_pool_connect(self.pool.connect, _connection)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3250, in _wrap_pool_connect
-     return fn()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 310, in connect
-     return _ConnectionFairy._checkout(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 868, in _checkout
-     fairy = _ConnectionRecord.checkout(pool)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 476, in checkout
-     rec = pool._do_get()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 145, in _do_get
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 143, in _do_get
-     return self._create_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 256, in _create_connection
-     return _ConnectionRecord(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 371, in __init__
-     self.__connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 665, in __connect
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 661, in __connect
-     self.dbapi_connection = connection = pool._invoke_creator(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/create.py"", line 590, in connect
-     return dialect.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py"", line 597, in connect
-     return self.dbapi.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/__init__.py"", line 88, in Connect
-     return Connection(*args, **kwargs)
- TypeError: Connection.__init__() got an unexpected keyword argument 'password'

","First, any container that wants to read data from an env file must have an env_file section and the path of the env file must be specified inside it. So set this for mysql and redis services as well.
version: ""3.9""
services:
  mysql:
    env_file:
      - .env
  redis:
    env_file:
      - .env

Second, in the mysql service inside docker compose, you read the name of the container from the env file, and inside the env file, you have set this value equal to app_mysql.
MYSQL_CONTAINER_NAME='app_mysql'

Since routing in the docker compose network is something like DNS, you must set the value of MYSQL_HOST equal to the name of the mysql container so that your fastapi service can connect to the mysql container.
MYSQL_HOST='app_mysql'

",,,false,
https://stackoverflow.com/questions/77429344,true,The issue involves the usage of SQL expressions in the @property decorator for an async session in SQLAlchemy.,SQLAlchemy,@property decorator,"The issue arises when using the @property decorator in the Tweet model to define the attachments property. This property executes a SQL expression using SQLAlchemy's select() function to retrieve the paths of attached images based on the tweet_media_ids. However, when trying to access the attachments property, a sqlalchemy.exc.MissingGreenlet error is raised.","The expected behavior is for the attachments property to return a list of paths of attached images based on the tweet_media_ids. However, due to the error, the property cannot be accessed.","The issue is triggered when attempting to access the attachments property in the Tweet model, specifically when executing the SQL expression using object_session(self).execute(query).","This issue might be challenging to detect during development and testing because it involves the usage of SQL expressions in an async session, which can lead to unexpected errors related to greenlet spawning and IO operations.",How use SQL Expressions in @property decorator for async session in SQLAlchemy?,"I've got two models Media and Tweet.
Unfortunately, they don't have a one-to-many relationship. The specifics of the API are: one router sends images (Media), and the other router in the body receives a ids list of images that will be attached to the tweet.
I would like to set a attachments field in the Tweet model that would contain a list of paths of attached images, so that later I can simply validate using pydantic schemas.
class Media(Base):
    __tablename__ = 'medias'
    id: Mapped[int] = mapped_column(Sequence('media_id_seq'), primary_key=True, index=True)
    path: Mapped[str] = mapped_column(nullable=False)
    ....

class Tweet(Base):
    __tablename__ = 'tweets'
    id: Mapped[int] = mapped_column(Sequence('tweet_id_seq'), primary_key=True, index=True)
    content: Mapped[str] = mapped_column(TEXT, nullable=False)
 
    tweet_media_ids: Mapped[list[int]] = mapped_column(ARRAY(Integer), nullable=True)
  
    @property
    def attachments(self):
        if not self.tweet_media_ids:
            return []
        query = select(Media.path).where(Media.id.in_(self.tweet_media_ids))
        result = object_session(self).execute(query)
        return result.scalars().all()

I've tried to get a tweet and a list of appended media in attachments, but got  *sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/20/xd2s)* while reffering to item.attachments
`@router.get(""/tweets"", response_model=schemas.TweetsList)
async def get_tweets_list(db: Annotated[AsyncSession, Depends(get_db_session)]):
    tweets_orm = await get_tweets(db)
    for item in tweets_orm:
        print(""att="", item.attachments)
    tweets = [schemas.Tweet.model_validate(tweet) for tweet in tweets_orm]
    tweets_list = schemas.TweetsList(tweets=tweets)
    return tweets_list`

172.18.0.3:49338 - ""GET /api/tweets HTTP/1.0"" 500
 [2023-11-06 05:43:43 +0000] [7] [ERROR] Exception in ASGI application
 Traceback (most recent call last):
 File ""/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py"", line 426, in run_asgi
 result = await app(  # type: ignore[func-returns-value]
 File ""/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py"", line 84, in __call__
 return await self.app(scope, receive, send)
 172.18.0.1 - - [06/Nov/2023:05:43:43 +0000] ""GET /api/tweets HTTP/1.1"" 500 21 ""http://localhost/"" ""Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/119.0"" ""-""
 File ""/usr/local/lib/python3.10/site-packages/fastapi/applications.py"", line 292, in __call__
 await super().__call__(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/applications.py"", line 122, in __call__
 await self.middleware_stack(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 184, in __call__
 raise exc
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 162, in __call__
 await self.app(scope, receive, _send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/cors.py"", line 83, in __call__
 await self.app(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py"", line 79, in __call__
 raise exc
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py"", line 68, in __call__
 await self.app(scope, receive, sender)
 File ""/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py"", line 20, in __call__
 raise e
 File ""/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py"", line 17, in __call__
 await self.app(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 718, in __call__
 await route.handle(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 276, in handle
 await self.app(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 66, in app
 response = await func(request)
 File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 273, in app
 raw_response = await run_endpoint_function(
 File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 190, in run_endpoint_function
 return await dependant.call(**values)
 File ""/api/app/routes/tweets.py"", line 66, in get_tweets_list
 print(""att="", item.attachments)
 File ""/api/app/models.py"", line 79, in attachments
 result = object_session(self).execute(query)
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 2306, in execute
 return self._execute_internal(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 2188, in _execute_internal
 result: Result[Any] = compile_state_cls.orm_execute_statement(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/context.py"", line 293, in orm_execute_statement
 result = conn.execute(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1416, in execute
 return meth(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/sql/elements.py"", line 516, in _execute_on_connection
 return connection._execute_clauseelement(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1639, in _execute_clauseelement
 ret = self._execute_context(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1848, in _execute_context
 return self._exec_single_context(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1988, in _exec_single_context
 self._handle_dbapi_exception(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 2346, in _handle_dbapi_exception
 raise exc_info[1].with_traceback(exc_info[2])
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1969, in _exec_single_context
 self.dialect.do_execute(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py"", line 922, in do_execute
 cursor.execute(statement, parameters)
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py"", line 586, in execute
 self._adapt_connection.await_(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/_concurrency_py3k.py"", line 116, in await_only
 raise exc.MissingGreenlet(
 sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/20/xd2s)

Question:
how to fix the error or advise another implementation of the attachments field
","Use hybrid_property instead of property decorator.
from sqlalchemy.ext.hybrid import hybrid_property  # add this line


class Tweet(Base):
    __tablename__ = 'tweets'
    id: Mapped[int] = mapped_column(Sequence('tweet_id_seq'), primary_key=True, index=True)
    content: Mapped[str] = mapped_column(TEXT, nullable=False)
 
    tweet_media_ids: Mapped[list[int]] = mapped_column(ARRAY(Integer), nullable=True)
  
    @hybrid_property
    def attachments(self):
        if not self.tweet_media_ids:
            return []
        query = select(Media.path).where(Media.id.in_(self.tweet_media_ids))
        result = object_session(self).execute(query)
        return result.scalars().all()

sqlalchemy.exc.MissingGreenlet raised by relations might be resolved using hybrid_property in your case. The error occurs when manipulating join commands in async functions with bad relationships, including lazy loading strategies.
In addition, SQLAlchemy doesn't have to have physical relationships. So you can use relationship in your code even if two tables do not have actual relations.
With that:
class Tweet(Base):
    __tablename__ = 'tweets'
    id: Mapped[int] = mapped_column(Sequence('tweet_id_seq'), primary_key=True, index=True)
    content: Mapped[str] = mapped_column(TEXT, nullable=False)
 
    medias: Mapped[list[Media]] = relationship(back_populates=""medias"")
  
    @hybrid_property
    def attachments(self) -&gt; list[Media]:
        return self.medias

SQLAlchemy will generate a query with the relationship automatically. In fact, when setting logical relationships, you don't need to write the attachments method.
Or use awaitable_attrs
You are using AsyncSession so that you can use awaitable_attrs
@router.get(""/tweets"", response_model=schemas.TweetsList)
async def get_tweets_list(db: Annotated[AsyncSession, Depends(get_db_session)]):
    tweets_orm = await get_tweets(db)
    for item in tweets_orm:
        print(""att="", item.awaitable_attrs.attachments)
    tweets = [schemas.Tweet.model_validate(tweet) for tweet in tweets_orm]
    tweets_list = schemas.TweetsList(tweets=tweets)
    return tweets_list

awaitable_attrs resolves deferred columns that are not loaded yet. It can await result = await object_session(self).execute(query) in method attachments.
","    async def get_tweets_list(
        db: Annotated[AsyncSession, Depends(get_db_session)]
):
    tweets_orm = await get_tweets(db)
    for item in tweets_orm:
        print(""att="", await item.awaitable_attrs.attachments)
    tweets = [schemas.Tweet.model_validate(tweet) for tweet in tweets_orm]

It seems item.attachments return coroutine.
await item.awaitable_attrs.attachments return desired list.
So I guess an addition hit to DB is needed. And those query in property decorator does not execute
",,false,
https://stackoverflow.com/questions/69466354,true,The issue involves a problematic API related to uploading artifacts to an S3 bucket using MLflow. The error message indicates a failure to upload the artifact due to an 'InvalidAccessKeyId' error.,MLflow,mlflow.keras.log_model,The issue occurs when attempting to log a Keras model using mlflow.keras.log_model.,"The expected behavior is for the Keras model to be logged as an artifact in the specified MLflow server. However, the upload fails with an 'InvalidAccessKeyId' error.",The issue is triggered when mlflow.keras.log_model is called with the Keras model that needs to be logged.,"This issue might be challenging to detect during development and testing because it involves the interaction between MLflow, the S3 storage backend, and the user's access credentials.",MLflow S3UploadFailedError: Failed to upload,"I've created with docker a MinioS3 artifact storage and a MySQL backend storage using the next Docker Compose:
    version: '3.8'
    services:
        db:
           environment:
              - MYSQL_DATABASE=${MYSQL_DATABASE}
              - MYSQL_USER=${MYSQL_USER}
              - MYSQL_PASSWORD=${MYSQL_PASSWORD}
              - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
           expose:
              - '3306'        
           volumes:
              - '(path)/server_backend:/var/lib/mysql '
           image: 'mysql'
           container_name: db

        storage:
            environment:
                - MINIO_ACCESS_KEY=${MINIO_USR}
                - MINIO_SECRET_KEY=${MINIO_PASS}
            expose:
                - '9000'
            ports:
                - '9000:9000'        
            depends_on:
                - db
            command: server /data
            volumes:
                - '(path)/server_artifact:/data'
            image: minio/minio:RELEASE.2021-02-14T04-01-33Z
            container_name: MinIO

        mlflow:
            build: ./mlflow
            environment:
                - AWS_ACCESS_KEY_ID=${MINIO_USR}
                - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       
            expose:
                - '5000'
            ports:
                - '5000:5000'
            depends_on:
                - storage                       
            image: 'mlflow:Dockerfile'
            container_name: server

The Mlflow server docker was created using the next Dockerfile:
    FROM python:3.8-slim-buster
    WORKDIR /usr/src/app
    RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql
    ENV MLFLOW_S3_ENDPOINT_URL=http://storage:9000
    CMD mlflow server \
        --backend-store-uri mysql+pymysql://MLFLOW:temporal@db:3306/DBMLFLOW \
        --default-artifact-root s3://artifacts \
        --host 0.0.0.0

The credantials are defined in a .env file.
The results of the docker-compose up command:

    [+] Running 21/22
     - mlflow Error                                                                                                                              5.6s
     - storage Pulled                                                                                                                           36.9s
       - a6b97b4963f5 Pull complete                                                                                                             24.6s
       - 13948a011eec Pull complete                                                                                                             24.7s
       - 40cdef9976a6 Pull complete                                                                                                             24.7s
       - f47162848743 Pull complete                                                                                                             24.8s
       - 5f2758d8e94c Pull complete                                                                                                             24.9s
       - c2950439edb8 Pull complete                                                                                                             25.0s
       - 1b08f8a15998 Pull complete                                                                                                             30.7s
     - db Pulled                                                                                                                                45.8s
       - 07aded7c29c6 Already exists                                                                                                             0.0s
       - f68b8cbd22de Pull complete                                                                                                              0.7s
       - 30c1754a28c4 Pull complete                                                                                                              2.1s
       - 1b7cb4d6fe05 Pull complete                                                                                                              2.2s
       - 79a41dc56b9a Pull complete                                                                                                              2.3s
       - 00a75e3842fb Pull complete                                                                                                              6.7s
       - b36a6919c217 Pull complete                                                                                                              6.8s
       - 635b0b84d686 Pull complete                                                                                                              6.8s
       - 6d24c7242d02 Pull complete                                                                                                             39.4s
       - 5be6c5edf16f Pull complete                                                                                                             39.5s
       - cb35eac1242c Pull complete                                                                                                             39.5s
       - a573d4e1c407 Pull complete                                                                                                             39.6s
    [+] Building 1.4s (7/7) FINISHED
     =&gt; [internal] load build definition from Dockerfile                                                                                         0.0s
     =&gt; =&gt; transferring dockerfile: 32B                                                                                                          0.0s
     =&gt; [internal] load .dockerignore                                                                                                            0.0s
     =&gt; =&gt; transferring context: 2B                                                                                                              0.0s
     =&gt; [internal] load metadata for docker.io/library/python:3.8-slim-buster                                                                    1.3s
     =&gt; [1/3] FROM docker.io/library/python:3.8-slim-buster@sha256:13a3f2bffb4b18ff7eda2763a3b0ba316dd82e548f52ea8b4fd11c94b97afa7d              0.0s
     =&gt; CACHED [2/3] WORKDIR /usr/src/app                                                                                                        0.0s
     =&gt; CACHED [3/3] RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql                                                           0.0s
     =&gt; exporting to image                                                                                                                       0.0s
     =&gt; =&gt; exporting layers                                                                                                                      0.0s
     =&gt; =&gt; writing image sha256:76d4e4462b5c7c1826734e59a54488b56660de0dd5ecc188c308202608a8f20b                                                 0.0s
     =&gt; =&gt; naming to docker.io/library/mlflow:Dockerfile                                                                                         0.0s
    
    Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them
    [+] Running 3/3
     - Container db  Created                                                                                                       0.5s
     - Container MinIO      Created                                                                                                       0.1s
     - Container server     Created                                                                                                       0.1s
    Attaching to server, MinIO, db
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Initializing database files
    db  | 2021-10-06T12:12:57.679527Z 0 [System] [MY-013169] [Server] /usr/sbin/mysqld (mysqld 8.0.26) initializing of server in progress as process 44
    db  | 2021-10-06T12:12:57.687748Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
    db  | 2021-10-06T12:12:58.230036Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
    db  | 2021-10-06T12:12:59.888820Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main
    db  | 2021-10-06T12:12:59.889102Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main
    db  | 2021-10-06T12:12:59.997461Z 6 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.
    MinIO      | Attempting encryption of all config, IAM users and policies on MinIO backend
    MinIO      | Endpoint: http://172.18.0.3:9000  http://127.0.0.1:9000
    MinIO      |
    MinIO      | Browser Access:
    MinIO      |    http://172.18.0.3:9000  http://127.0.0.1:9000
    MinIO      |
    MinIO      | Object API (Amazon S3 compatible):
    MinIO      |    Go:         https://docs.min.io/docs/golang-client-quickstart-guide
    MinIO      |    Java:       https://docs.min.io/docs/java-client-quickstart-guide
    MinIO      |    Python:     https://docs.min.io/docs/python-client-quickstart-guide
    MinIO      |    JavaScript: https://docs.min.io/docs/javascript-client-quickstart-guide
    MinIO      |    .NET:       https://docs.min.io/docs/dotnet-client-quickstart-guide
    server     | 2021/10/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 0.1 seconds
    server     | 2021/10/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 0.3 seconds
    server     | 2021/10/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 0.7 seconds
    server     | 2021/10/06 12:13:03 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 1.5 seconds
    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Database files initialized
    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Starting temporary server
    db  | 2021-10-06T12:13:04.422603Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.26) starting as process 93
    db  | 2021-10-06T12:13:04.439806Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
    db  | 2021-10-06T12:13:04.575773Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
    db  | 2021-10-06T12:13:04.827307Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:04.827865Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:04.832827Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
    db  | 2021-10-06T12:13:04.834132Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
    db  | 2021-10-06T12:13:04.841629Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
    db  | 2021-10-06T12:13:04.855748Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: /var/run/mysqld/mysqlx.sock
    db  | 2021-10-06T12:13:04.855801Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.26'  socket: '/var/run/mysqld/mysqld.sock'  port: 0  MySQL Community Server - GPL.
    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Temporary server started.
    server     | 2021/10/06 12:13:05 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 3.1 seconds
    db  | Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
    db  | Warning: Unable to load '/usr/share/zoneinfo/leap-seconds.list' as time zone. Skipping it.
    db  | Warning: Unable to load '/usr/share/zoneinfo/zone.tab' as time zone. Skipping it.
    db  | Warning: Unable to load '/usr/share/zoneinfo/zone1970.tab' as time zone. Skipping it.
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating database DBMLFLOW
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating user MLFLOW
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Giving user MLFLOW access to schema DBMLFLOW
    db  |
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Stopping temporary server
    db  | 2021-10-06T12:13:06.948482Z 13 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.26).
    server     | 2021/10/06 12:13:08 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 6.3 seconds
    db  | 2021-10-06T12:13:08.716131Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.26)  MySQL Community Server - GPL.
    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: Temporary server stopped
    db  |
    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: MySQL init process done. Ready for start up.
    db  |
    db  | 2021-10-06T12:13:09.159115Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.26) starting as process 1
    db  | 2021-10-06T12:13:09.167405Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
    db  | 2021-10-06T12:13:09.298925Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
    db  | 2021-10-06T12:13:09.488958Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:09.489087Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:09.489934Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
    db  | 2021-10-06T12:13:09.490169Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
    db  | 2021-10-06T12:13:09.494728Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
    db  | 2021-10-06T12:13:09.509856Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: /var/run/mysqld/mysqlx.sock
    db  | 2021-10-06T12:13:09.509982Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.26'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server - GPL.
    db  | mbind: Operation not permitted
    server     | 2021/10/06 12:13:14 INFO mlflow.store.db.utils: Creating initial MLflow database tables...
    server     | 2021/10/06 12:13:14 INFO mlflow.store.db.utils: Updating database tables
    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.
    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
    server     | INFO  [alembic.runtime.migration] Running upgrade  -&gt; 451aebb31d03, add metric step
    server     | INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tags
    server     | INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric values
    server     | INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags Table
    server     | INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limit
    server     | INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -&gt; 89d4b8295536, create latest metrics table
    server     | INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!
    server     | INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -&gt; 2b4d017a5e9b, add model registry tables to db
    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.
    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!
    server     | INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -&gt; cfd24bdc0731, Update run status constraint with killed
    server     | INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -&gt; 0a8213491aaa, drop_duplicate_killed_constraint
    server     | INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -&gt; 728d730b5ebd, add registered model tags table
    server     | INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -&gt; 27a6a02d2cf1, add model version tags table
    server     | INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -&gt; 84291f40a231, add run_link to model_version
    server     | INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -&gt; a8c4a736bde6, allow nulls for run_id
    server     | INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -&gt; 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary
    server     | INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -&gt; c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql
    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.
    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
    db  | mbind: Operation not permitted
    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Starting gunicorn 20.1.0
    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Listening at: http://0.0.0.0:5000 (17)
    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Using worker: sync
    server     | [2021-10-06 12:13:16 +0000] [19] [INFO] Booting worker with pid: 19
    server     | [2021-10-06 12:13:16 +0000] [20] [INFO] Booting worker with pid: 20
    server     | [2021-10-06 12:13:16 +0000] [21] [INFO] Booting worker with pid: 21
    server     | [2021-10-06 12:13:16 +0000] [22] [INFO] Booting worker with pid: 22


It makes me suspect because on the second line appears - mlflow Error but I think that this is why the other builds haven't finished.
Then I've set my environment variables on the client to create the information flow between my script and the storages:

    os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://localhost:9000/'
    os.environ['AWS_ACCESS_KEY_ID'] = 'key'
    os.environ['AWS_SECRET_ACCESS_KEY'] = 'pw'
    
    remote_server_uri = ""http://localhost:5000/"" # server URI
    mlflow.set_tracking_uri(remote_server_uri)
    
    mlflow.set_experiment(""mnist_mLflow_demo"")


finally I trained a TensorFlow network and I didn't have problems storing parameters and metrics but gave me some warnings (referring to next error). But the model haven't been auto log, so I tried to do it manually:
    with mlflow.start_run(run_name = ""test0"") as run:
    
        mlflow.keras.log_model(model2, 'model2')

    mlflow.end_run()

It dosen't work and it gives me the next INFO (but essencialy an error):
    INFO:tensorflow:Assets written to: (path)\Temp\tmpgr5eaha2\model\data\model\assets
    INFO:tensorflow:Assets written to: (path)\Temp\tmpgr5eaha2\model\data\model\assets
    2021/10/06 14:16:00 ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: (path)\AppData\Local\Temp\tmpgr5eaha2\model, flavor: keras)
    Traceback (most recent call last):
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\environment.py"", line 212, in infer_pip_requirements
        return _infer_requirements(model_uri, flavor)
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\requirements_utils.py"", line 263, in _infer_requirements
        modules = _capture_imported_modules(model_uri, flavor)
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\requirements_utils.py"", line 221, in _capture_imported_modules
        _run_command(
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\requirements_utils.py"", line 163, in _run_command
        stderr = stderr.decode(""utf-8"")
    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 349: invalid continuation byte


And the next error:

    ClientError                               Traceback (most recent call last)
    ~\Python\Python39\lib\site-packages\boto3\s3\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)
        278         try:
    --&gt; 279             future.result()
        280         # If a client error was raised, add the backwards compatibility layer
    
    ~\Python\Python39\lib\site-packages\s3transfer\futures.py in result(self)
        105             # out of this and propogate the exception.
    --&gt; 106             return self._coordinator.result()
        107         except KeyboardInterrupt as e:
    
    ~\Python\Python39\lib\site-packages\s3transfer\futures.py in result(self)
        264         if self._exception:
    --&gt; 265             raise self._exception
        266         return self._result
    
    ~\Python\Python39\lib\site-packages\s3transfer\tasks.py in __call__(self)
        125             if not self._transfer_coordinator.done():
    --&gt; 126                 return self._execute_main(kwargs)
        127         except Exception as e:
    
    ~\Python\Python39\lib\site-packages\s3transfer\tasks.py in _execute_main(self, kwargs)
        149 
    --&gt; 150         return_value = self._main(**kwargs)
        151         # If the task is the final task, then set the TransferFuture's
    
    ~\Python\Python39\lib\site-packages\s3transfer\upload.py in _main(self, client, fileobj, bucket, key, extra_args)
        693         with fileobj as body:
    --&gt; 694             client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
        695 
    
    ~\Python\Python39\lib\site-packages\botocore\client.py in _api_call(self, *args, **kwargs)
        385             # The ""self"" in this scope is referring to the BaseClient.
    --&gt; 386             return self._make_api_call(operation_name, kwargs)
        387 
    
    ~\Python\Python39\lib\site-packages\botocore\client.py in _make_api_call(self, operation_name, api_params)
        704             error_class = self.exceptions.from_code(error_code)
    --&gt; 705             raise error_class(parsed_response, operation_name)
        706         else:
    
    ClientError: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.
    
    During handling of the above exception, another exception occurred:
    
    S3UploadFailedError                       Traceback (most recent call last)
    C:\Users\FCAIZA~1\AppData\Local\Temp/ipykernel_7164/2476247499.py in &lt;module&gt;
          1 with mlflow.start_run(run_name = ""test0"") as run:
          2 
    ----&gt; 3     mlflow.keras.log_model(model2, 'model2')
          4 
          5 mlflow.end_run()
    
    ~\Python\Python39\lib\site-packages\mlflow\keras.py in log_model(keras_model, artifact_path, conda_env, custom_objects, keras_module, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, **kwargs)
        402             mlflow.keras.log_model(keras_model, ""models"")
        403     """"""
    --&gt; 404     Model.log(
        405         artifact_path=artifact_path,
        406         flavor=mlflow.keras,
    
    ~\Python\Python39\lib\site-packages\mlflow\models\model.py in log(cls, artifact_path, flavor, registered_model_name, await_registration_for, **kwargs)
        186             mlflow_model = cls(artifact_path=artifact_path, run_id=run_id)
        187             flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)
    --&gt; 188             mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)
        189             try:
        190                 mlflow.tracking.fluent._record_logged_model(mlflow_model)
    
    ~\Python\Python39\lib\site-packages\mlflow\tracking\fluent.py in log_artifacts(local_dir, artifact_path)
        582     """"""
        583     run_id = _get_or_start_run().info.run_id
    --&gt; 584     MlflowClient().log_artifacts(run_id, local_dir, artifact_path)
        585 
        586 
    
    ~\Python\Python39\lib\site-packages\mlflow\tracking\client.py in log_artifacts(self, run_id, local_dir, artifact_path)
        975             is_dir: True
        976         """"""
    --&gt; 977         self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)
        978 
        979     @contextlib.contextmanager
    
    ~\Python\Python39\lib\site-packages\mlflow\tracking\_tracking_service\client.py in log_artifacts(self, run_id, local_dir, artifact_path)
        332         :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.
        333         """"""
    --&gt; 334         self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)
        335 
        336     def list_artifacts(self, run_id, path=None):
    
    ~\Python\Python39\lib\site-packages\mlflow\store\artifact\s3_artifact_repo.py in log_artifacts(self, local_dir, artifact_path)
        102                 upload_path = posixpath.join(dest_path, rel_path)
        103             for f in filenames:
    --&gt; 104                 self._upload_file(
        105                     s3_client=s3_client,
        106                     local_file=os.path.join(root, f),
    
    ~\Python\Python39\lib\site-packages\mlflow\store\artifact\s3_artifact_repo.py in _upload_file(self, s3_client, local_file, bucket, key)
         78         if environ_extra_args is not None:
         79             extra_args.update(environ_extra_args)
    ---&gt; 80         s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)
         81 
         82     def log_artifact(self, local_file, artifact_path=None):
    
    ~\Python\Python39\lib\site-packages\boto3\s3\inject.py in upload_file(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)
        128     """"""
        129     with S3Transfer(self, Config) as transfer:
    --&gt; 130         return transfer.upload_file(
        131             filename=Filename, bucket=Bucket, key=Key,
        132             extra_args=ExtraArgs, callback=Callback)
    
    ~\Python\Python39\lib\site-packages\boto3\s3\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)
        283         # client error.
        284         except ClientError as e:
    --&gt; 285             raise S3UploadFailedError(
        286                 ""Failed to upload %s to %s: %s"" % (
        287                     filename, '/'.join([bucket, key]), e))
    
    S3UploadFailedError: Failed to upload (path)\AppData\Local\Temp\tmpgr5eaha2\model\conda.yaml to artifacts/1/5ae5fcef2d07432d811c3d7eb534382c/artifacts/model2/conda.yaml: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.


","I found the solution of this issue. It is a tricky problem due to spanish characters, my system's user profile in ""C:/"" is ""fcaizares"" (Caizares is my first last name). I have created another user named ""fcanizares"" and all is working fine. Hope you find this solution helpfull.
PS: Moral of the issue, get rid of the extrange characters!
",,,false,
https://stackoverflow.com/questions/64952367,true,"The issue involves the SQLAlchemy Core API, specifically the values() method. The behavior described in the issue is unexpected and does not align with the expected behavior of the API.",SQLAlchemy,values,"The issue is related to the behavior of the values() method in SQLAlchemy Core. The expectation is that the method should limit the columns in the INSERT statement to only those specified, but in this case, it does not.","The expected behavior is for the values() method to generate an INSERT statement that includes only the specified columns. However, in this case, it includes all columns in the users table.",The issue occurs when using the values() method in SQLAlchemy Core to generate an INSERT statement with limited columns.,"This issue might be challenging to detect because the behavior of the values() method might not be well-documented or widely known, leading to unexpected results for users who expect it to work differently.",SQLAlchemy Core. Values method does not limit columns in insert statement,"SQLALchemy Core insert expressions documentation says:

Notice above that the INSERT statement names every column in the users table. This can be limited by using the values() method, which establishes the VALUES clause of the INSERT explicitly:

With that in mind, I wrote the following snippet, which returns unexpected results.
from datetime import datetime
import sqlalchemy
from sqlalchemy import types
from sqlalchemy.dialects import postgresql


metadata = sqlalchemy.MetaData()

users = sqlalchemy.Table(
    ""users"",
    metadata,
    sqlalchemy.Column(
        ""id"",
        postgresql.UUID(as_uuid=True),
        default=uuid.uuid4(),
        primary_key=True,
    ),
    sqlalchemy.Column(""email"", types.String, unique=True, index=True),
    sqlalchemy.Column(
        ""created_at"",
        types.TIMESTAMP(timezone=True),
        default=datetime.utcnow(),
    ),
    sqlalchemy.Column(
        ""updated_at"",
        types.TIMESTAMP(timezone=True),
        default=datetime.utcnow(),
        onupdate=datetime.utcnow(),
    ),
)


email = ""god@olympus.org""
query = users.insert().values(email=email)

# (Pdb) print(query)
# INSERT INTO users (id, email, created_at, updated_at) VALUES (:id, :email, :created_at, :updated_at)
#
# (Pdb) print(query.compile().params)
# {'id': None, 'email': 'god@olympus.org', 'created_at': None, 'updated_at': None}

I expected the query to be INSERT INTO users (email) VALUES (:email)
Is there anything I'm missing?
I'm using SQLAlchemy==1.3.20 by the way.
","The issue does not relate to SQLAlchemy, but rather to encode/databases.
Support for ""default"" parameter in sqlalchemy.Column
",,,false,
https://stackoverflow.com/questions/64755423,true,"The issue involves the pandas to_sql() function and the pyhive library, indicating a potential problem with the interaction between the two APIs.","pandas, pyhive",pandas.to_sql,The issue is related to using pandas to_sql() function with pyhive to replace a table in Hive. It results in an error related to string formatting.,"The expected behavior is for the to_sql() function to replace the table in Hive with the new pandas dataframe. However, it throws a DatabaseError related to string formatting.",The issue is triggered when using pandas to_sql() function with pyhive to replace a table in Hive.,"This issue might be challenging to detect because it involves the interaction between two different libraries (pandas and pyhive), and the error message does not provide clear information about the root cause.",Execution failed when using pandas to_sql and pyhive to replace table - DatabaseError: &quot;... not all arguments converted during string formatting&quot;,"I need to replace a table in Hive with a new pandas dataframe. I am using pyhive to create a connection engine and subsequently using pandas.to_sql with 'if_exists' as replace.
from pyhive import hive

my_data = pd.read_csv('my_data.csv')

conn = hive.Connection(host=""111.11.11.11"", port=10000, username=""abcd"")

my_data.to_sql(name='table_name', con=conn, if_exists='replace', schema='my_schema')

conn.close()

However, this results in an unexpected error as follows:
DatabaseError: Execution failed on sql: SELECT name FROM sqlite_master WHERE type='table' AND name=?;
not all arguments converted during string formatting
unable to rollback

Other answers seem to indicate that this is related to to_sql expecting a SqlAlchemy engine - I was under the impression that this is what pyhive uses to create a connection. Any guidance on how to appropriately execute this command would be appreciated.
It is worth noting that elsewhere, this same connection has no problem reading data from Hive.
Works as expected:
conn = hive.Connection(host=""111.11.11.11"", port=10000, username=""abcd"")

my_data = pd.read_sql('select * from my_table', conn)

conn.close()

","
Other answers seem to indicate that this is related to to_sql expecting a SqlAlchemy engine - I was under the impression that this is what pyhive uses to create a connection.

PyHive can create a SQLAlchemy Engine object, but not the way you're doing it. As illustrated in the PyHive docs, you need to do something like
engine = create_engine('hive://localhost:10000/default')

and then pass the engine object to to_sql.

[read_sql] Works as expected

read_sql will often work with just a DBAPI connection, but to_sql requires a SQLAlchemy Connectable (Engine or Connection) because it may need to generate DDL. See this answer for more information.
",,,false,
https://stackoverflow.com/questions/59577085,false,The issue does not involve a problematic API. It is related to a user's attempt to learn and troubleshoot REST API development with Python and Flask.,,,,,,,Rest Api post definition failing | Python | Flask,"I am very new to python. I have basic idea with python and flask is totally new for me. Trying to learn rest api with python using flask. But, I am facing the issue while posting data. below are sample code and error. Please help me to fix this issue. Thanks in advance. I tried googling but did not get any solution.

Error while running

127.0.0.1 - - [03/Jan/2020 15:58:54] ""POST /api/User HTTP/1.1"" 500 -
  Traceback (most recent call last):
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1997, in __call__
      return self.wsgi_app(environ, start_response)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1985, in wsgi_app
      response = self.handle_exception(e)
    File ""/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py"", line 273, in error_router
      return original_handler(e)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1540, in handle_exception
      reraise(exc_type, exc_value, tb)
    File ""/usr/local/lib/python3.7/site-packages/flask/_compat.py"", line 32, in reraise
      raise value.with_traceback(tb)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1982, in wsgi_app
      response = self.full_dispatch_request()
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1614, in full_dispatch_request
      rv = self.handle_user_exception(e)
    File ""/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py"", line 273, in error_router
      return original_handler(e)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1517, in handle_user_exception
      reraise(exc_type, exc_value, tb)
    File ""/usr/local/lib/python3.7/site-packages/flask/_compat.py"", line 32, in reraise
      raise value.with_traceback(tb)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1612, in full_dispatch_request
      rv = self.dispatch_request()
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1598, in dispatch_request
      return self.view_functions[rule.endpoint](**req.view_args)
    File ""/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py"", line 480, in wrapper
      resp = resource(*args, **kwargs)
    File ""/usr/local/lib/python3.7/site-packages/flask/views.py"", line 84, in view
      return self.dispatch_request(*args, **kwargs)
    File ""/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py"", line 595, in dispatch_request
      resp = meth(*args, **kwargs)
    File ""/Users/z0034ff/Documents/music/resources/Users.py"", line 32, in post
      typeid=json_data['typeid'],
  TypeError: __init__() got an unexpected keyword argument 'fname'


File name: Model.py

from flask import Flask
  from marshmallow import Schema, fields, pre_load, validate
  from flask_marshmallow import Marshmallow
  from flask_sqlalchemy import SQLAlchemy


  ma = Marshmallow()
  db = SQLAlchemy()


  class Users(db.Model):
      __tablename__ = 'users'
      id = db.Column(db.Integer, primary_key=True)
      fname = db.Column(db.String(50), nullable=False)
      lname = db.Column(db.String(50), nullable=False)
      email = db.Column(db.String(100), nullable=False)
      phone = db.Column(db.String(50), nullable=False)
      typeid = db.Column(db.Integer, db.ForeignKey(
          'user_type.id', ondelete='CASCADE'), nullable=False)
      creation_date = db.Column(
          db.TIMESTAMP, server_default=db.func.current_timestamp(), nullable=False)

      def __init__(self, users, typeid):
          self.users = users
          self.typeid = typeid

  class UsersSchema(ma.Schema):
      id = fields.Integer(dump_only=True)
      fname = fields.String(required=True, validate=validate.Length(1))
      lname = fields.String(required=True, validate=validate.Length(1))
      email = fields.String(required=True, validate=validate.Length(1))
      phone = fields.String(required=True, validate=validate.Length(10))
      typeid = fields.Integer(required=True)
      creation_date = fields.DateTime()


File name: Users.py

from flask import jsonify, request
  from flask_restful import Resource
  from Model import db, Users, UsersSchema

  users_schema = UsersSchema(many=True)
  user_schema = UsersSchema()


  class UsersResource(Resource):
      def get(self):
          users = Users.query.all()
          users = users_schema.dump(users).data
          return {""status"": ""success"", ""data"": users}, 200

      def post(self):
          json_data = request.get_json(force=True)
          if not json_data:
              return {'message': 'No input data provided'}, 400
          # Validate and deserialize input
          data, errors = user_schema.load(json_data)
          if errors:
              return errors, 422
          user = Users.query.filter_by(email=data['email']).first()
          if user:
              return {'message': 'User already exists'}, 400
          print(json_data['fname'])
          user = Users(
              fname=json_data['fname'],
              lname=json_data['lname'],
              email=json_data['email'],
              phone=json_data['phone'],
              typeid=json_data['typeid'],
          )

          db.session.add(user)
          db.session.commit()

          result = user_schema.dump(user).data

          return {""status"": 'success', 'data': result}, 201

",,,,false,
https://stackoverflow.com/questions/49288255,false,"The issue does not provide enough information to determine if it involves a problematic API. The issue title suggests it is related to batch insert with SQLAlchemy, but the body of the issue is empty.",,,,,,,SQLAlchemy batch insert with database functions,,,,,false,
https://stackoverflow.com/questions/35693081,false,The issue does not provide enough information to determine if it involves a problematic API. Both the title and body of the issue are empty.,,,,,,,sqlalchemy: order of query result unexpected,,,,,false,
https://stackoverflow.com/questions/32786334,false,The issue does not provide enough information to determine if it involves a problematic API. Both the title and body of the issue are empty.,,,,,,,Python3 sqlalchemy pymysql gevent sqlalchemy.util.queue.Empty gevent.hub.LoopExit: This operation would block forever,,,,,false,
https://stackoverflow.com/questions/29308836,false,The issue does not provide enough information to determine if it involves a problematic API. Both the title and body of the issue are empty.,,,,,,,Turbogears nostests results in OperationalError when using SQLAlchemy-FullText-Search,,,,,false,
https://stackoverflow.com/questions/10739967,false,The issue does not provide enough information to determine if it involves a problematic API. Both the title and body of the issue are empty.,,,,,,,Python subprocess calling bcp on .csv: &#39;unexpected eof&#39;,,,,,false,
https://stackoverflow.com/questions/77644943,true,"The issue involves the SQLAlchemy and clickhouse-driver libraries, indicating a potential problem with the interaction between the two APIs.","SQLAlchemy, clickhouse-driver",SQLAlchemy.create_async_engine,The issue is related to connecting to ClickHouse with SQLAlchemy and clickhouse-driver. It results in an 'Unexpected packet from server' error.,"The expected behavior is for the connection to ClickHouse to be established successfully. However, it throws an 'Unexpected packet from server' error.",The issue is triggered when attempting to connect to ClickHouse using SQLAlchemy and clickhouse-driver.,"This issue might be challenging to detect because it involves the interaction between two different libraries (SQLAlchemy and clickhouse-driver), and the error message does not provide clear information about the root cause.","asynch.errors.UnexpectedPacketFromServerError: Code: 102. Unexpected packet from server &lt;host:port&gt; (expected Hello or Exception, got Unknown packet)","I'm trying to connect to clickhouse with sqlalchemy. I'm using:

python3.11
clickhouse-driver == 0.2.6
sqlalchemy == 2.0.23
clickhouse-sqlalchemy == 0.3.0
asynch == 0.2.3
asyncio == 3.4.3

Here is my script I used:
import asyncio
import contextlib
import pydantic
import traceback
import typing
from sqlalchemy import text, TextClause, engine
from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession, create_async_engine
from sqlalchemy.pool import Pool, QueuePool 

class AsyncDatabase:
    def __init__(self):
        self.ch_uri: str = ""clickhouse+asynch://admin:Password123@host:31123/db""
        self.ch_engine: AsyncEngine = create_async_engine(
            url=self.ch_uri,
            echo=False,
            pool_size=100,
            max_overflow=20,
            poolclass=QueuePool,
        )
        self.ch_session: AsyncSession = AsyncSession(bind=self.ch_engine)
        self.ch_pool: Pool = self.ch_engine.pool

async_db: AsyncDatabase = AsyncDatabase()

@contextlib.asynccontextmanager
async def get_ch_session() -&gt; typing.AsyncGenerator[AsyncSession, None]:
    try:
        yield async_db.ch_session
    except Exception as e:
        print(traceback.print_exc())
        await async_db.ch_session.rollback()
    finally:
        await async_db.ch_session.close()


async def hello() -&gt; str:
    session: AsyncSession = None
    async with get_ch_session() as session:
        stmt: TextClause = text(""SELECT * FROM table_name LIMIT 1"")
        result: engine.Result = await session.execute(stmt)
        print(result.all())

    return ""ok""


if __name__ == ""__main__"":
    asyncio.run(hello())

I did try without async but still got the same error
asynch.errors.UnexpectedPacketFromServerError: Code: 102. Unexpected packet from server host:31123 (expected Hello or Exception, got Unknown packet)
when I use DataGrid to connect with above creds, it works fine. so I think it's not about the 31123 port
I stuck on this for 5 hrs, all answers from the web doesn't help me at all.
","You tried to use TCP Native protocol, but connected to HTTP based port
look grep tcp_port /var/lib/clickhouse/preprocessed_configs/config.xml
and use this value
",,,false,
https://stackoverflow.com/questions/76663686,true,"The issue involves the AWS Lambda environment, Python 3.8, and the interaction between async SQLAlchemy and asyncpg libraries.","SQLAlchemy, asyncpg",SQLAlchemy.ext.asyncio.create_async_engine,The issue is related to using async SQLAlchemy and asyncpg in an AWS Lambda function. It results in an 'IllegalStateChangeError' related to calling the 'close()' method.,"The expected behavior is for the async SQLAlchemy and asyncpg libraries to work correctly in the AWS Lambda environment. However, it throws an 'IllegalStateChangeError' when calling the 'close()' method.",The issue is triggered when using async SQLAlchemy and asyncpg in an AWS Lambda function.,"This issue might be challenging to detect because it involves the interaction between multiple libraries (SQLAlchemy, asyncpg) and the AWS Lambda environment, and the error message does not provide clear information about the root cause.","aws python 3.8 lambda + async sqlalchemy 2.0.10, asyncpg 0.28.0","from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession

engine_async = create_async_engine(
    ""postgresql+asyncpg://postgres:password@host_name.rds.amazonaws.com/postgres"")

def get_data(event, context): # this is the lambda handler
    return asyncio.get_event_loop().run_until_complete(get_data_async(event, context))

async def get_datas_async(event, context):
    org_id = event['org_id']
    async_session = sessionmaker(bind=engine_async, future=True, class_=AsyncSession)
    async with async_session() as session1, async_session() as session2, async_session() as session3:
        users_count_query = select(func.count(User.user_id)).filter_by(org_id=org_id)
        org_name_query = select(Organization.name).filter_by(org_id=org_id)
        other_data_query = select(
            func.count(Transaction.transaction_id).label('transactions_count'),
            func.sum(Transaction.total_cost).label('total_cost'),
            func.sum(Transaction.total_time).label('total_time')).where(Transaction.org_id == org_id)

        tasks = [
            session1.execute(users_count_query),
            session2.execute(other_data_query),
            session3.execute(org_name_query)
        ]
        results = await asyncio.gather(*tasks)
        userss_count, data, org_name = results
        data = data.fetchone()
        uesrs_count = users_count.scalar_one()
        org_name = org_name.scalar_one()

        return {
            ""statusCode"": 200,
            ""users_count"": users_count,
            ""transactions_count"": data.transactions_count,
            ""total_cost"": 0 if not data.total_cost else data.total_cost,
            ""total_time"": data.total_time,
            ""org_name"": org_name,
        }

I get this error:
{
    ""errorMessage"": ""Method 'close()' can't be called here; method '_connection_for_bind()' is already in progress and this would cause an unexpected state change to &lt;SessionTransactionState.CLOSED: 5&gt;"",
    ""errorType"": ""IllegalStateChangeError"",
    ""stackTrace"": [
        ""  File \""/var/task/app.py\"", line 131, in get_data\n    return asyncio.get_event_loop().run_until_complete(get_data_async(event, context))\n"",
        ""  File \""/var/lang/lib/python3.8/asyncio/base_events.py\"", line 616, in run_until_complete\n    return future.result()\n"",
        ""  File \""/var/task/app.py\"", line 175, in get_data_async\n    return {\n"",
        ""  File \""/var/task/sqlalchemy/ext/asyncio/session.py\"", line 859, in __aexit__\n    await asyncio.shield(task)\n"",
        ""  File \""/var/task/sqlalchemy/ext/asyncio/session.py\"", line 840, in close\n    await greenlet_spawn(self.sync_session.close)\n"",
        ""  File \""/var/task/sqlalchemy/util/_concurrency_py3k.py\"", line 154, in greenlet_spawn\n    result = context.switch(*args, **kwargs)\n"",
        ""  File \""/var/task/sqlalchemy/orm/session.py\"", line 2382, in close\n    self._close_impl(invalidate=False)\n"",
        ""  File \""/var/task/sqlalchemy/orm/session.py\"", line 2424, in _close_impl\n    transaction.close(invalidate)\n"",
        ""  File \""&lt;string&gt;\"", line 2, in close\n"",
        ""  File \""/var/task/sqlalchemy/orm/state_changes.py\"", line 121, in _go\n    raise sa_exc.IllegalStateChangeError(\n""
    ]
}

","solution and explanation are here: https://github.com/sqlalchemy/sqlalchemy/discussions/9312
",,,false,
https://stackoverflow.com/questions/75071043,true,"The issue involves a problematic API, specifically the sqlalchemy library and the pyodbc driver, when connecting to a SQL Server running in a Docker container. The error message indicates a problem with reading from the server, which suggests an issue with the connection or query execution.",SQLAlchemy,session.execute,The error occurs when executing the query using the session.execute method in SQLAlchemy. The error message indicates a problem with reading from the server.,"The expected behavior is for the query to be executed successfully and return the desired result from the SQL Server. However, the error indicates a failure in reading from the server.",The issue is triggered when executing a query using session.execute in SQLAlchemy with a connection to a SQL Server running in a Docker container.,"This issue might be challenging to detect during development and testing because it involves the interaction between multiple components (SQLAlchemy, pyodbc, Docker) and may depend on specific configurations or network setups.",Read from the server failed (20004) (SQLExecDirectW)Read from the server failed (20004) (SQLExecDirectW),"I am using the sqlalchemy to access sql server using pyodbc in Ubuntu OS
the sql server runs in docker container
the url for connecting the server in sqlalchemy, which I used is
'mssql+pyodbc:///?odbc_connects=""Driver=;SERVER=;DATABASE=;UID=;PWD=;port=;TDS_Version=8.0""
I run this statement - result = session.query(User).filter(User.username == username).first()
I got this error
(pyodbc.OperationalError) ('08S01', '[08S01] [FreeTDS][SQL Server]Read from the server failed (20004) (SQLExecDirectW)')
[SQL: SELECT TOP 1 [user].user_id AS user_user_id, [user].username AS user_username, [user].password AS user_password, [user].first_name AS user_first_name, [user].last_name AS user_last_name, [user].designation AS user_designation, [user].last_login AS user_last_login
FROM [user]
WHERE [user].username = ?]
[parameters: ('device1',)]
(Background on this error at: http://sqlalche.me/e/e3q8)
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1249, in _execute_context
    cursor, statement, parameters, context
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 552, in do_execute
    cursor.execute(statement, parameters)
pyodbc.OperationalError: ('08S01', '[08S01] [FreeTDS][SQL Server]Read from the server failed (20004) (SQLExecDirectW)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""./database1.py"", line 43, in get_user_details
    result = session.query(User).filter(User.username == username).first()
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3232, in first
    ret = list(self[0:1])
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3018, in __getitem__
    return list(res)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3334, in __iter__
    return self._execute_and_instances(context)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3359, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 988, in execute
    return meth(self, multiparams, params)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/sql/elements.py"", line 287, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1107, in _execute_clauseelement
    distilled_params,
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1253, in _execute_context
    e, statement, parameters, cursor, context
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1473, in _handle_dbapi_exception
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 152, in reraise
    raise value.with_traceback(tb)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1249, in _execute_context
    cursor, statement, parameters, context
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 552, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (pyodbc.OperationalError) ('08S01', '[08S01] [FreeTDS][SQL Server]Read from the server failed (20004) (SQLExecDirectW)')
[SQL: SELECT TOP 1 [user].user_id AS user_user_id, [user].username AS user_username, [user].password AS user_password, [user].first_name AS user_first_name, [user].last_name AS user_last_name, [user].designation AS user_designation, [user].last_login AS user_last_login
FROM [user]
WHERE [user].username = ?]
[parameters: ('device1',)]
(Background on this error at: http://sqlalche.me/e/e3q8)

Due to this error, all the following queries are triggering the different error each time-
sqlalchemy.exc.StatementError: (sqlalchemy.exc.InvalidRequestError) Can't reconnect until invalid transaction is rolled back

sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('01000', '[01000] [FreeTDS][SQL Server]Unexpected EOF from the server (20017) (SQLExecDirectW)')

pyodbc.OperationalError: ('08S01', '[08S01] [FreeTDS][SQL Server]Communication link failure (0) (SQLExecDirectW)')

sqlalchemy.exc.ResourceClosedError: This result object does not return rows. It has been closed automatically.

sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('HY000', '[HY000] [FreeTDS][SQL Server]Unknown error (0) (SQLExecDirectW)')

pyodbc.OperationalError: ('08S01', '[08S01] [FreeTDS][SQL Server]Bad token from the server: Datastream processing out of sync (20020) (SQLExecDirectW)')

sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('HY008', '[HY008] [FreeTDS][SQL Server]Operation was cancelled (0) (SQLExecDirectW)')

2023-01-10 16:38:20,010 - ERROR - sqlalchemy.pool.impl.QueuePool - Exception during reset or similar
dbapi_connection.rollback()
pyodbc.Error: ('HY000', 'The driver did not supply an error!')

What is the cause of this error?
UPDATE
the sql server version - select @@verion -

Microsoft SQL Server 2017 (RTM-CU22) (KB4577467) - 14.0.3356.20 (X64)
Aug 20 2020 22:33:27
Copyright (C) 2017 Microsoft Corporation
Express Edition (64-bit) on Linux (Ubuntu 16.04.7 LTS)

","You are using FreeTDS instead of the supported Microsoft ODBC Driver for SQL Server.  Download instructions are here.
",,,false,
https://stackoverflow.com/questions/74461385,false,The issue does not involve a problematic API. It is related to a misunderstanding of how to select specific columns and use the cast function in SQLAlchemy.,,,,,,,Select specific columns with cast using SQLAlchemy,"I'm using SQLAlchemy (Version: 1.4.44) and I'm having some unexpected results when trying to select columns and using cast on those columns.
First, most of the examples and even current documentation suggests column selection should work by passing an array to the select function like this:
s = select([table.c.col1])

However, I get the following error if I try this:
s = my_table.select([my_table.columns.user_id])

sqlalchemy.exc.ArgumentError: SQL expression for WHERE/HAVING role expected, got [Column('user_id', String(), table=&lt;my_table&gt;)].

Some examples suggest just placing the field directly in the select query.
s = select(table.c.col1)


But this seems to do nothing more than create an idle where-clause out of the field.
I eventually was able to achieve column selection with this approach:
s = my_table.select().with_only_columns(my_table.columns.created_at)


But I am not able to use cast for some reason with this approach.
s = my_table.select().with_only_columns(cast(my_table.columns.created_at, Date))

ValueError: Couldn't parse date string '2022' - value is not a string.

All help appreciated!
","I don't think table.select() is common usage.  SQLAlchemy is in a big transition right now on its way to 2.0.  In 1.4 (and in 2) the following syntax should work, use whatever session handling you already have working I just mean the select(...):
from sqlalchemy.sql import select, cast
from sqlalchemy.dialects.postgresql import INTEGER

class User(Base):
    __tablename__ = ""users""
    id = Column(
        Integer, nullable=False, primary_key=True
    )
    name = Column(Text)

with Session(engine) as session:
    u1 = User(name=""1"")
    session.add(u1)
    session.commit()

with Session(engine) as session:
    my_table = User.__table__
    # Cast user name into integer.
    print (session.execute(select(cast(my_table.c.name, INTEGER))).all())

",,,false,
https://stackoverflow.com/questions/72381857,true,"The issue involves a problematic API, specifically the SQLAlchemy library and the async ORM feature. The error messages indicate problems with updating the database after inserting a record, and they are related to the session management and transaction handling.",SQLAlchemy,session.add,The error messages indicate problems with updating the database after inserting a record using the session.add method in SQLAlchemy. The errors suggest issues with session management and transaction handling.,"The expected behavior is for the database to be updated successfully after inserting a record. However, the errors indicate failures in updating the database.",The issue is triggered when using the session.add method in SQLAlchemy to insert a record into the database and then attempting to update the database.,"This issue might be challenging to detect during development and testing because it involves the interaction between multiple components (SQLAlchemy, async ORM, session management) and may depend on specific configurations or usage patterns.",Unable to get an id from the database after doing insert using sqlalchemy,"Assuming this model:
class Pipeline(Base):
    __tablename__ = ""pipeline""
    pipeline_id = Column(String, primary_key=True)
    pipeline_version_id = Column(Integer, primary_key=True, autoincrement=True)
    is_active = Column(Boolean, nullable=False)
    update_ts = Column(DateTime, nullable=False)
    config = Column(TEXT(), nullable=True)
    parser_version = Column(String, nullable=True)

And this insert code:
        async with self.sessionFactory() as session:
            async with session.begin():
                query = select(Pipeline).filter_by(pipeline_id=pipeline_id).filter_by(is_active=True)
                result = await session.execute(query)
                active_pipelines = result.scalars().all()
                for entry in active_pipelines:
                    entry.is_active = False
                    self.logger.info(f""Marking pipeline/version {entry.pipeline_id}/{entry.pipeline_version_id} inactive"")

                self.logger.info(f""Creating update_pipeline:  {updated_pipeline}"")

                session.add(updated_pipeline)
                pipeline_version_id = updated_pipeline.pipeline_version_id
                self.logger.info('adding updated pipeline with version id {id}'.format(id=str(pipeline_version_id)))

When I run this, it is returning a null pipeline_version_id
[Edit]
Here are other alternatives I tried:
        async with self.sessionFactory() as session:
            async with session.begin():
                query = select(Pipeline).filter_by(pipeline_id=pipeline_id).filter_by(is_active=True)
                result = await session.execute(query)
                active_pipelines = result.scalars().all()
                for entry in active_pipelines:
                    entry.is_active = False
                    self.logger.info(f""Marking pipeline/version {entry.pipeline_id}/{entry.pipeline_version_id} inactive"")
                self.logger.info(f""Creating update_pipeline:  {updated_pipeline}"")
                session.add(updated_pipeline)

        pipeline_version_id = updated_pipeline.pipeline_version_id
        self.logger.info('adding updated pipeline with version id {id}'.format(id=str(pipeline_version_id)))

Result: Failed to update DB. Error: Instance &lt;Pipeline at 0x110c31da0&gt; is not bound to a Session; attribute refresh operation cannot proceed (Background on this error at: https://sqlalche.me/e/14/bhk3)
        async with self.sessionFactory() as session:
            async with session.begin():
                query = select(Pipeline).filter_by(pipeline_id=pipeline_id).filter_by(is_active=True)
                result = await session.execute(query)
                active_pipelines = result.scalars().all()
                for entry in active_pipelines:
                    entry.is_active = False
                    self.logger.info(f""Marking pipeline/version {entry.pipeline_id}/{entry.pipeline_version_id} inactive"")
                self.logger.info(f""Creating update_pipeline:  {updated_pipeline}"")
                session.add(updated_pipeline)

            pipeline_version_id = updated_pipeline.pipeline_version_id
            self.logger.info('adding updated pipeline with version id {id}'.format(id=str(pipeline_version_id)))

Result: Failed to update DB. Error: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/14/xd2s)
How do i get the ID of the inserted record without doing another query?
These are my dependencies:
python = ""^3.7""
setuptools = ""&lt;58""
fastapi = ""^0.54.1""
gunicorn = ""&lt;20.0""
uvicorn = ""^0.11.5""
sqlalchemy = ""^1.4.25""
apache-airflow=""1.10.10""
psycopg2-binary = ""^2.9.1""

Here is my session creation:
def create_session_factory(db_conn=None):
    #
    # Using https://rogulski.it/blog/sqlalchemy-14-async-orm-with-fastapi/
    #
    if not db_conn:
        db_conn = get_default_parser_db_conn_str()
    global _engine
    if not _engine:
        _engine = create_async_engine(db_conn, **DB_ENGINE_OPTIONS)
    sessionFactory = sessionmaker(_engine, class_=AsyncSession)
    sessionFactory.configure()
    return sessionFactory

and the DB_ENGINE_OPTIONS
DB_ENGINE_OPTIONS = {
    ""poolclass"": QueuePool,
    ""pool_size"": 10,
    ""max_overflow"": 50,
    ""pool_recycle"": 3600,
    ""pool_timeout"": 30
}

Any help appreciated.
this is my first time at async DB calls with python, so I am sure i am missing something.
","Thank you all, who answered.
Ultimately, just adding a session.flush() after the call to session.add(obj) gave me what i needed.
",,,false,
https://stackoverflow.com/questions/71004077,false,The issue does not involve a problematic API. It is related to a query error when calculating the average timestamp in PostgreSQL using Superset and SQLAlchemy.,,,,,,,"How to get average value from SQL(Superset, SQLAlchemy under PostgreSQL) timestamp","I tried to calculate the average timestamp and the query is successfully executed directly in postgresql, but when executed through superset(python's SQLAlchemy), an error occurs when converting an integer to a timestamp. How can I fix the error or do it in another way?
Query1:
select 
    to_timestamp(
        avg(
            cast(
                extract(epoch from last_checked_at) as integer)
                )::integer
        ) as datetime
from
    the_best_table

Query1 postgresql result:




datetime




2022-02-06 03:15:19+00




Query1 superset(SQLAlchemy) result:

PostgreSQL Error postgresql error: '&gt;=' not supported between
instances of 'datetime.timedelta' and 'int'
This may be triggered by: Issue 1002 - The database returned an
unexpected error. link



 

 
Also below you can see the results of the query to convert an integer to datetime. The problem is in this part of the code.
Query2:
select to_timestamp(1644117319) as datetime

Query2 postgresql result:




datetime




2022-02-06 03:15:19+00




Query2 superset(SQLAlchemy) result:

PostgreSQL Error postgresql error: '&gt;=' not supported between
instances of 'datetime.timedelta' and 'int'
This may be triggered by: Issue 1002 - The database returned an
unexpected error. link

","I was not able to reproduce the error.
Works fine.
Setup in Postgresql Database:
postgres=&gt; create table the_best_table(last_checked_at timestamp);
CREATE TABLE
postgres=&gt; insert into the_best_table values(now());
INSERT 0 1
postgres=&gt; select
    to_timestamp(
        avg(
            cast(
                extract(epoch from last_checked_at) as integer)
                )::integer
        ) as datetime
from
    the_best_table;
        datetime
------------------------
 2022-02-07 04:54:06+09
(1 row)

The exact two queries run fine for me in SQLAlchemy:
&gt;&gt;&gt; from sqlalchemy import create_engine
&gt;&gt;&gt; engine = create_engine('postgresql+pg8000://&lt;username&gt;:&lt;passwd&gt;@192.xxx.xxx.123/postgres')
&gt;&gt;&gt;  metadata.create_all(engine)
&gt;&gt;&gt; with engine.connect() as con:
...     rs = con.execute('select to_timestamp(avg(cast(extract(epoch from last_checked_at) as integer))::integer ) as datetime from the_best_table;')
...     for row in rs:
...             print(row)
... 
(datetime.datetime(2022, 2, 7, 4, 54, 6, tzinfo=datetime.timezone(datetime.timedelta(seconds=32400))),)
&gt;&gt;&gt; with engine.connect() as con:
...     rs = con.execute('select to_timestamp(1644117319) as datetime;')
...     for row in rs:
...             print(row)
... 
(datetime.datetime(2022, 2, 6, 12, 15, 19, tzinfo=datetime.timezone(datetime.timedelta(seconds=32400))),)

",,,false,
https://stackoverflow.com/questions/69447368,true,"The issue involves a problematic API, specifically the SQLAlchemy library and the MySQL driver. The error message indicates a problem with unexpected keyword arguments when connecting to an RDS instance using a token.",SQLAlchemy,create_engine,The error occurs when creating the engine using the create_engine function in SQLAlchemy. The error message indicates an unexpected keyword argument related to the token parameter.,"The expected behavior is for the engine to be created successfully and establish a connection to the RDS instance using a token. However, the error indicates a failure in creating the engine due to an unexpected keyword argument.",The issue is triggered when creating the engine using create_engine in SQLAlchemy with the intention of connecting to an RDS instance using a token.,"This issue might be challenging to detect during development and testing because it involves the interaction between multiple components (SQLAlchemy, MySQL driver) and may depend on specific configurations or authentication mechanisms.",Connecting to MySQL using a token,"I've been trying to connect to an RDS instance using the sqlalchemy library using a token. According to the docs it should be possible via the cparams['token'] variable, but doing so I get an error of an unexpected argument.
Going deeper into the docs, there was this code where the connection could be established using the keyword argument attrs_before, but in this case such argument also does not exist.
Does anyone has any idea on how to connect to the DB using a token?
I've trying to do it via such code but with no effect:
import boto3
import struct
from sqlalchemy import create_engine, event
from sqlalchemy.engine.url import URL
from sqlalchemy.engine.url import URL
from sqlalchemy.orm import sessionmaker

SQL_COPT_SS_ACCESS_TOKEN = 1256
session = boto3.session.Session(profile_name='xxx')
rds = session.client('rds')

def get_authentication_token():
    return rds.generate_db_auth_token(DBHostname='xxxx', Port=3306, DBUsername='xxx')

engine_url = URL.create(drivername='mysql+pymysql', host='xxxx')
engine = create_engine(engine_url)

#@event.listens_for(engine, 'do_connect')
#def provide_token(dialect, conn_rec, cargs, cparams):
#    token_struct = struct.pack(f'&lt;I{len(token)}s', len(token), token)
#    cparams['attrs_before'] = {SQL_COPT_SS_ACCESS_TOKEN: token_struct}

@event.listens_for(engine, ""do_connect"")
def provide_token(dialect, conn_rec, cargs, cparams):
    cparams['token'] = get_authentication_token()

Session = sessionmaker(bind=engine)

with Session() as session:
    result = session.execute('select now()').first()


To be more precise I get the error:
TypeError: __init__() got an unexpected keyword argument 'token'
","I had the same issue, what solved for me was replacing token with password:
@event.listens_for(engine, ""do_connect"")
def provide_token(dialect, conn_rec, cargs, cparams):
    cparams['password'] = get_authentication_token()

Another remark is that you must also pass the SSL certificate in the connection, I simply added another param:
@event.listens_for(engine, ""do_connect"")
def provide_token(dialect, conn_rec, cargs, cparams):
    cparams['password'] = get_authentication_token()
    caparam['ssl'] = {""ca"": ""rds-combined-ca-bundle.pem""}

Bear in mind that you might have to pass the full path to the rds-combined-ca-bundle.pem file. If you don't pass the SSL file and the connection param you'll still get an Access Denied even if everything else is setup correctly.
The SSL file can be downloaded from https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem
",,,false,
https://stackoverflow.com/questions/66277687,false,The issue does not involve a problematic API. It appears to be a question related to using Celery and Gino in a specific software setup.,,,,,,,Save an object using Celery and Gino,"I have the following pipeline
Event model (based on Gino 'db' object):
class Event(db.Model):
    __tablename__ = ""events""

    id = db.Column(db.BigInteger(), primary_key=True)
    request_timestamp = db.Column(db.DateTime(), nullable=False)
    service = db.Column(db.String(), nullable=False)
    url = db.Column(db.String(), nullable=False)
    status_code = db.Column(db.Integer(), nullable=False)
    response_time = db.Column(db.DateTime(), nullable=False)

FastApi app has a POST view with call to celery task to pass the data of the Event object:
@router.post(""/events/add"")
async def add_event(event: EventModel):
    event_data = {'request_timestamp': event.request_timestamp.replace(tzinfo=None),
                  'service': event.service,
                  'url': event.url,
                  'status_code': event.status_code,
                  'response_time': event.response_time.replace(tzinfo=None)
                  }

    task = celery_app.send_task(""monitoring_service.src.monitoring_service.worker.celery_worker.add_to_db"",
                                kwargs=event_data)

    return JSONResponse(content=""Event recorded successfully"", status_code=200)

And celery worker which must save an Event object to database:
@celery_app.task(acks_late=True)
async def add_to_db(request_timestamp, service, url, status_code, response_time):
    event = await Event.create(
        request_timestamp=datetime.strptime(request_timestamp, '%Y-%m-%dT%H:%M:%S.%f'),
        service=service,
        url=url,
        status_code=status_code,
        response_time=datetime.strptime(response_time, '%Y-%m-%dT%H:%M:%S.%f'),
    )

    return {""status"": True}

Now I'm receiving an error during object saving process:
Traceback (most recent call last):

File ""/usr/local/lib/python3.8/site-packages/celery/app/trace.py"", line 479, in trace_task

mark_as_done(

File ""/usr/local/lib/python3.8/site-packages/celery/backends/base.py"", line 158, in mark_as_done

self.store_result(task_id, result, state, request=request)

File ""/usr/local/lib/python3.8/site-packages/celery/backends/base.py"", line 442, in store_result

self._store_result(task_id, result, state, traceback,

File ""/usr/local/lib/python3.8/site-packages/celery/backends/database/__init__.py"", line 51, in _inner

return fun(*args, **kwargs)

File ""/usr/local/lib/python3.8/site-packages/celery/backends/database/__init__.py"", line 130, in _store_result

session.commit()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 1042, in commit

self.transaction.commit()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 504, in commit

self._prepare_impl()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 483, in _prepare_impl

self.session.flush()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2523, in flush

self._flush(objects)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2664, in _flush

transaction.rollback(_capture_exception=True)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py"", line 68, in __exit__

compat.raise_(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_

raise exception

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2624, in _flush

flush_context.execute()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 422, in execute

rec.execute(self)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 586, in execute

persistence.save_obj(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 230, in save_obj

_emit_update_statements(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 994, in _emit_update_statements

c = cached_connections[connection].execute(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1014, in execute

return meth(self, multiparams, params)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py"", line 298, in _execute_on_connection

return connection._execute_clauseelement(self, multiparams, params)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1127, in _execute_clauseelement

ret = self._execute_context(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1207, in _execute_context

self._handle_dbapi_exception(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1511, in _handle_dbapi_exception

util.raise_(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_

raise exception

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1205, in _execute_context

context = constructor(dialect, self, conn, *args)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 858, in _init_compiled

param = dict(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 861, in &lt;genexpr&gt;

processors[key](compiled_params[key])

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/sqltypes.py"", line 1689, in process

value = dumps(value, protocol)

sqlalchemy.exc.SQLAlchemyError: (builtins.TypeError) cannot pickle 'coroutine' object


warn(RuntimeWarning(

[2021-02-19 10:19:39,010: ERROR/ForkPoolWorker-1] Task monitoring_service.src.monitoring_service.worker.celery_worker.add_to_db[994ae754-f216-4fa9-b502-5fbb0221011c] raised unexpected: SQLAlchemyError(""(builtins.TypeError) cannot pickle 'coroutine' object"")

Traceback (most recent call last):

File ""/usr/local/lib/python3.8/site-packages/celery/app/trace.py"", line 479, in trace_task

mark_as_done(

File ""/usr/local/lib/python3.8/site-packages/celery/backends/base.py"", line 158, in mark_as_done

self.store_result(task_id, result, state, request=request)

File ""/usr/local/lib/python3.8/site-packages/celery/backends/base.py"", line 442, in store_result

self._store_result(task_id, result, state, traceback,

File ""/usr/local/lib/python3.8/site-packages/celery/backends/database/__init__.py"", line 51, in _inner

return fun(*args, **kwargs)

File ""/usr/local/lib/python3.8/site-packages/celery/backends/database/__init__.py"", line 130, in _store_result

session.commit()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 1042, in commit

self.transaction.commit()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 504, in commit

self._prepare_impl()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 483, in _prepare_impl

self.session.flush()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2523, in flush

self._flush(objects)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2664, in _flush

transaction.rollback(_capture_exception=True)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py"", line 68, in __exit__

compat.raise_(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_

raise exception

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2624, in _flush

flush_context.execute()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 422, in execute

rec.execute(self)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 586, in execute

persistence.save_obj(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 230, in save_obj

_emit_update_statements(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 994, in _emit_update_statements

c = cached_connections[connection].execute(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1014, in execute

return meth(self, multiparams, params)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py"", line 298, in _execute_on_connection

return connection._execute_clauseelement(self, multiparams, params)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1127, in _execute_clauseelement

ret = self._execute_context(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1207, in _execute_context

self._handle_dbapi_exception(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1511, in _handle_dbapi_exception

util.raise_(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_

raise exception

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1205, in _execute_context

context = constructor(dialect, self, conn, *args)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 858, in _init_compiled

param = dict(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 861, in &lt;genexpr&gt;

processors[key](compiled_params[key])

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/sqltypes.py"", line 1689, in process

value = dumps(value, protocol)

sqlalchemy.exc.SQLAlchemyError: (builtins.TypeError) cannot pickle 'coroutine' object

If I call event = await Event.create(...) inside a view it is successfull and saves data to db.
If I delete 'async/await' from Celery worker, this error doesn't show up, Celery task has 'SUCCESS' state  but database is empty.
Can't really understand what is not right.
","Simple: currently, Celery is not able to handle asynchronous functions as tasks. You need to wrap it with asyncio.run (https://docs.python.org/3/library/asyncio-task.html#asyncio.run):
async def add_to_db(request_timestamp, service, url, status_code, response_time):
    event = await Event.create(
        request_timestamp=datetime.strptime(request_timestamp, '%Y-%m-%dT%H:%M:%S.%f'),
        service=service,
        url=url,
        status_code=status_code,
        response_time=datetime.strptime(response_time, '%Y-%m-%dT%H:%M:%S.%f'),
    )

@celery_app.task(acks_late=True)
def add_to_db_task(request_timestamp, service, url, status_code, response_time):
    asyncio.run(add_to_db(request_timestamp, service, url, status_code, response_time))
    return {""status"": True}

",,,false,
https://stackoverflow.com/questions/66274054,false,The issue does not involve a problematic API. It is a question about the behavior of a custom expectation in Great Expectations.,,,,,,,Great Expectations custom expectation not ignoring nulls as requested,"Versions of the libraries we're using:
snowconn==3.7.1
snowflake-connector-python==2.3.10
snowflake-sqlalchemy==1.2.3
SQLAlchemy==1.3.23
great_expectations==0.13.10
pandas==1.1.5

Note we're grabbing data from Snowflake on our own and then feeding a dataframe of it into Great Expectations. I'm aware GE has a Snowflake data source and it's on my list to add it. But I think this setup should work even without using that data source.
We have the following Great Expectations data context config:
    data_context_config = DataContextConfig(
        datasources={
            datasource_name: DatasourceConfig(
                class_name='PandasDatasource',
                data_asset_type={
                    'module_name': 'dataqa.dataset',
                    'class_name': 'CustomPandasDataset'
                }
            )
        },
        store_backend_defaults=S3StoreBackendDefaults(
            default_bucket_name=METADATA_BUCKET,
            expectations_store_prefix=EXPECTATIONS_PATH,
            validations_store_prefix=VALIDATIONS_PATH,
            data_docs_prefix=DATA_DOCS_PATH,
        ),
        validation_operators={
            ""action_list_operator"": {
                ""class_name"": ""ActionListValidationOperator"",
                ""action_list"": [
                    {
                        ""name"": ""store_validation_result"",
                        ""action"": {""class_name"": ""StoreValidationResultAction""},
                    },
                    {
                        ""name"": ""store_evaluation_params"",
                        ""action"": {""class_name"": ""StoreEvaluationParametersAction""},
                    },
                    {
                        ""name"": ""update_data_docs"",
                        ""action"": {""class_name"": ""UpdateDataDocsAction""},
                    },
                ],
            }
        }
    )
    ge_context = BaseDataContext(project_config=data_context_config)

CustomPandasDataset is defined as:
class CustomPandasDataset(PandasDataset):
    _data_asset_type = ""CustomPandasDataset""

    @MetaPandasDataset.multicolumn_map_expectation
    def expect_column_A_equals_column_B_column_C_ratio(
        self,
        column_list,
        ignore_row_if='any_value_is_missing'
    ):
        column_a = column_list.iloc[:,0]
        column_b = column_list.iloc[:,1]
        column_c = column_list.iloc[:,2]

        return abs(column_a - (1.0 - (column_b/column_c))) &lt;= 0.001

and called like:
    cols = ['a', 'b', 'c']
    batch.expect_column_A_equals_column_B_column_C_ratio(
        cols,
        catch_exceptions=True
    )

Later on we validate the data context like so:
    return ge_context.run_validation_operator(
        ""action_list_operator"",
        assets_to_validate=batches,
        run_id=run_id)[""success""]

Often times, columns a and b are null in our data. Given I've set the ignore_row_if='any_value_is_missing' flag on the custom expectation, I'm expecting rows with null values in any of columns a, b, or c to be skipped. But Great Expectations doesn't skip them, instead adding them to the unexpected, or ""failed"" field of output:
result  
element_count   1000
missing_count   0
missing_percent 0
unexpected_count    849
unexpected_percent  84.89999999999999
unexpected_percent_total    84.89999999999999
unexpected_percent_nonmissing   84.89999999999999result 
element_count   1000
missing_count   0
missing_percent 0
unexpected_count    849
unexpected_percent  84.89999999999999
unexpected_percent_total    84.89999999999999
unexpected_percent_nonmissing   84.89999999999999


partial_unexpected_list 

0   
a   null
b   null
c   1.63

I'm unsure why this is happening. In the Great Expectations source, the multicolumn_map_expectation does:
...
            elif ignore_row_if == ""any_value_is_missing"":
                boolean_mapped_skip_values = test_df.isnull().any(axis=1)
...
            boolean_mapped_success_values = func(
                self, test_df[boolean_mapped_skip_values == False], *args, **kwargs
            )
            success_count = boolean_mapped_success_values.sum()
            nonnull_count = (~boolean_mapped_skip_values).sum()
            element_count = len(test_df)

            unexpected_list = test_df[
                (boolean_mapped_skip_values == False)
                &amp; (boolean_mapped_success_values == False)
            ]
            unexpected_index_list = list(unexpected_list.index)

            success, percent_success = self._calc_map_expectation_success(
                success_count, nonnull_count, mostly
            )

which I interpret as ignoring null-containing rows (not adding them to the unexpected list and not using them to determine percent_success). I've dropped a pdb in our code and verified that the dataframe we're calling the expectation on can be manipulated in the correct way to get ""sensible"" data (test_df.isnull().any(axis=1)), but for some reason Great Expectations is allowing those nulls to slip through. Anyone know why?
","I believe the poster filed a Github issue here: https://github.com/great-expectations/great_expectations/issues/2460. The progress can be tracked there.
",,,false,
https://stackoverflow.com/questions/64046595,true,"The issue involves the use of the amancevice/superset docker image to connect to an Oracle database. The error message indicates a missing Oracle Client library, which is required for the connection to work.",,,,,,,Unable to use amancevice/superset docker image to use Oracle DB as datasource,"I'm trying to test if superset would be adecuate for our data visualization needs. As I've no knowledge of python, I'm using a docker image of a superset installation (v0.37.1) published with all the libraries needed to connect to different databases (https://github.com/amancevice/docker-superset). When I try to set up a new database source to an Oracle database I get this error in the superset.log:
Unexpected error (cx_Oracle.DatabaseError) DPI-1047: Cannot locate a 64-bit Oracle Client library: ""libclntsh.so: cannot open shared object file: No such file or directory"". See https://oracle.github.io/odpi/doc/installation.html#linux for help
(Background on this error at: http://sqlalche.me/e/13/4xp6)

After googling I've added to the container (when I have this solved I'll investigate how to create a new image with all the changes I've performed directly to the container) the Oracle Instant Client library and modified the .bashrc file of the container (for the user superset) to add a new env variable LD_LIBRARY_PATH pointing to the directory with the Oracle Instant Client.
I've restarted the container.
To check that SQLAlchemy can connect to my OracleDB correctly I've created a test_script.py:
import sqlalchemy as sa
engine = sa.create_engine('oracle+cx_oracle://user:password@host:port/?service_name=service')
with engine.connect() as connection:
    result = connection.execute(""select * from dual"")
    for row in result:
        print(row)

And then if I connect to the container directly docker exec -it my_container bash and execute my script python test_script.py, I get the correct results and no error message, however, connecting to the superset url and trying to create a new datasource to that Oracle DB, I'm still getting the same error.
Does someone know if there's another configuration change needed in the container to make this work?
","When you download Oracle Instant Client Basic or Basic Light ZIP packages from here, then run something like this in your Dockerfile:
RUN echo /opt/oracle/instantclient_19_8 &gt; /etc/ld.so.conf.d/oic.conf &amp;&amp; \
    ldconfig

Also see sample docker images for Linux developers and for basic Oracle Instant Client.  If you are not on an RPM based system, check out the sample Docker files in Docker for Oracle Database Applications in Node.js and Python.
The details vary with what base Docker image you are using, and whether you want to install Instant Client from ZIP files or RPMs.
Update: prebuilt containers are available from Oracle's GitHub Container Registry.
",,,false,
https://stackoverflow.com/questions/58691132,false,The issue does not involve a problematic API. It is a question about setting default filter values in Flask SQLAlchemy and does not indicate any unexpected behavior or unpredictable API failures.,,,,,,,Flask SQLAlchemy - Default filter value if no results for provided filter,"Using Flask SQLAlchemy I'm querying a MySQL database with a table called bar and looking for rows that match a filter consisting of foo and country_code:

foo_filter = 'hello'
country_code_filter = 'ES'
result = Bar.filter_by(foo=foo_filter, country_code=country_code_filter).first()


The above code will return the first row in which foo = foo_filter and country_code = country_code_filter.

However, it's possible that we might not have rows for certain country codes that match foo. In these cases (i.e, cases where the above query returns 0 results), I'd like to use a default country filter of 'RoW' as our dataset should always have an RoW value for each possible value of foo. In the unexpected occurrence that this also doesn't return any results then an error should be thrown. This is the code I have for this:

foo_filter = 'hello'
country_code_filter = 'ES'
result = Bar.filter_by(foo=foo_filter, country_code=country_code_filter).first()
if not result:
    result = Bar.filter_by(foo=foo_filter, country_code='RoW').first()
if not result:
    raise RuntimeException(f""No data for combination {foo_filter}, {country_code_filter} or {foo_filter}, RoW"")


This approach of running similar queries multiple times and checking the result each time until I get a row feels very messy/wrong but I haven't been able to find any better approaches that allow you to set an 'alternative' filter when your initial query returns 0 rows in Flask SQLAlchemy

Is there a cleaner approach to this?
",,,,false,
https://stackoverflow.com/questions/58534100,false,The issue does not involve a problematic API. It is a question about a specific error related to QueuePool limit overflow in SQLAlchemy and does not indicate any unexpected behavior or unpredictable API failures.,,,,,,,"TimeoutError QueuePool limit of size x overflow xy reached, connection timed out, timeout 30","This is the error that I get:
RemoteError: Remote error: TimeoutError QueuePool limit of size x overflow xy reached, connection timed out, timeout 30 (Background on this error at: http://sqlalche.me/e/3o7r)

We have an issue with metadata agent. One compute node hosts many vms and the vms can't get ip. I've tried to restart on the dhcp metadata server both service, tried to restart our rabbitmq server as well. Also restarted neutron server, but the issue still persist and the vms are down.

Errors like these:

2019-10-24 10:34:47.794 86154 ERROR neutron.agent.metadata.agent
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent [-] Unexpected error.: RemoteError: Remote error: TimeoutError QueuePool limit of size 5 overflow 50 reached, connection timed out, timeout 30 (Background on this error at: http://sqlalche.me/e/3o7r)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent Traceback (most recent call last):
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 89, in __call__
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     instance_id, tenant_id = self._get_instance_and_tenant_id(req)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 162, in _get_instance_and_tenant_id
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     ports = self._get_ports(remote_address, network_id, router_id)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 155, in _get_ports
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return self._get_ports_for_remote_address(remote_address, networks)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/common/cache_utils.py"", line 116, in __call__
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return self.func(target_self, *args, **kwargs)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 137, in _get_ports_for_remote_address
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     ip_address=remote_address)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 106, in _get_ports_from_server
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return self.plugin_rpc.get_ports(self.context, filters)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 72, in get_ports
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return cctxt.call(context, 'get_ports', filters=filters)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/common/rpc.py"", line 150, in call
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return self._original_context.call(ctxt, method, **kwargs)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py"", line 179, in call
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     retry=self.retry)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/oslo_messaging/transport.py"", line 133, in _send
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     retry=retry)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py"", line 584, in send
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     call_monitor_timeout, retry=retry)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py"", line 575, in _send
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     raise result
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent RemoteError: Remote error: TimeoutError QueuePool limit of size 5 overflow 50 reached, connection timed out, timeout 30 (Background on this error at: http://sqlalche.me/e/3o7r)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent [u'Traceback (most recent call last):\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_messaging/rpc/server.py"", line 163, in _process_incoming\n    res = self.dispatcher.dispatch(message)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py"", line 265, in dispatch\n    return self._do_dispatch(endpoint, method, ctxt, args)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py"", line 194, in _do_dispatch\n    result = func(ctxt, **new_args)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron/api/rpc/handlers/metadata_rpc.py"", line 43, in get_ports\n    return self.plugin.get_ports(context, filters=filters)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron/db/api.py"", line 123, in wrapped\n    return method(*args, **kwargs)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron_lib/db/api.py"", line 140, in wrapped\n    setattr(e, \'_RETRY_EXCEEDED\', True)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 220, in __exit__\n    self.force_reraise()\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 196, in force_reraise\n    six.reraise(self.type_, self.value, self.tb)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron_lib/db/api.py"", line 136, in wrapped\n    return f(*args, **kwargs)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_db/api.py"", line 154, in wrapper\n    ectxt.value = e.inner_exc\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 220, in __exit__\n    self.force_reraise()\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 196, in force_reraise\n    six.reraise(self.type_, self.value, self.tb)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_db/api.py"", line 142, in wrapper\n    return f(*args, **kwargs)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron_lib/db/api.py"", line 183, in wrapped\n    LOG.debug(""Retry wrapper got retriable exception: %s"", e)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 220, in __exit__\n    self.force_reraise()\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 196, in force_reraise\n    six.reraise(self.type_, self.value, self.tb)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron_lib/db/api.py"", line 179, in wrapped\n    return f(*dup_args, **dup_kwargs)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron/db/db_base_plugin_v2.py"", line 1435, in get_ports\n    items = [self._make_port_dict(c, fields) for c in query]\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2925, in __iter__\n    return self._execute_and_instances(context)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2946, in _execute_and_instances\n    close_with_result=True)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2955, in _get_bind_args\n    **kw\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2937, in _connection_from_session\n    conn = self.session.connection(**kw)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 1035, in connection\n    execution_options=execution_options)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 1042, in _connection_for_bind\n    conn = engine.contextual_connect(**kw)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2123, in contextual_connect\n    self._wrap_pool_connect(self.pool.connect, None),\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2158, in _wrap_pool_connect\n    return fn()\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py"", line 403, in connect\n    return _ConnectionFairy._checkout(self)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py"", line 788, in _checkout\n    fairy = _ConnectionRecord.checkout(pool)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py"", line 532, in checkout\n    rec = pool._do_get()\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py"", line 1186, in _do_get\n    (self.size(), self.overflow(), self._timeout), code=""3o7r"")\n', u'TimeoutError: QueuePool limit of size 5 overflow 50 reached, connection timed out, timeout 30 (Background on this error at: http://sqlalche.me/e/3o7r)\n'].
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent
2019-10-24 10:35:04.436 86151 WARNING oslo_messaging._drivers.amqpdriver [-] Number of call queues is 21, greater than warning threshold: 20. There could be a leak. Increasing threshold to: 40
2019-10-24 10:35:04.637 86156 WARNING oslo_messaging._drivers.amqpdriver [-] Number of call queues is 21, greater than warning threshold: 20. There could be a leak. Increasing threshold to: 40
2019-10-24 10:35:05.339 86149 WARNING oslo_messaging._drivers.amqpdriver [-] Number of call queues is 21, greater than warning threshold: 20. There could be a leak. Increasing threshold to: 40


What should I do to fix it :( ?

Or how can I finetune this sqlalchemy? I have no idea to be honest what is this.
",,,,false,
https://stackoverflow.com/questions/56877620,false,The issue does not involve a problematic API. It is a question about an error in a Reddit clone web application related to SQLAlchemy queries and does not indicate any unexpected behavior or unpredictable API failures.,,,,,,,SqlAlchemy Query.All() Unexpectedly Returning OperationalError,"I'm implementing search functionality using Elasticsearch in a ""Reddit clone"" web application that I'm developing. I want to support searching for threads, users, and subreddits, but when I enter a search query and search for one of the 3 above mentioned categories that does not hold any matches, I'm getting an unexpected ""OperationalError"" instead of an empty set of results.

As shown in the code I included, I attempted to use the sqlalchemy.orm.query.Query.all() function which returned the following error:

OperationalError: (sqlite3.OperationalError) near ""END"": syntax error
[SQL: SELECT user.id AS user_id, user.username AS user_username, user.email AS user_email, user.password_hash AS user_password_hash, user.last_sign_in AS user_last_sign_in 
FROM user 
WHERE 1 != 1 ORDER BY CASE user.id END]
(Background on this error at: http://sqlalche.me/e/e3q8)


I researched other StackOverflow posts and found that the first() function internally processes the database result and returns None if no results are found, but when I switched to that function, I faced this error:

OperationalError: (sqlite3.OperationalError) near ""END"": syntax error
[SQL: SELECT user.id AS user_id, user.username AS user_username, user.email AS user_email, user.password_hash AS user_password_hash, user.last_sign_in AS user_last_sign_in 
FROM user 
WHERE 1 != 1 ORDER BY CASE user.id END
 LIMIT ? OFFSET ?]
[parameters: (1, 0)]
(Background on this error at: http://sqlalche.me/e/e3q8)


Checking the documentation for SqlAlchemy, I don't see any mention of this error in either function, and reading the meaning of OperationalError, I'm concerned that my database setup is possibly incorrect. 

app/routes.py: This is the route that handles search requests made to the following URL: http://localhost:5000/search?q=&amp;index=

@app.route('/search', methods=['GET'])
def search():
    print 'Hit the /search route!'
    if not g.search_form.validate():
        return redirect(url_for('index'))
    page = request.args.get('page', 1, type=int)
    target_index = request.args.get('index', 'thread')
    if target_index == 'thread':
        results, total = Thread.search(g.search_form.q.data, page, app.config['POSTS_PER_PAGE'])
        print 'Called Thread.search(), total results = {}'.format(total['value'])
    elif target_index == 'user':
        results, total = User.search(g.search_form.q.data, page, app.config['POSTS_PER_PAGE'])
        print 'Called User.search(), total results = {}'.format(total['value'])
    elif target_index == 'subreddit':
        results, total = Subreddit.search(g.search_form.q.data, page, app.config['POSTS_PER_PAGE'])
        print 'Called Subreddit.search(), total results = {}'.format(total['value'])
    else:
        return render_template('404.html')
    try:
        results = results.all()
    except OperationalError:
        results = [None]
    total = total['value']
    next_url = url_for('search', index=target_index, q=g.search_form.q.data, page=page + 1) if total &gt; page * app.config['POSTS_PER_PAGE'] else None
    prev_url = url_for('search', index=target_index, q=g.search_form.q.data, page=page - 1) if page &gt; 1 else None
    results_list = zip(results, [None] * len(results)) # Temporarily to match expected input for template
    return render_template('search.html', title=_('Search'), results_list=results_list, next_url=next_url, prev_url=prev_url, query=g.search_form.q.data, index=target_index)


app/models.py:

class SearchableMixin(object):
    @classmethod
    def search(cls, expression, page, per_page):
        ids, total = query_index(cls.__tablename__, expression, page, per_page)
        if total == 0:
            return cls.query.filter_by(id=0), 0
        when = []
        for i in range(len(ids)):
            when.append((ids[i], i))
        return cls.query.filter(cls.id.in_(ids)).order_by(
            db.case(when, value=cls.id)), total

    @classmethod
    def before_commit(cls, session):
        session._changes = {
            'add': list(session.new),
            'update': list(session.dirty),
            'delete': list(session.deleted)
        }

    @classmethod
    def after_commit(cls, session):
        for obj in session._changes['add']:
            if isinstance(obj, SearchableMixin):
                add_to_index(obj.__tablename__, obj)
        for obj in session._changes['update']:
            if isinstance(obj, SearchableMixin):
                add_to_index(obj.__tablename__, obj)
        for obj in session._changes['delete']:
            if isinstance(obj, SearchableMixin):
                remove_from_index(obj.__tablename__, obj)
        session._changes = None

    @classmethod
    def reindex(cls):
        for obj in cls.query:
            add_to_index(cls.__tablename__, obj)

db.event.listen(db.session, 'before_commit', SearchableMixin.before_commit)
db.event.listen(db.session, 'after_commit', SearchableMixin.after_commit)

# Below is one model that implements SearchableMixin to allow searching # for users. Thread and Subreddit models follow the same logic.
class User(db.Model, UserMixin, SearchableMixin):
    __searchable__ = ['username']
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(64), index=True, unique=True)
    # &lt;Remaining User model fields here...&gt;


app/search.py: (Holds the underlying search functions to query Elasticsearch indices)

def add_to_index(index, model):
    if not app.elasticsearch:
        return
    payload = {}
    for field in model.__searchable__:
        payload[field] = getattr(model, field)
    app.elasticsearch.index(index=index, doc_type=index, id=model.id,
                                    body=payload)

def remove_from_index(index, model):
    if not app.elasticsearch:
        return
    app.elasticsearch.delete(index=index, doc_type=index, id=model.id)

def query_index(index, query, page, per_page):
    if not app.elasticsearch:
        return [], 0
    search = app.elasticsearch.search(
        index=index,
        body={'query': {'multi_match': {'query': query, 'fields': ['*']}},
              'from': (page - 1) * per_page, 'size': per_page})
    ids = [int(hit['_id']) for hit in search['hits']['hits']]
    return ids, search['hits']['total']


As my included app/routes.py shows, I made a workaround by catching the OperationalError and treating it as an indicator that no results were found, but since the all() documentation makes no mention of it, I did not expect there to be this exception being raised.
",,,,false,
https://stackoverflow.com/questions/28008124,false,"The issue does not provide sufficient information to determine if it involves a problematic API. The issue title suggests a question about finding columns with a particular number as an attribute, but the issue body is empty.",,,,,,,How to find all columns with a particular number as an attribute?,,,,,false,
https://stackoverflow.com/questions/17198595,true,The issue involves a problematic API in SQLAlchemy that leads to unexpected behavior when using both lazy-load and dynamic versions of the same relationship. This API-related problem causes a DELETE error when trying to delete a group that has multiple versions of the relationship with users.,SQLAlchemy,relationship,"The issue arises when a group object has two versions of the relationship with users: 'users' (lazy-load) and 'users_dynamic' (dynamic). When attempting to delete the group, an error occurs because the rows in the association table are already deleted, causing a mismatch in the number of rows expected to be deleted.","The expected behavior is for the group to be deleted along with the associated users. However, due to the conflicting versions of the relationship, the DELETE statement fails to delete the expected number of rows.",The issue is triggered when a group object with both 'users' and 'users_dynamic' relationships is deleted.,"This issue might be challenging to detect during development and testing, especially if the developer is not aware of the potential conflicts that can arise from using multiple versions of the same relationship.",SQLAlchemy DELETE Error caused by having a both lazy-load AND a dynamic version of the same relationship,"Here is some example code:

users_groups = Table('users_groups', Model.metadata,
    Column('user_id', Integer, ForeignKey('users.id')),
    Column('group_id', Integer, ForeignKey('groups.id'))
)

class User(Model):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)


class Group(Model):
    __tablename__ = 'groups'
    id = Column(Integer, primary_key=True)

    users = relationship('User', secondary=users_groups, lazy='select', backref='groups')
    users_dynamic = relationship('User', secondary=users_groups, lazy='dynamic')


So what happens here is that if you add a bunch of users to a group like so:

g = Group()
g.users = [User(), User(), User()]
session.add(g)
session.commit()


and then try to delete the group

session.delete(g)
session.commit()


You will get some form of this error:

DELETE statement on table 'users_groups' expected to delete 3 row(s); Only 0 were matched.


Removing the 2nd version of the relationship (the dynamic one in my case) fixes this problem. I am not even sure where to begin in terms of understanding why this is happening. I have been using 2 versions of various relationships in many cases throughout my SQLAlchemy models in order to make it easy to use the most appropriate query-strategy given a situation. This is the first time it has caused an unexpected issue.

Any advice is welcome.
","both the Group.users and Group.users_dynamic relationships are attempting to reconcile the fact that the Group is being deleted along with being able to manage the User() objects they refer to; one relationship succeeds while the second one fails, as the rows in the association table were already deleted.  The most straightforward solution is to mark all but one of the identical relationships as viewonly:

class Group(Base):
    __tablename__ = 'groups'
    id = Column(Integer, primary_key=True)

    users = relationship('User', secondary=users_groups, lazy='select', backref='groups')
    users_dynamic = relationship('User', viewonly=True, secondary=users_groups, lazy='dynamic')


if you're still wanting to have both relationships handle some degree of mutations, you'd need to do this carefully as SQLAlchemy doesn't know how to coordinate among changes in two relationships at the same time, so conflicts like this can continue to happen (like double inserts, etc) if you make equivalent mutations on both relationships.   To just take care of the ""delete"" issue by itself, you can also try setting Group.users_dynamic to passive_deletes=True:

class Group(Base):
    __tablename__ = 'groups'
    id = Column(Integer, primary_key=True)

    users = relationship('User', secondary=users_groups, lazy='select', backref='groups')
    users_dynamic = relationship('User', passive_deletes=True, secondary=users_groups, lazy='dynamic')

","I just add another simple workaround.

You can delete the collections before deleting the item itself:

&gt;&gt;&gt; for user in group.users:
        group.users.remove(user)
&gt;&gt;&gt; db.session.delete(group)
&gt;&gt;&gt; db.session.commit()


Alternatively, you can also set it as an empty list:

&gt;&gt;&gt; group.users = []
&gt;&gt;&gt; db.session.commit()
&gt;&gt;&gt; db.session.delete(group)
&gt;&gt;&gt; db.session.commit()

",,false,
https://stackoverflow.com/questions/21076105,true,The issue involves a potential problem with using SQLAlchemy's ORM in a Twisted application. The provided code and question indicate a misunderstanding of how sessions and threads should be handled in this context.,SQLAlchemy,scoped_session,"The issue arises from incorrect assumptions about the relationship between sessions and threads in a Twisted application. The code attempts to use scoped_session to constrain sessions to a single thread, but the provided explanations and conclusions suggest a lack of clarity on how to handle lazy/eager-loading and session management across threads.","The expected behavior is for scoped_session to provide a way to manage sessions within a single thread, ensuring that sessions are not passed to other threads. However, the code and question demonstrate confusion and uncertainty about the correct approach.",The issue is triggered by attempting to use scoped_session in a Twisted application without a clear understanding of how to handle sessions and threads.,"This issue might be challenging to detect during development and testing, especially if the developer is not familiar with the specific requirements and considerations of using SQLAlchemy's ORM in a Twisted application.",Is this an acceptable way to make threaded SQLAlchemy queries from Twisted?,"I've been doing some reading on using SQLAlchemy's ORM in the context of a Twisted application.  It's a lot of information to digest, so I'm having a bit of trouble putting all the pieces together.  So far, I've gathered the following absolute truths:


One session implies one thread.  Always.
scoped_session, by default, provides us with a way of constraining sessions to a given thread.  In other words, I am sure that by using scoped_session, I will not pass sessions to other threads (unless I do so explicitly, which I won't).


I also gathered that there are some issues relating to lazy/eager-loading and that one possible approach is to dissociate ORM objects from a session and reattach them to another session when changing threads.  I'm quite fuzzy on the details, but I also concluded that scoped_session renders many of these points moot.

My first question is whether or not I am severely mistaken in my above conclusions.

Beyond that, I've crafted this approach, which I hope is satisfactory.

I begin by creating a scoped_session object...

Session = scoped_session(sessionmaker(bind=_my_engine))


... which I will then use from a context manager, in order to handle exceptions and clean-up gracefully:

@contextmanager
def transaction_context():
    session = Session()
    try:
        yield session
        session.commit()
    except:
        session.rollback()
        raise
    finally:
        session.remove()  # dispose of the session


Now all I need to do is to use the above context manager in a function that is deferred to a separate thread.  I've thrown together a decorator to make things a bit prettier:

def threaded(fn):
    @wraps(fn)  # functools.wraps
    def wrapper(*args, **kwargs):
        return deferToThread(fn, *args, **kwargs)  # t.i.threads.deferToThread
    return wrapper


Here is an example of how I intend to use the whole shebang.  Below is a function that performs a DB lookup using the SQLAlchemy ORM:

@threaded
def get_some_attributes(group):
    with transaction_context() as session:
        return session.query(Attribute).filter(Attribute.group == group)


My second question is whether or not this approach is viable.


Am I making any fundamentally flawed assumptions?
Are there any caveats?
Is there a better way?


Edit: Here is a related question concerning the unexpected error in my context manager.
","Right now I work on this exact problem, and I think I found a solution. 

Indeed, you must defer all database access functions to a thread. But in your solution, you remove the session after querying the database, so all your results ORM objects will be detached and you wont have access to their fields.

You can't use scoped_session because in Twisted we have only one MainThread (except with things that work in deferToThread). We can, however, use scoped_sesssion with scopefunc. 

In Twisted there is a great thing known as ContextTracker:


  provides a way to pass arbitrary key/value data up and down a call
  stack without passing them as parameters to the functions on that call
  stack.


In my twisted web app in method render_GET I set a uuid parameter:

call = context.call({""uuid"": str(uuid.uuid4())}, self._render, request)


and then I call the _render method to do the actual work (work with db, render html, etc).

I create the scoped_session like this: 

scopefunc = functools.partial(context.get, ""uuid"")
Session = scoped_session(session_factory, scopefunc=scopefunc)


Now within any function calls of _render I can get session with:

Session()


and at the end of _render I have to do Session.remove() to remove the session.

It worksa with my webapp and I think can work for other tasks.

This is completely standalone example, show how all it work together.

from twisted.internet import reactor, threads
from twisted.web.resource import Resource
from twisted.web.server import Site, NOT_DONE_YET
from twisted.python import context
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import sessionmaker, scoped_session
from sqlalchemy.ext.declarative import declarative_base
import uuid
import functools

engine = create_engine(
    'sqlite:///test.sql',
    connect_args={'check_same_thread': False},
    echo=False)

session_factory = sessionmaker(bind=engine)
scopefunc = functools.partial(context.get, ""uuid"")
Session = scoped_session(session_factory, scopefunc=scopefunc)
Base = declarative_base()


class User(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)
    name = Column(String)

Base.metadata.create_all(bind=engine)


class TestPage(Resource):
    isLeaf = True

    def render_GET(self, request):
        context.call({""uuid"": str(uuid.uuid4())}, self._render, request)
        return NOT_DONE_YET

    def render_POST(self, request):
        return self.render_GET(request)

    def work_with_db(self):
        user = User(name=""TestUser"")
        Session.add(user)
        Session.commit()
        return user

    def _render(self, request):
        print ""session: "", id(Session())
        d = threads.deferToThread(self.work_with_db)

        def success(result):
            html = ""added user with name - %s"" % result.name
            request.write(html.encode('UTF-8'))
            request.finish()
            Session.remove()
        call = functools.partial(context.call, {""uuid"": scopefunc()}, success)
        d.addBoth(call)
        return d

if __name__ == ""__main__"":
    reactor.listenTCP(8888, Site(TestPage()))
    reactor.run()


I print out id of session, and you can see that its different for each request. If you remove scopefunc from scoped_session constructor and do two simultaneous request(insert time.sleep to work_with_db), you will get one common session for this two requests.


  The scoped_session object by default uses threading.local() as storage, so that a single Session is maintained for all who call upon the scoped_session registry, but only within the scope of a single thread


a problem here that in twisted we have only one thread for all requests. Thats why we have to create own scopefunc, that will show the difference between requests.

An other problem, that twisted didnt pass context to callbacks and we have to wrap callback and send current context to it.

call = functools.partial(context.call, {""uuid"": scopefunc()}, success)


Still I dont know how to make it work with defer.inLineCallback, that I use everywhere in my code.
",,,false,
https://stackoverflow.com/questions/76107844,true,The issue involves a problematic API in SQLAlchemy that leads to a StaleDataError when trying to update a user in the database. The error occurs due to unaccounted-for database state during the UPDATE statement.,SQLAlchemy,version_id_col,"The issue is related to the use of the 'version_id_col' attribute in the SQLAlchemy model. The error message mentions that the UPDATE statement expected to update a certain number of rows, but a mismatch occurred, indicating stale data.","The expected behavior is for the UPDATE statement to update the specified row with the new values. However, the StaleDataError occurs because the version identifier of the object does not match the version number in the database.",The issue is triggered when updating a user object in the database and encountering a StaleDataError.,"This issue might be challenging to detect during development and testing, especially if the developer is not familiar with the concept of versioning and how it is implemented in SQLAlchemy.",SqlAlchemy StaleDataError on simple update statement,"I'm trying to update a user in the database but keep running into a StaleDataError.
user = session.query(User).get(1)
user.first_name # John
user.first_name = 'Sally'
session.commit()

# &gt; sqlalchemy.orm.exc.StaleDataError: UPDATE statement on table 'user' expected to update 1 row(s);
#   -1 were matched.

From the SqlAlchemy docs on StaleDataError:

An operation encountered database state that is unaccounted for.
Conditions which cause this to happen include:
A flush may have attempted to update or delete rows and an unexpected
number of rows were matched during the UPDATE or DELETE statement.
Note that when version_id_col is used, rows in UPDATE or DELETE
statements are also matched against the current known version
identifier.
A mapped object with version_id_col was refreshed, and the version
number coming back from the database does not match that of the object
itself.
A object is detached from its parent object, however the object was
previously attached to a different parent identity which was garbage
collected, and a decision cannot be made if the new parent was really
the most recent parent.

The docs don't elaborate on version_id_col. Could this be the issue? What is this column and where can I find if it's active?
Why can't SqlAlchemy locate the row id (which is obviously there since it pulled the row just moments before) and what's wrong with my update command?
","I found the solution in an answer by @tamersalama. I'm reposting it here because that question was hard to find in connection to the StaleDataError.
Add __mapper_args__ to the SqlAlchemy model:
class User(Base):

    __tablename__ = 'user'
    user_id = Column(Integer, primary_key=True, nullable=False)
    first_name = Column(String(20))
    last_name = Column(String(20))

    __mapper_args__ = {
        'version_id_col': user_id, # this is the id column of the model
        'version_id_generator': False
    }

",,,false,
https://stackoverflow.com/questions/75758327,false,"The issue does not involve a problematic API but rather a specific error related to SQLAlchemy and the usage of asyncio. The error message suggests an unexpected state change during the commit operation, but it does not indicate an issue with a particular API or its behavior.",,,,,,,SQLAlchemy: method &#39;_connection_for_bind()&#39; is already in progress,"I recently updated SQLAlchemy (with [asyncio] package) to 1.4.46 and started to get the following exception when committing:

sqlalchemy.exc.IllegalStateChangeError: Method 'commit()' can't be called here; method '_connection_for_bind()' is already in progress and this would cause an unexpected state change to &lt;SessionTransactionState.CLOSED: 5&gt;

Before updating to the new version, it was working fine.
# -*- coding:utf-8 -*-

from sqlalchemy import exc, event, text
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession as SQLAlchemyAsyncSession
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.ext.asyncio import AsyncEngine
from sqlalchemy.pool import NullPool, Pool
from contextvars import ContextVar
from sanic import Sanic
import asyncio


class EngineNotInitialisedError(Exception):
    pass


class DBSessionContext:
    def __init__(self, session: Session, commit_on_exit: bool = True) -&gt; None:
        self.session = session
        self._query = None
        self.commit_on_exit = commit_on_exit
        self.token = None

    async def close(self, exc_type=None, exc_value=None, traceback=None):
        if self._query:
            if exc_value and getattr(exc_value, 'status_code', 500) &gt; 300:
                await self._query.rollback()
                self._post_processing.clear()
            else:
                await self._query.commit()
                await self.run_post_processing()

            await self._query.close()

        if self._post_processing:
            await self.run_post_processing()

    def set_token(self, token):
        self.token = token

    @property
    def query(self) -&gt; Session:
        if not self._query:
            self._query = self.session()

        return self._query


class AsyncSession(SQLAlchemyAsyncSession):
    async def execute(self, statement, **parameters):
        try:
            if isinstance(statement, str):
                # We wrap around the `text()` method automatically
                statement = text(statement)
            return await super().execute(statement, parameters)
        except exc.OperationalError as e:
            if e.orig.args[0] == 1205:
                # Lock wait timeout exceeded
                await self.rollback()
                return await super().execute(statement, parameters)

            raise e


class DBSession:
    def __init__(self):
        self.engine = None
        self.session = None
        self._session = None
        self.context = ContextVar(""context"", default=None)

    def init_app(self, app: Sanic, url: str, commit_on_exit: bool = True) -&gt; None:
        self.commit_on_exit = commit_on_exit

        engine_args = {
            'echo': app.config.get('DATABASE_ECHO', cast=bool, default=False),
            'echo_pool': app.config.get('DATABASE_ECHO_POOL', cast=bool, default=False),
            'poolclass': NullPool,  # will be used to create a connection pool instance using the connection parameters given in the URL
            # if pool_class is not NullPool:

            # the number of connections to allow in connection pool overflow
            # 'max_overflow': app.config.get('DATABASE_MAX_OVERFLOW', cast=int, default=10),
            # if True will enable the connection pool pre-ping feature that tests connections for liveness upon each checkout
            # 'pool_pre_ping': app.config.get('DATABASE_POOL_PRE_PING', cast=bool, default=True),
            # the number of connections to keep open inside the connection pool
            # 'pool_size': app.config.get('DATABASE_POOL_SIZE', cast=int, default=5),
            # this setting causes the pool to recycle connections after the given number of seconds has passed
            # 'pool_recycle': app.config.get('DATABASE_POOL_RECYCLE', cast=int, default=-1),
            # number of seconds to wait before giving up on getting a connection from the pool
            # 'pool_timeout': app.config.get('DATABASE_POOL_TIMEOUT', cast=int, default=3600),
        }

        self.engine = create_async_engine(
            url,
            **engine_args
        )

        self.session = sessionmaker(
            bind=self.engine,
            expire_on_commit=False,
            class_=AsyncSession,
            autoflush=False
        )

    async def __aenter__(self):
        if not isinstance(self.engine, AsyncEngine):
            raise EngineNotInitialisedError

        session_ctx = DBSessionContext(self.session, self.commit_on_exit)
        session_ctx.set_token(self.context.set(session_ctx))

        return session_ctx

    async def __aexit__(self, exc_type, exc_value, traceback):
        session_ctx = self.context.get()
        await asyncio.shield(session_ctx.close(exc_type, exc_value, traceback))

        self.context.reset(session_ctx.token)

    @property
    def query(self) -&gt; Session:
        return self.context.get().query


@event.listens_for(Pool, ""checkout"")
def check_connection(dbapi_con, con_record, con_proxy):
    '''Listener for Pool checkout events that pings every connection before using.
    Implements pessimistic disconnect handling strategy. See also:
    http://docs.sqlalchemy.org/en/rel_0_8/core/pooling.html#disconnect-handling-pessimistic'''

    cursor = dbapi_con.cursor()
    try:
        cursor.execute(""SELECT 1"")
    except exc.OperationalError as ex:
        if ex.args[0] in (2006,   # MySQL server has gone away
                          2013,   # Lost connection to MySQL server during query
                          2055):  # Lost connection to MySQL server at '%s', system error: %d
            raise exc.DisconnectionError()  # caught by pool, which will retry with a new connection
        else:
            raise

    cursor.close()


db = DBSession()

The code is called with the following :
async with db:
    await db.query.execute('INSERT INTO ...')

What is causing the InvalidStateChangeError I'm having? How can I avoid this issue?
","There is a discussion on the Github repository of SQLAlchemy, that gives a reason why the issue is occurring:
https://github.com/sqlalchemy/sqlalchemy/discussions/9312
The suggestion is that the code is calling something like
asyncio.gather(func(session), func2(session) with the two function sharing the same session, which causes the sqlalchemy.exc.IllegalStateChangeError
Removing the asyncio.gather call resolve the issue. (Or use two sessions, one for each functions).
",,,false,
https://stackoverflow.com/questions/77280663,false,The issue does not involve a problematic API. It is a question about the differences between two collations in MySQL.,,,,,,,What is the difference between utf8mb4_cs_0900_as_cs and utf8mb4_900_as_cs?,"I'm facing issues when building unit tests for my SQLALCHEMY api. I noticed that my MySQL DB returns an unexpected order when asking to order by a string column.
A little investigation showed that our DB is using utf8mb4_cs_0900_as_cs.
But when trying to order with utf8mb4_0900_as_cs, I get the expected order.
Can someone explain to me what are the differences between these 2 collations?
mysql&gt; SELECT
    -&gt;     column_0 AS name1
    -&gt; FROM
    -&gt;     (VALUES
    -&gt;         ROW('default_chanakah'),
    -&gt; ROW('default_aaa'),
    -&gt; ROW('default_zzz'),
    -&gt;         ROW('default_hsunan'),
    -&gt;         ROW('default_kourin1')
    -&gt;     ) AS hardcodedNames
    -&gt; ORDER BY
    -&gt; name1
    -&gt; COLLATE utf8mb4_cs_0900_as_cs;
+------------------+
| name1            |
+------------------+
| default_aaa      |
| default_hsunan   |
| default_chanakah |
| default_kourin1  |
| default_zzz      |
+------------------+
5 rows in set (0.00 sec)

mysql&gt; SELECT
    -&gt;     column_0 AS name1
    -&gt; FROM
    -&gt;     (VALUES
    -&gt;         ROW('default_chanakah'),
    -&gt; ROW('default_aaa'),
    -&gt; ROW('default_zzz'),
    -&gt;         ROW('default_hsunan'),
    -&gt;         ROW('default_kourin1')
    -&gt;     ) AS hardcodedNames
    -&gt; ORDER BY
    -&gt; name1
    -&gt; COLLATE utf8mb4_0900_as_cs;
+------------------+
| name1            |
+------------------+
| default_aaa      |
| default_chanakah |
| default_hsunan   |
| default_kourin1  |
| default_zzz      |
+------------------+
5 rows in set (0.00 sec)

","utf8mb4_cs_0900_as_cs is Czech utf8 (first cs indicate this), this alphabet contains ""letter"" Ch and it goes after h
this is why hsusan is before chanakah
from https://en.wikipedia.org/wiki/Czech_orthography

",,,false,
https://stackoverflow.com/questions/76038019,false,The issue does not involve a problematic API. It is a question about a specific error encountered while using SQLAlchemy and FastAPI.,,,,,,,SQL Alchemy error: MissingGreenlet: greenlet_spawn has not been called,"I am building backend application with fastapi and sqlalchemy, and while trying to get a relationship between tables, I am being hit with this error.
full error:

raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/20/xd2s)

I have stablished other relationships between other tables on the same project the exact same way and had no issues whatsoever up to this point. Since all of the other relationships worked with no issues I can't tell why this one isn't working.
This is what my models for both tables looks like:
class ServiceOrderModel(Settings.DB_BASE_MODEL):
    __tablename__ = 'service_order'
    __allow_unmapped__ = True

    id = Column(Integer, primary_key=True, autoincrement=True)
    identifier = Column(String(16))
    description = Column(String(256))
    execution_value = Column(Float, nullable=True)
    charged_value = Column(Float, nullable=True)
    status_id = Column(Integer, ForeignKey('service_status.id'))
    status = relationship('ServiceStatusModel', lazy='joined', back_populates='service_orders')


class ServiceStatusModel(Settings.DB_BASE_MODEL):
    __tablename__ = 'service_status'
    __allow_unmapped__ = True

    id=Column(Integer, primary_key=True, autoincrement=True)
    status=Column(String(32))
    service_orders=relationship('ServiceOrderModel', back_populates='status', lazy='joined', uselist=True)

Can anyone help me spot why is this error happening?
For more context, this is the function that I am trying to use prior to receiving the error:
@service_order_router.post('/', status_code=status.HTTP_201_CREATED, response_model=ServiceOrderReturnSchema)
async def post(data: ServiceOrderUpdateSchema,db: AsyncSession = Depends(get_session)) -&gt; Response:

    async with db as database:
        new_service_order = ServiceOrderModel(
            identifier=data.identifier,
            description=data.description,
            execution_value=None,
            charged_value=None,
            status_id=data.status_id,
        )
        database.add(new_service_order)
        await database.commit()

        return new_service_order

","Seems like status is probably referenced during rendering the response but isn't loaded during creation because just an id is passed.  Can you try adding this line before returning the service order?
    await database.commit()
    # Force status to load
    await database.refresh(new_service_order, [""status""])
    return new_service_order

",,,false,
https://stackoverflow.com/questions/77811401,false,The issue does not involve a problematic API. It is a question about the correct way of retaining database information with FastAPI and SQLAlchemy.,,,,,,,Correct way of retaining database information with fastapi and sqlalchemy,"we've recently changed tech stack for our python API so it now uses fastAPI and SQLAlchemy 2. Great combo, I'm sure, but I'm having issues creating something which I'd imagine would be pretty simple. I'm trying to build a simple post endpoint which takes a request body, processes it, adds it to the database, then return the inserted database entity. Something like this:
async def create_new_shipment_with_full_data(
    session: AsyncSession, data: FullShipmentCreationRequestBody
) -&gt; Shipment:
    async with session:
        shipment_data = data.model_dump(exclude_none=True)
        cursor = await session.execute(insert(Shipment).values(**shipment_data).returning(Shipment))
        inserted_shipment = cursor.scalar()
        await session.commit()
    
        return inserted_shipment

This of course doesn't work (wouldn't be here if it did) since the shipment data seems to be lost once the session commits, yielding me this error:
MissingGreenlet(""greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place?"")
Now, I've tried solving it by expunging the instance from the session, but that doesn't seem to help in this situation. I've also fixed a similar endpoint by converting the shipment to a dictionary, yet I don't know if this is the best solution.
I'm really asking about the correct way of implementing this type of endpoints using fastapi and sqlalchemy. It feels like a very standard use case and I've been trying to get help from chatgpt which has currently shown me around 467 ways which don't work. Why does this feel like rocket science?
Thanks in advance
","this error occurs when you try to pull up attributes synchronously, working in an asynchronous context, specifically in your case (I assume) that you have a Shipment object that has relation with any-obj object, and when you inserted something and indicated .returning, it will returns the Shipment object without any loaded relations, so after insert you have to session.refresh(inserted_shipment, attribute_names=[""any-obj""]) and its should fix your problem :)
also if you retrieve Shipment by id, for example, you have to load again any-obj attribute, it means:
async def retrieve(pk: int):
    stmt = select(Shipment).where(Shipment.id == pk).options(joinedload(Shipment.any-obj))
    result = await session.execute(stmt)
    return result.scalar()

or you may use selectinload (depends on your relations), read about it
you may read here about it
Speaking about how the greenlet pops up, Ill note that when you dont load these attributes yourself, the pydantic tries to get them synchronously
",,,false,
https://stackoverflow.com/questions/77383467,false,The issue does not involve a problematic API. It is a question about an error encountered when accessing the user.password attribute in a FastAPI application.,,,,,,,SQLAlchemy MissingGreenlet Error When Accessing User Object,"I'm encountering an issue in my FastAPI application when trying to access the user.password attribute. Here's a summary of the problem:
I'm using SQLAlchemy with an asynchronous MySQL database.
I've defined a User model with a password field in my application.
I'm trying to verify a user's password during the login process.
However, when I try to access user.password, I encounter the following error:
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place?
I've ensured that the user object is retrieved from the database, and I can print the user object successfully before the password verification step.
Here's the relevant code:
async def login(request: LoginSchema):
    user = await UserRepository.find_by_email(request.email)

    if user and pwd_context.verify(request.password, user.password):
        # ... (token generation and response)


My database session setup and UserRepository are as follows:
# UserRepository
class UserRepository(BaseRepo):
    model = User

    @staticmethod
    async def find_by_email(email: str):
        qry = select(User).where(User.email == email)
        return (await db.execute(qry)).scalar_one_or_none()
    
    ```

#db session
class DatabaseSession:
def init(self):
self.session = None
self.engine = None
def __getattr__(self, item):
    return getattr(self.session, item)

def init(self):
    self.engine = create_async_engine(DATABASE_URL, future=True, echo=True)
    self.session = async_sessionmaker(self.engine, expire_on_commit=False, class_=AsyncSession)()

async def create_all(self):
    async with self.engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

db = DatabaseSession()
`
I suspect that this issue may be related to SQLAlchemy's lazy loading mechanism. Could someone provide insights into what might be causing this error and how I can access the user.password attribute without triggering this exception?
Thank you in advance for your help.
","I realized in my models, the password field used the ""deferred"" keyword, in SQLAlchemy it's used to delay the loading of certain columns, which can be useful for optimizing database queries, especially when dealing with large datasets. However, in my case, using ""deferred"" on the password column caused unexpected behavior.
When I tried to access the ""user.password"" attribute, it triggered a database query because the password column was not loaded into memory until that point.
To resolve this issue, I removed the ""deferred"" keyword from the password column definition in my User model to ensure that the password column is loaded immediately when the User object is queried from the database, eliminating the greenlet spawn error.
",,,false,
https://stackoverflow.com/questions/76947178,false,The issue does not involve a problematic API. It is a question about connecting to a PostgreSQL database using ipython-sql.,,,,,,,"Update: Issue connecting to PostgreSQL database using ipython-sql (psycopg2==2.9.3, sqlalchemy==2.0.2, ipython-sql==0.3.9, postgres==15.3)","I am trying to connect to an existing PostgreSQL database. I am able to connect using the psql command-line interface, as well as pgAdmin. However, when I try to connect a python environment (JupyterLab and DataSpell), I am unable to connect.
Version Information:

psycopg2 version: 2.9.3
sqlalchemy version: 2.0.2
ipython-sql version: 0.3.9
postgres version: 15.3


%load_ext sql

%env DATABASE_URL=postgresql://retail_user:retail@localhost/retail_db

%%sql
SELECT * FROM orders LIMIT 10

The above code outputs:
__init__() got an unexpected keyword argument 'bind'
Connection info needed in SQLAlchemy format, example: postgresql://username:password@hostname/dbname or an existing connection: dict_keys([])

After some investigation, I discovered that downgrading SQLAlchemy to version 1.4.22 resolved this issue.
","The error was not due to incorrect connection details, but rather a compatibility issue between the versions of SQLAlchemy and ipython-sql I was using. I was initially using SQLAlchemy version 2.0.2, which resulted in this error.
To resolve the issue, I downgraded SQLAlchemy to version 1.4.22. After making this change, the connection worked as expected and I was able to execute SQL queries using ipython-sql without encountering the error.
For those who might face a similar problem:

Check the versions of SQLAlchemy and ipython-sql you are using.
If you are using a version of SQLAlchemy higher than 1.4.22, consider downgrading it to version 1.4.22.
Re-run your code after the downgrade to confirm that the issue is resolved.

Thank you to Adrian Klaver for suggesting the steps that ultimately led to the solution.
",,,false,
https://stackoverflow.com/questions/74809770,false,The issue does not involve a problematic API. It is related to a deprecation warning and a table not found error during the upgrade process of Airflow. It does not meet the criteria for deeper analysis as specified in the task instructions.,,,,,,,While upgrading Airflow version 2.0.0 to 2.2.5 got an error in initdb. the log is attached below,"
*While upgrading Airflow version 2.0.0 to 2.2.5 got an error in initdb. the log is attached below.
*





category=DeprecationWarning,
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361: DeprecationWarning: The logging_level option in [core] has been moved to the logging_level option in [logging] - the old setting has been used, but please update your config.
option = self._get_option_from_config_file(deprecated_key, deprecated_section, key, kwargs, section)
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The hide_sensitive_variable_fields option in [admin] has been moved to the hide_sensitive_var_conn_fields option in [core] - the old setting has been used, but please update your config.
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The base_log_folder option in [core] has been moved to the base_log_folder option in [logging] - the old setting has been used, but please update your config.
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The default_queue option in [celery] has been moved to the default_queue option in [operators] - the old setting has been used, but please update your config.
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The statsd_on option in [scheduler] has been moved to the statsd_on option in [metrics] - the old setting has been used, but please update your config.
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The default_queue option in [celery] has been moved to the default_queue option in [operators] - the old setting has been used, but please update your config.
DB: postgresql+psycopg2://airflow-dev-ipc:***@7a2dcbf5-4b02-4462-9ab2-f52f835fd961.c7e06sed0lktba7pbqj0.databases.appdomain.cloud:31604/airflow_dev_ipc
[2022-12-15 09:36:38,662] {db.py:919} INFO - Creating tables
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.s3.S3Hook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.emr.EmrHook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.s3.S3Hook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.emr.EmrHook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
INFO  [alembic.runtime.migration] Running upgrade c8ffec048a3b -&gt; a56c9515abdc, Remove dag_stat table
Traceback (most recent call last):
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context
cursor, statement, parameters, context
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 608, in do_execute
cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: table ""dag_stats"" does not exist
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
File ""/usr/local/bin/airflow"", line 8, in 
sys.exit(main())
File ""/usr/local/lib/python3.6/site-packages/airflow/main.py"", line 48, in main
args.func(args)
File ""/usr/local/lib/python3.6/site-packages/airflow/cli/cli_parser.py"", line 48, in command
return func(*args, **kwargs)
File ""/usr/local/lib/python3.6/site-packages/airflow/cli/commands/db_command.py"", line 31, in initdb
db.initdb()
File ""/usr/local/lib/python3.6/site-packages/airflow/utils/session.py"", line 70, in wrapper
return func(*args, session=session, **kwargs)
File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 592, in initdb
upgradedb(session=session)
File ""/usr/local/lib/python3.6/site-packages/airflow/utils/session.py"", line 67, in wrapper
return func(*args, **kwargs)
File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 920, in upgradedb
command.upgrade(config, 'heads')
File ""/usr/local/lib/python3.6/site-packages/alembic/command.py"", line 320, in upgrade
script.run_env()
File ""/usr/local/lib/python3.6/site-packages/alembic/script/base.py"", line 563, in run_env
util.load_python_file(self.dir, ""env.py"")
File ""/usr/local/lib/python3.6/site-packages/alembic/util/pyfiles.py"", line 92, in load_python_file
module = load_module_py(module_id, path)
File ""/usr/local/lib/python3.6/site-packages/alembic/util/pyfiles.py"", line 108, in load_module_py
spec.loader.exec_module(module)  # type: ignore
File """", line 678, in exec_module
File """", line 219, in _call_with_frames_removed
File ""/usr/local/lib/python3.6/site-packages/airflow/migrations/env.py"", line 107, in 
run_migrations_online()
File ""/usr/local/lib/python3.6/site-packages/airflow/migrations/env.py"", line 101, in run_migrations_online
context.run_migrations()
File """", line 8, in run_migrations
File ""/usr/local/lib/python3.6/site-packages/alembic/runtime/environment.py"", line 851, in run_migrations
self.get_context().run_migrations(**kw)
File ""/usr/local/lib/python3.6/site-packages/alembic/runtime/migration.py"", line 620, in run_migrations
step.migration_fn(**kw)
File ""/usr/local/lib/python3.6/site-packages/airflow/migrations/versions/a56c9515abdc_remove_dag_stat_table.py"", line 39, in upgrade
op.drop_table(""dag_stats"")
File """", line 8, in drop_table
File """", line 3, in drop_table
File ""/usr/local/lib/python3.6/site-packages/alembic/operations/ops.py"", line 1349, in drop_table
operations.invoke(op)
File ""/usr/local/lib/python3.6/site-packages/alembic/operations/base.py"", line 392, in invoke
return fn(self, operation)
File ""/usr/local/lib/python3.6/site-packages/alembic/operations/toimpl.py"", line 80, in drop_table
operation.to_table(operations.migration_context)
File ""/usr/local/lib/python3.6/site-packages/alembic/ddl/impl.py"", line 372, in drop_table
self._exec(schema.DropTable(table))
File ""/usr/local/lib/python3.6/site-packages/alembic/ddl/impl.py"", line 193, in _exec
return conn.execute(construct, multiparams)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1011, in execute
return meth(self, multiparams, params)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/sql/ddl.py"", line 72, in _execute_on_connection
return connection._execute_ddl(self, multiparams, params)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1073, in _execute_ddl
compiled,
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1317, in _execute_context
e, statement, parameters, cursor, context
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1511, in _handle_dbapi_exception
sqlalchemy_exception, with_traceback=exc_info[2], from_=e
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 182, in raise_
raise exception
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context
cursor, statement, parameters, context
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 608, in do_execute
cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) table ""dag_stats"" does not exist
[SQL:
DROP TABLE dag_stats]
(Background on this error at: http://sqlalche.me/e/13/f405)





I tried creating a table (dag_stats) in Postgres but still the issue continues.







","You will have to upgrade the database, please refer this link for uprading Airflow version
",,,false,
https://stackoverflow.com/questions/67743572,false,The issue does not involve a problematic API. It is related to exporting data into a CSV file using Flask-SQLAlchemy. The behavior observed is due to a misunderstanding of how to retrieve and write the actual data from the MySQL DB table to the CSV file. It does not meet the criteria for deeper analysis as specified in the task instructions.,,,,,,,Exporting data into CSV file from Flask-SQLAlchemy,"I'm looking to generate(export) a csv from a flask-sqlalchemy app i'm developing. But i'm getting some unexpected outcomes in my csv i.e. instead of the actual data from the MySQL DB table populated in the csv file, i get the declarative class model entries (placeholders??). The issue possibly could be the way i structured the query or even, the entire function.
Oddly enough - judging from the csv output (pic) - it would seem i'm on the right track since the row/column count is the same as the DB table but actual data is just not populated. I'm fairly new to SQLAlchemy ORM and Flask, so looking for some guidance here to pull through. Constructive feedback appreciated.
#class declaration with  DB object (divo)
class pearl(divo.Model):
    __tablename__ = 'users'                 
    work_id = divo.Column(divo.Integer, primary_key=True)
    user_fname = divo.Column(divo.String(length=255))
    user_lname = divo.Column(divo.String(length=255))
    user_category = divo.Column(divo.String(length=255))
    user_status = divo.Column(divo.String(length=1))
    login_id = divo.Column(divo.String(length=255))
    login_passwd = divo.Column(divo.String(length=255))

#user report function
@app.route(""/reports/users"")
def users_report():
    with open(r'C:\Users\Xxxxxxx\Projects\_repository\zzz.csv', 'w') as s_key:
        x15 = pearl.query.all()
        for i in x15:
#        x16 = tuple(x15)
            csv_out = csv.writer(s_key)
            csv_out.writerow(x15)
    flash(""Report generated. Please check designated repository."", ""green"")
    return redirect(url_for('reports_landing'))  # return redirect(url_for('other_tasks'))

#csv outcome (see attached pic)

","
instead of the actual data from the MySQL DB table populated in the csv file, i get the declarative class model entries (placeholders??)

Each object in the list
x15 = pearl.query.all()

represents a row in your users table.
What you're seeing in the spreadsheet are not placeholders, but string representations of each row object (See object.repr).
You could get the value of a column for a particular row object by the column name attribute, for example:
x15[0].work_id # Assumes there is at least one row object in x15

What you could do instead is something like this:
with open(r'C:\Users\Xxxxxxx\Projects\_repository\zzz.csv', 'w') as s_key:
    x15 = divo.session.query(pearl.work_id, pearl.user_fname) # Add columns to query as needed
    for i in x15:
        csv_out = csv.writer(s_key)
        csv_out.writerow(i)

i in the code above is a tuple of the form:
('work_id value', 'user_fname value')

",,,false,
https://stackoverflow.com/questions/65815257,false,"The issue does not involve a problematic API. It is related to handling transaction failures when using the sqlalchemy-aurora-data-api to connect to aurora-postgresql-serverless. The approach taken to handle the failures is being questioned, but it does not indicate an issue with the API itself. It does not meet the criteria for deeper analysis as specified in the task instructions.",,,,,,,"SQLAclhemy, auroa-serverless invalid transaction issue on commit (aurora_data_api.exceptions.DatabaseError)","I'm using the sqlalchemy-aurora-data-api to connect to aurora-postgresql-serverless, with SQLalchemy as an ORM.
For the most part, this has been working fine, but I keep hitting unexpected errors from the aurora_data_api (which sqlalchemy-aurora-data-api is built upon) during commits.
I've tried to handle this in the application logic by catching the exception and re-trying, however, this is still failing:

from aurora_data_api.exceptions import DatabaseError
from botocore.exceptions import ClientError

def handle_invalid_transaction_id(func):
    retries = 3

    @wraps(func)
    def inner(*args, **kwargs):
        for i in range(retries):
            try:
                return func(*args, **kwargs)
            except (DatabaseError, ClientError):
                if i != retries:
                    # The aim here is to try and force a new transaction 
                    # If an error occurs and retry
                    db.session.close()
                else:
                    raise

    return inner


And then in my models doing something like this:
class MyModel(db.Model):
    @classmethod
    @handle_invalid_transaction_id
    def create(cls, **kwargs):
        instance = cls(**kwargs)
        db.session.add(instance)
        db.session.commit()
        db.session.close()
        return kwargs

However, I keep hitting unpredictable transaction failures:
DatabaseError: (aurora_data_api.exceptions.DatabaseError) An error occurred (BadRequestException) when calling the ExecuteStatement operation: Transaction AXwQlogMJsPZgyUXCYFg9gUq4/I9FBEUy1zjMTzdZriEuBCF44s+wMX7+aAnyyJH/6arYcHxbCLW73WE8oRYsPMN17MOrqWfUdxkZRBrM/vBUfrP8FKv6Phfr6kK6o7/0mirCtRJUxDQAQPotaeP+hHj6/IOGUCaOnodt4M3015c0dAycuqhsy4= is not found [+26ms]
It is worth noting that these are not particularly long-running transactions, so I do not think that I'm hitting the transaction expiry issue that can occur with aurora-serverless as documented here.
Is there something fundamentally wrong with my approach to this or is there a better way to handle transactions failures when they occur?
","Just to close this off, and in case it helps anyone else, found the issue was in the transactions that were being created by in the cursor here
I can't answer the why, but we noticed that transactions were expiring despite the fact the data successfully committed. e.g:
request 1 - creates a bunch of transactions, write data, exits.
request 2 - creates a bunch of transactions, some transaction id for request 1 fails, exits.
So yeah, I don't think the issue is with the aurora-data-api, but somehow to do with transaction mgmt in general in aurora-serverless. In the end, we forked the repo and refactored so that everything is handled with ExecuteStatment calls rather than using transactions. It's been working fine so far (note we're using SQLalchemy so transactions are handled at the ORM level anyway).
",,,false,
https://stackoverflow.com/questions/64688620,false,"The issue does not involve a problematic API. It is related to connecting to IBM Db2 on Cloud using SQLAlchemy. The error message suggests a system error occurred, and the issue might be related to changes in the Python and Jupyterlab environment. It does not meet the criteria for deeper analysis as specified in the task instructions.",,,,,,,How do I fix connection to db2 using SQLAlchemy in python?,"I'm having trouble connecting to my database BLUDB in IBM Db2 on Cloud using SQLAlchemy. Here is the code I've always used and it's always worked fine:
%sql ibm_db_sa://user:pswd@some-host.services.dal.bluemix.net:50000/BLUDB

But now I get this error:

(ibm_db_dbi.ProgrammingError) ibm_db_dbi::ProgrammingError:
Exception('[IBM][CLI Driver] SQL1042C  An unexpected system error
occurred.  SQLSTATE=58004\r SQLCODE=-1042') (Background on this error
at: http://sqlalche.me/e/13/f405) Connection info needed in SQLAlchemy
format, example: postgresql://username:password@hostname/dbname or an
existing connection: dict_keys([])

These packages are loaded as always:
import ibm_db
import ibm_db_sa
import sqlalchemy
from sqlalchemy.engine import create_engine
I looked at the python db2 documentation on ibm and the sqlalchemy error message but couldn't get anywhere.
I am working in Jupyterlab locally. I've recently reinstalled Python and Jupyterlab. That's the only thing locally that's changed.
I am able to successfully run the notebooks in the cloud at kaggle and cognitive class. I am also able to connect and query sqlite3 via python without an issue using my local notebook.
All the ibm modules and version numbers are the same before and after installation. I used requirements.txt for reinstallation.
In db2diag.log here are the last two entries:
2020-11-05-14.06.47.081000-300 I13371F372           LEVEL: Warning
PID     : 17500                TID : 7808           PROC : python.exe
INSTANCE:                      NODE : 000
HOSTNAME: DESKTOP-6FFFO2E
EDUID   : 7808
FUNCTION: DB2 UDB, bsu security, sqlexLogPluginMessage, probe:20
DATA #1 : String with size, 43 bytes
loadAuthidMapper: GetModuleHandle rc = 126
2020-11-05-14.13.49.282000-300 I13745F373           LEVEL: Warning
PID     : 3060                 TID : 12756          PROC : python.exe
INSTANCE:                      NODE : 000
HOSTNAME: DESKTOP-6FFFO2E
EDUID   : 12756
FUNCTION: DB2 UDB, bsu security, sqlexLogPluginMessage, probe:20
DATA #1 : String with size, 43 bytes
loadAuthidMapper: GetModuleHandle rc = 126
","I think the root of this will be down to the new version of Python and pip caching.
What version did you move from and what version are you now on. Is this a Python 2 to Python 3 change?  When changing versions, normally you would need to clean pip install all components, but pip does use a cache. Even for components that may need to be compiled, and there is a good chance that Db2 components are being compiled.
So what you will need to do is to re-install the dependancies with
pip install --no-cache-dir
",,,false,
https://stackoverflow.com/questions/59162110,false,"The issue does not involve a problematic API. The error is related to the configuration of the ODBC driver and the connection to the SQL Server, rather than an unexpected behavior of an API.",,,,,,,Writing to SQL Server from Python,"This is the smallest example I can give that is a MRE

I am attempting to do the following:


Using pyodbc, read from a SQL Server instance (complete)
Then, print that data to verify it (complete)
Then, take that data, and insert it into a new table (or overwrite the table if it exists) &lt;- FAILURE


The code is below:

import pyodbc
import pandas as pd
import sqlalchemy as sa

sqlConn = pyodbc.connect(
    ""DRIVER={SQL Server};""
    ""SERVER=servername;""
    ""DATABASE=dbname;""
    ""Trusted_Connection=yes;""
)

sql = """"""
SELECT TOP (1000) [PART]
      ,[STEP]
      ,[COMPLETIONTIME]
  FROM [dbname].[dbo].[STEPS]
""""""

engine = sa.create_engine('mssql+pyodbc://servername/dbname')


df = pd.read_sql(sql, sqlConn)

df.to_sql(name = 'Test', con = engine, if_exists = 'replace', index = False)
sqlConn.commit()
sqlConn.close()


I get the following for an error:

  File ""WiP.py"", line 26, in &lt;module&gt;
    df.to_sql(name = 'Test', con = engine, if_exists = 'replace', index = False)
  File ""C:\Python367-64\lib\site-packages\pandas\core\generic.py"", line 2712, in to_sql
    method=method,
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 518, in to_sql
    method=method,
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 1319, in to_sql
    table.create()
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 641, in create
    if self.exists():
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 628, in exists
    return self.pd_sql.has_table(self.name, self.schema)
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 1344, in has_table
    self.connectable.dialect.has_table, name, schema or self.meta.schema
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2162, in run_callable
    with self._contextual_connect() as conn:
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2242, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2280, in _wrap_pool_connect
    e, dialect, self
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 152, in reraise
    raise value.with_traceback(tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2276, in _wrap_pool_connect
    return fn()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 492, in checkout
    rec = pool._do_get()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 139, in _do_get
    self._dec_overflow()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 153, in reraise
    raise value
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 136, in _do_get
    return self._create_connection()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 308, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 437, in __init__
    self.__connect(first_connect_check=True)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 639, in __connect
    connection = pool._invoke_creator(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\strategies.py"", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\default.py"", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
sqlalchemy.exc.InterfaceError: (pyodbc.InterfaceError) ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')
(Background on this error at: http://sqlalche.me/e/rvf5)


I looked that up on this site, and got the following:


  Exception raised for errors that are related to the databases
  operation and not necessarily under the control of the programmer,
  e.g. an unexpected disconnect occurs, the data source name is not
  found, a transaction could not be processed, a memory allocation error
  occurred during processing, etc.


I have also consulted:


Connecting to Microsoft SQL server using Python
List sql tables in pandas.read_sql
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql_table.html
https://github.com/mkleehammer/pyodbc/issues/300
Connecting to SQL Server 2012 using sqlalchemy and pyodbc


I also tried switching df.to_sql(name = 'Test', con = engine, if_exists = 'replace', index = False) to df.to_sql(name = 'Test', con = sqlConn, if_exists = 'replace', index = False) but got the exact same error.

What am I doing incorrectly? How can I wrote to a new table (or overwrite an existing one) from a Pandas dataframe?

UPDATE

It appears the connection is failing. The following:

import pyodbc
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

sqlConn = pyodbc.connect(
    ""DRIVER={SQL Server};""
    ""SERVER=servername;""
    ""DATABASE=dbname;""
    ""Trusted_Connection=yes;""
)

sql = """"""
SELECT TOP (1000) [ORDR_PART_NO]
      ,[OROP_ID]
      ,[COMPLETIONTIME]
  FROM [dbname].[dbo].[OpsLookup]
""""""
engine = sa.create_engine('mssql+pyodbc://servername/dbname')

cnxn = engine.connect()
result = cnxn.execute(""SELECT TOP (1000) * FROM [dbname].[dbo].[STEPS]"")
for row in result:
    print(row)
cnxn.close()


Yields:

C:\Python367-64\lib\site-packages\sqlalchemy\connectors\pyodbc.py:79: SAWarning: No driver name specified; this is expected by PyODBC when using DSN-less connections
  ""No driver name specified; ""
Traceback (most recent call last):
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2276, in _wrap_pool_connect
    return fn()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 303, in unique_connection
    return _ConnectionFairy._checkout(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 492, in checkout
    rec = pool._do_get()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 139, in _do_get
    self._dec_overflow()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 153, in reraise
    raise value
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 136, in _do_get
    return self._create_connection()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 308, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 437, in __init__
    self.__connect(first_connect_check=True)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 639, in __connect
    connection = pool._invoke_creator(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\strategies.py"", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\default.py"", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
pyodbc.InterfaceError: ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""WiP.py"", line 21, in &lt;module&gt;
    cnxn = engine.connect()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2209, in connect
    return self._connection_cls(self, **kwargs)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 103, in __init__
    else engine.raw_connection()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2307, in raw_connection
    self.pool.unique_connection, _connection
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2280, in _wrap_pool_connect
    e, dialect, self
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 152, in reraise
    raise value.with_traceback(tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2276, in _wrap_pool_connect
    return fn()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 303, in unique_connection
    return _ConnectionFairy._checkout(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 492, in checkout
    rec = pool._do_get()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 139, in _do_get
    self._dec_overflow()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 153, in reraise
    raise value
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 136, in _do_get
    return self._create_connection()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 308, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 437, in __init__
    self.__connect(first_connect_check=True)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 639, in __connect
    connection = pool._invoke_creator(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\strategies.py"", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\default.py"", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
sqlalchemy.exc.InterfaceError: (pyodbc.InterfaceError) ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')
(Background on this error at: http://sqlalche.me/e/rvf5)

","engine = sa.create_engine('mssql+pyodbc://servername/dbname')

needed to be changed to

engine = sa.create_engine('mssql+pyodbc://servername/dbname?trusted_connection=yes&amp;driver=ODBC Driver 13 for SQL Server')

Per: https://docs.sqlalchemy.org/en/13/dialects/mssql.html#hostname-connections
",,,false,
https://stackoverflow.com/questions/31325860,false,"The issue does not involve a problematic API. The user is seeking guidance on implementing dynamicism using SQLAlchemy, but there is no indication of unexpected behavior or unpredictable API. The provided answers suggest alternative solutions but do not address an API-related problem.",,,,,,,Dynamic Datasets and SQLAlchemy,"I am refactoring some old SQLite3 SQL statements in Python into SQLAlchemy.  In our framework, we have the following SQL statements that takes in a dict with certain known keys and potentially any number of unexpected keys and values (depending what information was provided).  

import sqlite3
import sys

def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d


def Create_DB(db):
    #    Delete the database
    from os import remove
    remove(db)

#   Recreate it and format it as needed
    with sqlite3.connect(db) as conn:
        conn.row_factory = dict_factory
        conn.text_factory = str

        cursor = conn.cursor()

        cursor.execute(""CREATE TABLE [Listings] ([ID] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL UNIQUE, [timestamp] REAL NOT NULL DEFAULT(( datetime ( 'now' , 'localtime' ) )), [make] VARCHAR, [model] VARCHAR, [year] INTEGER);"")


def Add_Record(db, data):
    with sqlite3.connect(db) as conn:
        conn.row_factory = dict_factory
        conn.text_factory = str

        cursor = conn.cursor()

        #get column names already in table
        cursor.execute(""SELECT * FROM 'Listings'"")
        col_names = list(map(lambda x: x[0], cursor.description))

        #check if column doesn't exist in table, then add it
        for i in data.keys():
            if i not in col_names:
                cursor.execute(""ALTER TABLE 'Listings' ADD COLUMN '{col}' {type}"".format(col=i, type='INT' if type(data[i]) is int else 'VARCHAR'))

        #Insert record into table
        cursor.execute(""INSERT INTO Listings({cols}) VALUES({vals});"".format(cols = str(data.keys()).strip('[]'), 
                    vals=str([data[i] for i in data]).strip('[]')
                    ))

#Database filename
db = 'test.db'

Create_DB(db)

data = {'make': 'Chevy',
    'model' : 'Corvette',
    'year' : 1964,
    'price' : 50000,
    'color' : 'blue',
    'doors' : 2}
Add_Record(db, data)

data = {'make': 'Chevy',
    'model' : 'Camaro',
    'year' : 1967,
    'price' : 62500,
    'condition' : 'excellent'}
Add_Record(db, data)


This level of dynamicism is necessary because there's no way we can know what additional information will be provided, but, regardless, it's important that we store all information provided to us.  This has never been a problem because in our framework, as we've never expected an unwieldy number of columns in our tables.

While the above code works, it's obvious that it's not a clean implementation and thus why I'm trying to refactor it into SQLAlchemy's cleaner, more robust ORM paradigm.  I started going through SQLAlchemy's official tutorials and various examples and have arrived at the following code:

from sqlalchemy import Column, String, Integer
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

Base = declarative_base()

class Listing(Base):
    __tablename__ = 'Listings'
    id = Column(Integer, primary_key=True)
    make = Column(String)
    model = Column(String)
    year = Column(Integer)

engine = create_engine('sqlite:///')

session = sessionmaker()
session.configure(bind=engine)
Base.metadata.create_all(engine)

data = {'make':'Chevy',
    'model' : 'Corvette',
    'year' : 1964}

record = Listing(**data)

s = session()
s.add(record)
s.commit()
s.close()


and it works beautifully with that data dict.  Now, when I add a new keyword, such as

data = {'make':'Chevy',
'model' : 'Corvette',
'year' : 1964,
'price' : 50000}


I get a TypeError: 'price' is an invalid keyword argument for Listing error.  To try and solve the issue, I modified the class to be dynamic, too:

class Listing(Base):
    __tablename__ = 'Listings'
    id = Column(Integer, primary_key=True)
    make = Column(String)
    model = Column(String)
    year = Column(Integer)

    def __checker__(self, data):
        for i in data.keys():
            if i not in [a for a in dir(self) if not a.startswith('__')]:
                if type(i) is int:
                    setattr(self, i, Column(Integer))
                else:
                    setattr(self, i, Column(String))
            else:
                self[i] = data[i]


But I quickly realized this would not work at all for several reasons, e.g. the class was already initialized, the data dict cannot be fed into the class without reinitializing it, it's a hack more than anything, et al.).  The more I think about it, the less obvious the solution using SQLAlchemy seems to me.  So, my main question is, how do I implement this level of dynamicism using SQLAlchemy?

I've researched a bit to see if anyone has a similar issue.  The closest I've found was Dynamic Class Creation in SQLAlchemy but it only talks about the constant attributes (""tablename"" et al.).  I believe the unanswered https://stackoverflow.com/questions/29105206/sqlalchemy-dynamic-attribute-change may be asking the same question.  While Python is not my forte, I consider myself a highly skilled programmer (C++ and JavaScript are my strongest languages) in the context scientific/engineering applications, so I may not hitting the correct Python-specific keywords in my searches.

I welcome any and all help.
","class Listing(Base):
    __tablename__ = 'Listings'
    id = Column(Integer, primary_key=True)
    make = Column(String)
    model = Column(String)
    year = Column(Integer)
    def __init__(self,**kwargs):
       for k,v in kwargs.items():
           if hasattr(self,k):
              setattr(self,k,v)
           else:
              engine.execute(""ALTER TABLE %s AD COLUMN %s""%(self.__tablename__,k)
              setattr(self.__class__,Column(k, String))
              setattr(self,k,v)


might work ... maybe ... I am not entirely sure I did not test it

a better solution would be to use a relational table

class Attribs(Base):
    listing_id = Column(Integer,ForeignKey(""Listing""))
    name = Column(String)
    val = Column(String)

class Listing(Base):
    id = Column(Integer,primary_key = True)
    attributes = relationship(""Attribs"",backref=""listing"")
    def __init__(self,**kwargs):
        for k,v in kwargs.items():
            Attribs(listing_id=self.id,name=k,value=v)
    def __str__(self):
        return ""\n"".join([""A LISTING"",] + [""%s:%s""%(a.name,a.val) for a in self.attribs])


another solution would be to store json

class Listing(Base):
    __tablename__ = 'Listings'
    id = Column(Integer, primary_key=True)
    data = Column(String)
    def __init__(self,**kwargs):
       self.data = json.dumps(kwargs)
       self.data_dict = kwargs


the best solution would be to use a no-sql key,value store (maybe even just a simple json file? or perhaps shelve? or even pickle I guess)
",,,false,
https://stackoverflow.com/questions/4253176,true,The issue involves a problematic API in SqlAlchemy that leads to unexpected behavior under specific runtime conditions.,SqlAlchemy,session.query,"The issue occurs when accessing the sqlalchemy object methods after the server runs for a while. The error message ""Parent instance is not bound to a Session; lazy load operation of attribute 'rate' cannot proceed"" indicates that the session object is getting cleared, invalidating the objects.","The expected behavior is for the sqlalchemy object methods to work properly without throwing the ""Parent instance is not bound to a Session"" error. However, after the server runs for a while, the session object gets cleared and the methods fail to execute.","The issue is triggered when the server runs for a while, causing the session object to get cleared and the objects to become invalid.","This issue might be challenging to detect during development and testing because it only affects some objects and there is no clear pattern to the behavior. Additionally, the issue may not be immediately apparent until the server has been running for a significant amount of time.",Issue with SqlAlchemy - &quot;Parent instance &lt;SomeClass&gt; is not bound to a Session; lazy load operation...&quot;,"I have a small thrift server in python that I use do some fast lookups. The server queries mysql via SqlAlchemy on the first request and shoves all returned objects into a dictionary so on subsequent requests no DB call is needed. I just get the object from the dict and then call some of the object methods needed to give the proper response.

Initially, everything is fine. However, after the server runs a while, I am getting this exception when accessing the sqlalchemy object methods:


  
    Parent instance  is not bound to a Session; lazy load operation of attribute 'rate' cannot proceed.
  


Strange, because I set eagerload('rate').

I cannot really see a pattern to this behavior, it only affects some objects. However, once it does affect an object it will continue to do so on each request until I restart my python server.

Any ideas?
","You probably cache objects between the requests, and when the commit happens, session object is getting cleared, invalidating your objects. If you start your server via some multithreaded web server that starts workers as needed, that explains why there's no pattern.
If you dont want to get the bottom of this and just need a quick fix, this will always work:  

if obj not in session:
    obj = session.query(ObjClass).get(obj.id)


The proper solution would be to make sure you don't cache objects between requests.
",,,false,
https://stackoverflow.com/questions/38357352,false,The issue does not involve a problematic API. It is related to the usage of getter/setter methods and the behavior of the SQLAlchemy query filter expression.,,,,,,,Convert datetime to unix timestamp in SQLAlchemy model before executing query?,"I am using SQLAlchemy to work with a remote database that uses a strange timestamp format--it stores timestamps as double-precision milliseconds since epoch. I'd like to work with python datetime objects, so I wrote getter/setter methods in my model, following this gist:

from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import synonym
from sqlalchemy.dialects.mysql import DOUBLE
import datetime

Base = declarative_base()
class Table(Base):
    __tablename__ = ""table""

    id = Column(Integer, primary_key=True)
    _timestamp = Column(""timestamp"", DOUBLE(asdecimal=False))

    @property
    def timestamp(self):
        return datetime.datetime.utcfromtimestamp(float(self._timestamp)/1000.)

    @timestamp.setter
    def timestamp(self, dt):
        self._timestamp = float(dt.strftime(""%s""))*1000.

    timestamp = synonym('_timestamp', descriptor=timestamp)


This works great for inserting new rows into the table and working with objects from the table:

&gt;&gt;&gt; table = session.query(Table).first()
&lt;Table id=1&gt;
&gt;&gt;&gt; table.timestamp
datetime.datetime(2016, 6, 27, 16, 9, 3, 320000)
&gt;&gt;&gt; table._timestamp
1467043743320.0


However, it breaks down when I try to use a datetime in a filter expression:

&gt;&gt;&gt; july = datetime.datetime(2016, 7, 1)
&gt;&gt;&gt; old = session.query(Table).filter(Table.timestamp &lt; july).first()
/lib/python2.7/site-packages/sqlalchemy/engine/default.py:450: Warning: Truncated incorrect DOUBLE value: '2016-07-01 00:00:00'
&gt;&gt;&gt; july_flt = float(july.strftime(""%s""))*1000.
&gt;&gt;&gt; old = session.query(Table).filter(Table.timestamp &lt; july_flt).first()
&lt;Table id=1&gt;


I assume this is because my getter/setter methods apply to instances of the table class, but don't change the behavior of the class itself. I've tried rewriting using a hybrid property instead of a synonym:

from sqlalchemy.ext.hybrid import hybrid_property

class Table(Base):
    __tablename__ = ""table""

    id = Column(Integer, primary_key=True)
    _timestamp = Column(""timestamp"", DOUBLE(asdecimal=False))

    @hybrid_property
    def timestamp(self):
        return datetime.datetime.utcfromtimestamp(float(self._timestamp)/1000.)

    @timestamp.setter
    def timestamp(self, dt):
        self._timestamp = float(dt.strftime(""%s""))*1000.


Again, this works with Table instances, but fails on a query--now it's hitting my getter method when I run the query:

&gt;&gt;&gt; july = datetime.datetime(2016, 7, 1)
&gt;&gt;&gt; old = session.query(Table).filter(Table.timestamp &lt; july).first()
Traceback:
  File ""models.py"", line 42, in timestamp
    return datetime.datetime.utcfromtimestamp(float(self._timestamp)/1000.)
TypeError: float() argument must be a string or a number


With the debugger, I can see that the getter is receiving the Table._timestamp class (not a specific Table._timestamp, and not 'july').

I see that I could use the hybrid_property.expression decorator to define a SQL expression for converting timestamps into datetime, but what I'd really like is to convert the datetime into a timestamp on the python side, then run the query using timestamps. In other words, I'd like to use datetimes everywhere (including in queries), but have everything done with the microsecond timestamps on the SQL side. How can I do this?
","You have to use a custom type, which isn't as scary as it sounds.

from sqlalchemy.types import TypeDecorator


class DoubleTimestamp(TypeDecorator):
    impl = DOUBLE

    def __init__(self):
        TypeDecorator.__init__(self, as_decimal=False)

    def process_bind_param(self, value, dialect):
        return value.replace(tzinfo=datetime.timezone.utc).timestamp() * 1000

    def process_result_value(self, value, dialect):
        return datetime.datetime.utcfromtimestamp(value / 1000)


Then Table becomes:

class Table(Base):
    __tablename__ = ""table""

    id = Column(Integer, primary_key=True)
    timestamp = Column(DoubleTimestamp)


And then everything you mention works. You insert, select and compare with datetimes but it's stored as a DOUBLE.

Here I've used different logic for converting between timestamps since strftime('%s') isn't the correct solution. It's a different question which has been answered correctly here. Oh and I noticed you said microseconds but only convert to milliseconds in the code you posted, unless it was a slip of the tongue .
",,,false,
