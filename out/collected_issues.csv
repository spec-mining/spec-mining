search_keywords,issue_link,issue_title,issue_body,issue_score,issue_views,answer_1,answer_2,answer_3
scikit-learn unexpected behavior,https://stackoverflow.com/questions/76876536,Unexpected Feature Names in scikit-learn TfidfVectorizer,"I'm trying to use the TfidfVectorizer from scikit-learn to convert a list of words into a TF-IDF matrix. However, I'm encountering some unexpected behavior where the feature names generated by the vectorizer don't match the words in my input text.
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import brown
import re

txt = brown.words()
txt_str = "" "".join(txt)

def custom_tokenizer(text):
    tokens = re.findall(r'\b\w+\b', text.lower())
    return tokens

documents = [txt_str]

vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)
tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

print(feature_names)

I've noticed that some feature names in the output are neither present in my txt nor in the tokens generated by the custom_tokenizer. Additionally, some feature names seem to be numeric values or arbitrary characters and words.
I've tried various approaches, including using a custom tokenizer to ensure accurate tokenization, updating scikit-learn. It works on smaller texts as well as when the max_features is set to low enough value. However, for my project I do need all the features so I am trying to figure out what is going on.
Why I'm getting these unexpected feature names and how I can ensure that the feature names correspond accurately to the words in my input text?
some of the features are zwei' 'zworykin'.
",1,52,"Have you some examples of unexpected feature names ?
From my side, I don't think there is any problem extracting features.
The words not present in feature names, are words with special characters like -  ' or $. WHen you extract the text with the function custom_tokenizer, you get two tokens for example polyether-type, gives ""polyether"" and ""type"" tokens
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import brown
import re

txt = brown.words()
txt_str = "" "".join(txt)

def custom_tokenizer(text):
    tokens = re.findall(r'\b\w+\b', text.lower())
    return tokens

documents = [txt_str]

vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)
tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

print(f""Nb features                     : { len(feature_names)}"")
print(f""Nb words in brown               : { len(txt) }"")

# nb uniq lowercase words in brown
mylist = list(dict.fromkeys(txt))
mylist = [ x.lower() for x in mylist]
print(f""Nb uniq lowercase words in brown: {len(mylist)}"")

# in li1 and not in li2
def Diff(li1, li2):
    set_dif = set(li1).difference(set(li2))
    temp3 = list(set_dif)
    return temp3
li3 = Diff(mylist,feature_names)

def info(text):
    custom_tokenizer(text)
    try:
        s=list(feature_names).index(text)
        print(f""The text {text} was found in features at position : {s}"")
    except:
        print(f""The text {text} was not found in features"")    
    

print(f""10 first words not in feature_names : \n { li3[0:10]} "")

info(""season"")
info(""mosquito"")

Nb features                     : 42432
Nb words in brown               : 1161192
Nb uniq lowercase words in brown: 56057
10 first words not in feature_names :
[""season's"", 'non-farm', ""chip-o's"",
'polyether-type', 'take-off', '1-0', 'spring-back', '$50',
'mosquito-plagued', 'anti-submarine']
The text season was found in features at position : 33478
The text mosquito was found in features at position : 24918
",,
scikit-learn unexpected result,https://stackoverflow.com/questions/41949101,scikit learn LDA giving unexpected results,"I am attempting to classify some data with the scikit learn LDA classifier.  I'm not entirely sure what to ""expect"" from it, but what I am getting is weird.  Seems like a good opportunity to learn about either a shortcoming of the technique, or a way in which I am applying it wrong.  I understand that no line could completely separate this data, but it seems that there are much ""better"" lines than the one it is finding.  I'm just using the default options.  Any thoughts on how to do this better?  I'm using LDA because it is linear in the size of my dataset.  Although I think a linear SVM has a similar complexity.  Perhaps it would be better for such data?  I will update when I have tested other possibilities.

The picture: (light blue is what my LDA classifier predicts will be dark blue)



The code:

import numpy as np
from numpy import array
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
import itertools

X = array([[ 0.23125754,  0.79170351],
       [ 0.78021491, -0.24999486],
       [ 0.00856446,  0.41452734],
       [ 0.66381753, -0.09872504],
       [-0.03178685,  0.04876317],
       [ 0.65574645, -0.68214948],
       [ 0.14290684,  0.38256002],
       [ 0.05156987,  0.11094875],
       [ 0.06843403,  0.19110019],
       [ 0.24070898, -0.07403764],
       [ 0.03184353,  0.4411446 ],
       [ 0.58708124, -0.38838008],
       [-0.00700369,  0.07540799],
       [-0.01907816,  0.07641038],
       [ 0.30778608,  0.30317186],
       [ 0.55774143, -0.38017325],
       [-0.00957214, -0.03303287],
       [ 0.8410637 ,  0.158594  ],
       [-0.00294113, -0.00380608],
       [ 0.26577841,  0.07833684],
       [-0.32249375,  0.49290502],
       [ 0.11313078,  0.35697211],
       [ 0.41153679, -0.4471876 ],
       [-0.00313315,  0.30065913],
       [ 0.14344143, -0.19127107],
       [ 0.04857767,  0.01339191],
       [ 0.5865007 ,  0.71209886],
       [ 0.08157439,  0.40909955],
       [ 0.72495202,  0.29583866],
       [-0.09391461,  0.17976605],
       [ 0.06149141,  0.79323099],
       [ 0.52208024, -0.2877661 ],
       [ 0.01992141, -0.00435266],
       [ 0.68492617, -0.46981335],
       [-0.00641231,  0.29699622],
       [ 0.2369677 ,  0.140319  ],
       [ 0.6602586 ,  0.11200433],
       [ 0.25311836, -0.03085372],
       [-0.0895014 ,  0.45147252],
       [-0.18485667,  0.43744524],
       [ 0.94636701,  0.16534406],
       [ 0.01887734, -0.07702135],
       [ 0.91586801,  0.17693792],
       [-0.18834833,  0.31944796],
       [ 0.20468328,  0.07099982],
       [-0.15506378,  0.94527383],
       [-0.14560083,  0.72027034],
       [-0.31037647,  0.81962815],
       [ 0.01719756, -0.01802322],
       [-0.08495304,  0.28148978],
       [ 0.01487427,  0.07632112],
       [ 0.65414479,  0.17391618],
       [ 0.00626276,  0.01200355],
       [ 0.43328095, -0.34016614],
       [ 0.05728525, -0.05233956],
       [ 0.61218382,  0.20922571],
       [-0.69803697,  2.16018536],
       [ 1.38616732, -1.86041621],
       [-1.21724616,  2.72682759],
       [-1.26584365,  1.80585403],
       [ 1.67900048, -2.36561699],
       [ 1.35537903, -1.60023078],
       [-0.77289615,  2.67040114],
       [ 1.62928969, -1.20851808],
       [-0.95174264,  2.51515935],
       [-1.61953649,  2.34420531],
       [ 1.38580104, -1.9908369 ],
       [ 1.53224512, -1.96537012]])

y = array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.])

classifier = LDA()
classifier.fit(X,y)

xx = np.array(list(itertools.product(np.linspace(-4,4,300), np.linspace(-4,4,300))))
yy = classifier.predict(xx)
b_colors = ['salmon' if yyy==0 else 'deepskyblue' for yyy in yy]
p_colors = ['r' if yyy==0 else 'b' for yyy in y]
plt.scatter(xx[:,0],xx[:,1],s=1,marker='o',edgecolor=b_colors,c=b_colors)
plt.scatter(X[:,0], X[:,1], marker='o', s=5, c=p_colors, edgecolor=p_colors)
plt.show()


UPDATE: Changing from using sklearn.discriminant_analysis.LinearDiscriminantAnalysis to sklearn.svm.LinearSVC also using the default options gives the following picture:



I think using the zero-one loss instead of the hinge loss would help, but sklearn.svm.LinearSVC doesn't seem to allow custom loss functions.

UPDATE: The loss function to sklearn.svm.LinearSVC approaches the zero-one loss as the parameter C goes to infinity.  Setting C = 1000 gives me what I was originally hoping for.  Not posting this as an answer, because the original question was about LDA.

picture:


",3,315,"LDA models each class as a Gaussian, so the model for each class is determined by the class' estimated mean vector and covariance matrix.
Judging by the eye only, your blue and red classes have approximately the same mean and same covariance, which means the 2 Gaussians will 'sit' on top of each other, and the discrimination will be poor. Actually it also means that the separator (the blue-pink border) will be noisy, that is it will change a lot between random samples of your data.

Btw your data is clearly not linearly-separable, so every linear model will have a hard time discriminating the data.

If you must use a linear model, try using LDA with 3 components, such that the top-left blue blob is classified as '0', the bottom-right blue blob as '1', and the red as '2'. This way you will get a much better linear model. You can do it by preprocessing the blue class with a clustering algorithm with K=2 classes.
",,
scikit-learn unexpected result,https://stackoverflow.com/questions/30066268,Provide Starting Positions to t-distributed Stochastic Neighbor Embedding (TSNE) in scikit-learn,"I've been looking at using scikit learns' TSNE method to visualize high dimensional data in 2D. However, I have some idea of where the starting positions should be in 2D space but I don't see any way of specifying this information. Any ideas how I might be able to provide the starting coordinates?

I see that there's an ""init"" parameter but it appears to only take ""random"" or ""pca"" as options. What I'm looking for is a way to specify exactly what the initial coordinates are.

Update

I'm trying to increase the distance between my data points (~6,500 points) but they are quite tightly clustered and overlapping. With default parameters, I get:

model = sklearn.manifold.TSNE(n_components=2, random_state=0)




A slight increase in n_iter and early_exaggeration didn't produce significantly different results as the points (within clusters) are still overlapping.

model = sklearn.manifold.TSNE(n_components=2, random_state=0, n_iter=10000, early_exaggeration=10)




However, increasing the early_exaggeration from 10 to 100 (which, according to the docs, should increase the distance between clusters) produced some unexpected results (I ran this twice and it was the same result):

model = sklearn.manifold.TSNE(n_components=2, random_state=0, n_iter=10000, early_exaggeration=100)




This link: https://beta.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm Provides an example (three quarters of the way down the page) for how to monkey patch the gradient_descent function to save coordinates/positions. 

Added Issue to scikit-learn
",2,906,"It is currently not possible, but it would be a two-line change.
I think it would be a good addition, and we do support init=array for things like k-means. So PR welcome.
",,
scikit-learn unexpected result,https://stackoverflow.com/questions/36865542,Adjusted Mutual Information (scikit-learn),"I have implemented a clustering algorithm for summarizing log files, and am currently testing it against ground-truth data with the Adjusted Rand index and the Adjusted Mutual Information index. 

Input to my algorithm is a list of log entries, and output is a list of integers (the cluster label that each item belongs to). The ground truth is similarly a list of integers where each integer represent the true cluster the item belongs to. For most of my test cases I receive normal/expected results, but one file is giving me unexpected output. I have enclosed the two lists, the ground-truth clustering as well as that of my algorithm's:

Ground truth list:
http://pastebin.com/9Y5TE6b7

Own clustering:
http://pastebin.com/hJz1M4sf

These two lists are fed into scikit-learn functions to get the ARI and AMI. The ARI score looks roughly correct, but AMI is above 1, which according to the documentation and definition of AMI should not be possible if I understand it correctly. This data set is highly unbalanced, but many of my other files are similarly balanced. I cannot figure this out. For reference, the scores I get for ARI and AMI is:

ARI: 0.99642743999922712

AMI: 1.0190170466324
",2,813,"This has been fixed in the development version.
",,
scikit-learn unexpected result,https://stackoverflow.com/questions/12358965,scikit-learn RandomForestClassifier produces &#39;unexpected&#39; results,"I'm trying to use sk-learn's RandomForestClassifier for a binary classification task (positive and negative examples). My training data contains 1.177.245 examples with 40 features, in SVM-light format (sparse vectors) which I load using sklearn.dataset's load_svmlight_file. It produces a sparse matrix of 'feature values' (1.177.245 * 40) and one array of 'target classes' (1s and 0s, 1.177.245 of them). I don't know whether this is worrysome, but the trainingdata has 3552 positives and the rest are all negative.

As the sk-learn's RFC doesn't accept sparse matrices, I convert the sparse matrix to a dense array (if I'm saying that right? Lots of 0s for absent features) using .toarray(). I print the matrix before and after converting to arrays and that seems to be going all right.

When I initiate the classifier and start fitting it to the data, it takes this long:

[Parallel(n_jobs=40)]: Done   1 out of  40 | elapsed: 24.7min remaining: 963.3min
[Parallel(n_jobs=40)]: Done  40 out of  40 | elapsed: 27.2min finished


(is that output right? Those 963 minutes take about 2 and a half...)

I then dump it using joblib.dump.
When I re-load it:

RandomForestClassifier: RandomForestClassifier(bootstrap=True, compute_importances=True,
        criterion=gini, max_depth=None, max_features=auto,
        min_density=0.1, min_samples_leaf=1, min_samples_split=1,
        n_estimators=1500, n_jobs=40, oob_score=False,
        random_state=&lt;mtrand.RandomState object at 0x2b2d076fa300&gt;,
        verbose=1)


And test it on real trainingdata (consisting out of 750.709 examples, exact same format as training data) I get ""unexpected"" results. To be exact; only one of the examples in the testingdata is classified as true. When I train on half the initial trainingdata and test on the other half, I get no positives at all.

Now I have no reason to believe anything is wrong with what's happening, it's just that I get weird results, and furthermore I think it's all done awfully quick. It's probably impossible to make a comparison, but training a RFClassifier on the same data using rt-rank (also with 1500 iterations, but with half the cores) takes over 12 hours...

Can anyone enlighten me whether I have any reason to believe something is not working the way it's supposed to? Could it be the ratio of positives to negatives in the training data? Cheers.
",1,2607,"Indeed this dataset is very very imbalanced. I would advise you to subsample the negative examples (e.g. pick n_positive_samples of them at random) or to oversample the positive example (the latter is more expensive and but might yield better models).

Also are you sure that all your features are numerical features (larger values means something in real life)? If some of them are categorical integer markers, those feature should be exploded as one-of-k boolean encodings instead as scikit-learn implementation of random forest s cannot directly deal with categorical data.
",,
scikit-learn unexpected result,https://stackoverflow.com/questions/53215887,unexpected result for inheritance in scikit-learn,"I am getting an unexpected result when trying to create a simple modification of the class KNeighborsClassifier:

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)

class my_Classifier(KNeighborsClassifier):  
  """"""My Nearest Neighbour classifier""""""
  def __init__(self, gamma=0):
    def my_dist(x, y):  # squared distance
      return np.sum((abs(x-y))**gamma)
    KNeighborsClassifier.__init__(self, n_neighbors=1, metric=my_dist)
    self.gamma = gamma
    print(gamma)
  def fit(self, X, y):
    KNeighborsClassifier.fit(self, X, y)
    return self
  def predict(self, X, y=None):
    return KNeighborsClassifier.predict(self, X)
  def score(self, X, y):
    return KNeighborsClassifier.score(self, X, y)


I have made the metric dependent on a parameter gamma&gt;=0.  If gamma=2, this is just the squared Euclidean metric, and if gamma=0, this is a useless metric (essentially a constant).  First it works as expected: for

knn = my_Classifier(gamma=2)
knn.fit(X_train, y_train)
knn.score(X_test,y_test)


the output is

2
0.9736842105263158


(the accuracy is good) and for

knn = my_Classifier(gamma=0)
knn.fit(X_train, y_train)
knn.score(X_test,y_test)


the output is

0
0.34210526315789475


(the accuracy is hopeless).  I am also printing the value of gamma used.

However, when I try

from sklearn.model_selection import GridSearchCV
param_grid = {'gamma': [0,2]}
grid_search = GridSearchCV(my_Classifier(), param_grid)
grid_search.fit(X_train, y_train)
grid_search.score(X_test, y_test)


the result is unexpected: 

0
0
0
0
0
0
0
0
0.34210526315789475


Why is the value gamma=0 used every time?  And gamma=2 (producing a much better result) is never tried.  I know I am making some silly mistake but can't see where.
",1,72,"The value gamma = 0 is not used every time !

What you see when you get this:

0
0
0
0
0
0
0
0
0.34210526315789475


Is the score



To see the gamma use this:

grid_search.cv_results_ 

",,
scikit-learn unexpected result,https://stackoverflow.com/questions/69916926,Unexpected error when trying to install conda environment from .yaml file,"I am trying to install a conda environment in WSL2 from a .yaml file, the instructions for the installation saying the first step is to run the following command:
conda env create -f devtools/conda-envs/ael-test.yaml

After the WSL2 terminal collects the package metadata and solves the environment, it encounters an error, which I don't really know how to solve. I run the command above from the base environment, from the directory where /devtools/conda-ens/ael-test.yaml is located.
Collecting package metadata (repodata.json): done
Solving environment: done
Preparing transaction: failed

# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

    Traceback (most recent call last):
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/exceptions.py"", line 1079, in __call__
        return func(*args, **kwargs)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/cli/main.py"", line 80, in do_call
        exit_code = getattr(module, func_name)(args, parser)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/cli/main_create.py"", line 141, in execute
        result[installer_type] = installer.install(prefix, pkg_specs, args, env)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/installers/conda.py"", line 59, in install
        unlink_link_transaction.execute()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 245, in execute
        self.verify()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 222, in verify
        self.prepare()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 211, in prepare
        grps = self._prepare(self.transaction_context, stp.target_prefix,
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 288, in _prepare
        packages_info_to_link = tuple(read_package_info(prec, pcrec)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 288, in &lt;genexpr&gt;
        packages_info_to_link = tuple(read_package_info(prec, pcrec)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/gateways/disk/read.py"", line 89, in read_package_info
        package_metadata = read_package_metadata(epd)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/gateways/disk/read.py"", line 144, in read_package_metadata
        data = json.loads(f.read())
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/__init__.py"", line 357, in loads
        return _default_decoder.decode(s)
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/decoder.py"", line 337, in decode
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/decoder.py"", line 355, in raw_decode
        raise JSONDecodeError(""Expecting value"", s, err.value) from None
    json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

`$ /home/gheorghe/anaconda3/bin/conda-env create -f devtools/conda-envs/ael-test.yaml`

  environment variables:
                 CIO_TEST=&lt;not set&gt;
  CONDA_AUTO_UPDATE_CONDA=false
        CONDA_DEFAULT_ENV=base
                CONDA_EXE=/home/gheorghe/anaconda3/bin/conda
             CONDA_PREFIX=/home/gheorghe/anaconda3
    CONDA_PROMPT_MODIFIER=(base)
         CONDA_PYTHON_EXE=/home/gheorghe/anaconda3/bin/python
               CONDA_ROOT=/home/gheorghe/anaconda3
              CONDA_SHLVL=1
           CURL_CA_BUNDLE=&lt;not set&gt;
                     PATH=/home/gheorghe/anaconda3/bin:/home/gheorghe/.vscode-server/bin/f4af3cb
                          f5a99787542e2a30fe1fd37cd644cc31f/bin:/home/gheorghe/anaconda3/bin:/ho
                          me/gheorghe/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbi
                          n:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Program Files
                          /WindowsApps/Microsoft.WindowsTerminal_1.11.2921.0_x64__8wekyb3d8bbwe:
                          /mnt/c/Program Files/copasi.org/COPASI 4.29.228/bin:/mnt/c/Windows/sys
                          tem32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/Syste
                          m32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Prog
                          ram Files (x86)/Intel/Intel(R) Management Engine
                          Components/DAL:/mnt/c/Program Files/Intel/Intel(R) Management Engine
                          Components/DAL:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA
                          NvDLISR:/mnt/c/Program Files (x86)/NVIDIA
                          Corporation/PhysX/Common:/mnt/c/Program Files/Mullvad VPN/resources:/m
                          nt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt
                          /c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/Ope
                          nSSH:/mnt/c/Program Files (x86)/Common Files/Propellerhead
                          Software/ReWire:/mnt/c/Program Files/Common Files/Propellerhead
                          Software/ReWire:/mnt/c/Program Files/NCBI/blast-2.12.0+/bin:/mnt/c/Use
                          rs/gheor/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/ghe
                          or/AppData/Local/Microsoft/WindowsApps:/snap/bin
       REQUESTS_CA_BUNDLE=&lt;not set&gt;
            SSL_CERT_FILE=&lt;not set&gt;

     active environment : base
    active env location : /home/gheorghe/anaconda3
            shell level : 1
       user config file : /home/gheorghe/.condarc
 populated config files : /home/gheorghe/.condarc
          conda version : 4.10.3
    conda-build version : 3.21.5
         python version : 3.8.11.final.0
       virtual packages : __linux=5.10.16.3=0
                          __glibc=2.31=0
                          __unix=0=0
                          __archspec=1=x86_64
       base environment : /home/gheorghe/anaconda3  (writable)
      conda av data dir : /home/gheorghe/anaconda3/etc/conda
  conda av metadata url : None
           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /home/gheorghe/anaconda3/pkgs
                          /home/gheorghe/.conda/pkgs
       envs directories : /home/gheorghe/anaconda3/envs
                          /home/gheorghe/.conda/envs
               platform : linux-64
             user-agent : conda/4.10.3 requests/2.26.0 CPython/3.8.11 Linux/5.10.16.3-microsoft-standard-WSL2 ubuntu/20.04.3 glibc/2.31
                UID:GID : 1000:1000
             netrc file : None
           offline mode : False


An unexpected error has occurred. Conda has prepared the above report.

If submitted, this report will be used by core maintainers to improve
future releases of conda.

Edit:
This is the .yaml file:
channels:
  - conda-forge
  - pytorch
dependencies:
  - python
  - pip

  - numpy
  - scipy
  - scikit-learn

  - qcelemental
  - openbabel

  - matplotlib
  - seaborn

  - mlflow

  - pytorch
  - torchvision

  - lark-parser
  - cython

  - pip:
    - torchani
    - ""git+https://github.com/RMeli/mdanalysis.git@develop#egg=MDAnalysis&amp;subdirectory=package""

  - black
  - flake8
  - mypy
  - isort

  - pytest
  - pytest-xdist
  - pytest-cov
  - codecov

",1,915,"There is no problem in your environment, I managed to install it, getting
Successfully built MDAnalysis
Installing collected packages: msgpack, tqdm, mrcfile, mmtf-python, gsd, fasteners, biopython, GridDataFormats, torchani, MDAnalysis
Successfully installed GridDataFormats-1.0.1 MDAnalysis-2.6.0.dev0 biopython-1.81 fasteners-0.18 gsd-3.1.1 mmtf-python-1.1.3 mrcfile-1.4.3 msgpack-1.0.5 torchani-2.2.3 tqdm-4.66.1

done
#
# To activate this environment, use
#
#     $ conda activate ael

in the end.
The problem you encountering is probably related to https://github.com/conda/conda/issues/9590
which has a workaround
sudo rm -r ~/.condarc

or editing the ~/.condarc like https://github.com/conda/conda/issues/9590#issuecomment-1003211237
",,
scikit-learn unexpected result,https://stackoverflow.com/questions/17794313,Unexpected results when using scikit-learn&#39;s SVM classification algorithm (RBF kernel),"Using the example on this page 
http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html,
I created my own graphs using some normally distributed data with a standard deviation of 10 instead of the iris data.

My graph turned out to be like this:


Notice how the RBF kernel graph is very different from the the one from the example. The entire area is classified to be yellow except the red and blue bits. In other words there are too many support vectors. I have tried changing C and degree but they didn't help. The code I used to produce this graph is shown below. 

Please note I need to use RBF kernel because polynomial kernels run significantly slower than RBF kernels.

import numpy as np
import pylab as pl
from sklearn import svm, datasets

FP_SIZE = 50
STD = 10

def gen(fp):

  data = []
  target = []

  fp_count = len(fp)

  # generate rssi reading for monitors / fingerprint points
  # using scikit-learn data structure
  for i in range(0, fp_count):
    for j in range(0,FP_SIZE):
      target.append(i)
      data.append(np.around(np.random.normal(fp[i],STD)))

  data = np.array(data)
  target = np.array(target)

  return data, target

fp = [[-30,-70],[-58,-30],[-60,-60]]

data, target = gen(fp)

# import some data to play with
# iris = datasets.load_iris()
X = data[:, :2]  # we only take the first two features. We could
                      # avoid this ugly slicing by using a two-dim dataset
Y = target

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, Y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)
lin_svc = svm.LinearSVC(C=C).fit(X, Y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# title for the plots
titles = ['SVC with linear kernel',
          'SVC with RBF kernel',
          'SVC with polynomial (degree 3) kernel',
          'LinearSVC (linear kernel)']


for i, clf in enumerate((svc, rbf_svc, poly_svc, lin_svc)):
    # Plot the decision boundary. For that, we will asign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    pl.subplot(2, 2, i + 1)
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    pl.contourf(xx, yy, Z, cmap=pl.cm.Paired)
    pl.axis('off')

    # Plot also the training points
    pl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)

    pl.title(titles[i])

pl.show()

",0,3941,"DId you use any other measure of correctness aside from what you get in the point.

Usually SVMs need to be run using a grid search, specially if you have  an RBF, C only will take care of the regularization, which will do little if your data is not sparse to begin with.

You need to run a grid search over gamma and C, they have a really good example of that here:

http://scikit-learn.org/0.13/auto_examples/grid_search_digits.html#example-grid-search-digits-py

Also, their library already takes care of the cross validation.

Remember that those examples are good for the toy datasets, the moment you enter with a new dataset, there is no reason to believe is going to behave anything like the one in the example.
",,
scikit-learn unexpected issue,https://stackoverflow.com/questions/59830510,Defining distance parameter (V) in knn crossval grid search (seuclidean/mahalanobis distance metrics),"I am trying to carry out a k-fold cross-validation grid search using the KNN algorithm using python sklearn, with parameters in the search being number of neighbors K and distance metric. I am including mahalanobis and seuclidean as distance metrics, and understand these have a parameter which needs to be specified, namely V or VI (covariance matrix of features or inverse of this). 

Below is my code: 

X_train, X_test, y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=10,stratify=y)

knn=KNeighborsClassifier()

grid_param={'n_neighbors':np.arange(1,51),'metric':['euclidean','minkowski','mahalanobis','seuclidean'],'metric_params':[{'V': np.cov(X_train)}]} 

knn_gscv=GridSearchCV(knn,grid_param,cv=5)

knn_gscv.fit(X_train,y_train) (*)


The (*) line throws this error when executed: 

TypeError: __init__() got an unexpected keyword argument 'V'

I have also tried VI instead of V but getting same error. 

I have come across potential solutions below but these don't help. 

https://github.com/scikit-learn/scikit-learn/issues/6915

Scikit-learn: How do we define a distance metric's parameter for grid search

Any help appreciated!

This is also my first question, so any feedback would be helpful also with this regard. 
",3,2128,"grid_params = [
    {'n_neighbors': np.arange(1, 51), 'metric': ['euclidean', 'minkowski']},
    {'n_neighbors': np.arange(1, 51), 'metric': ['mahalanobis', 'seuclidean'],
     'metric_params': [{'V': np.cov(X_train)}]}
]


The issue is that euclidean and minkowski metrics do not accepts V parameter. So you need to separate them. 
",,
scikit-learn unexpected issue,https://stackoverflow.com/questions/38966075,How to restrict anaconda from upgrading the module being installed if its a higher level dependency,"I'm trying to use continuum io anaconda packing system to package python-2.7.10 with other dependent modules for our environment. I want to automate the pack distribution to simply be a single installation of python with the modules we require.

The issue I'm having is when I specify the modules under the build parameter in meta.yaml it will upgrade the version of python being installed despite the fact that it is python-2.7.10. This will cause an error in the build process.

Is there a way to pin the version of python being installed so that if there is a dependency it will hard fail, or use an earlier version of the package?

meta.yaml, ive tried not pinning the version of the modules as well.

package:
  name: python
  version: 2.7.10

source:
  fn: Python-2.7.10.tgz
  url: https://www.python.org/ftp/python/2.7.10/Python-2.7.10.tgz
  md5: d7547558fd673bd9d38e2108c6b42521

build:
  no_link: bin/python

requirements:
  build:
    - bzip2 [unix]
    - zlib [unix]
    - sqlite [unix]
    - readline [unix]
    - tk [unix]
    - openssl [unix]
    - system [linux]
    - ipython 5.0.0
    - numpy 1.11.1
    - cython 0.24.1
    - scipy 0.18.0
    - pandas 0.18.1
    - patsy 0.4.1
    - statsmodels 0.6.1
    - matplotlib 1.5.2
    - ggplot 0.9.4
    - scikit-learn 0.17.1
    - distribute 0.6.45
    - backports.ssl-match-hostname 3.5.0.1
    - certifi 14.05.14
    - nose_parameterized 0.5.0
    - pyparsing 2.1.4
    - python-dateutil 2.5.3
    - pytz 2016.6.1
    - pyzmq 15.3.0
    - simplejson 3.3.3
    - six 1.10.0
    - sympy 1.0
    - tornado 4.4.1
    - virtualenv 13.0.1
    - wsgiref 0.1.2
    - python-swiftclient 2.7.0
    #- python-ceilometerclient #issue
    #- python-heatclient #issue
    #- python-keystoneclient 1.6.0
    #- python-novaclient 2.26.0
    #- python-troveclient #issue
    - python-cinderclient 1.1.2
    - python-glanceclient 0.17.2
    - python-neutronclient 2.4.0
    - networkx 1.11
    - pysal 1.11.1
    - pyyaml 3.11
    - shapely 1.5.13
    - beautifulsoup4 4.4.1
    - nltk 3.2.1
    - requests 2.10.0
    - seaborn 0.5.0
    - h5py 2.6.0
    - xlrd 1.0.0
    - markupsafe 0.23
    - crypto 1.1.0
    - jinja2 2.8
    - openpyxl 2.3.2
    - jaro_winkler 1.0.2
    - bokeh 0.12.1
    - numexpr 2.6.1
    - pytables 3.2.3.1
    - pycurl 7.43.0
    - mgrs 1.1.0
    - psutil 4.3.0
    - biopython 1.67
    - enaml 0.9.8
    - mdp 3.5
    - bitarray 0.8.1
    - clusterpy 0.9.9
    - pyside 1.2.1
    - pyqt 4.11.4
    - parsedatetime 1.4
    - pymysql 0.6.7
    - pyodbc 3.0.10
    - tabulate 0.7.2

  run:
    - zlib [unix]
    - sqlite [unix]
    - readline [unix]
    - tk [unix]
    - openssl [unix]
    - system [linux]

test:
  commands:
    - python -V [unix]
    - 2to3 -h [unix]
    - python-config --help [unix]

about:
  home: http://www.python.org/
  summary: general purpose programming language
  license: PSF


The output with the error:

$ conda build .
Removing old build environment
BUILD START: python-2.7.10-0
    (actual version deferred until further download or env creation)
Using Anaconda Cloud api site https://api.anaconda.org

The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    geos-3.5.0                 |                0        16.9 MB  defaults
    libgcc-4.8.5               |                1         922 KB  r
    pixman-0.32.6              |                0         2.4 MB  defaults
    unixodbc-2.3.4             |                0         688 KB  defaults
    yaml-0.1.6                 |                0         246 KB  defaults
    curl-7.49.0                |                1         543 KB  defaults
    glib-2.43.0                |                2         7.4 MB  r
    hdf5-1.8.17                |                1         1.9 MB  defaults
    atom-0.3.10                |           py27_0         676 KB  defaults
    backports_abc-0.4          |           py27_0           5 KB  defaults
    beautifulsoup4-4.4.1       |           py27_0         116 KB  defaults
    bitarray-0.8.1             |           py27_0          89 KB  defaults
    et_xmlfile-1.0.1           |           py27_0          15 KB  defaults
    future-0.15.2              |           py27_0         616 KB  defaults
    jaro_winkler-1.0.2         |           py27_0          24 KB  auto
    jdcal-1.2                  |           py27_1           9 KB  defaults
    kiwisolver-0.1.3           |           py27_0         571 KB  defaults
    markupsafe-0.23            |           py27_2          31 KB  defaults
    mgrs-1.1.0                 |           py27_0          48 KB  auto
    mpmath-0.19                |           py27_1         873 KB  defaults
    nltk-3.2.1                 |           py27_0         1.7 MB  defaults
    parsedatetime-1.2          |           py27_0          39 KB  auto
    ply-3.8                    |           py27_0          71 KB  defaults
    psutil-4.3.0               |           py27_0         224 KB  defaults
    pycurl-7.43.0              |           py27_0         128 KB  defaults
    pymysql-0.7.6              |           py27_0         116 KB  defaults
    pyodbc-3.0.10              |           py27_0         146 KB  defaults
    pyyaml-3.11                |           py27_4         297 KB  defaults
    pyzmq-15.4.0               |           py27_0         705 KB  defaults
    requests-2.10.0            |           py27_0         611 KB  defaults
    shapely-1.5.16             |           py27_0         494 KB  defaults
    tabulate-0.7.2             |           py27_0          18 KB  auto
    wsgiref-0.1.2              |           py27_0          943 B  auto
    xlrd-1.0.0                 |           py27_0         181 KB  defaults
    biopython-1.67             |      np111py27_0         2.2 MB  defaults
    clusterpy-0.9.9            |           py27_1         101 KB  conda-forge
    cmd2-0.6.7                 |           py27_0          33 KB  auto
    h5py-2.6.0                 |      np111py27_2         2.4 MB  defaults
    jinja2-2.8                 |           py27_1         264 KB  defaults
    jsonschema-2.5.1           |           py27_0          55 KB  defaults
    mdp-3.5                    |           py27_0         477 KB  defaults
    networkx-1.11              |           py27_0         1.1 MB  defaults
    numexpr-2.6.1              |      np111py27_0         347 KB  defaults
    openpyxl-2.3.2             |           py27_0         248 KB  defaults
    rsa-3.4.2                  |           py27_0          50 KB  conda-forge
    singledispatch-3.4.0.3     |           py27_1          17 KB  r
    ssl_match_hostname-3.4.0.2 |           py27_1           6 KB  defaults
    cliff-1.10.1               |           py27_0          36 KB  gus
    crypto-1.1.0               |           py27_0           3 KB  auto
    pysal-1.11.1               |           py27_0        11.2 MB  defaults
    pytables-3.2.3.1           |      np111py27_0         3.4 MB  defaults
    tornado-4.4.1              |           py27_0         552 KB  defaults
    bokeh-0.12.1               |           py27_0         3.2 MB  defaults
    harfbuzz-0.9.35            |                6         1.1 MB  r
    ipython-5.1.0              |           py27_0         936 KB  defaults
    pyopenssl-16.0.0           |           py27_0          66 KB  defaults
    pango-1.36.8               |                3         796 KB  r
    qt-4.8.7                   |                4        32.7 MB  defaults
    python-neutronclient-2.4.0 |           py27_0         222 KB  gus
    shiboken-1.2.1             |           py27_0         883 KB  defaults
    enaml-0.9.8                |           py27_1         944 KB  defaults
    pyside-1.2.1               |           py27_1         5.7 MB  defaults
    seaborn-0.7.1              |           py27_0         272 KB  defaults
    ------------------------------------------------------------
                                           Total:       107.8 MB

The following NEW packages will be INSTALLED:

    atom:                         0.3.10-py27_0       defaults
    babel:                        2.3.3-py27_0        defaults
    backports:                    1.0-py27_0          defaults
    backports.ssl-match-hostname: 3.5.0.1-py27_0      getpantheon
    backports_abc:                0.4-py27_0          defaults
    beautifulsoup4:               4.4.1-py27_0        defaults
    biopython:                    1.67-np111py27_0    defaults
    bitarray:                     0.8.1-py27_0        defaults
    bokeh:                        0.12.1-py27_0       defaults
    brewer2mpl:                   1.4.1-py27_1        conda-forge
    bzip2:                        1.0.6-3             defaults
    cairo:                        1.12.18-6           defaults
    certifi:                      2016.2.28-py27_0    defaults
    cffi:                         1.6.0-py27_0        defaults
    cliff:                        1.10.1-py27_0       gus
    clusterpy:                    0.9.9-py27_1        conda-forge
    cmd2:                         0.6.7-py27_0        auto
    crypto:                       1.1.0-py27_0        auto
    cryptography:                 1.4-py27_0          defaults
    curl:                         7.49.0-1            defaults
    cycler:                       0.10.0-py27_0       defaults
    cython:                       0.24.1-py27_0       defaults
    decorator:                    4.0.10-py27_0       defaults
    distribute:                   0.6.45-py27_1       defaults
    enaml:                        0.9.8-py27_1        defaults
    enum34:                       1.1.6-py27_0        defaults
    et_xmlfile:                   1.0.1-py27_0        defaults
    fontconfig:                   2.11.1-6            defaults
    freetype:                     2.5.5-1             defaults
    functools32:                  3.2.3.2-py27_0      defaults
    future:                       0.15.2-py27_0       defaults
    futures:                      3.0.5-py27_0        defaults
    geos:                         3.5.0-0             defaults
    get_terminal_size:            1.0.0-py27_0        defaults
    ggplot:                       0.11.1-py27_1       conda-forge
    glib:                         2.43.0-2            r
    h5py:                         2.6.0-np111py27_2   defaults
    harfbuzz:                     0.9.35-6            r
    hdf5:                         1.8.17-1            defaults
    idna:                         2.1-py27_0          defaults
    ipaddress:                    1.0.16-py27_0       defaults
    ipython:                      5.1.0-py27_0        defaults
    ipython_genutils:             0.1.0-py27_0        defaults
    iso8601:                      0.1.11-py27_0       defaults
    jaro_winkler:                 1.0.2-py27_0        auto
    jdcal:                        1.2-py27_1          defaults
    jinja2:                       2.8-py27_1          defaults
    jsonpatch:                    1.3-py27_0          auto
    jsonpointer:                  1.2-py27_0          auto
    jsonschema:                   2.5.1-py27_0        defaults
    kiwisolver:                   0.1.3-py27_0        defaults
    libffi:                       3.2.1-0             defaults
    libgcc:                       4.8.5-1             r
    libgfortran:                  3.0.0-1             defaults
    libpng:                       1.6.22-0            defaults
    libsodium:                    1.0.10-0            defaults
    libxml2:                      2.9.2-0             defaults
    markupsafe:                   0.23-py27_2         defaults
    matplotlib:                   1.5.1-np111py27_0   defaults
    mdp:                          3.5-py27_0          defaults
    mgrs:                         1.1.0-py27_0        auto
    mkl:                          11.3.3-0            defaults
    mpmath:                       0.19-py27_1         defaults
    msgpack-python:               0.4.7-py27_0        defaults
    netaddr:                      0.7.18-py27_0       conda-forge
    netifaces:                    0.10.4-py27_0       conda-forge
    networkx:                     1.11-py27_0         defaults
    nltk:                         3.2.1-py27_0        defaults
    nose_parameterized:           0.5.0-py27_0        conda-forge
    numexpr:                      2.6.1-np111py27_0   defaults
    numpy:                        1.11.1-py27_0       defaults
    openpyxl:                     2.3.2-py27_0        defaults
    openssl:                      1.0.2h-1            defaults
    oslo.config:                  1.9.3-py27_0        gus
    oslo.i18n:                    1.5.0-py27_0        gus
    oslo.serialization:           1.4.0-py27_0        gus
    oslo.utils:                   1.4.0-py27_0        gus
    pandas:                       0.18.1-np111py27_0  defaults
    pango:                        1.36.8-3            r
    parsedatetime:                1.2-py27_0          auto
    path.py:                      8.2.1-py27_0        defaults
    pathlib2:                     2.1.0-py27_0        defaults
    patsy:                        0.4.1-py27_0        defaults
    pbr:                          0.11.0-py27_0       defaults
    pexpect:                      4.0.1-py27_0        defaults
    pickleshare:                  0.7.3-py27_0        defaults
    pip:                          8.1.2-py27_0        defaults
    pixman:                       0.32.6-0            defaults
    ply:                          3.8-py27_0          defaults
    prettytable:                  0.7.2-py27_0        conda-forge
    prompt_toolkit:               1.0.3-py27_0        defaults
    psutil:                       4.3.0-py27_0        defaults
    ptyprocess:                   0.5.1-py27_0        defaults
    pyasn1:                       0.1.9-py27_0        defaults
    pycairo:                      1.10.0-py27_0       defaults
    pycparser:                    2.14-py27_1         defaults
    pycurl:                       7.43.0-py27_0       defaults
    pygments:                     2.1.3-py27_0        defaults
    pymysql:                      0.7.6-py27_0        defaults
    pyodbc:                       3.0.10-py27_0       defaults
    pyopenssl:                    16.0.0-py27_0       defaults
    pyparsing:                    2.1.4-py27_0        defaults
    pyqt:                         4.11.4-py27_4       defaults
    pysal:                        1.11.1-py27_0       defaults
    pyside:                       1.2.1-py27_1        defaults
    pytables:                     3.2.3.1-np111py27_0 defaults
    python:                       2.7.12-1            defaults
    python-cinderclient:          1.1.2-py27_0        gus
    python-dateutil:              2.5.3-py27_0        defaults
    python-glanceclient:          0.17.2-py27_0       gus
    python-keystoneclient:        1.3.2-py27_0        gus
    python-neutronclient:         2.4.0-py27_0        gus
    python-swiftclient:           2.7.0-py27_0        chenghlee
    pytz:                         2016.6.1-py27_0     defaults
    pyyaml:                       3.11-py27_4         defaults
    pyzmq:                        15.4.0-py27_0       defaults
    qt:                           4.8.7-4             defaults
    readline:                     6.2-2               defaults
    requests:                     2.10.0-py27_0       defaults
    rsa:                          3.4.2-py27_0        conda-forge
    scikit-learn:                 0.17.1-np111py27_2  defaults
    scipy:                        0.18.0-np111py27_0  defaults
    seaborn:                      0.7.1-py27_0        defaults
    setuptools:                   25.1.6-py27_0       defaults
    shapely:                      1.5.16-py27_0       defaults
    shiboken:                     1.2.1-py27_0        defaults
    simplegeneric:                0.8.1-py27_1        defaults
    simplejson:                   3.8.2-py27_0        defaults
    singledispatch:               3.4.0.3-py27_1      r
    sip:                          4.18-py27_0         defaults
    six:                          1.10.0-py27_0       defaults
    sqlite:                       3.13.0-0            defaults
    ssl_match_hostname:           3.4.0.2-py27_1      defaults
    statsmodels:                  0.6.1-np111py27_1   defaults
    stevedore:                    1.3.0-py27_0        gus
    sympy:                        1.0-py27_0          defaults
    system:                       5.8-2               defaults
    tabulate:                     0.7.2-py27_0        auto
    tk:                           8.5.18-0            defaults
    tornado:                      4.4.1-py27_0        defaults
    traitlets:                    4.2.2-py27_0        defaults
    unixodbc:                     2.3.4-0             defaults
    virtualenv:                   13.0.1-py27_0       defaults
    warlock:                      1.3.0-py27_0        conda-forge
    wcwidth:                      0.1.7-py27_0        defaults
    wheel:                        0.29.0-py27_0       defaults
    wsgiref:                      0.1.2-py27_0        auto
    xlrd:                         1.0.0-py27_0        defaults
    yaml:                         0.1.6-0             defaults
    zeromq:                       4.1.4-0             defaults
    zlib:                         1.2.8-3             defaults

Source cache directory is: /opt/app/anaconda2/conda-bld/src_cache
Found source in cache: Python-2.7.10.tgz
Extracting download
BUILD START: python-2.7.10-0
python is installed as a build dependency. Removing.
An unexpected error has occurred, please consider sending the
following traceback to the conda GitHub issue tracker at:

    https://github.com/conda/conda-build/issues

Include the output of the command 'conda info' in your report.


Traceback (most recent call last):
  File ""/opt/app/anaconda2/bin/conda-build"", line 5, in &lt;module&gt;
    sys.exit(main())
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 152, in main
    args_func(args, p)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 415, in args_func
    args.func(args, p)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 358, in execute
    debug=args.debug)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/build.py"", line 561, in build
    assert not plan.nothing_to_do(actions), actions
AssertionError: defaultdict(&lt;type 'list'&gt;, {'op_order': ('RM_FETCHED', 'FETCH', 'RM_EXTRACTED', 'EXTRACT', 'UNLINK', 'LINK', 'SYMLINK_CONDA'), 'PREFIX': '/opt/app/anaconda2/envs/_build_placehold_placehold_placehold_placehold_placehold'})

",3,577,"Unless there's a specific reason you need to compile python yourself, I think what you're actually going after is conda bundle (http://conda.pydata.org/docs/commands/conda-bundle.html).  Unfortunately we've removed it in conda 4.2 which will be coming out soon, intending to move it to conda-build.  Since that hasn't happened yet, and if it ends up actually being useful to people, we can add it back.



You could also try this using conda-build...

Remove the whole source block in your meta.yaml file. Also remove all of the build requirements that are also not run requirements.  Then in your build.sh file

conda install --yes --quiet \
    python=2.7.10 \
    ipython=5.0.0 \
    numpy=1.11.1 \
    cython=0.24.1 \
    scipy=0.18.0 \
    pandas=0.18.1 \
    patsy=0.4.1 \
    statsmodels=0.6.1 \
    matplotlib=1.5.2 \
    ggplot=0.9.4 \
    scikit-learn=0.17.1 \
    distribute=0.6.45 \
    backports.ssl-match-hostname=3.5.0.1 \
    certifi=14.05.14 \
    nose_parameterized=0.5.0 \
    pyparsing=2.1.4 \
    python-dateutil=2.5.3 \
    pytz=2016.6.1 \
    pyzmq=15.3.0 \
    simplejson=3.3.3 \
    six=1.10.0 \
    sympy=1.0 \
    tornado=4.4.1 \
    virtualenv=13.0.1 \
    wsgiref=0.1.2 \
    python-swiftclient=2.7.0 \
    python-cinderclient=1.1.2 \
    python-glanceclient=0.17.2 \
    python-neutronclient=2.4.0 \
    networkx=1.11 \
    pysal=1.11.1 \
    pyyaml=3.11 \
    shapely=1.5.13 \
    beautifulsoup4=4.4.1 \
    nltk=3.2.1 \
    requests=2.10.0 \
    seaborn=0.5.0 \
    h5py=2.6.0 \
    xlrd=1.0.0 \
    markupsafe=0.23 \
    crypto=1.1.0 \
    jinja2=2.8 \
    openpyxl=2.3.2 \
    jaro_winkler=1.0.2 \
    bokeh=0.12.1 \
    numexpr=2.6.1 \
    pytables=3.2.3.1 \
    pycurl=7.43.0 \
    mgrs=1.1.0 \
    psutil=4.3.0 \
    biopython=1.67 \
    enaml=0.9.8 \
    mdp=3.5 \
    bitarray=0.8.1 \
    clusterpy=0.9.9 \
    pyside=1.2.1 \
    pyqt=4.11.4 \
    parsedatetime=1.4 \
    pymysql=0.6.7 \
    pyodbc=3.0.10 \
    tabulate=0.7.2


The big difference: by listing all of those packages as build requirements, you're actually ensuring that they won't be in your final conda package.  Think of build requirements more like a compiler, or something that's necessary when you're building the package, but not when you're actually running it.
",,
scikit-learn unexpected issue,https://stackoverflow.com/questions/30066268,Provide Starting Positions to t-distributed Stochastic Neighbor Embedding (TSNE) in scikit-learn,,2,906,"It is currently not possible, but it would be a two-line change.
I think it would be a good addition, and we do support init=array for things like k-means. So PR welcome.
",,
scikit-learn unexpected issue,https://stackoverflow.com/questions/76309535,Quantile Forest error &quot;predict() got an unexpected keyword argument &#39;quantiles&#39;&quot;,"I continue to run into errors when run any form of quantile forest models with the prediction and quantile phases. I am following this example but with my own X and y. I have trained many a random forest and other derivations of tree models with this dataset, so I'm fairly certain it's not the input data issue.
https://github.com/zillow/quantile-forest
And have created an environment that supposedly follows all the installation requirements. I can provide my list of versions upon request. Time and time again, the RandomForestQuantileRegressor will work, but when I want to plot and see the quantiles, I get the error

**""TypeError: predict() got an unexpected keyword argument 'quantiles'""
**

Here is an example when I set up the environment:

Collecting quantile-forest
Downloading quantile_forest-1.1.2-cp310-cp310-macosx_10_9_x86_64.whl (188 kB)
 188.6/188.6 kB 2.7 MB/s eta 0:00:00
Requirement already satisfied: numpy&gt;=1.23 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from quantile-forest) (1.24.3)
Requirement already satisfied: scipy&gt;=1.4 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from quantile-forest) (1.10.1)
Requirement already satisfied: scikit-learn&gt;=1.0 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from quantile-forest) (1.2.2)
Requirement already satisfied: joblib&gt;=1.1.1 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from scikit-learn&gt;=1.0-&gt;quantile-forest) (1.2.0)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from scikit-learn&gt;=1.0-&gt;quantile-forest) (3.1.0)
Installing collected packages: quantile-forest
Successfully installed quantile-forest-1.1.2

This code works:
X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=0.5, random_state=0)

qrf = RandomForestQuantileRegressor(q=[0.05, 0.50, 0.95])
qrf.fit(X_train, y_train)

y_pred_5, y_pred_median, y_pred_95 = qrf.predict(X_test)
qrf.score(X_test, y_test)

But I cannot get any code that calls ""predict"" where I specify the quantiles to work without that type error.
",0,180,"I notice that the code you've provided is an example from the sklearn-quantile package. Perhaps confusingly, both packages -- sklearn-quantile and quantile-forest -- provide a RandomForestQuantileRegressor class, but the packages have different ways of passing the quantiles to the class methods. As a result, the RandomForestQuantileRegressor classes from the two packages are not currently interchangeable.
In the code snippet you've provided, it's not clear what imports are being used, but it appears that you may be using the RandomForestQuantileRegressor class from the sklearn-quantile package. This class expects the quantiles to be passed to the initialization function instead of the predict function and would lead to the error you've presented. If this is correct, then you can fix this by importing the RandomForestQuantileRegressor class from the quantile-forest package and passing the quantiles to the predict function instead of the initialization function.
If the above is not helpful or you still are running into errors, you're welcome to create an issue in the quantile-forest repository here for additional troubleshooting.
",,
scikit-learn unexpected issue,https://stackoverflow.com/questions/40877501,Error using conda to update a package,"I launched on AWS EC2 an instance using ubuntu as AMI.

Then I ran a script to install the jupyter notebook and be able to access to it with the IP address and the specific port.
The code I wrote was this one :

sudo apt-get install git

git clone https://gist.github.com/rashmibanthia/5a1e4d7e313d6832f2ff nb

. nb/jupyter_notebook_ec2.sh 

cd;mkdir notebook;cd notebook

tmux new -s nb

jupyter notebook --certfile=~/certs/mycert.pem --keyfile ~/certs/mycert.key


Now I am in the notebook. Here comes my problem. When I try to import some packages and class, I get an error. For example with this line:

from sklearn.gaussian_process import GaussianProcessRegressor


I get this:


  ImportError                               Traceback (most recent call
  last)  in ()
  ----&gt; 1 from sklearn.gaussian_process import GaussianProcessRegressor
  
  ImportError: cannot import name 'GaussianProcessRegressor'


So I tried to update the scikit learn package with conda with conda update scikit-learn but I have another error:


  Fetching package metadata ...An unexpected error has occurred. Please
  consider posting the following information to the conda GitHub issue
  tracker at:

https://github.com/conda/conda/issues



Current conda install:

           platform : linux-64
      conda version : 4.2.13
   conda is private : False
  conda-env version : 4.2.13
conda-build version : 1.18.2
     python version : 3.5.2.final.0
   requests version : 2.12.1
   root environment : /home/ubuntu/anaconda3  (writable)
default environment : /home/ubuntu/anaconda3
   envs directories : /home/ubuntu/anaconda3/envs
      package cache : /home/ubuntu/anaconda3/pkgs
       channel URLs : https://repo.continuum.io/pkgs/free/linux-64
                      https://repo.continuum.io/pkgs/free/noarch
                      https://repo.continuum.io/pkgs/pro/linux-64
                      https://repo.continuum.io/pkgs/pro/noarch
        config file : None
       offline mode : False




$ /home/ubuntu/anaconda3/bin/conda update scikit-learn

Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/exceptions.py"", line 479, in conda_exception_handler
    return_value = func(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/cli/main.py"", line 145, in _main
    exit_code = args.func(args, p)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/cli/main_update.py"", line 65, in execute
    install(args, parser, 'update')
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/cli/install.py"", line 238, in install
    prefix=prefix)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/api.py"", line 24, in get_index
    index = fetch_index(channel_urls, use_cache=use_cache, unknown=unknown)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 300, in fetch_index
    repodatas = [(u, f.result()) for u, f in zip(urls, futures)]
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 300, in &lt;listcomp&gt;
    repodatas = [(u, f.result()) for u, f in zip(urls, futures)]
  File ""/home/ubuntu/anaconda3/lib/python3.5/concurrent/futures/_base.py"", line 405, in result
    return self.__get_result()
  File ""/home/ubuntu/anaconda3/lib/python3.5/concurrent/futures/_base.py"", line 357, in __get_result
    raise self._exception
  File ""/home/ubuntu/anaconda3/lib/python3.5/concurrent/futures/thread.py"", line 55, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 75, in func
    res = f(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 117, in fetch_repodata
    timeout=(6.1, 60))
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 501, in get
    return self.request('GET', url, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 488, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 609, in send
    r = adapter.send(request, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/adapters.py"", line 423, in send
    timeout=timeout
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 594, in urlopen
    chunked=chunked)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 350, in _make_request
    self._validate_conn(conn)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 835, in _validate_conn
    conn.connect()
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connection.py"", line 330, in connect
    cert = self.sock.getpeercert()
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 324, in getpeercert
    'subjectAltName': get_subj_alt_name(x509)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 171, in get_subj_alt_name
    ext = cert.extensions.get_extension_for_class(
AttributeError: 'Extensions' object has no attribute 'get_extension_for_class'

",0,1164,"As kalefranz comments here 'https://github.com/conda/conda/issues/3898' it might be because pyopenssl and cryptography are out of sync and this should fix it:

CONDA_SSL_VERIFY=false conda update pyopenssl

In windows to set ssl_verify the following command can be used:

conda config --set ssl_verify False

As it is said in Problems with updating anaconda and installing new packages
",,
scikit-learn strange behavior,https://stackoverflow.com/questions/44729373,GaussianProcess regression results correct...up to a scale factor?,"I am running GaussianProcess regressions over some very noisy data. When I scatter plot predictions (which are, I know, predictions of means) vs actuals, I get a beautiful only slightly noisy y=x line.

Only one problem: the slope is completely wrong. Is there any way I can address this without building a second-stage linear regressor?

I regret I cannot share my data, but my model is fairly basic. X is a matrix with 10 columns, y a matrix with 1 column. I am using 1,000 examples to train and plot.

added: The below plot is plotting predicted versus actual. Given that I am using a nonlinear kernel, I find it strange that the GP regressor can find a relationship which is accurate up to a multiplier (slope).



kernel = (
    GP.kernels.RationalQuadratic(
        length_scale=.8,
        length_scale_bounds=(1e-3,1e3),
        alpha=.8,
        alpha_bounds=(1e-3,1e3),
        )
    + GP.kernels.WhiteKernel()
    )

gp = Pipeline( [
    ('scale',preproc.StandardScaler()),
    ('gp',GP.GaussianProcessRegressor(kernel=kernel)),
    ] )
gp.fit( X, y )


added: I'm a bit embarrassed, but I'm new to the GP world in particular and, really, regression as a ML problem in general. I had not plotted the model's performance over a test set, which revealed a strong overfit. Additionally, I've added an idiom to my code to deal with scikit-learn's default GP behavior, i.e., optimization makes me sad when I give it significant quantities of data, by ""pretraining"" on a small quantity of data, using the optimizer to find reasonable values for the kernel parameters, then ""training"" a much larger quantity of data. This allowed me to widen the parameter search and use multiple restarts on the optimizer, finding a much more generalizable model...which was almost all noise. Which was what I was expecting, really.

kernel = (
    GP.kernels.RationalQuadratic(
        length_scale=1,
        alpha=.5,
        )
    + GP.kernels.WhiteKernel(
        noise_level=1,
        )
    )*GP.kernels.ConstantKernel()

gp = Pipeline( [
    ('scale',preproc.StandardScaler()),
    ('gp',GP.GaussianProcessRegressor(
        kernel=kernel,
        n_restarts_optimizer=3,
        alpha=0,
        )),
    ] )
print(""pretraining model for target %s..."" % c)
x_pre = X_s.values[:500,:]
y_pre = y_s_scl[:500,:]
gp.fit( x_pre, y_pre )

gp = Pipeline( [
    ('scale',preproc.StandardScaler()),
    ('gp',GP.GaussianProcessRegressor(
        kernel=kernel,
        optimizer=None,
        alpha=0,
        )),
    ] )
print(""training model for target %s..."" % c)

",2,404,"EDIT: Have you tried centering your data before doing the regression? (subtracting the mean of all the output values from each output). I know the Gp Toolbox in Matlab doesn't need the data to be centered, but I am not sure about the GP in sklearn. See:
https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia

OLD COMMENT:
Your initial values for the hyperparameters in the kernel function (i.e. length-scale and alpha) are very important. During the fit(), the hyperparameters are optimized and local maximum of hyperpareters can be found, which could in turn affect your result. Depending on the bounds you set for these hyperparameters, many local maximum can be found depending on the initial conditions. 
On the sklearn site it says:
""As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying n_restarts_optimizer.""
You may try using the RBF function as it is a very traditional kernel function for the GP. 
",,
scikit-learn strange behavior,https://stackoverflow.com/questions/44527713,Python script works from one folder but not on subfolders.,"Good evening everyone, 

I am trying to do some machine learning with python so I imported the module scikit-learn:

from sklearn.preprocessing import MinMaxScaler


And it gives me this strange error: 

Traceback (most recent call last):

  File ""&lt;ipython-input-22-b55a4eaccb0b&gt;"", line 1, in &lt;module&gt;
    from sklearn.preprocessing import MinMaxScaler

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\preprocessing\__init__.py"", line 6, in &lt;module&gt;
    from ._function_transformer import FunctionTransformer

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\preprocessing\_function_transformer.py"", line 2, in &lt;module&gt;
    from ..utils import check_array

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\utils\__init__.py"", line 18, in &lt;module&gt;
    from ..externals.joblib import cpu_count

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\externals\joblib\__init__.py"", line 128, in &lt;module&gt;
    from .parallel import Parallel

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 24, in &lt;module&gt;
    from ._multiprocessing_helpers import mp

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\externals\joblib\_multiprocessing_helpers.py"", line 24, in &lt;module&gt;
    _sem = mp.Semaphore()

AttributeError: module 'multiprocessing' has no attribute 'Semaphore'


The errors are copied from Anaconda (Spider) but they are present even by starting a python session from the command line. The folder where I run the script is: 

C:\Users\Francesco\Desktop\script_python


The very strange thing is that if I run the same script from the Desktop, just a folder up, it works!

C:\Users\Francesco\Desktop\ &lt;-- Here it works!!


I found out that the problem is relative to the multiprocessing module, here a snapshot of this strange behavior: 


",1,403,"You probably have a module multiprocessing in the directory where you are running the script from.

Generally, in Python, the path where you run your script has precedence over the Python env, so it will try to pick that one first. This means that when sklearn imports multiprocessing, it uses your module, and not the built-in multiprocessing.

When this type of error appears, one way to identify the error is to run

python -c ""import multiprocessing; print(multiprocessing.__dir__)""


to double check which module is being used.
",,
scikit-learn strange behavior,https://stackoverflow.com/questions/48263740,strange memory consumption behaviour when running PCA in python,"At the moment I am trying to find the reason for a specific behavior in Python. 

First I want to describe my use case. The idea is a performance analysis on Python. Therefore I want to analyze the memory usage and runtime regarding the ""Principle Component Analysis""-algorithm. For that, I use scikit-learn (http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). 

To see, how the algorithm behaves for different datasets, I generated several randomly uniform distributed datasets. The strange thing is, there is an increase in memory usage and a faster runtime regarding two datasets.

Now I want to explain, how I measure the memory-usage and runtime shortly. 
Memory measurement (package: https://pypi.python.org/pypi/memory_profiler)


disable the garbage collection
Use the @profile decoration to find the increments of the function
Write the output into a log file
Use another python program to find the increments of the log files which belong to a specific dataset
sum up the increments - the result is the total memory allocation for the complete function
(I tested it with enabled garbage collection, too)


Time measurement (function: https://docs.python.org/2/library/timeit.html)


Use ""timeit.repeat()"" to get a list of many runtimes of the function
Write the list of those measurements to a CSV


After I had performed about 100 memory measurements and even more time measurements, I recognized the aforementioned behavior.
Hopefully, you can help me, find a reason for this behavior.

Here is the function, I measured:

@profile 
def pcaTrain(dataset):
 model = sklearn_pca.fit(dataset)
 model.variance = np.var(model.transform(dataset), axis=0, ddof=1)
 return model 


And here are some diagrams of the memory usage and the time measurements:

Memory measurements for dataset 4-17

Runtime measurements for dataset 4-17

Set up:


Python-version: 3.5.2
Memory: &gt; 100 GB


List of datasets:


dset04 -&gt; 1.000.000 rows per column (10 features x 1.000.000)
dset05 -&gt; 1.000.000 rows per column (12 features x 1.000.000)
dset06 -&gt; 1.000.000 rows per column (14 features x 1.000.000)
dset07 -&gt; 1.000.000 rows per column (16 features x 1.000.000)
dset08 -&gt; 1.000.000 rows per column (18 features x 1.000.000)
dset09 -&gt; 1.000.000 rows per column (20 features x 1.000.000)
dset10 -&gt; 1.000.000 rows per column (22 features x 1.000.000)
dset11 -&gt; 1.000.000 rows per column (24 features x 1.000.000)
dset12 -&gt; 1.000.000 rows per column (26 features x 1.000.000)
dset13 -&gt; 1.000.000 rows per column (28 features x 1.000.000)
dset14 -&gt; 1.000.000 rows per column (30 features x 1.000.000)
dset15 -&gt; 1.000.000 rows per column (35 features x 1.000.000)
dset16 -&gt; 1.000.000 rows per column (40 features x 1.000.000)
dset17 -&gt; 1.000.000 rows per column (45 features x 1.000.000)

",1,300,"The important question is, how many of the features you are extractiong.
Since version 0.18 of Scikit-learn, the svd_solver flag of the PCA algorithm determines which algorithm to use.  The default behavior is to select the ""best"" choice, which is described in detail in the official documentation which you mentioned. 
Possibly, one of those selections hits your performance, depending on the number of components. Otherwise, I would suggest you mention this behavior in the official GitHub of scikit-learn, since this could be interesting to them as well.
",,
scikit-learn strange behavior,https://stackoverflow.com/questions/53630329,Watershed analysis of coins - wrong output,"For the counting of round objects in an image I want to use the watershed algorithm.
In order to learn how it works and how I can use it for my needs, I have searched some working examples in python (https://docs.opencv.org/3.1.0/d3/db4/tutorial_py_watershed.html ; http://scikit-image.org/docs/dev/auto_examples/segmentation/plot_label.html)

I finaly found a working solution, which works more or less out-of-the-box for my own purposes (How to define the markers for Watershed in OpenCV?)

With this code I get nice results, both with the example file as with my own images.
I do get a strange behavior though after the watershed analysis. For some reason, the watershed step also adds a border around the image. So next to the objects that are detected, also the whole edge of the image gets detected and colored.

My guess is that I should change a parameters in the code to stop this from happening, but so far I'm unable to find what I should do.

this is the code:

import cv2
import numpy as np
from scipy.ndimage import label
def segment_on_dt(a, img):
    border = cv2.dilate(img, None, iterations=3)
    border = border - cv2.erode(border, None)
    dt = cv2.distanceTransform(img, cv2.DIST_L2, 3)
    dt = ((dt - dt.min()) / (dt.max() - dt.min()) * 255).astype(np.uint8)
    _, dt = cv2.threshold(dt, 200, 255, cv2.THRESH_BINARY)
    lbl, ncc = label(dt)
    # Completing the markers now. 
    lbl[border == 255] = 255 
    lbl = lbl.astype(np.int32)
    cv2.watershed(a, lbl)
    lbl[lbl == -1] = 0
    lbl = lbl.astype(np.uint8)
    return 255 - lbl

# Load image file
img = cv2.imread('coins.jpg')
# Pre-processing.
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img_gray = cv2.GaussianBlur(img_gray,(5,5),0)  
width, height = img_gray.shape
_, img_bin = cv2.threshold(img_gray, 0,  255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
img_bin = cv2.morphologyEx(img_bin, cv2.MORPH_OPEN,np.ones((5, 5),     dtype=int))
result = segment_on_dt(img, img_bin)
result[result != 255] = 0
result = cv2.dilate(result, None)
img[result == 255] = (0, 0, 255)
cv2.imwrite('Img_output.png',img)


Running this code will give this result (at least on my pc)



The result for detecting the coins is good enough for my purposes, but I'm a bit puzzled about the image edge that is also detected. From what I see during debugging, the watershed adds this edge, but it is unclear to me why this happens.
",1,514,"You can fix this by adding a background label using the tutorial provided by openCV. 
https://docs.opencv.org/3.1.0/d3/db4/tutorial_py_watershed.html

They added an extra step to insert sure background and sure foreground region to help the watershed algorithm to properly segment the coin regions.

*********** edit**************

After reading your code again. I found that your original code has no problem. 
Background label was set using the variable border.

You will probably get the same result by executing the code found in OpenCV tutorial. The problem is in the way your draw the results. Since this is a display problem, there are many ways we can tackle the problem. One of the many is to use the information of the sure-background

Here are the modification to the function segment_on_dt

def segment_on_dt(a, img):
    sure_background = cv2.dilate(img, None, iterations=3)
    border = sure_background - cv2.erode(sure_background, None)


    dt = cv2.distanceTransform(img, cv2.DIST_L2, 3)
    dt = ((dt - dt.min()) / (dt.max() - dt.min()) * 255).astype(np.uint8)
    _, dt = cv2.threshold(dt, 200, 255, cv2.THRESH_BINARY)
    lbl, ncc = label(dt)


    # Completing the markers now. 
    lbl[border == 255] = 255 


    lbl = lbl.astype(np.int32)
    cv2.watershed(a, lbl)
    lbl[lbl == -1] = 0
    # Only draw red line if its not in sure background
    lbl[sure_background == 0] = 255

    lbl = lbl.astype(np.uint8)
    cv2.imshow('lbl_2',lbl)

    return 255 - lbl


I have added a new condition for the red lines to be drawn. The line are only drawn if its not in sure background region.

Your final result should look like this.


",,
scikit-learn strange behavior,https://stackoverflow.com/questions/28726548,Text Documents Clustering - Non Uniform Clusters,"I have been trying to cluster a set of text documents. I have a sparse TFIDF matrix with around 10k documents (subset of a large dataset), and I try to run the scikit-learn k-means algorithm with different sizes of clusters (10,50,100). Rest all the parameters are default values.

I get a very strange behavior that no matter how many clusters I specify or even if I change the number of iterations, there would be 1 cluster in the lot which would contain most of the documents in itself and there will be many clusters which would have just 1 document in them. This is highly non-uniform behavior 

Does anyone know what kind of problem am I running into? 
",0,215,"Here are the possible things that might be going ""wrong"":


Your k-means cluster initialization points are chosen as the same set of points in each run. I recommend using the 'random' for the init parameter of k-means http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. If that doesn't work then supply to k-means your own set of random initial cluster centers. Remember to initialize your random generator using its seed() method as the current date and time. https://docs.python.org/2/library/random.html uses current date-time as the default value. 
Your distance function, i.e. euclidean distance might be the culprit. This is less likely but it is always good to run k-means using cosine similarity especially when you are using it for document similarity. scikits doesn't have this functionality at present but you should look here: Is it possible to specify your own distance function using scikit-learn K-Means Clustering?


These two combined should give you good clusters.
","I noticed with the help of above answers and comments that there was a problem with outliers and noise in original space. For this, we should use a dimensionality reduction method which eliminates the unwanted noise in the data. I tried random projections first but it failed to work with text data, simply because the problem was still not solved.
Then using Truncated Singular Value Decomposition, I was able to get perfect uniform clusters. Hence, the Truncated SVD is the way to go with textual data in my opinion.
",
scikit-learn strange behavior,https://stackoverflow.com/questions/62777025,GridSearchCV fails for own model class,"I'm trying to use a regression model I have implemented in combination with the GridSearchCV class of scikit-learn to optimize the hyper-parameters of my model. My modelclass is nicely build following the suggestions of the scikit-api:
class FOO(BaseEstimator, RegressorMixin):
def __init__(self,...)
    *** initialisation of all the parameters and hyperparameters (including the kernelfunction)***

def fit(self,X,y)
    *** implementation of fit: just takes input and performs fit of parameters.

def predict(self,X)
    *** implementation of predict: just takes input and calculates the result

The regression-class works as it should, but strangely enough, when I study the behavior of the hyperparameters, I tend to get inconsistencies. It appears one hyper-parameter is correctly applied by GridSearchCV, but the other one is clearly not.
So I am wondering, can someone explain to me how gridsearchCV is working (from the technical perspective)? How does it initialise the estimator, how does it run it over the grid?
My current assumption of the workings and required use of GridsearchCV is this:

Create a GridSearchCV instance  (CVmodel=GridSearchCV(MyRegressor,param_grid=Myparamgrid,...)
Fit the hyperparameter(s) via: CVmodel.fit(X,y). Which naively would work like this:

&gt; Loop over Parameter-values
&gt;          - create esimator instance with parameter value(and defaults for the other params)
&gt;          - estimator.fit
&gt;          - result[parameter-value]=estimator.predict

However, experience shows me this naive idea is quite wrong, as the hyper-parameter associated with the kernel-function of my regressor is not correctly initialized.
Can anyone provide some insight into what GridSearchCV is truly doing?
",0,184,"After quite some digging I discovered, scikit-learn does not create new instances (as would be expected in OOP) but rather updates the properties of the object via the set_params method. In my case, this worked fine for the hyperparameter which is directly defined by the same keyword in the __ init __ method, however, it breaks down when the hyperparameter is a property of the static method set during the __ init __ method. Overriding the set_params method (which many tutorials advise against) to deal with this fixes the problem.
For those interested in more details, I wrote this all up in a tutorial myself.
",,
scikit-learn strange behavior,https://stackoverflow.com/questions/63692895,Is there a way to build a logistic regression model even if there is only one class?,"Is there a way to build a scikit-learn logistic regression model for only 1 class? Obviously this model would predict the same class every time, regardless of the input data. My models are currently using liblinear as the solver, I'm not sure if there's another solver that would allow for this?
I realize this is a very strange question for ML but I am building many hierarchical models and in my situation it is easier to have a model for every case even if it predicts the same class every time.
Background: I have a hierarchical prediction task where I'm trying to predict three parts of a 9 digit code (e.g. for a code = 001010424, part 1 = 001, part 2 = 01, part 3= 0424). To do this I'm building hierarchical models. Using the input data we first predict part 1, then using the highest confidence decision for part 1 we use the input data again in a model for part 2 that is specific to the part 1 code. So for example, if I run the part 1 model and get a prediction that part 1 = 001 I then go to the part 2 model for 001 which then (is trained on and) predicts part 2 given part 1 = 001. This hierarchical behavior is repeated for part 3.
",0,1123,"Scitkit learn needs samples of at least two classes.
  import numpy as np
  from sklearn.linear_model import LogisticRegression

  x = np.random.rand(5,2)
  y = np.ones(5).astype(int)
  model = LogisticRegression().fit(x, y)

This yields the error:
  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1

You are probably better off having your training algorithm check if there is only one y label, and if there is, just have your code memorize that.  It seems that such an implementation would be straightforward to implement and much easier to understand for anyone looking at the code later.
",,
scikit-learn strange behavior,https://stackoverflow.com/questions/68155575,RandomForestClassifier takes less time to train when in Pipeline?,"I am experiencing strange behavior in Scikit-learn.
When I train a RandomForestClassifier standalone (not in a pipeline), the model takes 280 seconds to train, while the training process only takes 18 seconds in a pipeline.
Am I doing something wrong ?

",-1,223,"In 1st block, you have 6 lines of code. You are importing libraries, fitting your model, getting scores, and doing everything for the first time. Whereas in 2nd block you are importing the libraries like Scikit-learn which is already imported, so it won't take time. And, in 3rd block there is just one line of code. Another reason is that, In 1st block, you are getting some kind of warning which might be affecting the speed of running the code.
",,
scikit-learn strange result,https://stackoverflow.com/questions/22054964,OLS Regression: Scikit vs. Statsmodels?,"Short version: I was using the scikit LinearRegression on some data, but I'm used to p-values so put the data into the statsmodels OLS, and although the R^2 is about the same the variable coefficients are all different by large amounts. This concerns me since the most likely problem is that I've made an error somewhere and now I don't feel confident in either output (since likely I have made one model incorrectly but don't know which one).

Longer version: Because I don't know where the issue is, I don't know exactly which details to include, and including everything is probably too much. I am also not sure about including code or data. 

I am under the impression that scikit's LR and statsmodels OLS should both be doing OLS, and as far as I know OLS is OLS so the results should be the same.

For scikit's LR, the results are (statistically) the same whether or not I set normalize=True or =False, which I find somewhat strange.

For statsmodels OLS, I normalize the data using StandardScaler from sklearn. I add a column of ones so it includes an intercept (since scikit's output includes an intercept). More on that here: http://statsmodels.sourceforge.net/devel/examples/generated/example_ols.html (Adding this column did not change the variable coefficients to any notable degree and the intercept was very close to zero.) StandardScaler didn't like that my ints weren't floats, so I tried this: https://github.com/scikit-learn/scikit-learn/issues/1709
That makes the warning go away but the results are exactly the same.

Granted I'm using 5-folds cv for the sklearn approach (R^2 are consistent for both test and training data each time), and for statsmodels I just throw it all the data.

R^2 is about 0.41 for both sklearn and statsmodels (this is good for social science). This could be a good sign or just a coincidence.

The data is observations of avatars in WoW (from http://mmnet.iis.sinica.edu.tw/dl/wowah/) which I munged about to make it weekly with some different features. Originally this was a class project for a data science class.

Independent variables include number of observations in a week (int), character level (int), if in a guild (Boolean), when seen (Booleans on weekday day, weekday eve, weekday late, and the same three for weekend), a dummy for character class (at the time for the data collection, there were only 8 classes in WoW, so there are 7 dummy vars and the original string categorical variable is dropped), and others.

The dependent variable is how many levels each character gained during that week (int).

Interestingly, some of the relative order within like variables is maintained across statsmodels and sklearn. So, rank order of ""when seen"" is the same although the loadings are very different, and rank order for the character class dummies is the same although again the loadings are very different.

I think this question is similar to this one: Difference in Python statsmodels OLS and R's lm

I am good enough at Python and stats to make a go of it, but then not good enough to figure something like this out. I tried reading the sklearn docs and the statsmodels docs, but if the answer was there staring me in the face I did not understand it. 

I would love to know:


Which output might be accurate? (Granted they might both be if I missed a kwarg.)
If I made a mistake, what is it and how to fix it?
Could I have figured this out without asking here, and if so how?


I know this question has some rather vague bits (no code, no data, no output), but I am thinking it is more about the general processes of the two packages. Sure, one seems to be more stats and one seems to be more machine learning, but they're both OLS so I don't understand why the outputs aren't the same.

(I even tried some other OLS calls to triangulate, one gave a much lower R^2, one looped for five minutes and I killed it, and one crashed.)

Thanks!
",31,34799,"It sounds like you are not feeding the same matrix of regressors X to both procedures (but see below). Here's an example to show you which options you need to use for sklearn and statsmodels to produce identical results.

import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression

# Generate artificial data (2 regressors + constant)
nobs = 100 
X = np.random.random((nobs, 2)) 
X = sm.add_constant(X)
beta = [1, .1, .5] 
e = np.random.random(nobs)
y = np.dot(X, beta) + e 

# Fit regression model
sm.OLS(y, X).fit().params
&gt;&gt; array([ 1.4507724 ,  0.08612654,  0.60129898])

LinearRegression(fit_intercept=False).fit(X, y).coef_
&gt;&gt; array([ 1.4507724 ,  0.08612654,  0.60129898])


As a commenter suggested, even if you are giving both programs the same X, X may not have full column rank, and they sm/sk could be taking (different) actions under-the-hood to make the OLS computation go through (i.e. dropping different columns).

I recommend you use pandas and patsy to take care of this:

import pandas as pd
from patsy import dmatrices

dat = pd.read_csv('wow.csv')
y, X = dmatrices('levels ~ week + character + guild', data=dat)


Or, alternatively, the statsmodels formula interface:

import statsmodels.formula.api as smf
dat = pd.read_csv('wow.csv')
mod = smf.ols('levels ~ week + character + guild', data=dat).fit()


Edit: This example might be useful: http://statsmodels.sourceforge.net/devel/example_formulas.html
","If you use statsmodels, I would highly recommend using the statsmodels formula interface instead.  You will get the same old result from OLS using the statsmodels formula interface as you would from sklearn.linear_model.LinearRegression, or R, or SAS, or Excel.  

smod = smf.ols(formula ='y~ x', data=df)
result = smod.fit()
print(result.summary())


When in doubt, please 


try reading the source code
try a different language for benchmark, or 
try OLS from scratch, which is basic linear algebra. 

","i just wanted to add here, that in terms of sklearn, it does not use OLS method for linear regression under the hood. Since sklearn comes from the data-mining/machine-learning realm, they like to use Steepest Descent Gradient algorithm. This is a numerical method that is sensitive to initial conditions etc, while the OLS is an analytical closed form approach, so one should expect differences. So statsmodels comes from classical statistics field hence they would use OLS technique. So there are differences between the two linear regressions from the 2 different libraries
"
scikit-learn strange result,https://stackoverflow.com/questions/10407266,scikits learn and nltk: Naive Bayes classifier performance highly different,"I am comparing two Naive Bayes classifiers: one from NLTK and and one from scikit-learn. I'm dealing with a multi-class classification problem (3 classes: positive (1), negative (-1), and neutral (0)). 

Without performing any feature selection (that is, using all features available), and using a training dataset of 70,000 instances (noisy-labeled, with an instance distribution of  17% positive, 4%  negative and 78% neutral), I train two classifiers, the first one is a nltk.NaiveBayesClassifier, and the second one is a sklearn.naive_bayes.MultinomialNB (with fit_prior=True).

After training, I evaluated the classifiers on my test set of 30,000 instances and I get the following results:

**NLTK's NaiveBayes**
accuracy: 0.568740
class: 1
     precision: 0.331229
     recall: 0.331565
     F-Measure: 0.331355
class: -1
     precision: 0.079253 
     recall: 0.446331 
     F-Measure: 0.134596 
class: 0
     precision: 0.849842 
     recall: 0.628126 
     F-Measure: 0.722347 


**Scikit's MultinomialNB (with fit_prior=True)**
accuracy: 0.834670
class: 1
     precision: 0.400247
     recall: 0.125359
     F-Measure: 0.190917
class: -1
     precision: 0.330836
     recall: 0.012441
     F-Measure: 0.023939
class: 0
     precision: 0.852997
     recall: 0.973406
     F-Measure: 0.909191

**Scikit's MultinomialNB (with fit_prior=False)**
accuracy: 0.834680
class: 1
     precision: 0.400380
     recall: 0.125361
     F-Measure: 0.190934
class: -1
     precision: 0.330836
     recall: 0.012441
     F-Measure: 0.023939
class: 0
     precision: 0.852998
     recall: 0.973418
     F-Measure: 0.909197


I have noticed that while Scikit's classifier has better overall accuracy and precision, its recall is very low compared to the NLTK one, at least with my data. Taking into account that they might be (almost) the same classifiers, isn't this strange?
",14,5547,"Naive Bayes classifier usually means a Bayesian classfier over binary features that are assumed to be independent. This is what NLTK's Naive Bayes classifier implements. The corresponding scikit classifier is BernoulliNB classifier.

The restriction to boolean valued features is not actually necessary,
it is just the simplest to implement.
A naive Bayes classifier can be defined for (assumed) independent features from any parametric distribution.

MultinomialNB is for data with integer valued input features that are assumed to be multinomially distributed.

Sckit also has GaussianNB that for continuous valued features that are assumed to idependently Gaussian distributed.
",,
scikit-learn strange result,https://stackoverflow.com/questions/13301986,predict_proba or decision_function as estimator &quot;confidence&quot;,"I'm using LogisticRegression as a model to train an estimator in scikit-learn. The features I use are (mostly) categorical; and so are the labels. Therefore, I use a DictVectorizer and a LabelEncoder, respectively, to encode the values properly. 

The training part is fairly straightforward, but I'm having problems with the test part. The simple thing to do is to use the ""predict"" method of the trained model and get the predicted label. However, for the processing I need to do afterwards, I need the probability for each possible label (class) for each particular instance. I decided to use the ""predict_proba"" method. However, I get different results for the same test instance, whether I use this method when the instance is by itself or accompanied by others. 

Next, is a code that reproduces the problem.

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import LabelEncoder


X_real = [{'head': u'n\xe3o', 'dep_rel': u'ADVL'}, 
          {'head': u'v\xe3o', 'dep_rel': u'ACC'}, 
          {'head': u'empresa', 'dep_rel': u'SUBJ'}, 
          {'head': u'era', 'dep_rel': u'ACC'}, 
          {'head': u't\xeam', 'dep_rel': u'ACC'}, 
          {'head': u'import\xe2ncia', 'dep_rel': u'PIV'}, 
          {'head': u'balan\xe7o', 'dep_rel': u'SUBJ'}, 
          {'head': u'ocupam', 'dep_rel': u'ACC'}, 
          {'head': u'acesso', 'dep_rel': u'PRED'}, 
          {'head': u'elas', 'dep_rel': u'SUBJ'}, 
          {'head': u'assinaram', 'dep_rel': u'ACC'}, 
          {'head': u'agredido', 'dep_rel': u'SUBJ'}, 
          {'head': u'pol\xedcia', 'dep_rel': u'ADVL'}, 
          {'head': u'se', 'dep_rel': u'ACC'}] 
y_real = [u'AM-NEG', u'A1', u'A0', u'A1', u'A1', u'A1', u'A0', u'A1', u'AM-ADV', u'A0', u'A1', u'A0', u'A2', u'A1']

feat_encoder =  DictVectorizer()
feat_encoder.fit(X_real)

label_encoder = LabelEncoder()
label_encoder.fit(y_real)

model = LogisticRegression()
model.fit(feat_encoder.transform(X_real), label_encoder.transform(y_real))

print ""Test 1...""
X_test1 = [{'head': u'governo', 'dep_rel': u'SUBJ'}]
X_test1_encoded = feat_encoder.transform(X_test1)
print ""Features Encoded""
print X_test1_encoded
print ""Shape""
print X_test1_encoded.shape
print ""decision_function:""
print model.decision_function(X_test1_encoded)
print ""predict_proba:""
print model.predict_proba(X_test1_encoded)

print ""Test 2...""
X_test2 = [{'head': u'governo', 'dep_rel': u'SUBJ'}, 
           {'head': u'atrav\xe9s', 'dep_rel': u'ADVL'}, 
           {'head': u'configuram', 'dep_rel': u'ACC'}]

X_test2_encoded = feat_encoder.transform(X_test2)
print ""Features Encoded""
print X_test2_encoded
print ""Shape""
print X_test2_encoded.shape
print ""decision_function:""
print model.decision_function(X_test2_encoded)
print ""predict_proba:""
print model.predict_proba(X_test2_encoded)


print ""Test 3...""
X_test3 = [{'head': u'governo', 'dep_rel': u'SUBJ'}, 
           {'head': u'atrav\xe9s', 'dep_rel': u'ADVL'}, 
           {'head': u'configuram', 'dep_rel': u'ACC'},
           {'head': u'configuram', 'dep_rel': u'ACC'},]

X_test3_encoded = feat_encoder.transform(X_test3)
print ""Features Encoded""
print X_test3_encoded
print ""Shape""
print X_test3_encoded.shape
print ""decision_function:""
print model.decision_function(X_test3_encoded)
print ""predict_proba:""
print model.predict_proba(X_test3_encoded)


Following is the output obtained:

Test 1...
Features Encoded
  (0, 4)    1.0
Shape
(1, 19)
decision_function:
[[ 0.55372615 -1.02949707 -1.75474347 -1.73324726 -1.75474347]]
predict_proba:
[[ 1.  1.  1.  1.  1.]]
Test 2...
Features Encoded
  (0, 4)    1.0
  (1, 1)    1.0
  (2, 0)    1.0
Shape
(3, 19)
decision_function:
[[ 0.55372615 -1.02949707 -1.75474347 -1.73324726 -1.75474347]
 [-1.07370197 -0.69103629 -0.89306092 -1.51402163 -0.89306092]
 [-1.55921001  1.11775556 -1.92080112 -1.90133404 -1.92080112]]
predict_proba:
[[ 0.59710757  0.19486904  0.26065002  0.32612646  0.26065002]
 [ 0.23950111  0.24715931  0.51348452  0.3916478   0.51348452]
 [ 0.16339132  0.55797165  0.22586546  0.28222574  0.22586546]]
Test 3...
Features Encoded
  (0, 4)    1.0
  (1, 1)    1.0
  (2, 0)    1.0
  (3, 0)    1.0
Shape
(4, 19)
decision_function:
[[ 0.55372615 -1.02949707 -1.75474347 -1.73324726 -1.75474347]
 [-1.07370197 -0.69103629 -0.89306092 -1.51402163 -0.89306092]
 [-1.55921001  1.11775556 -1.92080112 -1.90133404 -1.92080112]
 [-1.55921001  1.11775556 -1.92080112 -1.90133404 -1.92080112]]
predict_proba:
[[ 0.5132474   0.12507868  0.21262531  0.25434403  0.21262531]
 [ 0.20586462  0.15864173  0.4188751   0.30544372  0.4188751 ]
 [ 0.14044399  0.3581398   0.1842498   0.22010613  0.1842498 ]
 [ 0.14044399  0.3581398   0.1842498   0.22010613  0.1842498 ]]


As can be seen, the values obtained with ""predict_proba"" for the instance in ""X_test1"" change when that same instance is with others in X_test2. Also, ""X_test3"" just reproduces the ""X_test2"" and adds one more instance (that is equal to the last in ""X_test2""), but the probability values for all of them change. Why does this happen?
Also, I find it really strange that ALL the probabilities for ""X_test1"" are 1, shouldn't the sum of all be 1?

Now, if instead of using ""predict_proba"" I use ""decision_function"", I get the consistency in the values obtained that I need. The problem is that I get negative coefficients, and even some of the positives ones are greater than 1. 

So, what should I use? Why do the values of ""predict_proba"" change that way? Am I not understanding correctly what those values mean?

Thanks in advance for any help you could give me.

UPDATE

As suggested, I changed the code so as to also print the encoded ""X_test1"", ""X_test2"" and ""X_test3"", as well as their shapes. This doesn't appear to be the problem, as the encoding is consistant for the same instances between the test sets. 
",12,3268,,,
scikit-learn strange result,https://stackoverflow.com/questions/33937532,Use one attribute only once in scikit-learn decision tree in python,"I am using scikit-learn to create a decision tree, and its working like a charm. I would like to achieve one more thing: to make the tree to split on an attribute only once.
The reason behind this is because of my very strange dataset. I use a noisy dataset, and i am really interested in the noise as well. My class outcomes are binary let say [+,-]. I have a bunch of attributes with numbers mostly in the range of (0,1).
When scikit-learn creates the tree it splits on attributes multiple times, to make the tree ""better"". I understand that in this way the leaf nodes become more pure, but thats not the case i would like to achieve.
The thing i did was to define cutoffs for every attribute by counting the the information gain in different cutoffs, and choosing the maximum. In this way with ""leave-one-out"" and ""1/3-2/3"" cross validation techniques i get better results then the original tree.
The problem is that when i try to automatize this, i run into a problem  around the lower and upper bound e.g. around 0 and 1 because most of the elements will be under/upper that and  i get really high informational gain, cause one of the sets are pure, even if it only contains 1-2% of the full data.
All in all i would like to do something to make scikit-learn to only split on an attribute once.
If it cannot be done, do you guys have any advice how to generate those cutoffs in a nice way?
",11,4162,"To answer your question briefly, no, there is no built-in parameter to do this in sklearn. I tried to do the same a year ago, so I opened an issue requesting the addition of this feature.
sklearn builds nodes by randomly picking max_features features from the training dataset and searching for the cutoff that reduces the loss function the most. This exact same process is ran iteratively until some stopping criteria is met (max_depth, min_samples_leaf, etc.).
Hence, every feature always has the same probability of being picked, regardless of whether or not it has been used before.
If you're up for it, you can edit the source code of the classifier. In essence, all you need to do is drop the feature that minimizes the loss function after it has been chosen to build a node. That way, the algorithm will be unable to pick that feature again when taking a new sample of max_features features.
","I am not giving a method to directly deal with stopping the Classifier from using a feature multiple times. (Although you could do it by defining your own splitter and wiring it in, it is a lot of work.)

I would suggest making sure that you are balancing your classes in the first place, take a look at the class_weight parameter for details. That should help a lot in your issue. But if that does not work you can still enforce that there are no leafs having too small weight in them using the min_weight_fraction_leaf or similar parameters as suggested by maxymoo.
",
scikit-learn strange result,https://stackoverflow.com/questions/28076232,Is Apache Spark less accurate than Scikit Learn?,"I've recently been trying to get to know Apache Spark as a replacement for Scikit Learn, however it seems to me that even in simple cases, Scikit converges to an accurate model far faster than Spark does.
For example I generated 1000 data points for a very simple linear function (z=x+y) with the following script:

from random import random

def func(in_vals):
    '''result = x (+y+z+w....)'''
    result = 0
    for v in in_vals:
        result += v
    return result

if __name__ == ""__main__"":
    entry_count = 1000
    dim_count = 2
    in_vals = [0]*dim_count
    with open(""data_yequalsx.csv"", ""w"") as out_file:
        for entry in range(entry_count):
            for i in range(dim_count):
                in_vals[i] = random()
            out_val = func(in_vals)
            out_file.write(','.join([str(x) for x in in_vals]))
            out_file.write("",%s\n"" % str(out_val))


I then ran the following Scikit script:

import sklearn
from sklearn import linear_model

import numpy as np

data = []
target = []
with open(""data_yequalsx.csv"") as inFile:
    for row in inFile:
        vals = row.split("","")
        data.append([float(x) for x in vals[:-1]])
        target.append(float(vals[-1]))

test_samples= len(data)/10

train_data = [0]*(len(data) - test_samples)
train_target = [0]*(len(data) - test_samples)
test_data = [0]*(test_samples)
test_target = [0]*(test_samples)
train_index = 0
test_index = 0
for j in range(len(data)):
    if j &gt;= test_samples:
        train_data[train_index] = data[j]
        train_target[train_index] = target[j]
        train_index += 1
    else:
        test_data[test_index] = data[j]
        test_target[test_index] = target[j]
        test_index += 1

model = linear_model.SGDRegressor(n_iter=100, learning_rate=""invscaling"", eta0=0.0001, power_t=0.5, penalty=""l2"", alpha=0.0001, loss=""squared_loss"")
model.fit(train_data, train_target)
print(model.coef_)
print(model.intercept_)

result = model.predict(test_data)
mse = np.mean((result - test_target) ** 2)
print(""Mean Squared Error = %s"" % str(mse))


And then this Spark script: (with spark-submit , no other arguments)

from pyspark.mllib.regression import LinearRegressionWithSGD, LabeledPoint
from pyspark import SparkContext

sc = SparkContext (appName=""mllib_simple_accuracy"")

raw_data = sc.textFile (""data_yequalsx.csv"", minPartitions=10) #MinPartitions doesnt guarantee that you get that many partitions, just that you wont have fewer than that many partitions
data = raw_data.map(lambda line: [float(x) for x in line.split ("","")]).map(lambda entry: LabeledPoint (entry[-1], entry[:-1])).zipWithIndex()
test_samples= data.count()/10

training_data = data.filter(lambda (entry, index): index &gt;= test_samples).map(lambda (lp,index): lp)
test_data = data.filter(lambda (entry, index): index &lt; test_samples).map(lambda (lp,index): lp)

model = LinearRegressionWithSGD.train(training_data, step=0.01, iterations=100, regType=""l2"", regParam=0.0001, intercept=True)
print(model._coeff)
print(model._intercept)

mse = (test_data.map(lambda lp: (lp.label - model.predict(lp.features))**2 ).reduce(lambda x,y: x+y))/test_samples;
print(""Mean Squared Error: %s"" % str(mse))

sc.stop ()


Strangely though, the error given by spark is an order of magnitude larger than that given by Scikit (0.185 and 0.045 respectively) despite the two models having a nearly identical setup (as far as I can tell)
I understand that this is using SGD with very few iterations and so the results may differ but I wouldn't have thought that it would be anywhere near such a large difference or such a large error, especially given the exceptionally simple data.



Is there something I'm misunderstanding in Spark? Is it not correctly configured? Surely I should be getting a smaller error than that?
",11,4567,,,
scikit-learn strange result,https://stackoverflow.com/questions/39550118,Cross Validation function for logistic regression in R,"I Come from a predominantly python + scikit learn background, and I was wondering how would one obtain the cross validation accuracy for a logistic regression model in R? I was searching and surprised that there's no easy way to this. I'm looking for the equivalent:

import pandas as pd
from sklearn.cross_validation import cross_val_score
from sklearn.linear_model import LogisticRegression

## Assume pandas dataframe of dataset and target exist.

scores = cross_val_score(LogisticRegression(),dataset,target,cv=10)
print(scores)


For R: I have:

model = glm(df$Y~df$X,family=binomial')
summary(model) 


And now I'm stuck. Reason being, the deviance for my R model is 1900, implying its a bad fit, but the python one gives me 85% 10 fold cross validation accuracy.. which means its good. Seems a bit strange... So i wanted to run cross val in R to see if its the same result.

Any help is appreciated!
",7,35743,"R version using caret package:

library(caret)

# define training control
train_control &lt;- trainControl(method = ""cv"", number = 10)

# train the model on training set
model &lt;- train(target ~ .,
               data = train,
               trControl = train_control,
               method = ""glm"",
               family=binomial())

# print cv scores
summary(model)

","Below I took an answer from here and made a few changes. 

The changes I made were to make it a logit (logistic) model, add modeling and prediction, store the CV's results, and to make it a fully working example.

Also note that there are many packages and functions you could use, including cv.glm() from boot.

data(ChickWeight)

df                    &lt;- ChickWeight
df$Y                  &lt;- 0
df$Y[df$weight &gt; 100] &lt;- 1
df$X                  &lt;- df$Diet 

df     &lt;- df[sample(nrow(df)),]
folds  &lt;- cut(seq(1,nrow(df)),breaks=10,labels=FALSE)
result &lt;- list()

for(i in 1:10){
  testIndexes &lt;- which(folds==i,arr.ind=TRUE)
  testData    &lt;- df[testIndexes, ]
  trainData   &lt;- df[-testIndexes, ]
  model       &lt;- glm(Y~X,family=binomial,data=trainData)
  result[[i]] &lt;- predict(model, testData) 
}
result


You could add a line to calculate accuracy within the loop or just do it after the loop completes.
",
scikit-learn strange result,https://stackoverflow.com/questions/49416716,LabelBinarizer yields different result in multiclass example,"When executing the multiclass example in the scikit-learn tutorial


http://scikit-learn.org/stable/tutorial/basic/tutorial.html#multiclass-vs-multilabel-fitting


I came across a slight oddity.

&gt;&gt;&gt; import sklearn
&gt;&gt;&gt; sklearn.__version__
0.19.1

&gt;&gt;&gt; from sklearn.svm import SVC
&gt;&gt;&gt; from sklearn.multiclass import OneVsRestClassifier
&gt;&gt;&gt; from sklearn.preprocessing import LabelBinarizer

&gt;&gt;&gt; X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]
&gt;&gt;&gt; y = [0, 0, 1, 1, 2] # Three classes

&gt;&gt;&gt; clf = OneVsRestClassifier(estimator=SVC(random_state=0))
&gt;&gt;&gt; clf.fit(X, y).predict(X)
array([0, 0, 1, 1, 2])


This is all fine. Now with one-hot encoding:

&gt;&gt;&gt; y = LabelBinarizer().fit_transform(y)
&gt;&gt;&gt; y
array([[1, 0, 0],
       [1, 0, 0],
       [0, 1, 0],
       [0, 1, 0],
       [0, 0, 1]])


I would expect the label binarizer to only encode the target, but not having an influence on the classifier. However it yields a different result:

&gt;&gt;&gt; clf.fit(X, y).predict(X)
array([[1, 0, 0],
       [1, 0, 0],
       [0, 1, 0],
       [0, 0, 0],
       [0, 0, 0]])


Notebook on Google Colab (where the same code yields yet a different error, strangely):


https://drive.google.com/file/d/13dZ2aVbKTMgPOxj2SLsas2U2mOoKng2M/view?usp=sharing

",5,1334,"OneVsRestClassifier is applying  LabelBinarizer itself under the hood (the source code in sklearn/multiclass.py):

def fit(self, X, y):
  ...
  self.label_binarizer_ = LabelBinarizer(sparse_output=True)
  Y = self.label_binarizer_.fit_transform(y)
  Y = Y.tocsc()
  self.classes_ = self.label_binarizer_.classes_


So extra manual conversion is unnecessary. In fact, it's interpreting your one-hot encoded y as multi-label input. From the documentation:


  y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]
  
  Multi-class targets. An indicator matrix turns on multilabel
  classification.

",,
scikit-learn strange result,https://stackoverflow.com/questions/59240556,LatentDirichletAllocation was not installed in decomposition module of SciKit-Learn,"I got some strange problem at SciKit-Learn package. 
There is ""decomposition"" module inside SciKit-Learn package, which should contain LatentDirichletAllocation([]) function. See documentation here:
""https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition""

When I tried to import ""decomposition"" module: 
from sklearn import as decomposition
it gives error:

Traceback (most recent call last):
  File ""tf_1_day_scikit_dnn.py"", line 12, in &lt;module&gt;
    from sklearn import decomposition
  File ""/home/developer1/.local/lib/python3.6/site-packages/sklearn/decomposition/__init__.py"", line 19, in &lt;module&gt;
    from ._online_lda import LatentDirichletAllocation
ImportError: cannot import name 'LatentDirichletAllocation'


Command: 
ls -al ~/.local/lib/python3.6/site-packages/sklearn/decomposition shows:

drwxr-xr-x  4 developer1 developer1   4096 Dec  9 00:45 .
drwxr-xr-x 33 developer1 developer1   4096 Dec  9 00:45 ..
-rw-r--r--  1 developer1 developer1   5490 Dec  9 00:44 _base.py
-rw-r--r--  1 developer1 developer1    480 Dec  9 00:44 base.py
-rwxr-xr-x  1 developer1 developer1 179440 Dec  9 00:44 _cdnmf_fast.cpython-36m-x86_64-linux-gnu.so
-rwxr-xr-x  1 developer1 developer1 175344 Dec  3 00:09 cdnmf_fast.cpython-36m-x86_64-linux-gnu.so
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 cdnmf_fast.py
-rw-r--r--  1 developer1 developer1  54528 Dec  9 00:44 _dict_learning.py
-rw-r--r--  1 developer1 developer1    507 Dec  9 00:44 dict_learning.py
-rw-r--r--  1 developer1 developer1  12572 Dec  9 00:44 _factor_analysis.py
-rw-r--r--  1 developer1 developer1    513 Dec  9 00:44 factor_analysis.py
-rw-r--r--  1 developer1 developer1  20866 Dec  9 00:44 _fastica.py
-rw-r--r--  1 developer1 developer1    490 Dec  9 00:44 fastica_.py
-rw-r--r--  1 developer1 developer1  14076 Dec  9 00:44 _incremental_pca.py
-rw-r--r--  1 developer1 developer1    513 Dec  9 00:44 incremental_pca.py
-rw-r--r--  1 developer1 developer1   1401 Dec  9 00:44 __init__.py
-rw-r--r--  1 developer1 developer1  13597 Dec  9 00:44 _kernel_pca.py
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 kernel_pca.py
-rw-r--r--  1 developer1 developer1  47255 Dec  9 00:44 _nmf.py
-rw-r--r--  1 developer1 developer1    477 Dec  9 00:44 nmf.py
-rwxr-xr-x  1 developer1 developer1  62056 Dec  3 00:09 _online_lda.cpython-36m-x86_64-linux-gnu.so
-rwxr-xr-x  1 developer1 developer1  62064 Dec  9 00:44 _online_lda_fast.cpython-36m-x86_64-linux-gnu.so
-rw-r--r--  1 developer1 developer1    513 Dec  9 00:44 online_lda_fast.py
-rw-r--r--  1 developer1 developer1  30471 Dec  9 00:44 _online_lda.py
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 online_lda.py
-rw-r--r--  1 developer1 developer1  22807 Dec  9 00:44 _pca.py
-rw-r--r--  1 developer1 developer1    477 Dec  9 00:44 pca.py
drwxr-xr-x  2 developer1 developer1   4096 Dec  9 00:45 __pycache__
-rw-r--r--  1 developer1 developer1    855 Dec  9 00:44 setup.py
-rw-r--r--  1 developer1 developer1  13654 Dec  9 00:44 _sparse_pca.py
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 sparse_pca.py
drwxr-xr-x  3 developer1 developer1   4096 Dec  9 00:45 tests
-rw-r--r--  1 developer1 developer1   8346 Dec  9 00:44 _truncated_svd.py
-rw-r--r--  1 developer1 developer1    507 Dec  9 00:44 truncated_svd.py


Most of the functions are here but there are no traces of the ""LatentDirichletAllocation"" function. Yet I did see LatentDirichletAllocation class defined in the _online_lda.py file.

Command python3 -c ""import sklearn; sklearn.show_versions()"" prints all versions required:

System:
    python: 3.6.8 (default, Oct  7 2019, 12:59:55)  [GCC 8.3.0]
executable: /usr/bin/python3
   machine: Linux-4.15.0-65-generic-x86_64-with-Ubuntu-18.04-bionic

Python dependencies:
       pip: 9.0.1
setuptools: 42.0.2
   sklearn: 0.22
     numpy: 1.17.4
     scipy: 1.3.3
    Cython: None
    pandas: 0.25.3
matplotlib: 3.1.2
    joblib: 0.14.0

Built with OpenMP: True


So I assume all requirements were met here. (BTW SciKit-Learn worked fine till I tried to import ""decomposition"" module).

I installed the SciKit-Learn package as it was described here: 
""https://scikit-learn.org/stable/install.html""
using command pip3 install -U scikit-learn
I did reinstall that several times but got same result. 

What do I do wrong? Do I need to add missing ""LatentDirichletAllocation"" function into the ""decomposition"" module? If so - how? Should I install the whole package somehow different way? 

Thanks.
",3,1271,"I had the same problem today. I solved it by going back to the previous version 0.21 of scikit-learn:

pip3 install scikit-learn==0.21


EDIT: I think the answer from glemaitre (https://stackoverflow.com/a/59328446/10429267) shows a better solution.
","The validated ""answered"" is not the right fix since it only downgrades scikit-learn. You will not be able to benefit from new features and bug fixes.

It will be helpful instead to report and give feedback in the issue tracker: https://github.com/scikit-learn/scikit-learn/issues/15884

It will allow us to find the root of the problem and propose the proper fix which seems to affect several persons.

EDIT:
After some investigation, you need to remove the following files:


~/.local/lib/python3.6/site-packages/sklearn/decomposition/_online_lda.cpython-36m-x86_64-linux-gnu.so
~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/_hashing.cpython-36m-x86_64-linux-gnu.so
~/.local/lib/python3.6/site-packages/sklearn/datasets/_svmlight_format.cpython-36m-x86_64-linux-gnu.so


Apparently, the so files do not get removed when updating to scikit-learn. This might be due to an old pip version.
","Try:



For Python 3

pip3 uninstall scikit-learn
pip3 install -U scikit-learn==0.21.3




For Python 2

pip uninstall scikit-learn
pip install -U scikit-learn==0.20.4

"
scikit-learn strange result,https://stackoverflow.com/questions/33317896,scikit learn TSNE transform returns strange results when applied to word vectors,"I am getting weird results when applying scikit learn's manifold t-sne implementation on word embeddings generated by word2vec.
Normally, the distance and direction of (king-queen) should be the same as (man-woman) as stated in plenty of examples on the internet.

I am sure that my word2vec model is trained correctly as I am using the Google News data set and the query of most_similar(king-man+woman) also returns queen as it is supposed to.

The plotting of the transformed 2d-vectors is also done correctly. Therefore, it must be related to the t-sne transformation from scikit learn.

I am using the following python code for transformation:

vecs = np.concatenate(vecs)
np_vecs = np.array(vecs, dtype='float')
ts = TSNE(2)
reduced_vecs = ts.fit_transform(np_vecs)




I am I doing anything wrong or is the library configuration wrong?
",3,1287,"I would like to add to iceui2's answer. Original TSNE algorithm, is a non-linear dimension reduction technique, which tries to make sure that distribution of distances between neighbours is preserved in lower and higher dimension (neighbours in high dimension will be neighbours in lower dimension). 

However, your directions are likely to get distorted, as the manifold in which points used to lie has changed drastically.

To preserve directions, I recommend using PCA for dimension reduction and then plotting. 
",,
scikit-learn strange result,https://stackoverflow.com/questions/21855410,Number of clusters increased with the increase of MinPts in scikit-learn DBSCAN,"I use DBSCAN implementation from scikit-learn library and I got strange results.
The number of estimated clusters increased with the increase of parameter MinPts (min_samples) and from my understanding of algorithm this should not happend. 

Here are my results:

Estimated number of clusters:34 eps=0.9 min_samples=13.0
Estimated number of clusters:35 eps=0.9 min_samples=12.0
Estimated number of clusters:42 eps=0.9 min_samples=11.0 &lt;- strange result here
Estimated number of clusters:37 eps=0.9 min_samples=10.0   
Estimated number of clusters:53 eps=0.9 min_samples=9.0
Estimated number of clusters:63 eps=0.9 min_samples=8.0


I use scikit-learn like this:

X = StandardScaler().fit_transform(X)
db = DBSCAN(eps=eps, min_samples=min_samples, algorithm='kd_tree').fit(X)


and X is an array that contains ~200k 12-dimensional points.

What can be the problem here?
",3,2858,,,
scikit-learn strange result,https://stackoverflow.com/questions/60673168,Weird linear regression learning curve,"I'm trying to build a prediction model for apartments price. I use python scikit-learn toolset. I'm using a dataset having total floor area and location of the apartment, which I have converted to dummy features. So the dataset looks like this:

Then I build a learning curve to see how the model is doing. 
I build the learning curve this way:

from matplotlib import pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import learning_curve

model = LinearRegression()
training_sizes, training_scores, validation_scores = learning_curve(
    estimator = model,
    X = X_train,
    y = y_train,
    train_sizes = np.linspace(5, len(X_train) * 0.8, dtype = int), 
    cv = 5
)
line1, line2 = plt.plot(
    training_sizes, training_scores.mean(axis = 1), 'g', 
    training_sizes, validation_scores.mean(axis = 1), 'r')
plt.legend((line1, line2), ('Training', 'Cross-validation'))


The picture I see is somewhat confusing:

Anomalies I see here are:


Huge error on cross-validation set
Error not steadily decreasing on training examples number growth. 


Is it normal? 

Learning curve of training set only is also not so smooth but at least the error isn't that huge:


Also I tried to add to add polynomial features of 2nd degree. But this didn't make the model perform any different. And because I have a lot of categorical features (total 106) it takes quite long even for 2nd degree polynomial. So I didn't try for higher degrees. 

Also I tried to build a model using as simple cost function and gradient descent as possible using Octave. The result with weird error was same.  

Update:
Thanks to tolik I made several amendments:

Data preparation:
Categorical data are independent. So I can't combine them into one feature.
Features were scaled using StandardScaler(). Thank you for that.

Feature extraction:
After features transformation with PCA I found out one new feature has explained variance ratio over 99%. Though it's strange I used only this one. That also allowed to increase polynomial degree though it didn't increase performance.

Model selection:
I tried several different models but none seem to perform better than LinearRegression. Interesting thing - all models perform worse on full data set. Probably it's because I sorted by price and higher prices are rather outliers. So when I start training sets on 1000 samples and go to the maximum, I get this picture (for nearly all models):

",2,1020,"My explanation have 3 steps: The data preparation, feature extraction, and model selection.

Data preparation:


In this dataset there are lots of Categorical and Ordinal values. If
the column has several non related categories it's ok to one-hot it.
but if the column has categories with order like
""bad"",""normal"",""good"" you can convert it to numerical as
{Good:1,Normal:0.5,Bad:0}.
Value ranges: the value ranges for each feature differs from the other, therefore the best thing to do is to normalise each feature along itself between 0:1.


Feature Extraction:


Your goal is to maximise the score so I guess you don't care about which feature is more important. Use PCA (has an implementation in scikit-learn library) , this algorithm convert your feature vectors into different features that each of them is a linear combination of the other features.  These new features are ordered by their explained variance. The first features describes the data better than the last one. You select the first features that their explained_variance_ sums to 99%. Now you have weigh less features.


Model Selection: 
You don't really know what is a good model, because No Free Lunch Theory but in this problem the best results that don't use deep learning , use these: XGBoost-Regressor , Random-Forest-Regressor ,Ada-Boost.

The most important thing is the Data Preparation!!!
",,
scikit-learn strange result,https://stackoverflow.com/questions/53218341,Up Sampling imbalanced dataset&#39;s minor classes,"i am using scikit-learn to classify my data, at the moment i am running a simple DecisionTree classifier.
I have three classes with a big imbalanced problem. The classes are 0,1 and 2. The minor classes are 1 and 2.

To give you an idea about the number of samples of the classes:

0 = 25.000 samples
1 = 15/20 less or more
2 = 15/20 less or more


so minor classes are about 0.06% of the dataset.
The approach that i am following to solve the imbalance problem is the UPSAMPLING of the minor classes. Code:

from sklearn.utils import resample,
resample(data, replace=True, n_samples=len_major_class, random_state=1234)


Now comes the problem. I did two tests:


If I upsample the minor classes and then divide my dataset in two groups one for training and one for testing... the accuracy is:



             precision    recall  f1-score   support

          0       1.00      1.00      1.00     20570
          1       1.00      1.00      1.00     20533
          2       1.00      1.00      1.00     20439

avg / total       1.00      1.00      1.00     61542



very good result.


If I ONLY upsample the training data and leave the original data for testing, the result is:



             precision    recall  f1-score   support

          0       1.00      1.00      1.00     20570
          1       0.00      0.00      0.00        15
          2       0.00      0.00      0.00        16

avg / total       1.00      1.00      1.00     20601



as you can see the global accuracy is high, but the accuracy of the class 1 and 2 is zero.

I am creating the classifier in this way:

DecisionTreeClassifier(max_depth=20, max_features=0.4, random_state=1234, criterion='entropy')


I also have tried adding the class_weight with balanced value, but it makes no difference.

I should only upsample the training data, why am i getting this strange problem?
",2,1572,"The fact that you obtain that behavior is quite normal when you do the re-sampling before the splitting; you are inducing a bias in your data.

If you oversample the data and then split, the minority samples in the test won't be anymore independent from the samples in the training set because they are generated together. In your case they are exact copies of the samples in the training set. Your accuracy is 100% because the classifier is classifying samples that have already been seen in the training.

Since your problem is strongly umbalanced I would suggest to use an ensemble of classifiers to handle it. 1) Split your dataset in training set and test set. Given the size of the dataset you can sample 1-2 samples from the minority class for test and leave the other for training. 2) From the training you generate N datasets containing all the remaining samples of the minority class and under-samples from the majority class (i would say 2*number of samples in the minority class). 3) For each one of the dataset obtained you train a model. 4) Use the test set to obtain the prediction; the final prediction will be the results of a majority vote of all the predictions of the classifiers. 

To have robust metrics perform different iterations with different initial splitting test/training. 
","You should not split the dataset after upsampling. You can do the upsampling within the training data.

Basically, you are leaking the test data into the training data. 
","I have a function that resamples the dataset for each class to have the same amount of instance.
from sklearn.utils import resample
import pandas as pd

def make_resample(_df, column):

  dfs_r = {}
  dfs_c = {}
  bigger = 0
  ignore = """"
  for c in _df[column].unique():
    dfs_c[c] = _df[df[column] == c]
    if dfs_c[c].shape[0] &gt; bigger:
      bigger = dfs_c[c].shape[0]
      ignore = c

  for c in dfs_c:
    if c == ignore:
      continue
    dfs_r[c] = resample(dfs_c[c], 
                        replace=True,
                        n_samples=bigger - dfs_c[c].shape[0],
                        random_state=0)
  return pd.concat([dfs_r[c] for c in dfs_r] + [_df])

"
scikit-learn strange result,https://stackoverflow.com/questions/46046360,Why is sklearn faster on CPU than Theano on GPU?,"I've compared processing time with theano(CPU), theano(GPU) and Scikit-learn(CPU) using Python.
But, I got strange result.
Here look at the graph that I plot.

Processing Time Comparison:



you can see the result of scikit-learn that is faster than theano(GPU).
The program that I checked its elapsed time is to compute euclidean distance matrix from a matrix which have n * 40 elements.

Here is the part of code.

points = T.fmatrix(""points"")
edm = T.zeros_like(points)

def get_point_to_points_euclidean_distances(point_id):
    euclideans = (T.sqrt((T.sqr(points- points[point_id, : ])).sum(axis=1)))

    return euclideans

def get_EDM_CPU(points):
    EDM = np.zeros((points.shape[0], points.shape[0])).astype(np.float32)
    for row in range(points.shape[0]):
        EDM[row, :] = np.sqrt(np.sum((points - points[row, :])**2, axis=1))

    return EDM

def get_sk(points):
    EDM = sk.pairwise_distances(a, metric='l2')

    return EDM

seq = T.arange(T.shape(points)[0])
(result, _) = theano.scan(fn = get_point_to_points_euclidean_distances, \
outputs_info = None , \
sequences = seq)

get_EDM_GPU = theano.function(inputs = [points], outputs = result, allow_input_downcast = True)


I thought that the reason why GPU is slower than sci-kit learn is probably transfer time. So I did profiling GPU with nvprof command. then I got this.

==27105== NVPROF is profiling process 27105, command: python ./EDM_test.py
Using gpu device 0: GeForce GTX 580 (CNMeM is disabled, cuDNN not available)
data shape :  (10000, 40)
get_EDM_GPU elapsed time :  1.84863090515 (s)
get_EDM_CPU elapsed time :  8.09937691689 (s)
get_EDM_sk elapsed time :  1.10968112946 (s)
ratio :  4.38128395145
==27105== Profiling application: python ./EDM_test.py
==27105== Warning: Found 9 invalid records in the result.
==27105== Warning: This could be because device ran out of memory when profiling.
==27105== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 71.34%  1.28028s      9998  128.05us  127.65us  128.78us  kernel_reduce_01_node_316e2e1cbfbe8cfb8e4a101f329ffeec_0(int, int, float const *, int, int, float*, int)
 19.95%  357.97ms      9997  35.807us  35.068us  36.948us  kernel_Sub_node_bc41b3f8f12c93d29f2c4360ad445d80_0_2(unsigned int, int, int, float const *, int, int, float const *, int, int, float*, int, int)
  7.32%  131.38ms         2  65.690ms  1.2480us  131.38ms  [CUDA memcpy DtoH]
  1.25%  22.456ms      9996  2.2460us  2.1140us  2.8420us  kernel_Sqrt_node_23508f8f49d12f3e8369d543f5620c15_0_Ccontiguous(unsigned int, float const *, float*)
  0.12%  2.1847ms         1  2.1847ms  2.1847ms  2.1847ms  [CUDA memset]
  0.01%  259.73us         5  51.946us     640ns  250.36us  [CUDA memcpy HtoD]
  0.00%  17.086us         1  17.086us  17.086us  17.086us  kernel_reduce_ccontig_node_97496c4d3cf9a06dc4082cc141f918d2_0(unsigned int, float const *, float*)
  0.00%  2.0090us         1  2.0090us  2.0090us  2.0090us  void copy_kernel&lt;float, int=0&gt;(cublasCopyParams&lt;float&gt;)


The transfer [CUDA memcpy DtoH] was performed twice { 1.248 [us],  131.38 [ms] }

The transfer [CUDA memcpy HtoD] was performed 5x { min: 640 [ns], max: 250.36 [us] }  

The transfer time is about 131.639 ms (131.88 ms + 259.73 us).
but the gap between GPU and scikit-learn is about 700ms (1.8 s - 1.1 s) So, the gap is over the transfer time.

does it compute only upper triangular matrix from symmetric matrix?

what makes scikit-learn so fast?
",2,2264,"What makes scikit-learn ( on pure CPU-side ) so fast?

My initial candidates would be a mix of:  


highly efficient use of available CPU-cores' L1-/ L2- sizes within the fastest [ns]-distances  
smart numpy vectorised execution being friendly to CPU cache-lines  
dataset so small, it can completely remain non-evicted from cache ( test to scale the dataset-under-review way above the L2-/L3-cache sizes to see the DDRx-memory-cost effects on the observed performance ( details are in the URL below ) )  
might enjoy even better timing on numpy, if avoiding .astype() conversions ( test it )


Facts on the GPU-side


auto-generated GPU-kernels do not have much chance to get ultimate levels of global memory latency-masking, compared to manually tweaked kernel-designs, tailor fit to respective GPU-silicon-architecture / latencies observed in-vivo  
data-structures larger than just a few KB remain paying GPU-SM/GDDR-MEM distances of ~ large hundreds of [ns], nearly [us] -v/s- compared to small units ~ small tens of [ns] at CPU/L1/L2/L3/DDRx ) ref. timing details in &gt;&gt;&gt; https://stackoverflow.com/a/33065382  
not being able to enjoy much of the GPU/SMX power, due to this task's obvious low-reuse of data points and dataset size beyond the GPU/SM-silicon limits, that causes and must cause GPU/SM-register capacity spillovers in any kind of GPU-kernel design attempts and tweaking  
the global task is not having a minimum reasonable amount of asynchronous, isolated ( non-communicating islands ) mathematically-dense, yet SMX-local, GPU-kernel processing steps ( there is not much to compute so as to adjust for the add-on overheads and expensive SMX/GDDR memory costs )  


GPU-s can lovely exhibit it's best-performance, if sufficiently enough densely-convoluted re-processing operations take place -- like in large-scale/high-resolution image-processing -- on [m,n,o]-convolution-kernel matrices so small, so as that all these m*n*o constant values can reside local to SM, inside an available set of SMX-SM_registers and if the GPU-kernel-launchers are optimally tweaked by the 3D-tblock/grid processing-layout geometries, so that the global memory access latencies are at its best-masked performance, having all the GPU-threads enforced within the hardware WARP-aligned SMx:WarpScheduler RoundRobin thread-scheduling capabilites ( the first swap from Round-Robin into Greedy-WarpSchedule mode loses the whole battle in case of divergent execution-paths in GPU-kernel-code ).
",,
scikit-learn strange result,https://stackoverflow.com/questions/39366120,Numpy Cosine Similarity difference over big collections,"I need to use the Scikit-learn sklearn.metric.pairwise.cosine_similarity over big matrixes. 
For some optimizations i need to compute only some rows of the matrixes, and so i tried different methods.

I found that in some cases the results were different depending on the size of the vectors, and I saw this strange behaviour on this test case (big vectors, transpose and estimate cosine):

from sklearn.metrics.pairwise import cosine_similarity
from scipy import spatial
import numpy as np
from scipy.sparse import csc_matrix

size=200
a=np.array([[1,0,1,0]]*size)
sparse_a=csc_matrix(a.T)
#standard cosine similarity between the whole transposed matrix, take only the first row
res1=cosine_similarity(a.T,a.T)[0]
#take the row obtained by the multiplication of the first row of the transposed matrix with transposed matrix itself (optimized for the first row calculus only)
res2=cosine_similarity([a.T[0]],a.T)[0]
#sparse matrix implementation with the transposed, which should be faster
res3=cosine_similarity(sparse_a,sparse_a)[0]
print(""res1: "",res1)
print(""res2: "",res2)
print(""res3: "",res3)
print(""res1 vs res2: "",res1==res2)
print(""res1 vs res3: "",res1==res3)
print(""res2 vs res3: "", res2==res3)


If ""size"" is set to 200 I got this result, that is ok:

res1:  [ 1.  0.  1.  0.]
res2:  [ 1.  0.  1.  0.]
res3:  [ 1.  0.  1.  0.]
res1 vs res2:  [ True  True  True  True]
res1 vs res3:  [ True  True  True  True]
res2 vs res3:  [ True  True  True  True]


But if ""size"" is set to 2000 or more, some strange things happen:

res1:  [ 1.  0.  1.  0.]
res2:  [ 1.  0.  1.  0.]
res3:  [ 1.  0.  1.  0.]
res1 vs res2:  [False  True False  True]
res1 vs res3:  [False  True False  True]
res2 vs res3:  [ True  True  True  True]


Does anybody know what am I missing?

Thanks in advance
",2,471,"In order to compare numpy.array you have to use np.isclose instead of equality operator. Try:

from sklearn.metrics.pairwise import cosine_similarity
from scipy import spatial
import numpy as np
from scipy.sparse import csc_matrix

size=2000
a=np.array([[1,0,1,0]]*size)
sparse_a=csc_matrix(a.T)
#standard cosine similarity between the whole transposed matrix, take only the first row
res1=cosine_similarity(a.T,a.T)[0]
#take the row obtained by the multiplication of the first row of the transposed matrix with transposed matrix itself (optimized for the first     row calculus only)
res2=cosine_similarity([a.T[0]],a.T)[0]
#sparse matrix implementation with the transposed, which should befaster
res3=cosine_similarity(sparse_a,sparse_a)[0]
print(""res1: "",res1)
print(""res2: "",res2)
print(""res3: "",res3)
print(""res1 vs res2: "", np.isclose(res1, res2))
print(""res1 vs res3: "", np.isclose(res1, res3))
print(""res2 vs res3: "", np.isclose(res2, res2))


The results are:

res1:  [ 1.  0.  1.  0.]
res2:  [ 1.  0.  1.  0.]
res3:  [ 1.  0.  1.  0.]
res1 vs res2:  [ True  True  True  True]
res1 vs res3:  [ True  True  True  True]
res2 vs res3:  [ True  True  True  True]


as expected.
",,
scikit-learn strange result,https://stackoverflow.com/questions/48263740,strange memory consumption behaviour when running PCA in python,,1,300,"The important question is, how many of the features you are extractiong.
Since version 0.18 of Scikit-learn, the svd_solver flag of the PCA algorithm determines which algorithm to use.  The default behavior is to select the ""best"" choice, which is described in detail in the official documentation which you mentioned. 
Possibly, one of those selections hits your performance, depending on the number of components. Otherwise, I would suggest you mention this behavior in the official GitHub of scikit-learn, since this could be interesting to them as well.
",,
scikit-learn strange result,https://stackoverflow.com/questions/53630329,Watershed analysis of coins - wrong output,,1,514,"You can fix this by adding a background label using the tutorial provided by openCV. 
https://docs.opencv.org/3.1.0/d3/db4/tutorial_py_watershed.html

They added an extra step to insert sure background and sure foreground region to help the watershed algorithm to properly segment the coin regions.

*********** edit**************

After reading your code again. I found that your original code has no problem. 
Background label was set using the variable border.

You will probably get the same result by executing the code found in OpenCV tutorial. The problem is in the way your draw the results. Since this is a display problem, there are many ways we can tackle the problem. One of the many is to use the information of the sure-background

Here are the modification to the function segment_on_dt

def segment_on_dt(a, img):
    sure_background = cv2.dilate(img, None, iterations=3)
    border = sure_background - cv2.erode(sure_background, None)


    dt = cv2.distanceTransform(img, cv2.DIST_L2, 3)
    dt = ((dt - dt.min()) / (dt.max() - dt.min()) * 255).astype(np.uint8)
    _, dt = cv2.threshold(dt, 200, 255, cv2.THRESH_BINARY)
    lbl, ncc = label(dt)


    # Completing the markers now. 
    lbl[border == 255] = 255 


    lbl = lbl.astype(np.int32)
    cv2.watershed(a, lbl)
    lbl[lbl == -1] = 0
    # Only draw red line if its not in sure background
    lbl[sure_background == 0] = 255

    lbl = lbl.astype(np.uint8)
    cv2.imshow('lbl_2',lbl)

    return 255 - lbl


I have added a new condition for the red lines to be drawn. The line are only drawn if its not in sure background region.

Your final result should look like this.


",,
scikit-learn strange result,https://stackoverflow.com/questions/73491673,Strange results when scaling data using scikit learn,"I have an input dataset that has 4 time series with 288 values for 80 days. So the actual shape is (80,4,288). I would like to cluster differnt days. I have 80 days and all of them have 4 time series: outside temperature, solar radiation, electrical demand, electricity prices. What I want is to group similar days with regard to these 4 time series combined into clusters. Days belonging to the same cluster should have similar time series.
Before clustering the days using k-means or Ward's method, I would like to scale them using scikit learn. For this I have to transform the data into a 2 dimensional shape array with the shape (80, 4*288) = (80, 1152), as the Standard Scaler of scikit learn does not accept 3-dimensional input. The Standard Scaler just standardizes features by removing the mean and scaling to unit variance.
Now I scale this data using sckit learn's standard scaler:
import numpy as np
from sklearn.preprocessing import StandardScaler
import pandas as pd

data_Unscaled = pd.read_csv(""C:/Users/User1/Desktop/data_Unscaled.csv"", sep="";"")
scaler = StandardScaler()
data_Scaled = scaler.fit_transform(data_Unscaled)
np.savetxt(""C:/Users/User1/Desktop/data_Scaled.csv"", data_Scaled, delimiter="";"")

When I now compare the unscaled and scaled data e.g. for the first day (1 row) and the 4th time series (columns 864 - 1152 in the csv file), the results look quite strange as you can see in the following figure:

As far as I see it, they are not in line with each other. For example in the timeslots between 111 and 201 the unscaled data does not change at all whereas the scaled data fluctuates. I can't explain that. Do you have any idea why this is happening and why they don't seem to be in line?
Here is the unscaled input data with shape (80,1152): https://filetransfer.io/data-package/CfbGV9Uk#link
and here the scaled output of the scaling with shape (80,1152): https://filetransfer.io/data-package/23dmFFCb#link
",1,469,"You have two issues here: scaling and clustering. As the question title refers to scaling, I'll handle that one in detail. The clustering issue is probably better suited for CrossValidated.
You don't say it, but it seems natural that all temperatures, be it on day 1 or day 80, are measured on a same scale. The same holds for the other three variables. So, for the purpose of scaling you essentially have four time series.
StandardScaler, like basically everything in sklearn, expects your observations to be organised in rows and variables in columns. It treats each column separately, deducting its mean from all the values in the column and dividing the resulting values by their standard deviation.
I reckon from your data that the first 288 entries in each row correspond to one variable, the next 288 to the second one etc. You need to reshape these data to form 288*80=23040 rows and 4 columns, one for each variable.
You apply StandardScaler on that array and reformat the data into the original shape, with 80 rows and 4*288=1152 columns. The code below should do the trick:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

data_Unscaled = pd.read_csv(""C:/Users/User1/Desktop/data_Unscaled.csv"", sep="";"", header=None)

X = data_Unscaled.to_numpy()
X_narrow = np.array([X[:, i*288:(i+1)*288].ravel() for i in range(4)]).T
scaler = StandardScaler()
X_narrow_scaled = scaler.fit_transform(X_narrow)
X_scaled = np.array([X_narrow_scaled[i*288:(i+1)*288, :].T.ravel() for i in range(80)])

# Plot the original data:
i=3
j=0
plt.plot(X[j, i*288:(i+1)*288])
plt.title('TimeSeries_Unscaled')
plt.show()

# plot the scaled data:
plt.plot(X_scaled[j, i*288:(i+1)*288])
plt.title('TimeSeries_Scaled')
plt.show()

resulting in the following graphs:


The line
X_narrow = np.array([X[:, i*288:(i+1)*288].ravel() for i in range(4)]).T

uses list comprehension to generate the four columns of the long, narrow array X_narrow. Basically, it is just a shorthand for a for-loop over your four variables. It takes the first 288 columns of X, flattens them into a vector, which it then puts into the first column of X_narrow. Then it does the same for the next 288 columns, X[:, 288:576], and then for the third and the fourth block of the 288 observed values per day. This way, each column in X_narrow contains a long time series, spanning 80 days (and 288 observations per day), of exactly one of your variables (outside temperature, solar radiation, electrical demand, electricity prices).
Now, you might try to cluster X_scaled using K-means, but I doubt it will work. You have just 80 points in a 1152-dimensional space, so the curse of dimensionality will almost certainly kick in. You'll most probably need to perform some kind of dimensionality reduction, but, as I noted above, that's a different question.
",,
scikit-learn strange result,https://stackoverflow.com/questions/59140556,Can / should I use past (e.g. monthly) label columns from a database as features in an ML prediction (no time-series!)?,"The question: Is it normal / usual / professional to use the past of the labels as features?
I could not find anything reliable on this, although it is a basic question.
Edited: Please mind, this is not a time-series question, I have deleted the time-series tag now and I changed the question. This question is about features that change regularly over time, yes! But we do not create a time-series from this, as there are many other features as well which are not like the label and are also important features in the model. Now please think of using past labels as normal features without a time-series approach.
I try to predict a certain month of data that is available monthly, thus a time-series, but I am not using it as a time-series, it is just monthly avaiable data of various different features.
It is a classification model, and now I want to predict a label column of a selected month of that time-series. The previous months before the selected label month are now the point of the question.
I do not want to just drop the past months of the label just because they are ""almost"" a label (or in other words: they were just the label columns of the preceding models in time). I know the past of the label, why not considering it as features as well?
My predictions are of course much better when adding the past labels of the time-series of labels to the features. This is logical as the labels usually do not change so much from one month to the other and thus can be predicted very well if you have fed the data with the past of the label. It would be strange not to use such ""past labels"" as features, as any simple time-series regression would then be better than the ml model.
Example: Let's say I predict the IQ test result of a person, and I use her past IQ test results as features in addition to other normal ""non-label"" features like age, education aso. I use the first 11 months of ""past labels"" of a year as features in addition to my normal ""non-label"" features. I predict the label of the 12th month.
Predicting the label of the 12th month works much better if you add the past of the labels to the features - obviously. This is because the historical labels, if there are any, are of course better indicators of the final outcome than normal columns like age and education.
Possibly related p.s.:
p.s.1: In auto-regressive models, the past of the dependent variable can well be used as independent variable, see: https://de.wikipedia.org/wiki/Regressionsanalyse
p.s.2: In ML you can perhaps just try any features and take what gives you the best results, a bit like &gt;Good question, try them [feature selection methods] all and see what works best&lt; in https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/ &gt;If the features are relevant to the outcome, the model will figure out how to use them. Or most models will.&lt; The same is said in Does the feature selection matter for learning algorithm with regularization?
p.s.3: Also probably relevant is the problem of multicollinearity: https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/ though multicollinearity is said to be no issue for the prediction: &gt;Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you dont need to understand the role of each independent variable, you dont need to reduce severe multicollinearity.
",1,1210,"It is perfectly possible and also good practice to include past label columns as features, though it depends on your question: do you want to explain the label only with other features (on purpose), or do you want to consider other and your past label columns to get the next label predicted, as a sort of adding a time-series character to the model without using a time-series?
The sequence in time is not even important, as long as all of such monthly columns are shifted in time consistently by the same time when going over to the predicting set. The model does not care if it is just January and February of the same column type, for the model, every feature is isolated.
Example: You can perfectly run a random forest model on various features, including their past label columns that repeat the same column type again and again, only representing different months. Any month's column can be dealt with as an independent new feature in the ml model, the only importance is to shift all of those monthly columns by the exactly same period to reach a consistent predicting set. In other words, obviously you should avoid replacing January with March column when you go from a training set January-June to a predicting set February-July, instead you must replace January with February of course.
Update 202301: model name is ""walk-forward""
This model setup is called ""walk-forward"", see Why isnt out-of-time validation more ubiquitous? --&gt; option 3 almost at the bottom of the page.
I got this from a comment at Splitting Time Series Data into Train/Test/Validation Sets.
In the following, it shows only training and testing set. It writes ""validation set"", but it is known that this gets mixed up all over the place, see What is the Difference Between Test and Validation Datasets?, and it must be meant as the testing set in the default understanding of it.

Thus, with the right wording, it is:

This should be the best model for labels that become features in time.
validation set in a ""walk-forward"" model?
As you can see in the model, no validation set is needed since the test data must be biased ""forward"" in time, that is the whole idea of predicting the ""step forward in time"", and any validation set would have to be in that same biased artificial future - which is already the past at the time of training, but the model does not know this.
The validation happens by default, without a needed dataset split, during the walk-forward, when the model learns again and again to predict the future and the output metrics can be put against each other. As the model is to predict the time-biased future, there is no need to prove that or how the artificial future is biased and sort of ""overtrained by time"". It is the aim of the model to have the validation in the artificial future and predict the real future as a last step only.
But then, why not still having a validation set on top of this, at least if it is just a small k-fold validation? It could play a role if the testing set has a few strong changes that happen in small time windows but which are still important to be predicted, or at least hinted at, but should also not be overtrained within each training step. The validation set would hit some of these time windows and might show whether the model can handle them well enough. Any other method than k-fold would shrink the power of the model too much. The more you take away from the testing set during training, the less it can predict the future.
Wrap up:
Try it out, and in doubt, leave the validation aside and judge upon the model by checking its metrics over time, during the ""walk-forward"". This model is not like the others.
Thus, in the end, you can, but you do not have to, split a k-fold validation from the testing set. That would look like:

After predicting a lot of known futures, the very last step in time is then the prediction of the unknown future.
This also answers Does the training+testing set have to be different from the predicting set (so that you need to apply a time-shift to ALL columns)?.
",,
scikit-learn strange result,https://stackoverflow.com/questions/42265434,significant test using scikit-learn&#39;s permutation test results in the same p-value for all classifiers,"I'm trying to find out the significance of the results using scikit-learn's permutation test as in:

score, permutation_scores, pvalue = permutation_test_score(clf.best_estimator_, X_train, Y_train, cv=10, n_jobs=10, n_permutations=100, scoring='accuracy')


where the clf.best_estimator is the result of cross-validation.

I use it for several classifiers (several independent clf.best_estimator_) but the p-values for all of them is the same 0.00990099009901.

I have no idea why this happens. The strange thing is that this is the same number that is reported in the linked code in scikit-learn user guide.
",1,690,"I asked the same question in scikit-learn's issues and the answer was: for most of the good classifiers if the random classifier is better than the trained classifier in 1 test out of 100, this magic number would be the result.

so there's nothing wrong with this magic number.
",,
scikit-learn strange result,https://stackoverflow.com/questions/39865557,sentiment analysis joint list,"im doing the sentiment analysis with scikit-learn python, now I'm using the nltk to do the words lemmatization in order to increase processing speed, for example:

I get the following arrays after nltk processing:

array([ ['Really', 'a', 'terrible', 'course', u'lecture', u'be', 'so', 'boring', 'i', u'contemplate', 'suicide', 'on', 'numerous', u'occasion', 'and', 'the', 'tutes', u'go', 'for', 'two', u'hour', 'and', u'be', 'completely'], ['Management', 'accounting', u'require', 'sufficient', 'practice', 'to', 'get', 'a', 'hang', 'of', 'Made', 'easier', 'with', 'a', 'great', 'lecturer']], dtype=object)


but the scklearn require the array is 

array([ 'Really a terrible course  lectures were so boring i contemplated suicide on numerous occasions and the tutes went for two hours and were completely ', 'Management accounting requires sufficient practice to get a hang of  Made easier with a great lecturer '],dtype=object)


so what is the best way to convert this array into the right form? I try to use joint list but the result is strange
",1,68,"You would do:

second_array = [' '.join(each) for each in first_array]


Alternatively you can tell sklearn.CountVectorizer to just use your tokens:

vect = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)
X = vect.fit_transform(first_array)

",,
scikit-learn strange result,https://stackoverflow.com/questions/23914472,strange F1 score result using scikit learn,"I am doing some classification and I was looking at the f1 score and noticed something strange.

When I do: 

""f1:"" + str(f1_score(y_test_bin, target, average=""weighted""))


I get :

f1:0.444444444444


When I do:

print ""f1:"" + str(f1_score(y_test_bin, target,pos_label=0, average=""weighted""))


I get:

f1:0.823529411765


Which is stange since I set the average to be 'weighted'. This should give me the weighted average of those two scores. Which is independent of the ""True label""

I can also see this in the classification report:

         precision    recall  f1-score   support

      0       0.76      0.90      0.82        39
      1       0.60      0.35      0.44        17

avg / total       0.71      0.73      0.71        56


In the classification report I get the weighted average, but not when I use the f1 score function. Why is this ?
",1,2729,"I was struggling with this problem as well and found a solution after reading eickenberg's answer on this thread which is definitely worth a read for the background on this.

In short, sklearn automatically overrides the averaging to take the positive class score when it interprets the data as binary. It does this automatically or when you specify a pos_label. The solution then is to redefine the pos_label as None.

print ""f1:"" + str(f1_score(y_test_bin, target, pos_label=None, average=""weighted""))


Hope this helps!
",,
scikit-learn strange result,https://stackoverflow.com/questions/65856432,Using Perceptron sklearn.ensemble.AdaBoostClassifier() gives an error,"I have the problem of using perceptrons for AdaBoost classifier.
The training and testing data from here
should be turned to 0 and 1 in the last column (""Poker Hand""), (it is from 1 to 9 inclusively originally), then both Decision Tree Classifier and AdaBoost Classifier with the total of 15 weak perceptron classifiers should be implemented in the data. I try to use scikit-learn libraries, but while my Decision Tree Classifier provides good results, AdaBoost Classifier throws error:
ValueError: BaseClassifier in AdaBoostClassifier ensemble is worse than random, ensemble can not be fit.
Here, the crucial parts of the code.
import pandas as pd
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import Perceptron
from sklearn import metrics

if __name__ == ""__main__"":
   
    data_train = pd.read_csv(""poker-hand-testing.data"",header=None)
    data_test = pd.read_csv(""poker-hand-training-true.data"",header=None)
    

    for value in range(0, len(data_train)):
        if data_train[10][value] != 0:
            data_train[10][value] = 1
    
    for value in range(0, len(data_test)):
        if data_test[10][value] != 0:
            data_test[10][value] = 1

    col=['Suit of card #1','Rank of card #1',
     'Suit of card #2','Rank of card #2',
     'Suit of card #3','Rank of card #3',
     'Suit of card #4','Rank of card #4',
     'Suit of card #5','Rank of card #5',
     'Poker Hand']
    
    data_train.columns=col
    data_test.columns=col
    
    y_train=data_train['Poker Hand']
    y_test=data_test['Poker Hand']
    
    x_train=data_train.drop('Poker Hand',axis=1)
    x_test=data_test.drop('Poker Hand',axis=1)
    
#The problematic part
    classifier = AdaBoostClassifier(base_estimator=Perceptron(), n_estimators=15, algorithm='SAMME')
    classifier = classifier.fit(x_train, y_train)
    y_pred = classifier.predict(x_test)
    
    print(""Accuracy of AdaBoost:"", metrics.accuracy_score(y_test, y_pred))

The strange thing is that this error occurs only once per 9-10 times when I don't change values to binary ones, while binary values almost always gives an error. Also, changing Perceptron() to SGDClassifier(loss=""perceptron"", eta0=1, learning_rate=""constant"", penalty=None) also throws such errors.
My questions are:

What is the solution with the possibility of using scikit-learn library?

Is there any way to handle such an exception? For example, if it gives error, execute it again until desired results?

Are there any other alternatives where I can use both Decision Tree and AdaBoost with Perceptron, if it is couldn't be solved in scikit-learn library?


",0,421,"The problematic part could be solved by try-catch block. For example,
#The problematic part solution
AdaBoost_accuracy = 0

while AdaBoost_accuracy == 0:
    try:
        classifier = AdaBoostClassifier(base_estimator=Perceptron(), n_estimators=15, algorithm='SAMME')
        classifier = classifier.fit(x_train, y_train)
        y_pred = classifier.predict(x_test)
        AdaBoost_accuracy = metrics.accuracy_score(y_test, y_pred)
    except:
        print(""Let me reclassify AdaBoost again"")

print(""Accuracy of AdaBoost:"", AdaBoost_accuracy)

",,
scikit-learn strange result,https://stackoverflow.com/questions/37907775,Scikitklearns TfidfTransformer makes my pipeline predict just one label,"I have a pandas dataframe containing texts and labels, and I'm trying to predict the labels using scikit-learn's CountVectorizer, TfidfTransformer and MultinomialNB. Here's what the dataframe looks like:

                                                text party
0  Herr lderspresident! Att vara talman i Sverig...     S
1  Herr lderspresident! rade ledamter av Sveri...     M
2  Herr lderspresident! Som fretrdare fr Alli...     M
3  Val av andre vice talman Herr lderspresident!...    SD
4  Herr lderspresident! Vnsterpartiet vill utny...     V


When I construct a pipeline with the three estimators mentioned above, I only get a ~35% accuracy in my predictions, but when I remove the TfidfTransformer the accuracy is bumped up to a more reasonable ~75% accuracy.

text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()), # problematic row
                     ('clf', MultinomialNB()),
                   ])

text_clf = text_clf.fit(df.text.values, df.party.values)

test = df.sample(500, random_state=42)
docs_test = test.text.values
predicted = text_clf.predict(docs_test)
np.mean(predicted == test.party.values)
# Out: either 0.35 or 0.75 depending on whether I comment out the problematic row above


When I get 0.35 and inspect predicted I see that it almost exclusively contains one label ('S'). This is the most common label in the original dataset, but that shouldn't impact the predictions, right? Any ideas on why I get these strange results?

EDIT: Link to data where anforandetext and parti are the relevant columns.
",0,392,"The reason that you are getting so much difference is because of smoothing. If you checkout the documentation of MultinomialNB class, checkout the alpha parameter. The default value for that is 1.0. This means that it implements Plus One smoothing by default. Plus one smoothing is a very common technique used with relative frequency estimates to account for unseen data. In Plus One smoothing, we add 1 to all raw counts to account for unseen terms and the sparsity of the document-term matrix.

However, when you end up using TF-IDF weights, the numbers that you get are very small and mostly between 0 - 1. To illustrate, if I use your data and only convert it into TF-IDF weights, this is the small snapshot of the TF-IDF weights that I obtain.

  (0, 80914)    0.0698184481033
  (0, 80552)    0.0304609466459
  (0, 80288)    0.0301759343786
  (0, 80224)    0.103630302925
  (0, 80204)    0.0437500703747
  (0, 80192)    0.0808649191625


You can see that these are really small numbers and adding 1 to them for smoothing will have a drastic effect on the calculations that Multinomial Naive Bayes makes. By adding 1 to these numbers, you completely change their scale for classification and hence your estimates mess up. I am assuming, you have a good idea about how Multinomial Naive Bayes works. If not, then definitely see this video. The video and my answer will be sufficient to understand what is going wrong over here. 

You should either use a small value of alpha in TF-IDF case or you should build TF-IDF weights after doing smoothing on the raw counts. Also on a secondary note, please use cross-validation to get any accuracy estimates. By testing the model on a sample of the training data, your accuracy numbers will be extremely biased. I would recommend using cross-validation or a separate hold-out set to evaluate your model.

Hope that helps.
",,
scikit-learn strange result,https://stackoverflow.com/questions/35232804,Scikit-learn: precision_recall_fscore_support returns strange results,"I am doing some text minining/classification and attempt to evaluate performance with the precision_recall_fscore_support function from the sklearn.metrics module. I am not sure how I can create a really small example reproducing the problem, but maybe somebody can help even so because it is something obvious I am missing. 

The aforementioned function returns among other things the support for each class. The documentation states


  support: int (if average is not None) or array of int, shape = [n_unique_labels] :
  The number of occurrences of each label in y_true.


But in my case, the number of classes for which support is returned is not the same as the number of different classes in the testing data.     

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)

classifier = svm.SVC(kernel=""linear"")
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
prec, rec, fbeta, supp = precision_recall_fscore_support(y_test, y_pred)

print(len(classifier.classes_)) # prints 18
print(len(supp))                # prints 19
print(len(np.unique(y_test)))   # prints 18


How can this be? How can there be support for a class which is not in the data?
",0,1943,"I am not sure what the problem is, but in my case there seems to be a mismatch between the classes learned by the classifier and the ones occurring in the test data. One can force the the function to compute the performance measures for the right classes by explicitly naming them.

prec, rec, fbeta, supp = precision_recall_fscore_support(y_test, y_pred, labels=classifier.classes_)

",,
scikit-learn strange result,https://stackoverflow.com/questions/54166529,How to fix strange prediction results in scikit-learn,"I have a simple example in scikit-learn for prediction. Here is my data file (data.csv):

first second third target1 target2
 800   400    240    400     25
 400   200    120    200     50
 200   100    60     100     100
 100   50     30     50      200
 50    25     15     25      400


The features include : first ,second and third 

The targets include : target1 and target2

Now I want to provide new values for the features and predict target1 and target2. As you can see, there is a trend between the values of features and the values of the target1 and target2:

By doubling the features values the value of target1 also doubles while the value of target2 cuts in half 

For example if I provide the values : 1600 , 800 and 480 for the first, second and the third features, I expect to get 800 and 12.5 for the target1 and target2 respectively. Here is the code:

import pandas as pd
from sklearn.model_selection import train_test_split
from collections import *
from sklearn.linear_model import LinearRegression

features = pd.read_csv('data.csv')

features.head()
features_name = ['first' , 'second' , 'third']
target_name = ['target1','target2']

X = features[features_name]
y = features[target_name]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.155, random_state = 42)

linear_regression_model = LinearRegression()
linear_regression_model.fit(X_train,y_train)

new_data  = OrderedDict([('first',1600) ,('second',800),('third',480) ])
new_data = pd.Series(new_data).values.reshape(1,-1)
ss = linear_regression_model.predict(new_data)

print (ss)


Here is the output:

[[ 800.         -284.58549223]]


As you can see the predicted value for the target1 is exactly what is expected while the predicted value for the target2 (-284.58) is far away from what is expected (12.5)

Does anybody know what is wrong here and how I can fix this issue? Thanks in advance for your help. 
",-1,270,"Edit: there is no linear correlation between your inputs (any of them, they are essentially all the same) and your target2. It seems to be an exponential decay. Try transforming your feature to another feature (1/exp(x)) and then you can do a linear regression on that feature

Edit2: my mistake, it is simply 1/x

Also.. (original post below)
I'm not an expert, but it appears that your input features are not only highly correlated, they are actually perfectly correlated (linear correlation that is) - this generally makes regression breaks.
The intuitive reason is that because they are correlated, there exists an infinite amount of possible coefficients, where they counter one another, and the prediction would still be exactly the same (imagine in your case first-second is exactly the same as 2first-3second or 0first-1second etc).
I also think this causes the normal equation of linear regression to generate a matrix that cannot be inverted..
",,
scikit-learn strange issue,https://stackoverflow.com/questions/22054964,OLS Regression: Scikit vs. Statsmodels?,"Short version: I was using the scikit LinearRegression on some data, but I'm used to p-values so put the data into the statsmodels OLS, and although the R^2 is about the same the variable coefficients are all different by large amounts. This concerns me since the most likely problem is that I've made an error somewhere and now I don't feel confident in either output (since likely I have made one model incorrectly but don't know which one).

Longer version: Because I don't know where the issue is, I don't know exactly which details to include, and including everything is probably too much. I am also not sure about including code or data. 

I am under the impression that scikit's LR and statsmodels OLS should both be doing OLS, and as far as I know OLS is OLS so the results should be the same.

For scikit's LR, the results are (statistically) the same whether or not I set normalize=True or =False, which I find somewhat strange.

For statsmodels OLS, I normalize the data using StandardScaler from sklearn. I add a column of ones so it includes an intercept (since scikit's output includes an intercept). More on that here: http://statsmodels.sourceforge.net/devel/examples/generated/example_ols.html (Adding this column did not change the variable coefficients to any notable degree and the intercept was very close to zero.) StandardScaler didn't like that my ints weren't floats, so I tried this: https://github.com/scikit-learn/scikit-learn/issues/1709
That makes the warning go away but the results are exactly the same.

Granted I'm using 5-folds cv for the sklearn approach (R^2 are consistent for both test and training data each time), and for statsmodels I just throw it all the data.

R^2 is about 0.41 for both sklearn and statsmodels (this is good for social science). This could be a good sign or just a coincidence.

The data is observations of avatars in WoW (from http://mmnet.iis.sinica.edu.tw/dl/wowah/) which I munged about to make it weekly with some different features. Originally this was a class project for a data science class.

Independent variables include number of observations in a week (int), character level (int), if in a guild (Boolean), when seen (Booleans on weekday day, weekday eve, weekday late, and the same three for weekend), a dummy for character class (at the time for the data collection, there were only 8 classes in WoW, so there are 7 dummy vars and the original string categorical variable is dropped), and others.

The dependent variable is how many levels each character gained during that week (int).

Interestingly, some of the relative order within like variables is maintained across statsmodels and sklearn. So, rank order of ""when seen"" is the same although the loadings are very different, and rank order for the character class dummies is the same although again the loadings are very different.

I think this question is similar to this one: Difference in Python statsmodels OLS and R's lm

I am good enough at Python and stats to make a go of it, but then not good enough to figure something like this out. I tried reading the sklearn docs and the statsmodels docs, but if the answer was there staring me in the face I did not understand it. 

I would love to know:


Which output might be accurate? (Granted they might both be if I missed a kwarg.)
If I made a mistake, what is it and how to fix it?
Could I have figured this out without asking here, and if so how?


I know this question has some rather vague bits (no code, no data, no output), but I am thinking it is more about the general processes of the two packages. Sure, one seems to be more stats and one seems to be more machine learning, but they're both OLS so I don't understand why the outputs aren't the same.

(I even tried some other OLS calls to triangulate, one gave a much lower R^2, one looped for five minutes and I killed it, and one crashed.)

Thanks!
",31,34799,"It sounds like you are not feeding the same matrix of regressors X to both procedures (but see below). Here's an example to show you which options you need to use for sklearn and statsmodels to produce identical results.

import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression

# Generate artificial data (2 regressors + constant)
nobs = 100 
X = np.random.random((nobs, 2)) 
X = sm.add_constant(X)
beta = [1, .1, .5] 
e = np.random.random(nobs)
y = np.dot(X, beta) + e 

# Fit regression model
sm.OLS(y, X).fit().params
&gt;&gt; array([ 1.4507724 ,  0.08612654,  0.60129898])

LinearRegression(fit_intercept=False).fit(X, y).coef_
&gt;&gt; array([ 1.4507724 ,  0.08612654,  0.60129898])


As a commenter suggested, even if you are giving both programs the same X, X may not have full column rank, and they sm/sk could be taking (different) actions under-the-hood to make the OLS computation go through (i.e. dropping different columns).

I recommend you use pandas and patsy to take care of this:

import pandas as pd
from patsy import dmatrices

dat = pd.read_csv('wow.csv')
y, X = dmatrices('levels ~ week + character + guild', data=dat)


Or, alternatively, the statsmodels formula interface:

import statsmodels.formula.api as smf
dat = pd.read_csv('wow.csv')
mod = smf.ols('levels ~ week + character + guild', data=dat).fit()


Edit: This example might be useful: http://statsmodels.sourceforge.net/devel/example_formulas.html
","If you use statsmodels, I would highly recommend using the statsmodels formula interface instead.  You will get the same old result from OLS using the statsmodels formula interface as you would from sklearn.linear_model.LinearRegression, or R, or SAS, or Excel.  

smod = smf.ols(formula ='y~ x', data=df)
result = smod.fit()
print(result.summary())


When in doubt, please 


try reading the source code
try a different language for benchmark, or 
try OLS from scratch, which is basic linear algebra. 

","i just wanted to add here, that in terms of sklearn, it does not use OLS method for linear regression under the hood. Since sklearn comes from the data-mining/machine-learning realm, they like to use Steepest Descent Gradient algorithm. This is a numerical method that is sensitive to initial conditions etc, while the OLS is an analytical closed form approach, so one should expect differences. So statsmodels comes from classical statistics field hence they would use OLS technique. So there are differences between the two linear regressions from the 2 different libraries
"
scikit-learn strange issue,https://stackoverflow.com/questions/57484399,Issue importing scikit-learn: module &#39;scipy&#39; has no attribute &#39;_lib&#39;,"I'm new to Python and am using Anaconda on Windows 10 to learn how to implement machine learning. Running this code on Spyder:

import sklearn as skl


Originally got me this:

Traceback (most recent call last):

  File ""&lt;ipython-input-1-7135d3f24347&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/julia/.spyder-py3/temp.py', wdir='C:/Users/julia/.spyder-py3')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/julia/.spyder-py3/temp.py"", line 3, in &lt;module&gt;
    from sklearn.family import Model

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\__init__.py"", line 76, in &lt;module&gt;
    from .base import clone

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\base.py"", line 16, in &lt;module&gt;
    from .utils import _IS_32BIT

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\__init__.py"", line 20, in &lt;module&gt;
    from .validation import (as_float_array,

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py"", line 21, in &lt;module&gt;
    from .fixes import _object_dtype_isnan

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\fixes.py"", line 289, in &lt;module&gt;
    from scipy.sparse.linalg import lsqr as sparse_lsqr

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\__init__.py"", line 114, in &lt;module&gt;
    from .isolve import *

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\isolve\__init__.py"", line 6, in &lt;module&gt;
    from .iterative import *

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\isolve\iterative.py"", line 10, in &lt;module&gt;
    from . import _iterative

ImportError: DLL load failed: The specified module could not be found.


I then went to the command line and did

pip uninstall scipy
pip install scipy

pip uninstall scikit-learn
pip install scikit-learn


and got no errors when doing so, with scipy 1.3.1 (along with numpy 1.17.0) and scikit-learn 0.21.3 being installed according to the command line.

However, now when I try to import sklearn I get a different error:

 File ""&lt;ipython-input-2-7135d3f24347&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/julia/.spyder-py3/temp.py', wdir='C:/Users/julia/.spyder-py3')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/julia/.spyder-py3/temp.py"", line 3, in &lt;module&gt;
    from sklearn.family import Model

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\__init__.py"", line 76, in &lt;module&gt;
    from .base import clone

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\base.py"", line 16, in &lt;module&gt;
    from .utils import _IS_32BIT

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\__init__.py"", line 20, in &lt;module&gt;
    from .validation import (as_float_array,

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py"", line 21, in &lt;module&gt;
    from .fixes import _object_dtype_isnan

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\fixes.py"", line 289, in &lt;module&gt;
    from scipy.sparse.linalg import lsqr as sparse_lsqr

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\__init__.py"", line 113, in &lt;module&gt;
    from .isolve import *

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\isolve\__init__.py"", line 6, in &lt;module&gt;
    from .iterative import *

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\isolve\iterative.py"", line 136, in &lt;module&gt;
    def bicg(A, b, x0=None, tol=1e-5, maxiter=None, M=None, callback=None, atol=None):

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\_lib\_threadsafety.py"", line 59, in decorator
    return lock.decorate(func)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\_lib\_threadsafety.py"", line 47, in decorate
    return scipy._lib.decorator.decorate(func, caller)

AttributeError: module 'scipy' has no attribute '_lib'


Any suggestions? I've uninstalled and reinstalled Anaconda and I'm still getting the same issue.

EDIT: When I do 

conda list --show-channel-urls


I get

# packages in environment at C:\ProgramData\Anaconda3:
#
# Name                    Version                   Build  Channel
_ipyw_jlab_nb_ext_conf    0.1.0                    py37_0    defaults
alabaster                 0.7.12                   py37_0    defaults
anaconda-client           1.7.2                    py37_0    defaults
anaconda-navigator        1.9.7                    py37_0    defaults
asn1crypto                0.24.0                   py37_0    defaults
astroid                   2.2.5                    py37_0    defaults
attrs                     19.1.0                   py37_1    defaults
babel                     2.7.0                      py_0    defaults
backcall                  0.1.0                    py37_0    defaults
backports                 1.0                        py_2    defaults
backports.functools_lru_cache 1.5                        py_2    defaults
backports.tempfile        1.0                        py_1    defaults
backports.weakref         1.0.post1                  py_1    defaults
beautifulsoup4            4.7.1                    py37_1    defaults
blas                      1.0                         mkl    defaults
bleach                    3.1.0                    py37_0    defaults
bzip2                     1.0.8                he774522_0    defaults
ca-certificates           2019.5.15                     1    defaults
certifi                   2019.6.16                py37_1    defaults
cffi                      1.12.3           py37h7a1dbc1_0    defaults
chardet                   3.0.4                 py37_1003    defaults
click                     7.0                      py37_0    defaults
cloudpickle               1.2.1                      py_0    defaults
clyent                    1.2.2                    py37_1    defaults
colorama                  0.4.1                    py37_0    defaults
conda                     4.7.11                   py37_0    defaults
conda-build               3.18.8                   py37_0    defaults
conda-env                 2.6.0                         1    defaults
conda-package-handling    1.3.11                   py37_0    defaults
conda-verify              3.4.2                      py_1    defaults
console_shortcut          0.1.1                         3    defaults
cryptography              2.7              py37h7a1dbc1_0    defaults
decorator                 4.4.0                    py37_1    defaults
defusedxml                0.6.0                      py_0    defaults
docutils                  0.15.1                   py37_0    defaults
entrypoints               0.3                      py37_0    defaults
filelock                  3.0.12                     py_0    defaults
freetype                  2.9.1                ha9979f8_1    defaults
future                    0.17.1                   py37_0    defaults
glob2                     0.7                        py_0    defaults
icc_rt                    2019.0.0             h0cc432a_1    defaults
icu                       58.2                 ha66f8fd_1    defaults
idna                      2.8                      py37_0    defaults
imagesize                 1.1.0                    py37_0    defaults
intel-openmp              2019.4                      245    defaults
ipykernel                 5.1.1            py37h39e3cac_0    defaults
ipython                   7.7.0            py37h39e3cac_0    defaults
ipython_genutils          0.2.0                    py37_0    defaults
ipywidgets                7.5.1                      py_0    defaults
isort                     4.3.21                   py37_0    defaults
jedi                      0.13.3                   py37_0    defaults
jinja2                    2.10.1                   py37_0    defaults
joblib                    0.13.2                   py37_0    defaults
jpeg                      9b                   hb83a4c4_2    defaults
json5                     0.8.5                      py_0    defaults
jsonschema                3.0.1                    py37_0    defaults
jupyter_client            5.3.1                      py_0    defaults
jupyter_core              4.5.0                      py_0    defaults
jupyterlab                1.0.2            py37hf63ae98_0    defaults
jupyterlab_server         1.0.0                      py_1    defaults
keyring                   18.0.0                   py37_0    defaults
lazy-object-proxy         1.4.1            py37he774522_0    defaults
libarchive                3.3.3                h0643e63_5    defaults
libiconv                  1.15                 h1df5818_7    defaults
liblief                   0.9.0                ha925a31_2    defaults
libpng                    1.6.37               h2a8f88b_0    defaults
libsodium                 1.0.16               h9d3ae62_0    defaults
libtiff                   4.0.10               hb898794_2    defaults
libxml2                   2.9.9                h464c3ec_0    defaults
lz4-c                     1.8.1.2              h2fa13f4_0    defaults
lzo                       2.10                 h6df0209_2    defaults
m2w64-gcc-libgfortran     5.3.0                         6    defaults
m2w64-gcc-libs            5.3.0                         7    defaults
m2w64-gcc-libs-core       5.3.0                         7    defaults
m2w64-gmp                 6.1.0                         2    defaults
m2w64-libwinpthread-git   5.0.0.4634.697f757               2    defaults
markupsafe                1.1.1            py37he774522_0    defaults
mccabe                    0.6.1                    py37_1    defaults
menuinst                  1.4.16           py37he774522_0    defaults
mistune                   0.8.4            py37he774522_0    defaults
mkl                       2019.4                      245    defaults
mkl-service               2.0.2            py37he774522_0    defaults
mkl_fft                   1.0.12           py37h14836fe_0    defaults
mkl_random                1.0.2            py37h343c172_0    defaults
msys2-conda-epoch         20160418                      1    defaults
navigator-updater         0.2.1                    py37_0    defaults
nbconvert                 5.5.0                      py_0    defaults
nbformat                  4.4.0                    py37_0    defaults
notebook                  6.0.0                    py37_0    defaults
numpy                     1.17.0                   pypi_0    pypi
numpy-base                1.16.4           py37hc3f5095_0    defaults
numpydoc                  0.9.1                      py_0    defaults
olefile                   0.46                     py37_0    defaults
openssl                   1.1.1c               he774522_1    defaults
packaging                 19.0                     py37_0    defaults
pandas                    0.25.0           py37ha925a31_0    defaults
pandoc                    2.2.3.2                       0    defaults
pandocfilters             1.4.2                    py37_1    defaults
parso                     0.5.0                      py_0    defaults
pickleshare               0.7.5                    py37_0    defaults
pillow                    6.1.0            py37hdc69c19_0    defaults
pip                       19.2.2                   pypi_0    pypi
pkginfo                   1.5.0.1                  py37_0    defaults
powershell_shortcut       0.0.1                         2    defaults
prometheus_client         0.7.1                      py_0    defaults
prompt_toolkit            2.0.9                    py37_0    defaults
psutil                    5.6.3            py37he774522_0    defaults
py-lief                   0.9.0            py37ha925a31_2    defaults
pycodestyle               2.5.0                    py37_0    defaults
pycosat                   0.6.3            py37hfa6e2cd_0    defaults
pycparser                 2.19                     py37_0    defaults
pyflakes                  2.1.1                    py37_0    defaults
pygments                  2.4.2                      py_0    defaults
pylint                    2.3.1                    py37_0    defaults
pyopenssl                 19.0.0                   py37_0    defaults
pyparsing                 2.4.0                      py_0    defaults
pyqt                      5.9.2            py37h6538335_2    defaults
pyrsistent                0.14.11          py37he774522_0    defaults
pysocks                   1.7.0                    py37_0    defaults
python                    3.7.3                h8c8aaf0_1    defaults
python-dateutil           2.8.0                    py37_0    defaults
python-libarchive-c       2.8                     py37_13    defaults
pytz                      2019.1                     py_0    defaults
pywin32                   223              py37hfa6e2cd_1    defaults
pywinpty                  0.5.5                 py37_1000    defaults
pyyaml                    5.1.1            py37he774522_0    defaults
pyzmq                     18.0.0           py37ha925a31_0    defaults
qt                        5.9.7            vc14h73c81de_0    defaults
qtawesome                 0.5.7                    py37_1    defaults
qtconsole                 4.5.2                      py_0    defaults
qtpy                      1.8.0                      py_0    defaults
requests                  2.22.0                   py37_0    defaults
rope                      0.14.0                     py_0    defaults
ruamel_yaml               0.15.46          py37hfa6e2cd_0    defaults
scikit-learn              0.21.3                   pypi_0    pypi
scipy                     1.3.0                    pypi_0    pypi
send2trash                1.5.0                    py37_0    defaults
setuptools                41.0.1                   py37_0    defaults
sip                       4.19.8           py37h6538335_0    defaults
six                       1.12.0                   py37_0    defaults
snowballstemmer           1.9.0                      py_0    defaults
soupsieve                 1.9.2                    py37_0    defaults
sphinx                    2.1.2                      py_0    defaults
sphinxcontrib-applehelp   1.0.1                      py_0    defaults
sphinxcontrib-devhelp     1.0.1                      py_0    defaults
sphinxcontrib-htmlhelp    1.0.2                      py_0    defaults
sphinxcontrib-jsmath      1.0.1                      py_0    defaults
sphinxcontrib-qthelp      1.0.2                      py_0    defaults
sphinxcontrib-serializinghtml 1.1.3                      py_0    defaults
spyder                    3.3.6                    py37_0    defaults
spyder-kernels            0.5.1                    py37_0    defaults
sqlite                    3.29.0               he774522_0    defaults
terminado                 0.8.2                    py37_0    defaults
testpath                  0.4.2                    py37_0    defaults
tk                        8.6.8                hfa6e2cd_0    defaults
tornado                   6.0.3            py37he774522_0    defaults
tqdm                      4.32.1                     py_0    defaults
traitlets                 4.3.2                    py37_0    defaults
urllib3                   1.24.2                   py37_0    defaults
vc                        14.1                 h0510ff6_4    defaults
vs2015_runtime            14.15.26706          h3a45250_4    defaults
wcwidth                   0.1.7                    py37_0    defaults
webencodings              0.5.1                    py37_1    defaults
wheel                     0.33.4                   py37_0    defaults
widgetsnbextension        3.5.0                    py37_0    defaults
win_inet_pton             1.1.0                    py37_0    defaults
wincertstore              0.2                      py37_0    defaults
winpty                    0.4.3                         4    defaults
wrapt                     1.11.2           py37he774522_0    defaults
xz                        5.2.4                h2fa13f4_4    defaults
yaml                      0.1.7                hc54c509_2    defaults
zeromq                    4.3.1                h33f27b4_3    defaults
zlib                      1.2.11               h62dcd97_3    defaults
zstd                      1.3.7                h508b16e_0    defaults


with the version of scipy not matching up with the version that pip installed. Not sure how significant it is but it seemed strange to me.

EDIT 2:
Doing pip list returns

Package                       Version
----------------------------- ---------
-cipy                         1.3.0
alabaster                     0.7.12
anaconda-client               1.7.2
anaconda-navigator            1.9.7
asn1crypto                    0.24.0
astroid                       2.2.5
attrs                         19.1.0
Babel                         2.7.0
backcall                      0.1.0
backports.functools-lru-cache 1.5
backports.tempfile            1.0
backports.weakref             1.0.post1
beautifulsoup4                4.7.1
bleach                        3.1.0
certifi                       2019.6.16
cffi                          1.12.3
chardet                       3.0.4
Click                         7.0
cloudpickle                   1.2.1
clyent                        1.2.2
colorama                      0.4.1
conda                         4.7.11
conda-build                   3.18.8
conda-package-handling        1.3.11
conda-verify                  3.4.2
cryptography                  2.7
decorator                     4.4.0
defusedxml                    0.6.0
docutils                      0.15.1
entrypoints                   0.3
filelock                      3.0.12
future                        0.17.1
glob2                         0.7
idna                          2.8
imagesize                     1.1.0
ipykernel                     5.1.1
ipython                       7.7.0
ipython-genutils              0.2.0
ipywidgets                    7.5.1
isort                         4.3.21
jedi                          0.13.3
Jinja2                        2.10.1
joblib                        0.13.2
json5                         0.8.5
jsonschema                    3.0.1
jupyter-client                5.3.1
jupyter-core                  4.5.0
jupyterlab                    1.0.2
jupyterlab-server             1.0.0
keyring                       18.0.0
lazy-object-proxy             1.4.1
libarchive-c                  2.8
MarkupSafe                    1.1.1
mccabe                        0.6.1
menuinst                      1.4.16
mistune                       0.8.4
mkl-fft                       1.0.12
mkl-random                    1.0.2
mkl-service                   2.0.2
navigator-updater             0.2.1
nbconvert                     5.5.0
nbformat                      4.4.0
notebook                      6.0.0
numpy                         1.17.0
numpydoc                      0.9.1
olefile                       0.46
packaging                     19.0
pandas                        0.25.0
pandocfilters                 1.4.2
parso                         0.5.0
pickleshare                   0.7.5
Pillow                        6.1.0
pio                           0.0.3
pip                           19.2.2
pkginfo                       1.5.0.1
prometheus-client             0.7.1
prompt-toolkit                2.0.9
psutil                        5.6.3
pycodestyle                   2.5.0
pycosat                       0.6.3
pycparser                     2.19
pyflakes                      2.1.1
Pygments                      2.4.2
pylint                        2.3.1
pyOpenSSL                     19.0.0
pyparsing                     2.4.0
pyrsistent                    0.14.11
PySocks                       1.7.0
python-dateutil               2.8.0
pytz                          2019.1
pywin32                       223
pywinpty                      0.5.5
PyYAML                        5.1.1
pyzmq                         18.0.0
QtAwesome                     0.5.7
qtconsole                     4.5.2
QtPy                          1.8.0
requests                      2.22.0
rope                          0.14.0
ruamel-yaml                   0.15.46
scikit-learn                  0.21.3
scipy                         1.3.1
Send2Trash                    1.5.0
setuptools                    41.0.1
six                           1.12.0
snowballstemmer               1.9.0
soupsieve                     1.9.2
Sphinx                        2.1.2
sphinxcontrib-applehelp       1.0.1
sphinxcontrib-devhelp         1.0.1
sphinxcontrib-htmlhelp        1.0.2
sphinxcontrib-jsmath          1.0.1
sphinxcontrib-qthelp          1.0.2
sphinxcontrib-serializinghtml 1.1.3
spyder                        3.3.6
spyder-kernels                0.5.1
terminado                     0.8.2
testpath                      0.4.2
tornado                       6.0.3
tqdm                          4.32.1
traitlets                     4.3.2
urllib3                       1.24.2
wcwidth                       0.1.7
webencodings                  0.5.1
wheel                         0.33.4
widgetsnbextension            3.5.0
win-inet-pton                 1.1.0
wincertstore                  0.2
wrapt                         1.11.2


pip list says scipy is version 1.3.1, while conda list says it's version 1.3.0. Again, not sure how relevant it is, but seems strange

EDIT 3: I got this error after putting the following lines (suggested by @Brennan) in my command prompt then running the file

pip uninstall scikit-learn
pip uninstall scipy
conda uninstall scikit-learn
conda uninstall scipy

conda update --all
conda install scipy
conda install scikit-learn


This is the new error I get when trying to import sklearn:

Traceback (most recent call last):

  File ""&lt;ipython-input-15-7135d3f24347&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/julia/.spyder-py3/temp.py', wdir='C:/Users/julia/.spyder-py3')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/julia/.spyder-py3/temp.py"", line 2, in &lt;module&gt;
    import sklearn as skl

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\__init__.py"", line 76, in &lt;module&gt;
    from .base import clone

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\base.py"", line 13, in &lt;module&gt;
    import numpy as np

  File ""C:\ProgramData\Anaconda3\lib\site-packages\numpy\__init__.py"", line 140, in &lt;module&gt;
    from . import _distributor_init

  File ""C:\ProgramData\Anaconda3\lib\site-packages\numpy\_distributor_init.py"", line 34, in &lt;module&gt;
    from . import _mklinit

ImportError: DLL load failed: The specified module could not be found.


A possible cause of this might be me deleting the mkl_rt.dll file from my Anaconda/Library/bin after encountering the error described here: https://github.com/ContinuumIO/anaconda-issues/issues/10182

This puts me in a predicament, because reinstalling Anaconda to repair this will get me the same ""ordinal 242 could not be located"" error that I faced earlier, but not repairing it will continue the issue with sklearn...

FINAL EDIT: Solved by installing old version of Anaconda. Will mark as solved when I am able to (2 days)
",3,12402,"I encountered the same error after letting my PC sit for 4 days unattended. Restarting the kernel solved it.
This probably won't work for everyone, but it might save someone a little agony.
","I ended up fixing this by uninstalling my current version of Anaconda and installing a version from a few months ago. I didn't get the ""ordinal 242"" error nor the issues with scikit-learn.
","had a similar problem on google collaboratory, simply uninstalling and reinstalling the scipy module alone solved it for me

and the simple fact that collab ai suggested this move emboldened me
"
scikit-learn strange issue,https://stackoverflow.com/questions/59140556,Can / should I use past (e.g. monthly) label columns from a database as features in an ML prediction (no time-series!)?,,1,1210,"It is perfectly possible and also good practice to include past label columns as features, though it depends on your question: do you want to explain the label only with other features (on purpose), or do you want to consider other and your past label columns to get the next label predicted, as a sort of adding a time-series character to the model without using a time-series?
The sequence in time is not even important, as long as all of such monthly columns are shifted in time consistently by the same time when going over to the predicting set. The model does not care if it is just January and February of the same column type, for the model, every feature is isolated.
Example: You can perfectly run a random forest model on various features, including their past label columns that repeat the same column type again and again, only representing different months. Any month's column can be dealt with as an independent new feature in the ml model, the only importance is to shift all of those monthly columns by the exactly same period to reach a consistent predicting set. In other words, obviously you should avoid replacing January with March column when you go from a training set January-June to a predicting set February-July, instead you must replace January with February of course.
Update 202301: model name is ""walk-forward""
This model setup is called ""walk-forward"", see Why isnt out-of-time validation more ubiquitous? --&gt; option 3 almost at the bottom of the page.
I got this from a comment at Splitting Time Series Data into Train/Test/Validation Sets.
In the following, it shows only training and testing set. It writes ""validation set"", but it is known that this gets mixed up all over the place, see What is the Difference Between Test and Validation Datasets?, and it must be meant as the testing set in the default understanding of it.

Thus, with the right wording, it is:

This should be the best model for labels that become features in time.
validation set in a ""walk-forward"" model?
As you can see in the model, no validation set is needed since the test data must be biased ""forward"" in time, that is the whole idea of predicting the ""step forward in time"", and any validation set would have to be in that same biased artificial future - which is already the past at the time of training, but the model does not know this.
The validation happens by default, without a needed dataset split, during the walk-forward, when the model learns again and again to predict the future and the output metrics can be put against each other. As the model is to predict the time-biased future, there is no need to prove that or how the artificial future is biased and sort of ""overtrained by time"". It is the aim of the model to have the validation in the artificial future and predict the real future as a last step only.
But then, why not still having a validation set on top of this, at least if it is just a small k-fold validation? It could play a role if the testing set has a few strong changes that happen in small time windows but which are still important to be predicted, or at least hinted at, but should also not be overtrained within each training step. The validation set would hit some of these time windows and might show whether the model can handle them well enough. Any other method than k-fold would shrink the power of the model too much. The more you take away from the testing set during training, the less it can predict the future.
Wrap up:
Try it out, and in doubt, leave the validation aside and judge upon the model by checking its metrics over time, during the ""walk-forward"". This model is not like the others.
Thus, in the end, you can, but you do not have to, split a k-fold validation from the testing set. That would look like:

After predicting a lot of known futures, the very last step in time is then the prediction of the unknown future.
This also answers Does the training+testing set have to be different from the predicting set (so that you need to apply a time-shift to ALL columns)?.
",,
scikit-learn strange issue,https://stackoverflow.com/questions/73491958,Installation always stuck on PyCaret 2.2.2 + Package problems,"I'm stuck on an issue that I can't seem to solve. I was fine using PyCaret on my other PC and had recently got a new desktop.
I was working on one dataset on my old PC and had no problems with setup() and PyCaret preprocessed my data without any issues. When I worked on my the same dataset with my new desktop and Jupyter newly installed, I noticed I ran into an ValueError: Setting a random_state has no effect since shuffle is False. You should leave random_state to its default (None), or set shuffle=True. I thought it was strange but went on to set fold_shuffle=True to get through this.
Next, I encountered AttributeError: 'Simple_Imputer' object has no attribute 'fill_value_categorical'. It seems I'm getting failures at every step of setup(). I went through the forums and found a thread where at the bottom of it, @eddygeek mentioned that PyCaret was set up to fail if the sklearn version is wrong. This got me looking into the packages I have that may meet dependencies between packages.
I noticed the following issues:

I get several errors:
ERROR: Command errored out with exit status 1: C:\Users\%%USER%%\anaconda3\python.exe'
Ignoring numpy: markers 'python_version &gt;= ""3.8"" and platform_system == ""AIX""' don't match your environment
ERROR: Could not find a version that satisfies the requirement scikit-learn==0.23.2
Screenshot of more errors attached

Jupyter Notebook fails to launch because of Pandas Profiling Import Error: cannot import name 'soft_unicode' from 'markupsafe'. I got around this by installing markupsafe===2.0.1 but this leads to incompatibility warning by pandas-profiling 3.2.0 saying it needs markupsafe 2.1.1

PyCaret keeps getting installed as 2.2.2 version. I think that's why it keeps looking for scikit-learn 0.23.2 when the latest PyCaret 2.3.10 works with scikit-learn &gt;=1.0. I've tried uninstalling and reinstalling PyCaret several times but it's still the same.


What I've done
I'm on Python 3.9.12 that was installed together with Anaconda3. My PyCaret was installed with pip install pycaret[full] --user on Anaconda Prompt.
In my pip list, I have:

scikit-learn 1.1.2
markupsafe 2.1.1
pandas-profiling 3.2.0
pycaret 2.2.2

I've added C:\Users\%%USER%%\AppData\Roaming\Python\Python39\Scripts to PATH
I'm really at my wits end so I hope I can get some advice on this. Thank you.
",1,977,"You are using a very old version of pycaret which does not work in Python 3.9. Please install the latest version in a fresh (conda) environment. Make sure it is a new environment in order to avoid any package issues.
# This installs the pre-release 3.0.0 release which has reduced dependencies.
pip install --pre pycaret

","I've encountered the very same issues and solved as follows.
According to the documentation, there are a few problems with your setup:

PyCaret is not yet compatible with sklearn&gt;=0.23.2

PyCaret is tested and supported on the following 64-bit systems:
Python 3.6  3.8
Python 3.9 for Ubuntu only


So if you're using python 3.9 on Windows, I'd start with that.
I went into a rabbit hole of downgrading the packages and getting one error after another.
Long story short, the setup that finally worked was:
sklearn 0.23.1
scipy 1.5.2

Both installed on a virtual conda environment but at the end I had to run:
pip3 install pycaret[full]

Notice pip3 intead of pip because I was getting permission errors.
",
scikit-learn strange issue,https://stackoverflow.com/questions/44490337,Import module works in Jupyter notebook but not in IDLE,"I dont understand what Im doing wrong. I re-installed my windows last week; after that I got python (3.6), and all the libraries I need, but when I try to import them in the IDLE returns an error (ModuleNotFoundError: No module named), and when I check in the command window with pip list or conda list, the package is already there. The strange thing is when I tried to import them from a Jupyter notebook worked without any problem. 

Im working with a laptop Windows 7, 64 bits. No previous version of python was installed before. I am having issues with scikit-learn, pandas-datareader and beautifulsoup4. I tried to install the packages with pip, conda and the wheel file. Nothing seems to work.  (since Im in my laptop, I only download miniconda, and Im installing the modules I need).

Any suggestions? The safety of my laptop is in your hands. :D 
",1,3390,"The reason is that your pip/conda installed library paths are not accessible by python IDLE.    You have to add those library paths to your environment variable(PATH). To do this open my computer &gt; properties &gt; advanced system settings &gt; system.

Under environment variables look for PATH and at the end add the location of installed libraries. Refer this for more details on how to add locations in path variable. Once you do these you will be able to import the libraries. In order to know what locations python searches for libraries you can use 

import sys 
print sys.path


This will give you a list of locations where python searches for libraries. Once you edit the PATH variable those locations will be reflected here.

Refer this also in order to know how to add python library path.

Note: The tutorial is a reference on how to edit PATH variable. I encourage you to find the location of installed libraries and follow the steps to edit the same.
",,
scikit-learn strange issue,https://stackoverflow.com/questions/66023482,Sklearn and Pymesh import causes conflicts,"I am working on processing 3D meshes using PyMesh. pip install pymesh2 has build error. Hence I installed from another release from the official github page. I am using pymesh2-0.3-cp36-cp36m-linux_x86_64.whl.
I installed scikit-learn using pip install. Both installed successfully in my system.
While importing them, I observed something strange.
While importing Pymesh and Sklearn. Pymesh imported correctly. Sklearn is not

While importing Sklearn and Pymesh. Sklearn imported correctly. Pymesh is not

Seems like these two packages are conflicting each other!
Can someone help me what is the issue and any possible resolution?
",0,140,"As suggested in the comments, I used conda instead of virtual env and pip installed the pymesh using the wheel file from the github. That worked well for me!
",,
scikit-learn strange issue,https://stackoverflow.com/questions/37165344,python - sklearn 0.17 and 0.18 - how to retrieve only the first value of an StratifiedKFold object?,"I have a python problem.
I use scikit-learn 0.17
Someone give me an example code that I have to run.
The code was in python 2, and scikit-learn 0.18, the dev version.
I transformed the code in python 3 without issue.
But he used the function :

sklearn.model_selection.StratifiedKFold(n_folds=3, shuffle=False, random_state=None)


In scikit-learn 0.17, I have function that looks like this one : 

sklearn.cross_validation.StratifiedKFold(y, n_folds=3, shuffle=False, random_state=None)


As you can see, they seem very similar : there is only the ""y"" that change.

So I have two solution : update to 0.18 (I'm using windows, and I can't use linux) I tried this one, but I don't succeed to compile sources.

The second solution is to use the StratifiedKFold of 0.17.
I succeed this solution. (I don't know if it doing the things it suppose to do, however x) ).

So, there is my problem (finally) :

I have to adapt this two line : 

skf = StratifiedKFold(n_folds=5)
train_idx, test_idx = skf.split(patch_arr, labels).next()


for the moment, I wrote this (cover your eyes):

skf = StratifiedKFold(labels, n_folds=5)
for train_idx, test_idx in skf:
    break 


And the program run, but my code is awful. Is there any other method to retrieve only the first couple of (train_idx, test_idx) ?
I tried with skf[0] and skf.next(), but none of this works.

(for your information, train_idx and test_idx are two arrays)

Have you got any idea ?
I find strange that I can't use patch_arr in my solution too, but if I can avoid to spend more hours trying to update scikit-image, I will be very happy :)
",0,331,"The following should suffice.

train_idx, test_idx = next(iter(skf))

",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/5941234,Profiling SQL query,"I'm using sqlalchemy (expression language, not full ORM) with MySQL and experiencing some unexpected slowness.  Particularly, the time spent performing a select query in by sqlalchemy is ten times greater than the time spent performing the same query from the mysql command line.

Output from cprofile:

ncalls  tottime  percall  cumtime  percall filename:lineno(function)
100  206.703    2.067  206.703    2.067 {method 'query' of '_mysql.connection' objects}


MySQL time:  0.26 seconds

The consensus seems to be that there is some overhead using sqlalchemy, but not nearly this much.  Any suggestions as to what could cause behavior like this?

The queries are generally of the form:

SELECT fieldnames.minage, fieldnames.maxage, fieldnames.race,    
fieldnames.sex, sum( pop.population ) AS pop, pop.zip5
FROM pop
INNER JOIN fieldnames ON fieldnames.fieldname = pop.fieldname_id
WHERE fieldnames.race IN (""White alone"")
AND fieldnames.sex IN (""Female"")
AND fieldnames.maxage &gt;=101
AND fieldnames.minage &lt;=107
GROUP BY fieldnames.minage, fieldnames.maxage

",6,852,"One possible reason for slowness - Does sql alchemy use prepared statements? If yes, then a reason why you may be experiencing a difference in performance is because the mysql optimizer has different information when creating the two query plans.

When you run the query from the command-line, the mysql optimizer has the complete query with all where clause values filled in (as you showed above3), thus can optimize explicitly for these values.

When you run from sql alchemy, the mysql optimizer may only see this (perhaps fieldnames.race and fieldnames.sex are parameterized as well):

SELECT fieldnames.minage, fieldnames.maxage, fieldnames.race,    
fieldnames.sex, sum( pop.population ) AS pop, pop.zip5
FROM pop
INNER JOIN fieldnames ON fieldnames.fieldname = pop.fieldname_id
WHERE fieldnames.race IN (""White alone"")
AND fieldnames.sex IN (""Female"")
AND fieldnames.maxage &gt;= ?
AND fieldnames.minage &lt;= ?
GROUP BY fieldnames.minage, fieldnames.maxage


Thus the optimizer has to make a guess on what values you might use then optimize around that. Unfortunately, it may make a bad guess, and thus in a worst case create a query plan that makes the query run significantly slower than you expect.
",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/28901050,Default value doesn&#39;t work in SQLAlchemy + PostgreSQL + aiopg + psycopg2,"I've found an unexpected behavior in SQLAlchemy. I'm using the following versions:


SQLAlchemy (0.9.8)
PostgreSQL (9.3.5)
psycopg2 (2.5.4)
aiopg (0.5.1)


This is the table definition for the example:

import asyncio
from aiopg.sa import create_engine
from sqlalchemy import (
  MetaData,
  Column,
  Integer,
  Table,
  String,
)
metadata = MetaData()

users = Table('users', metadata,
  Column('id_user', Integer, primary_key=True, nullable=False),
  Column('name', String(20), unique=True),
  Column('age', Integer, nullable=False, default=0),
)


Now if I try to execute a simple insert to the table just populating the id_user and name, the column age should be auto-generated right? Lets see...

@asyncio.coroutine
def go():
  engine = yield from create_engine('postgresql://USER@localhost/DB')
  data = {'id_user':1, 'name':'Jimmy' }
  stmt = users.insert(values=data, inline=False)
  with (yield from engine) as conn:
    result = yield from conn.execute(stmt)


loop = asyncio.get_event_loop()
loop.run_until_complete(go())


This is the resulting statement with the corresponding error:

INSERT INTO users (id_user, name, age) VALUES (1, 'Jimmy', null);

psycopg2.IntegrityError: null value in column ""age"" violates not-null constraint


I didn't provide the age column, so where is that age = null value coming from? I was expecting something like this:

INSERT INTO users (id_user, name) VALUES (1, 'Jimmy');


Or if the default flag actually works should be:

INSERT INTO users (id_user, name, Age) VALUES (1, 'Jimmy', 0);


Could you put some light on this?
",4,1565,"This issue has been confirmed has an aiopg bug. Seems like at the moment it's ignoring the default argument on data manipulation. 

I've fixed the issue using server_default instead:

users = Table('users', metadata,
          Column('id_user', Integer, primary_key=True, nullable=False),
          Column('name', String(20), unique=True),
          Column('age', Integer, nullable=False, server_default='0'))

","I think you need to use inline=True in your insert. This turns off 'pre-execution'.
Docs are a bit cryptic on what exactly this 'pre-execution' entails, but they mentions default parameters:

    :param inline:
      if True, SQL defaults present on :class:`.Column` objects via
      the ``default`` keyword will be compiled 'inline' into the statement
      and not pre-executed.  This means that their values will not
      be available in the dictionary returned from
      :meth:`.ResultProxy.last_updated_params`.


This piece of docstring is from Update class, but they have a shared behavior with Insert. 

Besides, that's the only way they test it: 
https://github.com/zzzeek/sqlalchemy/blob/rel_0_9/test/sql/test_insert.py#L385
",
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/17959320,Flask-Login &amp; Flask-Principle authenticated user drops to flask_login.AnonymousUserMixin,"I'm getting two problems


My authenticated user constantly drops to flask_login.AnonymousUserMixin
I get unexpected signalling using Flask-Login &amp; Flask-Principal


Trying to get /projects/10 URL which is protected with
@admin_permission.require(http_exception=403)

This is my console output:

127.0.0.1 - - [30/Jul/2013 16:22:58] ""GET /projects/10 HTTP/1.1"" 302 -
127.0.0.1 - - [30/Jul/2013 16:22:58] ""GET /login HTTP/1.1"" 200 -


Getting on login form (All good so far). Typing valid login &amp; password and getting crazy signalling and behavior is not what I expect:

127.0.0.1 - - [30/Jul/2013 16:24:06] ""POST /login HTTP/1.1"" 302 -
&lt;Employee('103','Dmitry Semenov')&gt;
&lt;Identity id=""103"" auth_type=""None"" provides=set([Need(method='role', value='manager'), Need(method='id', value=103L), Need(method='role', value='admin')])&gt;
&lt;flask_login.AnonymousUserMixin object at 0x03258790&gt;
&lt;Identity id=""103"" auth_type=""None"" provides=set([])&gt;
127.0.0.1 - - [30/Jul/2013 16:24:06] ""GET /projects/10 HTTP/1.1"" 302 -
&lt;flask_login.AnonymousUserMixin object at 0x03342AF0&gt;
&lt;Identity id=""103"" auth_type=""None"" provides=set([])&gt;
127.0.0.1 - - [30/Jul/2013 16:24:06] ""GET /login HTTP/1.1"" 200 -
&lt;flask_login.AnonymousUserMixin object at 0x03342E90&gt;
&lt;Identity id=""103"" auth_type=""None"" provides=set([])&gt;


As you see I got current_user pointing to valid Employee instance (class) and identity id=103, but then immediately it becamse flask_login.AnonymousUserMixin for some reason and then the auth system passes that user and don't allow me to open /projects/10 URL.

Any Ideas what is wrong? And Why I'm getting that many signals - according to the code they should happen only on successfull login. What do I miss?

Source code:

# flask-principal
principals = Principal()
normal_role = RoleNeed('normal')
normal_permission = Permission(normal_role)
admin_permission  = Permission(RoleNeed('admin'))
principals._init_app(app)

login_manager    = LoginManager()
login_manager.init_app(app)

@login_manager.user_loader
def load_user(userid):
    return mysqlsess.query(Employee).get(userid)


@app.route(""/"")
@app.route(""/dashboard"")
def vDashboard():
    return render_template('dashboard.html')

@app.route('/projects')
def vPojects():
    return ""Projects""


@app.route('/projects/&lt;ID&gt;')
@admin_permission.require(http_exception=403)
def vProject(ID):
    return current_user.roles[1]

# somewhere to login    
@app.route('/login', methods=['GET', 'POST'])
def login():
    # A hypothetical login form that uses Flask-WTF
    form = LoginForm()

    # Validate form input
    if form.validate_on_submit():
        # Retrieve the user from the hypothetical datastore
        user = mysqlsess.query(Employee).get(form.email.data)

        # Compare passwords (use password hashing production)
        if form.password.data == str(user.ID):
            # Keep the user info in the session using Flask-Login
            login_user(user)
            # Tell Flask-Principal the identity changed
            identity_changed.send(app,
                                  identity=Identity(user.ID))
            return redirect(session['redirected_from'] or '/')
        else:
            return abort(401)

    return render_template('login.html', form=form)


# somewhere to logout
@app.route(""/logout"")
def logout():
    logout_user()

    for key in ['identity.name', 'identity.auth_type', 'redirected_from']:
        try:
            del session[key]
        except:
            pass
    return Response('&lt;p&gt;Logged out&lt;/p&gt;')


# handle login failed
@app.errorhandler(401)
def page_not_found(e):
    return Response('&lt;p&gt;Login failed&lt;/p&gt;')


@app.errorhandler(403)
def page_not_found(e):
    session['redirected_from'] = request.url
    return redirect(url_for('login'))


@identity_loaded.connect_via(app)
def on_identity_loaded(sender, identity):
        identity.user = current_user
        print identity.user


        if hasattr(current_user, 'ID'):
            identity.provides.add(UserNeed(current_user.ID))

        if hasattr(current_user, 'roles'):
            for role in current_user.roles:
                identity.provides.add(RoleNeed(role))

        print identity


class LoginForm(Form):
    email = TextField()
    password = PasswordField()

if __name__ == ""__main__"":
    app.run()


And my Employee SQLAlchemy class

class Employee(Base):

__tablename__  = ""Employees""

# Properties
ID           = Column(BigInteger,   primary_key=True)
name         = Column(VARCHAR(255), nullable=False)
created      = Column(DateTime,     nullable=False, default=datetime.now())
updated      = Column(DateTime)
deleted      = Column(DateTime)
branchID     = Column(BigInteger,   ForeignKey('Branches.ID'),    nullable=False)
departmentID = Column(BigInteger,   ForeignKey('Departments.ID'), nullable=False)
utilization  = Column(SmallInteger, nullable=False, default=1)
statusID     = Column(Enum('active', 'fired', 'vacation'), default='active')
birthday     = Column(Date)

# Relationships
Branch       = relationship(""Branch"")
Department   = relationship(""Department"")
ProjectStat  = relationship(""ProjectStat"",  lazy=""dynamic"")

roles        = [""admin"", ""manager""]

# Methods
def zzz(self):
    session = object_session(self)

    stats = self.ProjectStat.filter(and_(ProjectStat.metricID=='hb', ProjectStat.metricValue&gt;=6)).all()
    for s in stats:
        print s.metricValue

# Constructor
def __init__(self, ID, name):
    self.ID   = ID
    self.name = name

# Friendly Print
def __repr__(self):
    return ""&lt;Employee('%s','%s')&gt;"" % (self.ID, self.name)

def is_active(self):
    return True

def get_id(self):
    return unicode(self.ID)

def is_authenticated(self):
    return True

def is_anonymous(self):
    return False

",3,2099,"You need to instantiate principle after login.

This is a repeat question, see here Flask Login and Principal - current_user is Anonymous even though I'm logged in
",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/48462859,"SQLAlchemy selects give different results on SQLite table, raw sql versus selectable","While reading SQLite tables using pandas and dask, I came across some unexpected behavior of SQLAlchemy when selecting from SQLite tables with datetimes (ISO formatted strings) stored as NUMERIC data type. An SQLAlchemy raw SQL query works fine, but a query using a selectable constructed from reflection fails. The two queries appear to be equivalent.

I have pasted an example below, along with the traceback. Can someone explain what is wrong with the third query in the example?

Set up table with NUMERIC datetime:

import sqlalchemy as sa
from sqlalchemy import text

connString = ""sqlite:///c:\\temp\\test.db""
engine = sa.create_engine(connString)
conn = engine.connect()
conn.execute(""create table testtable (uid INTEGER Primary Key, datetime NUMERIC)"")
conn.execute(""insert into testtable values (1, '2017-08-03 01:11:31')"")
print(conn.execute('PRAGMA table_info(testtable)').fetchall())
# [(0, 'uid', 'INTEGER', 0, None, 1), (1, 'datetime', 'NUMERIC', 0, None, 0)]


Query with raw SQL works:

resultList1 = conn.execute(""SELECT testtable.uid, testtable.datetime \nFROM testtable"").fetchall()
print(resultList1)
# [(1, '2017-08-03 01:11:31')]


Query with this selectable works:

resultList2 = conn.execute(sa.sql.select(columns=[text('uid'),text('datetime')]).select_from(text('testtable'))).fetchall() 
print(resultList2)
# [(1, '2017-08-03 01:11:31')]


Query with this selectable fails:

m = sa.MetaData()
table = sa.Table('testtable', m, autoload=True, autoload_with=engine)
selectble = sa.sql.select(table.columns).select_from(table)
print(selectble.compile().string)
#  note: same raw sql query as above
# ""SELECT testtable.uid, testtable.datetime \nFROM testtable""

resultList3 = conn.execute(sa.sql.select(table.columns).select_from(table)).fetchall()
# SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively...
print(resultList3)

conn.close()


The error:

Traceback (most recent call last):

  File ""&lt;ipython-input-20-188c84a35d95&gt;"", line 1, in &lt;module&gt;
    print(resultList3)

  File ""c:\program files\python36\lib\site-packages\sqlalchemy\engine\result.py"", line 156, in __repr__
    return repr(sql_util._repr_row(self))

  File ""c:\program files\python36\lib\site-packages\sqlalchemy\sql\util.py"", line 329, in __repr__
    "", "".join(trunc(value) for value in self.row),

TypeError: must be real number, not str

",3,1302,"SQLite has a very different type system from most SQL databases: it uses dynamic typing, and after conversion the typename you give a column determines its affinity, such as NUMERIC:


  A column with NUMERIC affinity may contain values using all five storage classes. When text data is inserted into a NUMERIC column, the storage class of the text is converted to INTEGER or REAL (in order of preference) if such conversion is lossless and reversible. For conversions between TEXT and REAL storage classes, SQLite considers the conversion to be lossless and reversible if the first 15 significant decimal digits of the number are preserved. If the lossless conversion of TEXT to INTEGER or REAL is not possible then the value is stored using the TEXT storage class. No attempt is made to convert NULL or BLOB values.


Since you've inserted values for which a (lossless) conversion to INTEGER or REAL1 is not possible, your values use the TEXT storage class, and SQLAlchemy/pysqlite is unhappy since it on the other hand expected values that it can convert to float, which fails.

The typing system causes other similar issues, such as when reflecting the resulting table from a CREATE TABLE ... AS against a SELECT from a table using DATETIME typename, which is converted to NUMERIC affinity.

A shorter code example that demonstrates the issue:

In [2]: foo = Table('foo', metadata, Column('bar', NUMERIC))

In [3]: foo.create(engine)
CREATE TABLE foo (
        bar NUMERIC
)

In [4]: engine.execute(""insert into foo values ('not really a number, no')"")
Out[4]: &lt;sqlalchemy.engine.result.ResultProxy at 0x7fbcd7ee8f98&gt;

In [5]: foo.select().execute().fetchall()
Out[5]: ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
  ...
~/Work/SO/lib/python3.6/site-packages/sqlalchemy/sql/util.py in __repr__(self)
    327         trunc = self.trunc
    328         return ""(%s%s)"" % (
--&gt; 329             "", "".join(trunc(value) for value in self.row),
    330             "","" if len(self.row) == 1 else """"
    331         )

TypeError: must be real number, not str


1 Probably the reason why the sqlite+pysqlite dialect does not support Decimal natively  neither does SQLite
",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/62170836,How do I fix this Elastic Beanstalk error when deploying a Flask app: Python 3.7.0 was not found on your system,"I am trying to deploy my Flask app using the EB CLI following this official AWS tutorial. I receive the error 'Create environment operation is complete, but with errors'. Observing the environment logs, the server seems to be lacking the correct Python version.

Below are the EB environment details, my PipFile, the EB CLI output and the error in the deploy logs.

How can I fix this problem?

Environment details:

 Application name: server_eb
  Region: eu-west-2
  Deployed Version: XXX
  Environment ID: XXX
  Platform: arn:aws:elasticbeanstalk:eu-west-2::platform/Python 3.7 running on 64bit Amazon Linux 2/3.0.1
  Tier: WebServer-Standard-1.0
  CNAME: UNKNOWN
  Updated: 2020-06-02 16:53:10.321000+00:00


PipFile:

[[source]]
name = ""pypi""
url = ""https://pypi.org/simple""
verify_ssl = true

[dev-packages]

[packages]
flask = ""~=1.1.1""
python-dotenv = ""~=0.13.0""
psycopg2 = ""~=2.8.5""
flask-sqlalchemy = ""~=2.4.1""
flask-migrate = ""~=2.5.3""
flask-dance = {extras = [""sqla""],version = ""~=3.0.0""}
flask-login = ""~=0.4.1""
blinker = ""~=1.4""
flask-session = ""==0.3.1""
redis = ""==3.4.1""

[scripts]
migrate=""flask db upgrade""
start=""flask run --cert=cert.pem --key=key.pem""

[requires]
python_version = ""3.7.0""


CLI output:

2020-06-02 16:54:55    ERROR   [Instance: XXX] Command failed on instance. An unexpected error has occurred [ErrorCode: 0000000001].
2020-06-02 16:54:55    INFO    Command execution completed on all instances. Summary: [Successful: 0, Failed: 1].
2020-06-02 16:55:57    ERROR   Create environment operation is complete, but with errors. For more information, see troubleshooting documentation.

ERROR: ServiceError - Create environment operation is complete, but with errors. For more information, see troubleshooting documentation.


Logs: 

2020/06/02 16:54:55.030996 [ERROR] An error occurred during execution of command [app-deploy] - [SetUpPythonEnvironment]. Stop running the command. Error: fail to install Gunicorn with error Command /bin/sh -c python3 -m pipenv install gunicorn --skip-lock failed with error exit status 1. Stderr:Warning: the environment variable LANG is not set!
We recommend setting this in ~/.profile (or equivalent) for proper expected behavior.
Warning: Python 3.7.0 was not found on your system
You can specify specific versions of Python with:
  $ pipenv --python path/to/python

",1,993,"Based on the comments, the issue was that the EB uses Python 3.7.6, while the OP's dependencies required version 3.7.0. 

The solution was to change the dependency to Python 3.7.6.

The current python version running on EB can be found here:


  64bit Amazon Linux 2 v3.0.1 running Python 3.7 - Python 3.7.6

",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/66656355,How to access only filtered objects when filtering on joined tables in sqlalchemy,"I am using sqlalchemy for the first time and I have an unexpected behavior in reading results of a query.
Let's say these are my database models:
class Parent(Base):
    __table__ = ""parents""
    id = Column(Integer, primary_key=True)
    children = relationship(""Child"", back_populates=""parent"")

class Child(Base):
    __table__ = ""children""
    id = Column(Integer, primary_key=True)
    parent_id = Column(Integer, ForeignKey(""parents.id""))
    did_homework = Column(Boolean, default=False)
    parent = relationship(""Parent"", back_populates=""children"")

When I query the database like this:
parents = session.query(Parent).join(Parent.children).filter(Child.did_homework == True).all()
good_children = parents[0].children

I expected all good_children to have did_homework set to True.
But it seems the query returns all parents which have at least one Child with did_homework set to True and then when I access parents[0].children I get all children of that parent, not just the ones that satisfy my criteria.
I have tried querying like this:
result = (
        db.query(Parent, Child)
        .filter(Parent.id == Child.parent_id)
        .filter(Child.did_homework == True)
        .all()
    )

which gives the expected tuples as the result (every Child in the tuple has did_homework == True), but every Parent in the tuple still has all children, not just the ones that did their homework.
Question:
Is there a way of writing a query that would assign only the children which are not filtered out to the parent they belong to?
So that when I write: good_children = parents[0].children I could be sure all objects in good_children have did_homework set to True.
",1,202,"After posting this I found the same question here with an answer that didn't work for me, but with a small modification I managed to do it:
parents = (
        db.query(Parent)
        .join(Child)
        .filter(Child.did_homework == True)
        .options(contains_eager(""children""))
        .all()
    )

",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/53726041,SQLAlchemy not committing update,"I am get some unexpected behavior when trying to commit an update with SQLalchemy..

I have an object like so..

updatedINfo = {'id': 1, 'type': 'NewData'}
# The ID is an id from the table, the 'type' is a column from that table
# The 'NewData' is what needs to replace whatever is currently stored in
# 'type' column.


Because I want this function to be reusble and not have to hardcode the column names, I figured I could do something like this;

# Find the event first
the_event = db.query(EventsPending).filter(EventsPending.id == updatedInfo.get('id')).one_or_none()

# If it returns None, something went wrong
if not the_event:
    return {'Error': 'Something went wrong..'}

# Take the id out of the JSON post
del updatedInfo['id']

# Turn our db query into a dict
event_dict = the_event.as_dict()

# Set the keys for cross checking
for key in event_dict.keys():
    if key in updatedInfo.keys():
        the_event.__dict__[key] = updatedInfo.get(key)
        print(the_event.type)
        # THIS PRINTS THE UPDATED TYPE
        print(the_event.as_dict())
        # THIS ALSO PRINTS THE UPDATED TYPE

       db.commit()
       print(the_event.type)
       # HOWEVER THIS PRINTS THE OLD TYPE AFTER THE COMMIT
       # The commit does not make it to the database
       # I see no errors, and logging shows no useful info


Obviously I can work around this by coding in the different types but really seems ugly and inefficient. Any help is appreciated.
",0,211,"For anyone running into this.. You need

setattr(the_event, key, data)
# the_event being the sqlalchemy object
# key being the column name
# data being the new value


Hope this helps someone.
",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/76536770,SQLAlchemy: Understanding Select statements within transactions,"I'm trying to understand the behavior of SQLAlchemy Sessions and their relations to PostgreSQL transactions, as they don't seem to behave quite the same. I've created a simple program to insert, update, and subsequently update a row:
engine = sa.create_engine(f""postgresql+psycopg2://{pg_user}:@{pg_host}:{pg_port}/{pg_db}"", echo=True)
Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)
    name = Column(String)

Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine, expire_on_commit=True)
session = Session()

# Add a user
u1 = User(id=1, name=""u1"")
session.add(u1)
session.commit()

# Modify user's name
session.query(User).filter_by(id=1).update({""name"": ""u2""})
session.commit()

# Get user's name
user = session.query(User).filter_by(id=1).one()
print(user.name) # prints ""u2"" as expected

However, if I now remove the session.commit() statement between the modification and select, I get an unexpected result:
# ...

# Modify user's name
session.query(User).filter_by(id=1).update({""name"": ""u2""})

# Get user's name
user = session.query(User).filter_by(id=1).one()
print(user.name) # prints ""u1"" -- NOT expected!

Presumably this is because the session is querying the database, in which the user's name has not been updated (because the transaction has not yet been committed). However, I would have thought that because I'm using the same session object to query the user, it should recognize the changes that have occurred on the User object within the transaction.
What am I not understanding here?
",0,106,"PostgreSQL uses ""Read Committed"" isolation level. This means that the each query being executed sees data committed earlier.
So, when you are making changes without ""session.commit()"", the changes are not made in the database, so the queries does not see those changes. Once you have committed the changes, the changes are made in database and upcoming queries within same session will see updated data.
To avoid ""session.commit()"" after every query, you can set ""autocommit"" to true.
",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/77024615,Redshift Sqlalchemy transactions and session management,"Database - Redshift
SQLAlchemy version - 1.4.2
I'm trying to create a re-usable query using the SQLAlchemy ORM. What I do is something like this.
def get_page(query, page_size, page_number):
    return query.limit(page_size).offset((page_number - 1) * page_size).all()

def my_query(self):
    with Session(engine) as session:
        query = session.query(my_model).order_by(
            my_model.id
        )
        return query

query = my_query()

while True:
    page_number = 1
    results = get_page(query, 100, page_number)
    if not results:
        break
    for result in results:
        print(result)
        
    page_number+=1

I'm experiencing unexpected behavior using this method on very large data sets. Where it will miss a bunch of entries, or will repeat some of the entries it has already done.  (entry A could appear on page 1 and 4 for some reason)
I'm assuming my problem is that every time i get a page and call .all() it is performing a brand new query so if new data is put onto the table it will now effect my results.
This is a watered down example of my real world use case.
The real scenarios query is a left join on two tables.
Table X (a join table) and table Y (The data to act on)
I join table Y to table X and find all of Y that does not have a corresponding X entry.
I iterate through my results using the method above and after 500 entries are processed i create 500 join entries on table X that reference the processed table Y entry.(keeping track of what data has been processed this way)
What I want to have happen is have all the results from my query -&gt; Stream them by chunks of 100, process them, and be able to perform bulk inserts onto the join table without effecting my initial query results or committing my query session(resulting in me requerying and ending up with different results). (meaning if the data changes it is not changed in my results of the initial query)
Tell me what is wrong with my method?
",0,84,"For this to be happening these different queries would need to be in different transactions (xid).  Can you confirm this is what is happening?  (I say this b/c if they happen in the same transaction you should get the same version of the tables.  And it sounds like from your description that this isn't just result ordering differences.)
So the first solution would be to have everything happen in a single transaction (if possible).  The best way to run this would be with a cursor to store the results between fetches.  See: https://docs.aws.amazon.com/redshift/latest/dg/declare.html and https://docs.aws.amazon.com/redshift/latest/dg/fetch.html
If it isn't possible to be in a single transaction, then can it be in a single session?  This way you could keep the results in a temp table and read from there.
If these queries absolutely need to be made across separate connections then you will need to set up a perm table to hold results.  This has the disadvantage of needing to be cleaned up when the process ends.  You will want to add some per iteration random chars to the table name so that no iteration collisions can happen on the table name.
I'd start by examining the system tables for these queries on Redshift and note the pid and xid of each.  This will give you more specific information on how SQLAlchemy is interacting with Redshift and what changes are needed.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/14719507,Unit tests for Query in SQLAlchemy,"How does one go about testing queries in SQLAlchemy? For example suppose we have this models.py

from sqlalchemy import (
        Column,
        Integer,
        String,
)
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class Panel(Base):
    __tablename__ = 'Panels'

    id = Column(Integer, primary_key=True)
    category = Column(Integer, nullable=False)
    platform = Column(String, nullable=False)
    region = Column(String, nullable=False)

    def __init__(self, category, platform, region):
        self.category = category
        self.platform = platform
        self.region = region


    def __repr__(self):
        return (
            ""&lt;Panel('{self.category}', '{self.platform}', ""
            ""'{self.region}')&gt;"".format(self=self)
        )


and this tests.py

import unittest

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from models import Base, Panel


class TestQuery(unittest.TestCase):

    engine = create_engine('sqlite:///:memory:')
    Session = sessionmaker(bind=engine)
    session = Session()

    def setUp(self):
        Base.metadata.create_all(self.engine)
        self.session.add(Panel(1, 'ion torrent', 'start'))
        self.session.commit()

    def tearDown(self):
        Base.metadata.drop_all(self.engine)

    def test_query_panel(self):
        expected = [Panel(1, 'ion torrent', 'start')]
        result = self.session.query(Panel).all()
        self.assertEqual(result, expected)


When we try running the test, it fails, even though the two Panels look identical.

$ nosetests
F
======================================================================
FAIL: test_query_panel (tests.TestQuery)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/clasher/tmp/tests.py"", line 31, in test_query_panel
    self.assertEqual(result, expected)
AssertionError: Lists differ: [&lt;Panel('1', 'ion torrent', 's... != [&lt;Panel('1', 'ion torrent', 's...

First differing element 0:
&lt;Panel('1', 'ion torrent', 'start')&gt;
&lt;Panel('1', 'ion torrent', 'start')&gt;

  [&lt;Panel('1', 'ion torrent', 'start')&gt;, &lt;Panel('2', 'ion torrent', 'end')&gt;]

----------------------------------------------------------------------
Ran 1 test in 0.063s

FAILED (failures=1)


One solution I've found is to make a query for every single instance I expect to find in the query:

class TestQuery(unittest.TestCase):

    ...

    def test_query_panel(self):
        expected = [
            (1, 'ion torrent', 'start'),
            (2, 'ion torrent', 'end')
        ]
        successful = True
        # Check to make sure every expected item is in the query
        try:
            for category, platform, region in expected:
                self.session.query(Panel).filter_by(
                        category=category, platform=platform,
                        region=region).one()
        except (NoResultFound, MultipleResultsFound):
            successful = False
        self.assertTrue(successful)
        # Check to make sure no unexpected items are in the query
        self.assertEqual(self.session.query(Panel).count(),
                         len(expected))


This strikes me as pretty ugly, though, and I'm not even getting to the point where I have a complex filtered query that I'm trying to test. Is there a more elegant solution, or do I always have to manually make a bunch of individual queries?
",40,48388,"your original test is on the right track, you just have to do one of two things: either make sure that two Panel objects of the same primary key identity compare as True:
import unittest

from sqlalchemy import create_engine
from sqlalchemy.orm import Session

from database.models import Base

class Panel(Base):
    # ...

    def __eq__(self, other):
        return isinstance(other, Panel) and other.id == self.id

or you can organize your test such that you make sure you're checking against the same Panel instance (because here we take advantage of the identity map):
class TestQuery(unittest.TestCase):
    def setUp(self):
        self.engine = create_engine('sqlite:///:memory:')
        self.session = Session(self.engine)
        Base.metadata.create_all(self.engine)
        self.panel = Panel(1, 'ion torrent', 'start')
        self.session.add(self.panel)
        self.session.commit()

    def tearDown(self):
        Base.metadata.drop_all(self.engine)

    def test_query_panel(self):
        expected = [self.panel]
        result = self.session.query(Panel).all()
        self.assertEqual(result, expected)

as far as the engine/session setup/teardown, I'd go for a pattern where you use a single engine for all tests, and assuming your schema is fixed, a single schema for all tests, then you make sure the data you work with is performed within a transaction that can be rolled back.   The Session can be made to work this way, such that calling commit() doesn't actually commit the ""real"" transaction, by wrapping the whole test within an explicit Transaction.  The example at https://docs.sqlalchemy.org/en/latest/orm/session_transaction.html#joining-a-session-into-an-external-transaction-such-as-for-test-suites illustrates this usage.    Having a "":memory:"" engine on every test fixture will take up a lot of memory and not really scale out to other databases besides SQLite.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/42681231,SQLAlchemy: unexpected results when using `and` and `or`,"I have a declarative base class News:

class News(Base):
    __tablename__ = ""news""
    id = Column(Integer, primary_key = True)
    title = Column(String)
    author = Column(String)
    url = Column(String)
    comments = Column(Integer)
    points = Column(Integer)
    label = Column(String)


I also have a function f(title), that gets a string and returns one of 3 variants of strings: 'good', 'maybe' or 'never'. 
I try to get filtered rows:

rows = s.query(News).filter(News.label == None and f(News.title) == 'good').all()


But the program fails, raising this error:

raise TypeError(""Boolean value of this clause is not defined"")


How can I resolve it? 
",25,32145,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/70104873,how to access relationships with async sqlalchemy?,"import asyncio

from sqlalchemy import Column
from sqlalchemy import DateTime
from sqlalchemy import ForeignKey
from sqlalchemy import func
from sqlalchemy import Integer
from sqlalchemy import String
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.ext.asyncio import create_async_engine
from sqlalchemy.future import select
from sqlalchemy.orm import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.orm import selectinload
from sqlalchemy.orm import sessionmaker

engine = create_async_engine(
        ""postgresql+asyncpg://user:pass@localhost/db"",
        echo=True,
    )


# expire_on_commit=False will prevent attributes from being expired
# after commit.
async_session = sessionmaker(
    engine, expire_on_commit=False, class_=AsyncSession
)


Base = declarative_base()

class A(Base):
    __tablename__ = ""a""

    id = Column(Integer, primary_key=True)
    name = Column(String, unique=True)
    data = Column(String)
    create_date = Column(DateTime, server_default=func.now())
    bs = relationship(""B"")

    # required in order to access columns with server defaults
    # or SQL expression defaults, subsequent to a flush, without
    # triggering an expired load
    __mapper_args__ = {""eager_defaults"": True}


class B(Base):
    __tablename__ = ""b""
    id = Column(Integer, primary_key=True)
    a_id = Column(ForeignKey(""a.id""))
    data = Column(String)
    
    
       

async with engine.begin() as conn:
    await conn.run_sync(Base.metadata.drop_all)
    await conn.run_sync(Base.metadata.create_all)


async with async_session() as session:
    async with session.begin():
        session.add_all(
            [
                A(bs=[B(), B()], data=""a1""),
                A(bs=[B()], data=""a2""),
            ]
        )


async with async_session() as session:
    result = await session.execute(select(A).order_by(A.id))
    a1 = result.scalars().first()

    # no issue: 
    print(a1.name, a1.data)

    # throws error:
    print(a1.bs)
    

Trying to access a1.bs  gives this error:
     59     current = greenlet.getcurrent()
     60     if not isinstance(current, _AsyncIoGreenlet):
---&gt; 61         raise exc.MissingGreenlet(
     62             ""greenlet_spawn has not been called; can't call await_() here. ""
     63             ""Was IO attempted in an unexpected place?""

MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/14/xd2s)



",18,11578,"This is how:
from sqlalchemy.orm import selectinload

async with async_session() as session:
    result = await session.execute(select(A).order_by(A.id)
                                            .options(selectinload(A.bs)))
    a = result.scalars().first()

    print(a.bs)


key is using the selectinload method to prevent implicit IO
UPDATE
There are a few alternatives to selectinload like joinedload, lazyload. I am still trying to understand the differences.
","muons answer is correct if you want eager loading (which is better).
But if for some reason you already have loaded your model and later want to load a relationship, there is a way starting with SQLAlchemy 2.0.4:
Using session.refresh, you can tell it to load a1.bs:
await session.refresh(a1, attribute_names=[""bs""])
print(a1.bs)  # This works

From the docs:

New in version 2.0.4: Added support for AsyncSession.refresh() and the underlying Session.refresh() method to force lazy-loaded relationships to load, if they are named explicitly in the Session.refresh.attribute_names parameter.

","There is an additional way of accessing the relationship attributes in SQLAlchemy v2, for the case where you have already loaded your model and later want to load a relationship.
As the documentation says, you can use AsyncAttrs: ""when added to a specific class or more generally to the Declarative Base superclass, provides an accessor AsyncAttrs.awaitable_attrs which delivers any attribute as an awaitable"". So you can create a Declarative Base class like this:
class Base(AsyncAttrs, DeclarativeBase):
    pass

And then use awaitable_attrs to access the relationship that must be previously loaded:
a1_bs = await a1.awaitable_attrs.bs
print(a1_bs)

"
SQLAlchemy unexpected result,https://stackoverflow.com/questions/40681371,Bug in SQLAlchemy Rollback after DB Exception?,,18,1040,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/68195361,How to properly handle many to many in async sqlalchemy?,"I was trying to implement many to many relationship between tables.
When I use backpopulates all tags for a specific user must be in the tags field.
The tables are successfully created.
Users and tags are added.
Link table too.
import asyncio
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.util import await_only, greenlet_spawn

from sqlalchemy import Column, Table, ForeignKey
from sqlalchemy.orm import declarative_base, relationship
from sqlalchemy.dialects.postgresql import VARCHAR, INTEGER

Base = declarative_base()

user_tag = Table('user_tag', Base.metadata,
                 Column('user_id', INTEGER, ForeignKey('users.id')),
                 Column('tag_id', INTEGER, ForeignKey('tags.id'))
                 )


class User(Base):
    __tablename__ = 'users'
    id = Column(INTEGER, primary_key=True)
    name = Column(VARCHAR(32), nullable=False, unique=True)
    tags = relationship(""Tag"",
                        secondary=user_tag,
                        back_populates=""users"")


class Tag(Base):
    __tablename__ = 'tags'
    id = Column(INTEGER, primary_key=True)
    tag = Column(VARCHAR(255), nullable=False, unique=True)
    users = relationship(""User"",
                         secondary=user_tag,
                         back_populates=""tags"")


async def main():
    engine = create_async_engine(
        ""postgresql+asyncpg://postgres:pgs12345@localhost/test"",
        echo=False,
    )

    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)
        await conn.run_sync(Base.metadata.create_all)

    users = [User(name=""p1""), User(name=""p2""), User(name=""p3"")]
    tags = [Tag(tag=""tag1""), Tag(tag=""tag2""), Tag(tag=""tag3"")]

    async with AsyncSession(engine) as session:
        async with session.begin():
            session.add_all(users)
            session.add_all(tags)

        for user in users:
            await session.refresh(user)
        for tag in tags:
            await session.refresh(tag)

        for user in users:
            for i in range(3, user.id - 1, -1):
                await session.execute(user_tag.insert().values(user_id=user.id, tag_id=i))
        await session.commit()

        for user in users:
            await session.refresh(user)
        for tag in tags:
            await session.refresh(tag)

        tags = await greenlet_spawn(users[0].tags)
        print(tags)


loop = asyncio.get_event_loop()
loop.run_until_complete(main())

When I run the program, it crashes with:
 File ""C:\Sources\asyncSQLAl test\main.py"", line 48, in &lt;module&gt;
    loop.run_until_complete(main())
  File ""C:\Users\Stanislav\AppData\Local\Programs\Python\Python39\lib\asyncio\base_events.py"", line 
642, in run_until_complete
    return future.result()
  File ""C:\Sources\asyncSQLAl test\main.py"", line 41, in main
    tags = await greenlet_spawn(await users[0].tags)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\attributes.py"", line 480, in __get__
    return self.impl.get(state, dict_)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\attributes.py"", line 931, in get
    value = self.callable_(state, passive)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\strategies.py"", line 879, in _load_for_state
    return self._emit_lazyload(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\strategies.py"", line 1036, 
in _emit_lazyload
    result = session.execute(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1689, in 
execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1582, in 
_execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\sql\lambdas.py"", line 481, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1451, in 
_execute_clauseelement
    ret = self._execute_context(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1813, in 
_execute_context
    self._handle_dbapi_exception(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1998, in 
_handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1770, in 
_execute_context
    self.dialect.do_execute(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 717, in do_execute
    cursor.execute(statement, parameters)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py"", line 449, in execute
    self._adapt_connection.await_(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\util\_concurrency_py3k.py"", line 60, in await_only
    raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)      
sys:1: RuntimeWarning: coroutine 'AsyncAdapt_asyncpg_cursor._prepare_and_execute' was never awaited

I don't quite understand how greenlet_spawn works here and where it should be used in this example.
For example, same program, but in sync style
from sqlalchemy import Column, Table, ForeignKey
from sqlalchemy.orm import declarative_base, relationship, sessionmaker
from sqlalchemy import create_engine
from sqlalchemy.dialects.postgresql import VARCHAR, INTEGER

Base = declarative_base()

user_tag = Table('user_tag', Base.metadata,
                 Column('user_id', INTEGER, ForeignKey('users.id')),
                 Column('tag_id', INTEGER, ForeignKey('tags.id'))
                 )


class User(Base):
    __tablename__ = 'users'
    id = Column(INTEGER, primary_key=True)
    name = Column(VARCHAR(32), nullable=False, unique=True)
    tags = relationship(""Tag"",
                        secondary=user_tag,
                        back_populates=""users"")


class Tag(Base):
    __tablename__ = 'tags'
    id = Column(INTEGER, primary_key=True)
    tag = Column(VARCHAR(255), nullable=False, unique=True)
    users = relationship(""User"",
                         secondary=user_tag,
                         back_populates=""tags"")
    
    def __str__(self):
        return self.tag


def main():
    engine = create_engine(
        ""postgresql+psycopg2://postgres:pgs12345@localhost/test"",
        echo=False,
    )

    Base.metadata.drop_all(engine)
    Base.metadata.create_all(engine)

    Session = sessionmaker(bind=engine)
    session = Session()

    users = [User(name=""p1""), User(name=""p2""), User(name=""p3"")]
    tags = [Tag(tag=""tag1""), Tag(tag=""tag2""), Tag(tag=""tag3"")]

    with session.begin():
        session.add_all(users)
        session.add_all(tags)

    for user in users:
        for i in range(3, user.id - 1, -1):
            session.execute(user_tag.insert().values(
                user_id=user.id, tag_id=i))
    session.commit()

    for tag in users[0].tags:
        print(tag, end="" "")

main()

Gives me:
tag1 tag2 tag3 

",8,7537,"I've been stuck on this today too and I've narrowed it down to the fact that a lazyload is attempted, which GreenLet is not happy about. I wasn't sure whether this was just my lack of skill but I've found this article that details some of the common errors:
https://matt.sh/sqlalchemy-the-async-ening, where it's mentioned that this very issue will occur in this way. Furthermore, the docs go into detail about needing to avoid lazyloading: https://docs.sqlalchemy.org/en/14/orm/extensions/asyncio.html.
My solution at the moment is to effectively prefetch the Child relation upon the initial query of the Parent object and then manipulate it from there. Whether this is a true bug, in the sense that it should work in async when it already works in sync or simply a limitation of the async method, I've no idea.
Edit 06/08/21, here is how I am prefetching relationships:
import sqlalchemy as sa
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload
from . import models

async def get_parent_prefetch_children(db: AsyncSession, parent_id: int) -&gt; models.Parent:
    result = await db.execute(
        sa.select(models.Parent).where(models.Parent.id == parent_id).options(
            selectinload(models.Parent.children)
        )
    )
    return result.scalar()

In your case, you call users[0].tags, which makes a lazyload and fails. In order for you to avoid this, you must refetch users with their tags eagerly loaded.
","From their official doc for relationship, you can use lazy=""joined"" to ask the SQLAlchemy do prefetch for you on session.refresh(...) or session.execute(select(...)). This will do joined style eagerly load at the time you query for the object. Note that this may introduce performance issue to your application since sometime you are not going to access the foreign'd objects while the database will still do joined style query for you.
tags = relationship(""Tag"",
    secondary=user_tag,
    back_populates=""users"",
    lazy=""joined"")

The results and SQL stmt looks like:
&gt;&gt;&gt;tags = users[0].tags
&gt;&gt;&gt;print(tags)
[&lt;__main__.Tag object at 0x10bc68610&gt;, &lt;__main__.Tag object at 0x10bc41d00&gt;, &lt;__main__.Tag object at 0x10bc12b80&gt;]
&gt;&gt;&gt;from sqlalchemy.future import select
&gt;&gt;&gt;print(select(User))
FROM users LEFT OUTER JOIN (user_tag AS user_tag_1 JOIN tags AS tags_1 ON tags_1.id = user_tag_1.tag_id) ON users.id = user_tag_1.user_id

",
SQLAlchemy unexpected result,https://stackoverflow.com/questions/67897872,Exception has occurred: MissingGreenlet whe making connection query in sql alchemy 1.4,"I'm trying to do a simple query with SQLAlchemy 1.4.17 from within pytest
def test_first():
    engine = create_engine(settings.SQLALCHEMY_DATABASE_URI)
    result = engine.execute(text(""SELECT email FROM user""))

but am getting this error
Exception has occurred: MissingGreenlet
greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)
  File ""/Users/mattc/Development/inference/server/inference_server/app/tests/test_01_user.py"", line 27, in test_first
    result = engine.execute(text(""SELECT email FROM user""))

and do not know why? Any suggestions?
",8,20905,"You are trying to use an async connector package in the same way as a synchronous connector
&gt;&gt;&gt; import sqlalchemy as sa
&gt;&gt;&gt; engine = sa.create_engine('postgresql+asyncpg:///')
&gt;&gt;&gt; res = engine.execute(sa.text('SELECT 1'))
&lt;stdin&gt;:1: RemovedIn20Warning: The Engine.execute() method is considered legacy as of the 1.x series of SQLAlchemy and will be removed in 2.0. All statement execution in SQLAlchemy 2.0 is performed by the Connection.execute() method of Connection, or in the ORM by the Session.execute() method of Session. (Background on SQLAlchemy 2.0 at: http://sqlalche.me/e/b8d9)
Traceback (most recent call last):
...
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)

You need to either use a synchronous connector, for example psycopg2, pg8000, or write async code:
import sqlalchemy as sa

import asyncio

from sqlalchemy.ext.asyncio import create_async_engine


async def async_main():
    engine = create_async_engine(
        ""postgresql+asyncpg:///test"", echo=True,
    )

    async with engine.connect() as conn:

        # select a Result, which will be delivered with buffered
        # results
        result = await conn.execute(sa.text('select email from users'))

        print(result.fetchall())
    await engine.dispose()


asyncio.run(async_main())

",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/28901050,Default value doesn&#39;t work in SQLAlchemy + PostgreSQL + aiopg + psycopg2,,4,1565,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/71116549,sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) with PostgreSQL,"I searched for this error a lot, but I only find some with more information behind that like ""FATAL: ..."". Mine has none. It only says
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) 

I have a postgres database inside a docker container that has set his port to the standard 5432.
I've created the container with the following command:
docker run --name some-postgres -e POSTGRES_PASSWORD=mysecretpassword -p 5432:5432 -d postgres

It is clean so no database created. The API should automatically create them.
I'm using Pycharm IDE, maybe it has something todo with that.
Traceback (most recent call last):
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\protocols\http\h11_impl.py"", line 373, in run_asgi
    result = await app(self.scope, self.receive, self.send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\proxy_headers.py"", line 75, in __call__
    return await self.app(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\debug.py"", line 96, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\debug.py"", line 93, in __call__
    await self.app(scope, receive, inner_send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\applications.py"", line 208, in __call__
    await super().__call__(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\applications.py"", line 112, in __call__
    await self.middleware_stack(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\middleware\errors.py"", line 181, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\middleware\errors.py"", line 159, in __call__
    await self.app(scope, receive, _send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\exceptions.py"", line 82, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\exceptions.py"", line 71, in __call__
    await self.app(scope, receive, sender)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 580, in __call__
    await route.handle(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 241, in handle
    await self.app(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 52, in app
    response = await func(request)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\routing.py"", line 226, in app
    raw_response = await run_endpoint_function(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\routing.py"", line 159, in run_endpoint_function
    return await dependant.call(**values)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\app\routers\v1\users.py"", line 31, in create_user
    session.commit()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1428, in commit
    self._transaction.commit(_to_root=self.future)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 829, in commit
    self._prepare_impl()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 808, in _prepare_impl
    self.session.flush()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3339, in flush
    self._flush(objects)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3479, in _flush
    transaction.rollback(_capture_exception=True)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3439, in _flush
    flush_context.execute()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 209, in save_obj
    for (
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 370, in _organize_states_for_save
    for state, dict_, mapper, connection in _connections_for_states(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 1709, in _connections_for_states
    connection = uowtransaction.transaction.connection(base_mapper)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 626, in connection
    return self._connection_for_bind(bind, execution_options)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 735, in _connection_for_bind
    conn = self._parent._connection_for_bind(bind, execution_options)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 747, in _connection_for_bind
    conn = bind.connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\future\engine.py"", line 419, in connect
    return super(Engine, self).connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3194, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 96, in __init__
    else engine.raw_connection()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3273, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3243, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 2097, in _handle_dbapi_exception_noconnection
    util.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3240, in _wrap_pool_connect
    return fn()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 476, in checkout
    rec = pool._do_get()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\impl.py"", line 146, in _do_get
    self._dec_overflow()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\impl.py"", line 143, in _do_get
    return self._create_connection()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 256, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 371, in __init__
    self.__connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 666, in __connect
    pool.logger.debug(""Error on connect(): %s"", e)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\create.py"", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\psycopg2\__init__.py"", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) 
(Background on this error at: https://sqlalche.me/e/14/e3q8)

This is my error that i get. My code looks like this:
main.py:
import os
import uvicorn
if __name__ == '__main__':
    port = int(os.getenv(""PORT""))
    uvicorn.run(""main:app"", host='0.0.0.0', port=port, reload=True, debug=True, workers=3)

app/main.py:
import os
from fastapi import FastAPI
from .database import engine
from .routers import v1
engine.init_db()
port = int(os.getenv(""PORT"")) #Port is 8000
app = FastAPI()

app.include_router(v1.router, prefix=""/v1"")

app/database/engine.py: (referenced in the file above)
import os

from fastapi.security import HTTPBearer
from sqlmodel import create_engine, SQLModel, Session
DATABASE_URL = ""postgresql+psycopg2://postgres:mysecretpassword@localhost:5432""
engine = create_engine(DATABASE_URL, echo=True)
token_auth_scheme = HTTPBearer()


async def init_db():
    async with engine.begin() as conn:
        # await conn.run_sync(SQLModel.metadata.drop_all)
        await conn.run_sync(SQLModel.metadata.create_all)


async def get_session():
    session = Session(engine)
    try:
        yield session
    finally:
        session.close()

The route ""users"" inside routers/v1/users.py:
from typing import Optional

from fastapi import APIRouter, Depends, HTTPException, Query, Path, Response, status
from pydantic import ValidationError
from sqlalchemy.exc import IntegrityError
from sqlalchemy.sql.functions import concat
from sqlalchemy import func
from sqlmodel import Session, select, col
from starlette import status

from app.database import models
from app.database.authentication import VerifyToken
from app.database.engine import get_session, token_auth_scheme

router = APIRouter()


@router.post("""", status_code=status.HTTP_201_CREATED, response_model=models.UserRead,
             response_model_exclude_none=True, name=""Create User"", tags=[""users""])
async def create_user(user_data: models.UserCreate,
                      session: Session = Depends(get_session)):

    try:
        new_user = models.User(**dict(user_data))
        session.add(new_user)
        session.commit()
        session.refresh(new_user)

        return new_user
    except IntegrityError:
        session.rollback()
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT, detail=""IntegrityError"")
    except ValidationError:
        session.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, detail=""ValidationError"")

The models.User:
class UserBase(SQLModel):
    id: str
    username: Optional[str]
    country_code: Optional[str]
    phone: Optional[str]

    class Config:
        allow_population_by_field_name = True

class User(UserBase, table=True):
    __tablename__ = 'users'
    id: str = Field(primary_key=True)
    username: Optional[str] = Field(sa_column=Column('username', VARCHAR(length=50), unique=True, default=None))
    phone: Optional[str] = Field(sa_column=Column('phone', VARCHAR(length=20), unique=True, default=None))

I hope that this is everything you guys need to find something. If you need something else contact me.
Best regards
Colin
EDIT:
After I changed the link from `postgresql+psycopg2` to `postgresql+asyncpg` I get a new error:

ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\protocols\http\h11_impl.py"", line 373, in run_asgi
    result = await app(self.scope, self.receive, self.send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\proxy_headers.py"", line 75, in __call__
    return await self.app(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\debug.py"", line 96, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\debug.py"", line 93, in __call__
    await self.app(scope, receive, inner_send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\applications.py"", line 208, in __call__
    await super().__call__(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\applications.py"", line 112, in __call__
    await self.middleware_stack(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\middleware\errors.py"", line 181, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\middleware\errors.py"", line 159, in __call__
    await self.app(scope, receive, _send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\exceptions.py"", line 82, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\exceptions.py"", line 71, in __call__
    await self.app(scope, receive, sender)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 580, in __call__
    await route.handle(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 241, in handle
    await self.app(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 52, in app
    response = await func(request)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\routing.py"", line 226, in app
    raw_response = await run_endpoint_function(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\routing.py"", line 159, in run_endpoint_function
    return await dependant.call(**values)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\app\routers\v1\users.py"", line 26, in create_user
    session.commit()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1428, in commit
    self._transaction.commit(_to_root=self.future)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 829, in commit
    self._prepare_impl()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 808, in _prepare_impl
    self.session.flush()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3339, in flush
    self._flush(objects)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3479, in _flush
    transaction.rollback(_capture_exception=True)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3439, in _flush
    flush_context.execute()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 209, in save_obj
    for (
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 370, in _organize_states_for_save
    for state, dict_, mapper, connection in _connections_for_states(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 1709, in _connections_for_states
    connection = uowtransaction.transaction.connection(base_mapper)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 626, in connection
    return self._connection_for_bind(bind, execution_options)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 735, in _connection_for_bind
    conn = self._parent._connection_for_bind(bind, execution_options)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 747, in _connection_for_bind
    conn = bind.connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\future\engine.py"", line 419, in connect
    return super(Engine, self).connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3194, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 96, in __init__
    else engine.raw_connection()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3273, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3240, in _wrap_pool_connect
    return fn()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 476, in checkout
    rec = pool._do_get()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\impl.py"", line 146, in _do_get
    self._dec_overflow()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\impl.py"", line 143, in _do_get
    return self._create_connection()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 256, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 371, in __init__
    self.__connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 666, in __connect
    pool.logger.debug(""Error on connect(): %s"", e)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\create.py"", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py"", line 748, in connect
    await_only(self.asyncpg.connect(*arg, **kw)),
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\_concurrency_py3k.py"", line 61, in await_only
    raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/14/xd2s)

EDIT 2:
If I use my local postgres database in docker i get this error. If I use my extern database from Heroku it works perfectly fine!
EDIT 3:
So apparently it won't work with my local docker postgres database. Now I am using my production database over heroku (so externally) and everything works with psycopg2. I didn't found the error why it won't let me use my local db but whatever.
",4,15895,"Even though I can see that you are using default port, maybe this can help to somebody with the same problem.
For me the problem was in explicit port definition - as I'm running two different postgres DB and (both from containers), one of them I set to listen to 5433 port - and this gives me exact same problem, so instead
SQLALCHEMY_DATABASE_URL = ""postgresql://postgres:password@localhost/fastapi""

I just put:
SQLALCHEMY_DATABASE_URL = ""postgresql://postgres:password@localhost:5433/fastapi""

Problem solved right away.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/46977257,SQLAlchemy returns unexpected results with limit/offset,,4,7032,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/54836672,"Using a Celery worker to interact with a SQLAlchemy DB, including knowing the user from the request",,3,4997,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/29222426,Comparing Dates in Flask-SQLAlchemy,"I've been trying to compare dates in a query given to SQLALchemy as follows:  

start = time.strptime(start, ""%d%m%y"")
end = time.strptime(end, ""%d%m%y"")
list_ideas = Idea.query.filter(time &gt;= start, time &lt;= end).all()


However, this does not return results regardless of dates given (where Idea.time = db.Column(db.DateTime, default=db.func.now())). I've searched through some other answers regarding this topic and from what I have gathered, I am not making the same mistakes.

In addition, changing the query to Idea.query.filter(time &gt;= start, time &lt;= end, deleted=False).all() gives the error:  


  TypeError: filter() got an unexpected keyword argument 'deleted'


Any pointers would be appreciated.

--
EDIT: I noticed that I was using import time, which may have caused the error. However, after changing it to from time import strptime, I now experience the error:  


  NameError: global name 'time' is not defined

",3,7372,"Try to use the func.DATE() function:
from sqlalchemy import func

start = time.strptime(start, ""%d%m%y"")
end = time.strptime(end, ""%d%m%y"")
list_ideas = Idea.query.filter(func.DATE(time) &gt;= start, func.DATE(time) &lt;= end).all()

refer to similar issue.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/68212540,How to use marshmallow-sqlalchemy with async code?,"I'm trying to use marshmallow-sqlalchemy with aiohttp and I have followed their docs with the basic example and I'm getting an error.
I have this schema:
from marshmallow_sqlalchemy import SQLAlchemyAutoSchema

from db.customer import Customer

class CustomerSchema(SQLAlchemyAutoSchema):
    class Meta:
        model = Customer
        include_relationships = True
        load_instance = True


And then the following code for the query:
from sqlalchemy import select
from db import db_conn
from db.customer import Customer
from queries.schema import CustomerSchema

customer_schema = CustomerSchema()

async def get_all_users():
    async with db_conn.get_async_sa_session() as session:
        statement = select(Customer)
        results = await session.execute(statement)
        _ = (results.scalars().all())
        print(_)
        response = customer_schema.dump(_, many=True)
        print(response)

For the first print statement I'm getting
[&lt;db.customer.Customer object at 0x10a183340&gt;, &lt;db.customer.Customer object at 0x10a183940&gt;, &lt;db.customer.Customer object at 0x10b0cd9d0&gt;]

But then it fails with
File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/util/_concurrency_py3k.py"", line 60, in await_only
    raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)

So how can I use marshmallow-sqlalchemy to serialize the SqlAlchemy reponse?
Another options (packages, etc) or a generic custom solutions are OK too.
For the time being I'm using this:
statement = select(Customer)
results = await session.execute(statement)
_ = (results.scalars().all())
response = {}
for result in _:
    value = {k: (v if not isinstance(v, sqlalchemy.orm.state.InstanceState) else '_') for k, v in result.__dict__.items()}
    response[f'customer {value[""id""]}'] = value
return response

Full traceback:

Traceback (most recent call last):
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/aiohttp/web_protocol.py"", line 422, in _handle_request
    resp = await self._request_handler(request)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/aiohttp/web_app.py"", line 499, in _handle
    resp = await handler(request)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/aiohttp/web_urldispatcher.py"", line 948, in _iter
    resp = await method()
  File ""/Users/ruslan/OneDrive/Home/Dev/projects/code/education/other/cft/views/user.py"", line 24, in get
    await get_all_users()
  File ""/Users/ruslan/OneDrive/Home/Dev/projects/code/education/other/cft/queries/user.py"", line 18, in get_all_users
    response = customer_schema.dump(_, many=True)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 547, in dump
    result = self._serialize(processed_obj, many=many)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 509, in _serialize
    return [
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 510, in &lt;listcomp&gt;
    self._serialize(d, many=False)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 515, in _serialize
    value = field_obj.serialize(attr_name, obj, accessor=self.get_attribute)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/fields.py"", line 310, in serialize
    value = self.get_value(obj, attr, accessor=accessor)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow_sqlalchemy/fields.py"", line 27, in get_value
    return super(fields.List, self).get_value(obj, attr, accessor=accessor)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/fields.py"", line 239, in get_value
    return accessor_func(obj, check_key, default)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 472, in get_attribute
    return get_value(obj, attr, default)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/utils.py"", line 239, in get_value
    return _get_value_for_key(obj, key, default)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/utils.py"", line 253, in _get_value_for_key
    return getattr(obj, key, default)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/attributes.py"", line 480, in __get__
    return self.impl.get(state, dict_)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/attributes.py"", line 931, in get
    value = self.callable_(state, passive)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/strategies.py"", line 879, in _load_for_state
    return self._emit_lazyload(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/strategies.py"", line 1036, in _emit_lazyload
    result = session.execute(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 1689, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1582, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/sql/lambdas.py"", line 481, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1451, in _execute_clauseelement
    ret = self._execute_context(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1813, in _execute_context
    self._handle_dbapi_exception(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1998, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
    raise exception
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1770, in _execute_context
    self.dialect.do_execute(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/default.py"", line 717, in do_execute
    cursor.execute(statement, parameters)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py"", line 449, in execute
    self._adapt_connection.await_(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/util/_concurrency_py3k.py"", line 60, in await_only
    raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)

",2,485,"The problem in this case is that the Marshmallow schema is configured to load related models (include_relationships=True).  Since the initial query doesn't load them automatically, the schema triggers a query to fetch them, and this causes the error.
The simplest solution, demonstrated in the docs, is to eagerly load the related objects with their ""parent"":
async def get_all_users():
    async with db_conn.get_async_sa_session() as session:

        # Let's assume a Customer has a 1 to many relationship with an Order model
        statement = select(Customer).options(orm.selectinload(Customer.orders))

        results = await session.execute(statement)
        _ = (results.scalars().all())
        print(_)
        response = customer_schema.dump(_, many=True)
        print(response)

There is more discussion in the Preventing Implicit IO when Using AsyncSession section of the docs.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/66656355,How to access only filtered objects when filtering on joined tables in sqlalchemy,,1,202,"After posting this I found the same question here with an answer that didn't work for me, but with a small modification I managed to do it:
parents = (
        db.query(Parent)
        .join(Child)
        .filter(Child.did_homework == True)
        .options(contains_eager(""children""))
        .all()
    )

",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/78010668,Docker fastapi unable to connect to mysql,"I'm new to docker and fastapi and I'm trying to connect fastapi with mysql service but it always say that it cannot connect to mysql in the logs. What am I missing here?
.env file
API_PATH='api'
API_VERSION='v1'
FASTAPI_CONTAINER_NAME='app_fastapi'
FASTAPI_PORT=8000
REDIS_CONTAINER_NAME='app_redis'
REDIS_PORT=8001
MYSQL_CONTAINER_NAME='app_mysql'
MYSQL_DATABASE_NAME='fastapi_db'
MYSQL_ROOT_PASSWORD='p@ssw0rD'
MYSQL_USER='fastapi_mysql_admin'
MYSQL_USER_PASSWORD='p@ssw0rD'
MYSQL_HOST='mysql'
MYSQL_PORT=3306

docker-compose file
version: ""3.9""  # Specify a compatible Docker Compose version

services:
  fastapi:
    build:
      context: ./fastapi  # Build your FastAPI app
      dockerfile: Dockerfile  # Use your app's Dockerfile
    container_name: ${FASTAPI_CONTAINER_NAME}
    ports:
      - ""${FASTAPI_PORT}:${FASTAPI_PORT}""  # Expose port 8000 for the FastAPI app
    volumes:
      - .:/app
    env_file:
      - .env
    depends_on:
      mysql:  # Ensure mysql starts first
        condition: service_healthy
      redis: # Ensure redis starts first
        condition: service_healthy
    restart: on-failure
    tty: true
    networks:
      - backend

  mysql:
    build:
      context: ./mysql  # Build from the current directory
      dockerfile: Dockerfile  # Use the specific Dockerfile
    container_name: ${MYSQL_CONTAINER_NAME}
    ports:
      - ""${MYSQL_PORT}:${MYSQL_PORT}""  # Expose port 3306 for database access
    environment:
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_PASSWORD=${MYSQL_USER_PASSWORD}
      - MYSQL_DATABASE=${MYSQL_DATABASE_NAME}
    healthcheck:
      test: [""CMD"", ""mysqladmin"", ""ping"", ""-h"", ""localhost"", ""-p${MYSQL_USER_PASSWORD}""]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 7s
    tty: true
    restart: on-failure
    networks:
      - backend
  
  redis:
    build:
      context: ./redis  # Build from the current directory
      dockerfile: Dockerfile  # Use the specific Dockerfile
    container_name: ${REDIS_CONTAINER_NAME}
    ports:
      - ""${REDIS_PORT}:${REDIS_PORT}""  # Expose Redis port
    healthcheck:
      test: [""CMD"", ""redis-cli"", ""ping""]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 7s
    tty: true
    restart: on-failure
    networks:
      - backend

networks:
  backend:

database.py
import sqlalchemy
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.exc import OperationalError
from sqlalchemy.orm import sessionmaker
from os import environ as env

user = env['MYSQL_USER']
password = env['MYSQL_USER_PASSWORD']
host = env['MYSQL_HOST']
port = env['MYSQL_PORT']
database = env['MYSQL_DATABASE_NAME']

DATABASE_URL = ""mysql+pymysql://{}@{}:{}/{}"".format(
    user,
    host,
    port,
    database
)

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

main.py
from fastapi import FastAPI
from os import environ as env
from app.modules.todo import router as TodoRouter
app = FastAPI()
apiPath = '/' + env['API_PATH'] + '/' + env['API_VERSION']
# routers
app.include_router(TodoRouter.router, prefix=apiPath + '/todo', tags=['Todo'])

todo/router.py
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from app.database import get_db
from app.modules.todo.data_models import TodoRequestModel

router = APIRouter()

@router.post('/')
def create(todo: TodoRequestModel, db: Session = Depends(get_db)):
    return TodoController.create(todo, db)

On docker start


Docker container log when /post endpoint is hit
- INFO:     172.31.0.1:37760 - ""POST /api/v1/todo/ HTTP/1.1"" 500 Internal Server Error
- ERROR:    Exception in ASGI application
- Traceback (most recent call last):
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 796, in _connect
-     self._get_server_information()
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 994, in _get_server_information
-     self.server_charset = charset_by_id(lang).name
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/charset.py"", line 34, in by_id
-     return self._by_id[id]
- KeyError: 255
- 
- During handling of the above exception, another exception occurred:
- 
- Traceback (most recent call last):
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3250, in _wrap_pool_connect
-     return fn()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 310, in connect
-     return _ConnectionFairy._checkout(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 868, in _checkout
-     fairy = _ConnectionRecord.checkout(pool)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 476, in checkout
-     rec = pool._do_get()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 145, in _do_get
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 143, in _do_get
-     return self._create_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 256, in _create_connection
-     return _ConnectionRecord(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 371, in __init__
-     self.__connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 665, in __connect
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 661, in __connect
-     self.dbapi_connection = connection = pool._invoke_creator(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/create.py"", line 590, in connect
-     return dialect.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py"", line 597, in connect
-     return self.dbapi.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/__init__.py"", line 88, in Connect
-     return Connection(*args, **kwargs)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 634, in __init__
-     self._connect()
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 817, in _connect
-     raise OperationalError(
- pymysql.err.OperationalError: (2003, ""Can't connect to MySQL server on 'mysql' (255)"")
- 
- The above exception was the direct cause of the following exception:
- 
- Traceback (most recent call last):
-   File ""/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py"", line 404, in run_asgi
-     result = await app(  # type: ignore[func-returns-value]
-   File ""/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py"", line 84, in __call__
-     return await self.app(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/applications.py"", line 1054, in __call__
-     await super().__call__(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/applications.py"", line 123, in __call__
-     await self.middleware_stack(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 186, in __call__
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 164, in __call__
-     await self.app(scope, receive, _send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py"", line 62, in __call__
-     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 64, in wrapped_app
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
-     await app(scope, receive, sender)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 762, in __call__
-     await self.middleware_stack(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 782, in app
-     await route.handle(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 297, in handle
-     await self.app(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 77, in app
-     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 64, in wrapped_app
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
-     await app(scope, receive, sender)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 72, in app
-     response = await func(request)
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 299, in app
-     raise e
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 294, in app
-     raw_response = await run_endpoint_function(
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 193, in run_endpoint_function
-     return await run_in_threadpool(dependant.call, **values)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/concurrency.py"", line 40, in run_in_threadpool
-     return await anyio.to_thread.run_sync(func, *args)
-   File ""/usr/local/lib/python3.10/site-packages/anyio/to_thread.py"", line 56, in run_sync
-     return await get_async_backend().run_sync_in_worker_thread(
-   File ""/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 2134, in run_sync_in_worker_thread
-     return await future
-   File ""/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 851, in run
-     result = context.run(func, *args)
-   File ""/app/fastapi/app/modules/todo/router.py"", line 23, in create
-     return TodoController.create(todo, db)
-   File ""/app/fastapi/app/modules/todo/controller.py"", line 17, in create
-     return TodoService.create(student, db)
-   File ""/app/fastapi/app/modules/todo/repository.py"", line 62, in create
-     db.commit()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 1431, in commit
-     self._transaction.commit(_to_root=self.future)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 829, in commit
-     self._prepare_impl()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 808, in _prepare_impl
-     self.session.flush()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3363, in flush
-     self._flush(objects)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3502, in _flush
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3463, in _flush
-     flush_context.execute()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
-     rec.execute(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
-     util.preloaded.orm_persistence.save_obj(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 211, in save_obj
-     for (
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 372, in _organize_states_for_save
-     for state, dict_, mapper, connection in _connections_for_states(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 1711, in _connections_for_states
-     connection = uowtransaction.transaction.connection(base_mapper)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 626, in connection
-     return self._connection_for_bind(bind, execution_options)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 735, in _connection_for_bind
-     conn = self._parent._connection_for_bind(bind, execution_options)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 747, in _connection_for_bind
-     conn = bind.connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3204, in connect
-     return self._connection_cls(self, close_with_result=close_with_result)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 96, in __init__
-     else engine.raw_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3283, in raw_connection
-     return self._wrap_pool_connect(self.pool.connect, _connection)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3253, in _wrap_pool_connect
-     Connection._handle_dbapi_exception_noconnection(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 2100, in _handle_dbapi_exception_noconnection
-     util.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3250, in _wrap_pool_connect
-     return fn()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 310, in connect
-     return _ConnectionFairy._checkout(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 868, in _checkout
-     fairy = _ConnectionRecord.checkout(pool)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 476, in checkout
-     rec = pool._do_get()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 145, in _do_get
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 143, in _do_get
-     return self._create_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 256, in _create_connection
-     return _ConnectionRecord(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 371, in __init__
-     self.__connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 665, in __connect
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 661, in __connect
-     self.dbapi_connection = connection = pool._invoke_creator(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/create.py"", line 590, in connect
-     return dialect.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py"", line 597, in connect
-     return self.dbapi.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/__init__.py"", line 88, in Connect
-     return Connection(*args, **kwargs)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 634, in __init__
-     self._connect()
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 817, in _connect
-     raise OperationalError(
- sqlalchemy.exc.OperationalError: (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'mysql' (255)"")
- (Background on this error at: https://sqlalche.me/e/14/e3q8)

I've tried adding a password in the database connection string but gives me a different error like the following
DATABASE_URL = ""mysql+pymysql://{}:{}@{}:{}/{}"".format(
        user,
        password,
        host,
        port,
        database
    )

new log
- INFO:     172.31.0.1:52082 - ""POST /api/v1/todo/ HTTP/1.1"" 500 Internal Server Error
- ERROR:    Exception in ASGI application
- Traceback (most recent call last):
-   File ""/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py"", line 404, in run_asgi
-     result = await app(  # type: ignore[func-returns-value]
-   File ""/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py"", line 84, in __call__
-     return await self.app(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/applications.py"", line 1054, in __call__
-     await super().__call__(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/applications.py"", line 123, in __call__
-     await self.middleware_stack(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 186, in __call__
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 164, in __call__
-     await self.app(scope, receive, _send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py"", line 62, in __call__
-     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 64, in wrapped_app
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
-     await app(scope, receive, sender)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 762, in __call__
-     await self.middleware_stack(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 782, in app
-     await route.handle(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 297, in handle
-     await self.app(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 77, in app
-     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 64, in wrapped_app
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
-     await app(scope, receive, sender)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 72, in app
-     response = await func(request)
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 299, in app
-     raise e
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 294, in app
-     raw_response = await run_endpoint_function(
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 193, in run_endpoint_function
-     return await run_in_threadpool(dependant.call, **values)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/concurrency.py"", line 40, in run_in_threadpool
-     return await anyio.to_thread.run_sync(func, *args)
-   File ""/usr/local/lib/python3.10/site-packages/anyio/to_thread.py"", line 56, in run_sync
-     return await get_async_backend().run_sync_in_worker_thread(
-   File ""/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 2134, in run_sync_in_worker_thread
-     return await future
-   File ""/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 851, in run
-     result = context.run(func, *args)
-   File ""/app/fastapi/app/modules/todo/router.py"", line 23, in create
-     return TodoController.create(todo, db)
-   File ""/app/fastapi/app/modules/todo/controller.py"", line 17, in create
-     return TodoService.create(student, db)
-   File ""/app/fastapi/app/modules/todo/repository.py"", line 62, in create
-     db.commit()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 1431, in commit
-     self._transaction.commit(_to_root=self.future)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 829, in commit
-     self._prepare_impl()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 808, in _prepare_impl
-     self.session.flush()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3363, in flush
-     self._flush(objects)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3502, in _flush
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3463, in _flush
-     flush_context.execute()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
-     rec.execute(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
-     util.preloaded.orm_persistence.save_obj(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 211, in save_obj
-     for (
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 372, in _organize_states_for_save
-     for state, dict_, mapper, connection in _connections_for_states(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 1711, in _connections_for_states
-     connection = uowtransaction.transaction.connection(base_mapper)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 626, in connection
-     return self._connection_for_bind(bind, execution_options)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 735, in _connection_for_bind
-     conn = self._parent._connection_for_bind(bind, execution_options)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 747, in _connection_for_bind
-     conn = bind.connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3204, in connect
-     return self._connection_cls(self, close_with_result=close_with_result)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 96, in __init__
-     else engine.raw_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3283, in raw_connection
-     return self._wrap_pool_connect(self.pool.connect, _connection)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3250, in _wrap_pool_connect
-     return fn()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 310, in connect
-     return _ConnectionFairy._checkout(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 868, in _checkout
-     fairy = _ConnectionRecord.checkout(pool)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 476, in checkout
-     rec = pool._do_get()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 145, in _do_get
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 143, in _do_get
-     return self._create_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 256, in _create_connection
-     return _ConnectionRecord(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 371, in __init__
-     self.__connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 665, in __connect
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 661, in __connect
-     self.dbapi_connection = connection = pool._invoke_creator(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/create.py"", line 590, in connect
-     return dialect.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py"", line 597, in connect
-     return self.dbapi.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/__init__.py"", line 88, in Connect
-     return Connection(*args, **kwargs)
- TypeError: Connection.__init__() got an unexpected keyword argument 'password'

",1,47,"First, any container that wants to read data from an env file must have an env_file section and the path of the env file must be specified inside it. So set this for mysql and redis services as well.
version: ""3.9""
services:
  mysql:
    env_file:
      - .env
  redis:
    env_file:
      - .env

Second, in the mysql service inside docker compose, you read the name of the container from the env file, and inside the env file, you have set this value equal to app_mysql.
MYSQL_CONTAINER_NAME='app_mysql'

Since routing in the docker compose network is something like DNS, you must set the value of MYSQL_HOST equal to the name of the mysql container so that your fastapi service can connect to the mysql container.
MYSQL_HOST='app_mysql'

",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/77429344,How use SQL Expressions in @property decorator for async session in SQLAlchemy?,"I've got two models Media and Tweet.
Unfortunately, they don't have a one-to-many relationship. The specifics of the API are: one router sends images (Media), and the other router in the body receives a ids list of images that will be attached to the tweet.
I would like to set a attachments field in the Tweet model that would contain a list of paths of attached images, so that later I can simply validate using pydantic schemas.
class Media(Base):
    __tablename__ = 'medias'
    id: Mapped[int] = mapped_column(Sequence('media_id_seq'), primary_key=True, index=True)
    path: Mapped[str] = mapped_column(nullable=False)
    ....

class Tweet(Base):
    __tablename__ = 'tweets'
    id: Mapped[int] = mapped_column(Sequence('tweet_id_seq'), primary_key=True, index=True)
    content: Mapped[str] = mapped_column(TEXT, nullable=False)
 
    tweet_media_ids: Mapped[list[int]] = mapped_column(ARRAY(Integer), nullable=True)
  
    @property
    def attachments(self):
        if not self.tweet_media_ids:
            return []
        query = select(Media.path).where(Media.id.in_(self.tweet_media_ids))
        result = object_session(self).execute(query)
        return result.scalars().all()

I've tried to get a tweet and a list of appended media in attachments, but got  *sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/20/xd2s)* while reffering to item.attachments
`@router.get(""/tweets"", response_model=schemas.TweetsList)
async def get_tweets_list(db: Annotated[AsyncSession, Depends(get_db_session)]):
    tweets_orm = await get_tweets(db)
    for item in tweets_orm:
        print(""att="", item.attachments)
    tweets = [schemas.Tweet.model_validate(tweet) for tweet in tweets_orm]
    tweets_list = schemas.TweetsList(tweets=tweets)
    return tweets_list`

172.18.0.3:49338 - ""GET /api/tweets HTTP/1.0"" 500
 [2023-11-06 05:43:43 +0000] [7] [ERROR] Exception in ASGI application
 Traceback (most recent call last):
 File ""/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py"", line 426, in run_asgi
 result = await app(  # type: ignore[func-returns-value]
 File ""/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py"", line 84, in __call__
 return await self.app(scope, receive, send)
 172.18.0.1 - - [06/Nov/2023:05:43:43 +0000] ""GET /api/tweets HTTP/1.1"" 500 21 ""http://localhost/"" ""Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/119.0"" ""-""
 File ""/usr/local/lib/python3.10/site-packages/fastapi/applications.py"", line 292, in __call__
 await super().__call__(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/applications.py"", line 122, in __call__
 await self.middleware_stack(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 184, in __call__
 raise exc
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 162, in __call__
 await self.app(scope, receive, _send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/cors.py"", line 83, in __call__
 await self.app(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py"", line 79, in __call__
 raise exc
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py"", line 68, in __call__
 await self.app(scope, receive, sender)
 File ""/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py"", line 20, in __call__
 raise e
 File ""/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py"", line 17, in __call__
 await self.app(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 718, in __call__
 await route.handle(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 276, in handle
 await self.app(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 66, in app
 response = await func(request)
 File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 273, in app
 raw_response = await run_endpoint_function(
 File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 190, in run_endpoint_function
 return await dependant.call(**values)
 File ""/api/app/routes/tweets.py"", line 66, in get_tweets_list
 print(""att="", item.attachments)
 File ""/api/app/models.py"", line 79, in attachments
 result = object_session(self).execute(query)
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 2306, in execute
 return self._execute_internal(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 2188, in _execute_internal
 result: Result[Any] = compile_state_cls.orm_execute_statement(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/context.py"", line 293, in orm_execute_statement
 result = conn.execute(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1416, in execute
 return meth(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/sql/elements.py"", line 516, in _execute_on_connection
 return connection._execute_clauseelement(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1639, in _execute_clauseelement
 ret = self._execute_context(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1848, in _execute_context
 return self._exec_single_context(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1988, in _exec_single_context
 self._handle_dbapi_exception(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 2346, in _handle_dbapi_exception
 raise exc_info[1].with_traceback(exc_info[2])
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1969, in _exec_single_context
 self.dialect.do_execute(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py"", line 922, in do_execute
 cursor.execute(statement, parameters)
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py"", line 586, in execute
 self._adapt_connection.await_(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/_concurrency_py3k.py"", line 116, in await_only
 raise exc.MissingGreenlet(
 sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/20/xd2s)

Question:
how to fix the error or advise another implementation of the attachments field
",1,210,"Use hybrid_property instead of property decorator.
from sqlalchemy.ext.hybrid import hybrid_property  # add this line


class Tweet(Base):
    __tablename__ = 'tweets'
    id: Mapped[int] = mapped_column(Sequence('tweet_id_seq'), primary_key=True, index=True)
    content: Mapped[str] = mapped_column(TEXT, nullable=False)
 
    tweet_media_ids: Mapped[list[int]] = mapped_column(ARRAY(Integer), nullable=True)
  
    @hybrid_property
    def attachments(self):
        if not self.tweet_media_ids:
            return []
        query = select(Media.path).where(Media.id.in_(self.tweet_media_ids))
        result = object_session(self).execute(query)
        return result.scalars().all()

sqlalchemy.exc.MissingGreenlet raised by relations might be resolved using hybrid_property in your case. The error occurs when manipulating join commands in async functions with bad relationships, including lazy loading strategies.
In addition, SQLAlchemy doesn't have to have physical relationships. So you can use relationship in your code even if two tables do not have actual relations.
With that:
class Tweet(Base):
    __tablename__ = 'tweets'
    id: Mapped[int] = mapped_column(Sequence('tweet_id_seq'), primary_key=True, index=True)
    content: Mapped[str] = mapped_column(TEXT, nullable=False)
 
    medias: Mapped[list[Media]] = relationship(back_populates=""medias"")
  
    @hybrid_property
    def attachments(self) -&gt; list[Media]:
        return self.medias

SQLAlchemy will generate a query with the relationship automatically. In fact, when setting logical relationships, you don't need to write the attachments method.
Or use awaitable_attrs
You are using AsyncSession so that you can use awaitable_attrs
@router.get(""/tweets"", response_model=schemas.TweetsList)
async def get_tweets_list(db: Annotated[AsyncSession, Depends(get_db_session)]):
    tweets_orm = await get_tweets(db)
    for item in tweets_orm:
        print(""att="", item.awaitable_attrs.attachments)
    tweets = [schemas.Tweet.model_validate(tweet) for tweet in tweets_orm]
    tweets_list = schemas.TweetsList(tweets=tweets)
    return tweets_list

awaitable_attrs resolves deferred columns that are not loaded yet. It can await result = await object_session(self).execute(query) in method attachments.
","    async def get_tweets_list(
        db: Annotated[AsyncSession, Depends(get_db_session)]
):
    tweets_orm = await get_tweets(db)
    for item in tweets_orm:
        print(""att="", await item.awaitable_attrs.attachments)
    tweets = [schemas.Tweet.model_validate(tweet) for tweet in tweets_orm]

It seems item.attachments return coroutine.
await item.awaitable_attrs.attachments return desired list.
So I guess an addition hit to DB is needed. And those query in property decorator does not execute
",
SQLAlchemy unexpected result,https://stackoverflow.com/questions/69466354,MLflow S3UploadFailedError: Failed to upload,"I've created with docker a MinioS3 artifact storage and a MySQL backend storage using the next Docker Compose:
    version: '3.8'
    services:
        db:
           environment:
              - MYSQL_DATABASE=${MYSQL_DATABASE}
              - MYSQL_USER=${MYSQL_USER}
              - MYSQL_PASSWORD=${MYSQL_PASSWORD}
              - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
           expose:
              - '3306'        
           volumes:
              - '(path)/server_backend:/var/lib/mysql '
           image: 'mysql'
           container_name: db

        storage:
            environment:
                - MINIO_ACCESS_KEY=${MINIO_USR}
                - MINIO_SECRET_KEY=${MINIO_PASS}
            expose:
                - '9000'
            ports:
                - '9000:9000'        
            depends_on:
                - db
            command: server /data
            volumes:
                - '(path)/server_artifact:/data'
            image: minio/minio:RELEASE.2021-02-14T04-01-33Z
            container_name: MinIO

        mlflow:
            build: ./mlflow
            environment:
                - AWS_ACCESS_KEY_ID=${MINIO_USR}
                - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       
            expose:
                - '5000'
            ports:
                - '5000:5000'
            depends_on:
                - storage                       
            image: 'mlflow:Dockerfile'
            container_name: server

The Mlflow server docker was created using the next Dockerfile:
    FROM python:3.8-slim-buster
    WORKDIR /usr/src/app
    RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql
    ENV MLFLOW_S3_ENDPOINT_URL=http://storage:9000
    CMD mlflow server \
        --backend-store-uri mysql+pymysql://MLFLOW:temporal@db:3306/DBMLFLOW \
        --default-artifact-root s3://artifacts \
        --host 0.0.0.0

The credantials are defined in a .env file.
The results of the docker-compose up command:

    [+] Running 21/22
     - mlflow Error                                                                                                                              5.6s
     - storage Pulled                                                                                                                           36.9s
       - a6b97b4963f5 Pull complete                                                                                                             24.6s
       - 13948a011eec Pull complete                                                                                                             24.7s
       - 40cdef9976a6 Pull complete                                                                                                             24.7s
       - f47162848743 Pull complete                                                                                                             24.8s
       - 5f2758d8e94c Pull complete                                                                                                             24.9s
       - c2950439edb8 Pull complete                                                                                                             25.0s
       - 1b08f8a15998 Pull complete                                                                                                             30.7s
     - db Pulled                                                                                                                                45.8s
       - 07aded7c29c6 Already exists                                                                                                             0.0s
       - f68b8cbd22de Pull complete                                                                                                              0.7s
       - 30c1754a28c4 Pull complete                                                                                                              2.1s
       - 1b7cb4d6fe05 Pull complete                                                                                                              2.2s
       - 79a41dc56b9a Pull complete                                                                                                              2.3s
       - 00a75e3842fb Pull complete                                                                                                              6.7s
       - b36a6919c217 Pull complete                                                                                                              6.8s
       - 635b0b84d686 Pull complete                                                                                                              6.8s
       - 6d24c7242d02 Pull complete                                                                                                             39.4s
       - 5be6c5edf16f Pull complete                                                                                                             39.5s
       - cb35eac1242c Pull complete                                                                                                             39.5s
       - a573d4e1c407 Pull complete                                                                                                             39.6s
    [+] Building 1.4s (7/7) FINISHED
     =&gt; [internal] load build definition from Dockerfile                                                                                         0.0s
     =&gt; =&gt; transferring dockerfile: 32B                                                                                                          0.0s
     =&gt; [internal] load .dockerignore                                                                                                            0.0s
     =&gt; =&gt; transferring context: 2B                                                                                                              0.0s
     =&gt; [internal] load metadata for docker.io/library/python:3.8-slim-buster                                                                    1.3s
     =&gt; [1/3] FROM docker.io/library/python:3.8-slim-buster@sha256:13a3f2bffb4b18ff7eda2763a3b0ba316dd82e548f52ea8b4fd11c94b97afa7d              0.0s
     =&gt; CACHED [2/3] WORKDIR /usr/src/app                                                                                                        0.0s
     =&gt; CACHED [3/3] RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql                                                           0.0s
     =&gt; exporting to image                                                                                                                       0.0s
     =&gt; =&gt; exporting layers                                                                                                                      0.0s
     =&gt; =&gt; writing image sha256:76d4e4462b5c7c1826734e59a54488b56660de0dd5ecc188c308202608a8f20b                                                 0.0s
     =&gt; =&gt; naming to docker.io/library/mlflow:Dockerfile                                                                                         0.0s
    
    Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them
    [+] Running 3/3
     - Container db  Created                                                                                                       0.5s
     - Container MinIO      Created                                                                                                       0.1s
     - Container server     Created                                                                                                       0.1s
    Attaching to server, MinIO, db
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Initializing database files
    db  | 2021-10-06T12:12:57.679527Z 0 [System] [MY-013169] [Server] /usr/sbin/mysqld (mysqld 8.0.26) initializing of server in progress as process 44
    db  | 2021-10-06T12:12:57.687748Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
    db  | 2021-10-06T12:12:58.230036Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
    db  | 2021-10-06T12:12:59.888820Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main
    db  | 2021-10-06T12:12:59.889102Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main
    db  | 2021-10-06T12:12:59.997461Z 6 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.
    MinIO      | Attempting encryption of all config, IAM users and policies on MinIO backend
    MinIO      | Endpoint: http://172.18.0.3:9000  http://127.0.0.1:9000
    MinIO      |
    MinIO      | Browser Access:
    MinIO      |    http://172.18.0.3:9000  http://127.0.0.1:9000
    MinIO      |
    MinIO      | Object API (Amazon S3 compatible):
    MinIO      |    Go:         https://docs.min.io/docs/golang-client-quickstart-guide
    MinIO      |    Java:       https://docs.min.io/docs/java-client-quickstart-guide
    MinIO      |    Python:     https://docs.min.io/docs/python-client-quickstart-guide
    MinIO      |    JavaScript: https://docs.min.io/docs/javascript-client-quickstart-guide
    MinIO      |    .NET:       https://docs.min.io/docs/dotnet-client-quickstart-guide
    server     | 2021/10/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 0.1 seconds
    server     | 2021/10/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 0.3 seconds
    server     | 2021/10/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 0.7 seconds
    server     | 2021/10/06 12:13:03 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 1.5 seconds
    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Database files initialized
    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Starting temporary server
    db  | 2021-10-06T12:13:04.422603Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.26) starting as process 93
    db  | 2021-10-06T12:13:04.439806Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
    db  | 2021-10-06T12:13:04.575773Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
    db  | 2021-10-06T12:13:04.827307Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:04.827865Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:04.832827Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
    db  | 2021-10-06T12:13:04.834132Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
    db  | 2021-10-06T12:13:04.841629Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
    db  | 2021-10-06T12:13:04.855748Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: /var/run/mysqld/mysqlx.sock
    db  | 2021-10-06T12:13:04.855801Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.26'  socket: '/var/run/mysqld/mysqld.sock'  port: 0  MySQL Community Server - GPL.
    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Temporary server started.
    server     | 2021/10/06 12:13:05 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 3.1 seconds
    db  | Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
    db  | Warning: Unable to load '/usr/share/zoneinfo/leap-seconds.list' as time zone. Skipping it.
    db  | Warning: Unable to load '/usr/share/zoneinfo/zone.tab' as time zone. Skipping it.
    db  | Warning: Unable to load '/usr/share/zoneinfo/zone1970.tab' as time zone. Skipping it.
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating database DBMLFLOW
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating user MLFLOW
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Giving user MLFLOW access to schema DBMLFLOW
    db  |
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Stopping temporary server
    db  | 2021-10-06T12:13:06.948482Z 13 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.26).
    server     | 2021/10/06 12:13:08 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 6.3 seconds
    db  | 2021-10-06T12:13:08.716131Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.26)  MySQL Community Server - GPL.
    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: Temporary server stopped
    db  |
    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: MySQL init process done. Ready for start up.
    db  |
    db  | 2021-10-06T12:13:09.159115Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.26) starting as process 1
    db  | 2021-10-06T12:13:09.167405Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
    db  | 2021-10-06T12:13:09.298925Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
    db  | 2021-10-06T12:13:09.488958Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:09.489087Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:09.489934Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
    db  | 2021-10-06T12:13:09.490169Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
    db  | 2021-10-06T12:13:09.494728Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
    db  | 2021-10-06T12:13:09.509856Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: /var/run/mysqld/mysqlx.sock
    db  | 2021-10-06T12:13:09.509982Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.26'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server - GPL.
    db  | mbind: Operation not permitted
    server     | 2021/10/06 12:13:14 INFO mlflow.store.db.utils: Creating initial MLflow database tables...
    server     | 2021/10/06 12:13:14 INFO mlflow.store.db.utils: Updating database tables
    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.
    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
    server     | INFO  [alembic.runtime.migration] Running upgrade  -&gt; 451aebb31d03, add metric step
    server     | INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tags
    server     | INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric values
    server     | INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags Table
    server     | INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limit
    server     | INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -&gt; 89d4b8295536, create latest metrics table
    server     | INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!
    server     | INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -&gt; 2b4d017a5e9b, add model registry tables to db
    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.
    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!
    server     | INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -&gt; cfd24bdc0731, Update run status constraint with killed
    server     | INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -&gt; 0a8213491aaa, drop_duplicate_killed_constraint
    server     | INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -&gt; 728d730b5ebd, add registered model tags table
    server     | INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -&gt; 27a6a02d2cf1, add model version tags table
    server     | INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -&gt; 84291f40a231, add run_link to model_version
    server     | INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -&gt; a8c4a736bde6, allow nulls for run_id
    server     | INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -&gt; 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary
    server     | INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -&gt; c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql
    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.
    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
    db  | mbind: Operation not permitted
    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Starting gunicorn 20.1.0
    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Listening at: http://0.0.0.0:5000 (17)
    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Using worker: sync
    server     | [2021-10-06 12:13:16 +0000] [19] [INFO] Booting worker with pid: 19
    server     | [2021-10-06 12:13:16 +0000] [20] [INFO] Booting worker with pid: 20
    server     | [2021-10-06 12:13:16 +0000] [21] [INFO] Booting worker with pid: 21
    server     | [2021-10-06 12:13:16 +0000] [22] [INFO] Booting worker with pid: 22


It makes me suspect because on the second line appears - mlflow Error but I think that this is why the other builds haven't finished.
Then I've set my environment variables on the client to create the information flow between my script and the storages:

    os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://localhost:9000/'
    os.environ['AWS_ACCESS_KEY_ID'] = 'key'
    os.environ['AWS_SECRET_ACCESS_KEY'] = 'pw'
    
    remote_server_uri = ""http://localhost:5000/"" # server URI
    mlflow.set_tracking_uri(remote_server_uri)
    
    mlflow.set_experiment(""mnist_mLflow_demo"")


finally I trained a TensorFlow network and I didn't have problems storing parameters and metrics but gave me some warnings (referring to next error). But the model haven't been auto log, so I tried to do it manually:
    with mlflow.start_run(run_name = ""test0"") as run:
    
        mlflow.keras.log_model(model2, 'model2')

    mlflow.end_run()

It dosen't work and it gives me the next INFO (but essencialy an error):
    INFO:tensorflow:Assets written to: (path)\Temp\tmpgr5eaha2\model\data\model\assets
    INFO:tensorflow:Assets written to: (path)\Temp\tmpgr5eaha2\model\data\model\assets
    2021/10/06 14:16:00 ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: (path)\AppData\Local\Temp\tmpgr5eaha2\model, flavor: keras)
    Traceback (most recent call last):
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\environment.py"", line 212, in infer_pip_requirements
        return _infer_requirements(model_uri, flavor)
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\requirements_utils.py"", line 263, in _infer_requirements
        modules = _capture_imported_modules(model_uri, flavor)
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\requirements_utils.py"", line 221, in _capture_imported_modules
        _run_command(
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\requirements_utils.py"", line 163, in _run_command
        stderr = stderr.decode(""utf-8"")
    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 349: invalid continuation byte


And the next error:

    ClientError                               Traceback (most recent call last)
    ~\Python\Python39\lib\site-packages\boto3\s3\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)
        278         try:
    --&gt; 279             future.result()
        280         # If a client error was raised, add the backwards compatibility layer
    
    ~\Python\Python39\lib\site-packages\s3transfer\futures.py in result(self)
        105             # out of this and propogate the exception.
    --&gt; 106             return self._coordinator.result()
        107         except KeyboardInterrupt as e:
    
    ~\Python\Python39\lib\site-packages\s3transfer\futures.py in result(self)
        264         if self._exception:
    --&gt; 265             raise self._exception
        266         return self._result
    
    ~\Python\Python39\lib\site-packages\s3transfer\tasks.py in __call__(self)
        125             if not self._transfer_coordinator.done():
    --&gt; 126                 return self._execute_main(kwargs)
        127         except Exception as e:
    
    ~\Python\Python39\lib\site-packages\s3transfer\tasks.py in _execute_main(self, kwargs)
        149 
    --&gt; 150         return_value = self._main(**kwargs)
        151         # If the task is the final task, then set the TransferFuture's
    
    ~\Python\Python39\lib\site-packages\s3transfer\upload.py in _main(self, client, fileobj, bucket, key, extra_args)
        693         with fileobj as body:
    --&gt; 694             client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
        695 
    
    ~\Python\Python39\lib\site-packages\botocore\client.py in _api_call(self, *args, **kwargs)
        385             # The ""self"" in this scope is referring to the BaseClient.
    --&gt; 386             return self._make_api_call(operation_name, kwargs)
        387 
    
    ~\Python\Python39\lib\site-packages\botocore\client.py in _make_api_call(self, operation_name, api_params)
        704             error_class = self.exceptions.from_code(error_code)
    --&gt; 705             raise error_class(parsed_response, operation_name)
        706         else:
    
    ClientError: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.
    
    During handling of the above exception, another exception occurred:
    
    S3UploadFailedError                       Traceback (most recent call last)
    C:\Users\FCAIZA~1\AppData\Local\Temp/ipykernel_7164/2476247499.py in &lt;module&gt;
          1 with mlflow.start_run(run_name = ""test0"") as run:
          2 
    ----&gt; 3     mlflow.keras.log_model(model2, 'model2')
          4 
          5 mlflow.end_run()
    
    ~\Python\Python39\lib\site-packages\mlflow\keras.py in log_model(keras_model, artifact_path, conda_env, custom_objects, keras_module, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, **kwargs)
        402             mlflow.keras.log_model(keras_model, ""models"")
        403     """"""
    --&gt; 404     Model.log(
        405         artifact_path=artifact_path,
        406         flavor=mlflow.keras,
    
    ~\Python\Python39\lib\site-packages\mlflow\models\model.py in log(cls, artifact_path, flavor, registered_model_name, await_registration_for, **kwargs)
        186             mlflow_model = cls(artifact_path=artifact_path, run_id=run_id)
        187             flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)
    --&gt; 188             mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)
        189             try:
        190                 mlflow.tracking.fluent._record_logged_model(mlflow_model)
    
    ~\Python\Python39\lib\site-packages\mlflow\tracking\fluent.py in log_artifacts(local_dir, artifact_path)
        582     """"""
        583     run_id = _get_or_start_run().info.run_id
    --&gt; 584     MlflowClient().log_artifacts(run_id, local_dir, artifact_path)
        585 
        586 
    
    ~\Python\Python39\lib\site-packages\mlflow\tracking\client.py in log_artifacts(self, run_id, local_dir, artifact_path)
        975             is_dir: True
        976         """"""
    --&gt; 977         self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)
        978 
        979     @contextlib.contextmanager
    
    ~\Python\Python39\lib\site-packages\mlflow\tracking\_tracking_service\client.py in log_artifacts(self, run_id, local_dir, artifact_path)
        332         :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.
        333         """"""
    --&gt; 334         self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)
        335 
        336     def list_artifacts(self, run_id, path=None):
    
    ~\Python\Python39\lib\site-packages\mlflow\store\artifact\s3_artifact_repo.py in log_artifacts(self, local_dir, artifact_path)
        102                 upload_path = posixpath.join(dest_path, rel_path)
        103             for f in filenames:
    --&gt; 104                 self._upload_file(
        105                     s3_client=s3_client,
        106                     local_file=os.path.join(root, f),
    
    ~\Python\Python39\lib\site-packages\mlflow\store\artifact\s3_artifact_repo.py in _upload_file(self, s3_client, local_file, bucket, key)
         78         if environ_extra_args is not None:
         79             extra_args.update(environ_extra_args)
    ---&gt; 80         s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)
         81 
         82     def log_artifact(self, local_file, artifact_path=None):
    
    ~\Python\Python39\lib\site-packages\boto3\s3\inject.py in upload_file(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)
        128     """"""
        129     with S3Transfer(self, Config) as transfer:
    --&gt; 130         return transfer.upload_file(
        131             filename=Filename, bucket=Bucket, key=Key,
        132             extra_args=ExtraArgs, callback=Callback)
    
    ~\Python\Python39\lib\site-packages\boto3\s3\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)
        283         # client error.
        284         except ClientError as e:
    --&gt; 285             raise S3UploadFailedError(
        286                 ""Failed to upload %s to %s: %s"" % (
        287                     filename, '/'.join([bucket, key]), e))
    
    S3UploadFailedError: Failed to upload (path)\AppData\Local\Temp\tmpgr5eaha2\model\conda.yaml to artifacts/1/5ae5fcef2d07432d811c3d7eb534382c/artifacts/model2/conda.yaml: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.


",1,2123,"I found the solution of this issue. It is a tricky problem due to spanish characters, my system's user profile in ""C:/"" is ""fcaizares"" (Caizares is my first last name). I have created another user named ""fcanizares"" and all is working fine. Hope you find this solution helpfull.
PS: Moral of the issue, get rid of the extrange characters!
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/64952367,SQLAlchemy Core. Values method does not limit columns in insert statement,"SQLALchemy Core insert expressions documentation says:

Notice above that the INSERT statement names every column in the users table. This can be limited by using the values() method, which establishes the VALUES clause of the INSERT explicitly:

With that in mind, I wrote the following snippet, which returns unexpected results.
from datetime import datetime
import sqlalchemy
from sqlalchemy import types
from sqlalchemy.dialects import postgresql


metadata = sqlalchemy.MetaData()

users = sqlalchemy.Table(
    ""users"",
    metadata,
    sqlalchemy.Column(
        ""id"",
        postgresql.UUID(as_uuid=True),
        default=uuid.uuid4(),
        primary_key=True,
    ),
    sqlalchemy.Column(""email"", types.String, unique=True, index=True),
    sqlalchemy.Column(
        ""created_at"",
        types.TIMESTAMP(timezone=True),
        default=datetime.utcnow(),
    ),
    sqlalchemy.Column(
        ""updated_at"",
        types.TIMESTAMP(timezone=True),
        default=datetime.utcnow(),
        onupdate=datetime.utcnow(),
    ),
)


email = ""god@olympus.org""
query = users.insert().values(email=email)

# (Pdb) print(query)
# INSERT INTO users (id, email, created_at, updated_at) VALUES (:id, :email, :created_at, :updated_at)
#
# (Pdb) print(query.compile().params)
# {'id': None, 'email': 'god@olympus.org', 'created_at': None, 'updated_at': None}

I expected the query to be INSERT INTO users (email) VALUES (:email)
Is there anything I'm missing?
I'm using SQLAlchemy==1.3.20 by the way.
",1,182,"The issue does not relate to SQLAlchemy, but rather to encode/databases.
Support for ""default"" parameter in sqlalchemy.Column
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/64755423,Execution failed when using pandas to_sql and pyhive to replace table - DatabaseError: &quot;... not all arguments converted during string formatting&quot;,"I need to replace a table in Hive with a new pandas dataframe. I am using pyhive to create a connection engine and subsequently using pandas.to_sql with 'if_exists' as replace.
from pyhive import hive

my_data = pd.read_csv('my_data.csv')

conn = hive.Connection(host=""111.11.11.11"", port=10000, username=""abcd"")

my_data.to_sql(name='table_name', con=conn, if_exists='replace', schema='my_schema')

conn.close()

However, this results in an unexpected error as follows:
DatabaseError: Execution failed on sql: SELECT name FROM sqlite_master WHERE type='table' AND name=?;
not all arguments converted during string formatting
unable to rollback

Other answers seem to indicate that this is related to to_sql expecting a SqlAlchemy engine - I was under the impression that this is what pyhive uses to create a connection. Any guidance on how to appropriately execute this command would be appreciated.
It is worth noting that elsewhere, this same connection has no problem reading data from Hive.
Works as expected:
conn = hive.Connection(host=""111.11.11.11"", port=10000, username=""abcd"")

my_data = pd.read_sql('select * from my_table', conn)

conn.close()

",1,1703,"
Other answers seem to indicate that this is related to to_sql expecting a SqlAlchemy engine - I was under the impression that this is what pyhive uses to create a connection.

PyHive can create a SQLAlchemy Engine object, but not the way you're doing it. As illustrated in the PyHive docs, you need to do something like
engine = create_engine('hive://localhost:10000/default')

and then pass the engine object to to_sql.

[read_sql] Works as expected

read_sql will often work with just a DBAPI connection, but to_sql requires a SQLAlchemy Connectable (Engine or Connection) because it may need to generate DDL. See this answer for more information.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/59577085,Rest Api post definition failing | Python | Flask,"I am very new to python. I have basic idea with python and flask is totally new for me. Trying to learn rest api with python using flask. But, I am facing the issue while posting data. below are sample code and error. Please help me to fix this issue. Thanks in advance. I tried googling but did not get any solution.

Error while running

127.0.0.1 - - [03/Jan/2020 15:58:54] ""POST /api/User HTTP/1.1"" 500 -
  Traceback (most recent call last):
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1997, in __call__
      return self.wsgi_app(environ, start_response)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1985, in wsgi_app
      response = self.handle_exception(e)
    File ""/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py"", line 273, in error_router
      return original_handler(e)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1540, in handle_exception
      reraise(exc_type, exc_value, tb)
    File ""/usr/local/lib/python3.7/site-packages/flask/_compat.py"", line 32, in reraise
      raise value.with_traceback(tb)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1982, in wsgi_app
      response = self.full_dispatch_request()
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1614, in full_dispatch_request
      rv = self.handle_user_exception(e)
    File ""/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py"", line 273, in error_router
      return original_handler(e)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1517, in handle_user_exception
      reraise(exc_type, exc_value, tb)
    File ""/usr/local/lib/python3.7/site-packages/flask/_compat.py"", line 32, in reraise
      raise value.with_traceback(tb)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1612, in full_dispatch_request
      rv = self.dispatch_request()
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1598, in dispatch_request
      return self.view_functions[rule.endpoint](**req.view_args)
    File ""/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py"", line 480, in wrapper
      resp = resource(*args, **kwargs)
    File ""/usr/local/lib/python3.7/site-packages/flask/views.py"", line 84, in view
      return self.dispatch_request(*args, **kwargs)
    File ""/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py"", line 595, in dispatch_request
      resp = meth(*args, **kwargs)
    File ""/Users/z0034ff/Documents/music/resources/Users.py"", line 32, in post
      typeid=json_data['typeid'],
  TypeError: __init__() got an unexpected keyword argument 'fname'


File name: Model.py

from flask import Flask
  from marshmallow import Schema, fields, pre_load, validate
  from flask_marshmallow import Marshmallow
  from flask_sqlalchemy import SQLAlchemy


  ma = Marshmallow()
  db = SQLAlchemy()


  class Users(db.Model):
      __tablename__ = 'users'
      id = db.Column(db.Integer, primary_key=True)
      fname = db.Column(db.String(50), nullable=False)
      lname = db.Column(db.String(50), nullable=False)
      email = db.Column(db.String(100), nullable=False)
      phone = db.Column(db.String(50), nullable=False)
      typeid = db.Column(db.Integer, db.ForeignKey(
          'user_type.id', ondelete='CASCADE'), nullable=False)
      creation_date = db.Column(
          db.TIMESTAMP, server_default=db.func.current_timestamp(), nullable=False)

      def __init__(self, users, typeid):
          self.users = users
          self.typeid = typeid

  class UsersSchema(ma.Schema):
      id = fields.Integer(dump_only=True)
      fname = fields.String(required=True, validate=validate.Length(1))
      lname = fields.String(required=True, validate=validate.Length(1))
      email = fields.String(required=True, validate=validate.Length(1))
      phone = fields.String(required=True, validate=validate.Length(10))
      typeid = fields.Integer(required=True)
      creation_date = fields.DateTime()


File name: Users.py

from flask import jsonify, request
  from flask_restful import Resource
  from Model import db, Users, UsersSchema

  users_schema = UsersSchema(many=True)
  user_schema = UsersSchema()


  class UsersResource(Resource):
      def get(self):
          users = Users.query.all()
          users = users_schema.dump(users).data
          return {""status"": ""success"", ""data"": users}, 200

      def post(self):
          json_data = request.get_json(force=True)
          if not json_data:
              return {'message': 'No input data provided'}, 400
          # Validate and deserialize input
          data, errors = user_schema.load(json_data)
          if errors:
              return errors, 422
          user = Users.query.filter_by(email=data['email']).first()
          if user:
              return {'message': 'User already exists'}, 400
          print(json_data['fname'])
          user = Users(
              fname=json_data['fname'],
              lname=json_data['lname'],
              email=json_data['email'],
              phone=json_data['phone'],
              typeid=json_data['typeid'],
          )

          db.session.add(user)
          db.session.commit()

          result = user_schema.dump(user).data

          return {""status"": 'success', 'data': result}, 201

",1,508,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/49288255,SQLAlchemy batch insert with database functions,,1,1498,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/35693081,sqlalchemy: order of query result unexpected,,1,653,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/32786334,Python3 sqlalchemy pymysql gevent sqlalchemy.util.queue.Empty gevent.hub.LoopExit: This operation would block forever,,1,2022,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/29308836,Turbogears nostests results in OperationalError when using SQLAlchemy-FullText-Search,,1,59,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/10739967,Python subprocess calling bcp on .csv: &#39;unexpected eof&#39;,,1,3082,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/76536770,SQLAlchemy: Understanding Select statements within transactions,,0,106,"PostgreSQL uses ""Read Committed"" isolation level. This means that the each query being executed sees data committed earlier.
So, when you are making changes without ""session.commit()"", the changes are not made in the database, so the queries does not see those changes. Once you have committed the changes, the changes are made in database and upcoming queries within same session will see updated data.
To avoid ""session.commit()"" after every query, you can set ""autocommit"" to true.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/77024615,Redshift Sqlalchemy transactions and session management,,0,84,"For this to be happening these different queries would need to be in different transactions (xid).  Can you confirm this is what is happening?  (I say this b/c if they happen in the same transaction you should get the same version of the tables.  And it sounds like from your description that this isn't just result ordering differences.)
So the first solution would be to have everything happen in a single transaction (if possible).  The best way to run this would be with a cursor to store the results between fetches.  See: https://docs.aws.amazon.com/redshift/latest/dg/declare.html and https://docs.aws.amazon.com/redshift/latest/dg/fetch.html
If it isn't possible to be in a single transaction, then can it be in a single session?  This way you could keep the results in a temp table and read from there.
If these queries absolutely need to be made across separate connections then you will need to set up a perm table to hold results.  This has the disadvantage of needing to be cleaned up when the process ends.  You will want to add some per iteration random chars to the table name so that no iteration collisions can happen on the table name.
I'd start by examining the system tables for these queries on Redshift and note the pid and xid of each.  This will give you more specific information on how SQLAlchemy is interacting with Redshift and what changes are needed.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/77644943,"asynch.errors.UnexpectedPacketFromServerError: Code: 102. Unexpected packet from server &lt;host:port&gt; (expected Hello or Exception, got Unknown packet)","I'm trying to connect to clickhouse with sqlalchemy. I'm using:

python3.11
clickhouse-driver == 0.2.6
sqlalchemy == 2.0.23
clickhouse-sqlalchemy == 0.3.0
asynch == 0.2.3
asyncio == 3.4.3

Here is my script I used:
import asyncio
import contextlib
import pydantic
import traceback
import typing
from sqlalchemy import text, TextClause, engine
from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession, create_async_engine
from sqlalchemy.pool import Pool, QueuePool 

class AsyncDatabase:
    def __init__(self):
        self.ch_uri: str = ""clickhouse+asynch://admin:Password123@host:31123/db""
        self.ch_engine: AsyncEngine = create_async_engine(
            url=self.ch_uri,
            echo=False,
            pool_size=100,
            max_overflow=20,
            poolclass=QueuePool,
        )
        self.ch_session: AsyncSession = AsyncSession(bind=self.ch_engine)
        self.ch_pool: Pool = self.ch_engine.pool

async_db: AsyncDatabase = AsyncDatabase()

@contextlib.asynccontextmanager
async def get_ch_session() -&gt; typing.AsyncGenerator[AsyncSession, None]:
    try:
        yield async_db.ch_session
    except Exception as e:
        print(traceback.print_exc())
        await async_db.ch_session.rollback()
    finally:
        await async_db.ch_session.close()


async def hello() -&gt; str:
    session: AsyncSession = None
    async with get_ch_session() as session:
        stmt: TextClause = text(""SELECT * FROM table_name LIMIT 1"")
        result: engine.Result = await session.execute(stmt)
        print(result.all())

    return ""ok""


if __name__ == ""__main__"":
    asyncio.run(hello())

I did try without async but still got the same error
asynch.errors.UnexpectedPacketFromServerError: Code: 102. Unexpected packet from server host:31123 (expected Hello or Exception, got Unknown packet)
when I use DataGrid to connect with above creds, it works fine. so I think it's not about the 31123 port
I stuck on this for 5 hrs, all answers from the web doesn't help me at all.
",0,54,"You tried to use TCP Native protocol, but connected to HTTP based port
look grep tcp_port /var/lib/clickhouse/preprocessed_configs/config.xml
and use this value
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/76663686,"aws python 3.8 lambda + async sqlalchemy 2.0.10, asyncpg 0.28.0","from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession

engine_async = create_async_engine(
    ""postgresql+asyncpg://postgres:password@host_name.rds.amazonaws.com/postgres"")

def get_data(event, context): # this is the lambda handler
    return asyncio.get_event_loop().run_until_complete(get_data_async(event, context))

async def get_datas_async(event, context):
    org_id = event['org_id']
    async_session = sessionmaker(bind=engine_async, future=True, class_=AsyncSession)
    async with async_session() as session1, async_session() as session2, async_session() as session3:
        users_count_query = select(func.count(User.user_id)).filter_by(org_id=org_id)
        org_name_query = select(Organization.name).filter_by(org_id=org_id)
        other_data_query = select(
            func.count(Transaction.transaction_id).label('transactions_count'),
            func.sum(Transaction.total_cost).label('total_cost'),
            func.sum(Transaction.total_time).label('total_time')).where(Transaction.org_id == org_id)

        tasks = [
            session1.execute(users_count_query),
            session2.execute(other_data_query),
            session3.execute(org_name_query)
        ]
        results = await asyncio.gather(*tasks)
        userss_count, data, org_name = results
        data = data.fetchone()
        uesrs_count = users_count.scalar_one()
        org_name = org_name.scalar_one()

        return {
            ""statusCode"": 200,
            ""users_count"": users_count,
            ""transactions_count"": data.transactions_count,
            ""total_cost"": 0 if not data.total_cost else data.total_cost,
            ""total_time"": data.total_time,
            ""org_name"": org_name,
        }

I get this error:
{
    ""errorMessage"": ""Method 'close()' can't be called here; method '_connection_for_bind()' is already in progress and this would cause an unexpected state change to &lt;SessionTransactionState.CLOSED: 5&gt;"",
    ""errorType"": ""IllegalStateChangeError"",
    ""stackTrace"": [
        ""  File \""/var/task/app.py\"", line 131, in get_data\n    return asyncio.get_event_loop().run_until_complete(get_data_async(event, context))\n"",
        ""  File \""/var/lang/lib/python3.8/asyncio/base_events.py\"", line 616, in run_until_complete\n    return future.result()\n"",
        ""  File \""/var/task/app.py\"", line 175, in get_data_async\n    return {\n"",
        ""  File \""/var/task/sqlalchemy/ext/asyncio/session.py\"", line 859, in __aexit__\n    await asyncio.shield(task)\n"",
        ""  File \""/var/task/sqlalchemy/ext/asyncio/session.py\"", line 840, in close\n    await greenlet_spawn(self.sync_session.close)\n"",
        ""  File \""/var/task/sqlalchemy/util/_concurrency_py3k.py\"", line 154, in greenlet_spawn\n    result = context.switch(*args, **kwargs)\n"",
        ""  File \""/var/task/sqlalchemy/orm/session.py\"", line 2382, in close\n    self._close_impl(invalidate=False)\n"",
        ""  File \""/var/task/sqlalchemy/orm/session.py\"", line 2424, in _close_impl\n    transaction.close(invalidate)\n"",
        ""  File \""&lt;string&gt;\"", line 2, in close\n"",
        ""  File \""/var/task/sqlalchemy/orm/state_changes.py\"", line 121, in _go\n    raise sa_exc.IllegalStateChangeError(\n""
    ]
}

",0,209,"solution and explanation are here: https://github.com/sqlalchemy/sqlalchemy/discussions/9312
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/75071043,Read from the server failed (20004) (SQLExecDirectW)Read from the server failed (20004) (SQLExecDirectW),"I am using the sqlalchemy to access sql server using pyodbc in Ubuntu OS
the sql server runs in docker container
the url for connecting the server in sqlalchemy, which I used is
'mssql+pyodbc:///?odbc_connects=""Driver=;SERVER=;DATABASE=;UID=;PWD=;port=;TDS_Version=8.0""
I run this statement - result = session.query(User).filter(User.username == username).first()
I got this error
(pyodbc.OperationalError) ('08S01', '[08S01] [FreeTDS][SQL Server]Read from the server failed (20004) (SQLExecDirectW)')
[SQL: SELECT TOP 1 [user].user_id AS user_user_id, [user].username AS user_username, [user].password AS user_password, [user].first_name AS user_first_name, [user].last_name AS user_last_name, [user].designation AS user_designation, [user].last_login AS user_last_login
FROM [user]
WHERE [user].username = ?]
[parameters: ('device1',)]
(Background on this error at: http://sqlalche.me/e/e3q8)
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1249, in _execute_context
    cursor, statement, parameters, context
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 552, in do_execute
    cursor.execute(statement, parameters)
pyodbc.OperationalError: ('08S01', '[08S01] [FreeTDS][SQL Server]Read from the server failed (20004) (SQLExecDirectW)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""./database1.py"", line 43, in get_user_details
    result = session.query(User).filter(User.username == username).first()
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3232, in first
    ret = list(self[0:1])
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3018, in __getitem__
    return list(res)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3334, in __iter__
    return self._execute_and_instances(context)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3359, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 988, in execute
    return meth(self, multiparams, params)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/sql/elements.py"", line 287, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1107, in _execute_clauseelement
    distilled_params,
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1253, in _execute_context
    e, statement, parameters, cursor, context
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1473, in _handle_dbapi_exception
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 152, in reraise
    raise value.with_traceback(tb)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1249, in _execute_context
    cursor, statement, parameters, context
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 552, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (pyodbc.OperationalError) ('08S01', '[08S01] [FreeTDS][SQL Server]Read from the server failed (20004) (SQLExecDirectW)')
[SQL: SELECT TOP 1 [user].user_id AS user_user_id, [user].username AS user_username, [user].password AS user_password, [user].first_name AS user_first_name, [user].last_name AS user_last_name, [user].designation AS user_designation, [user].last_login AS user_last_login
FROM [user]
WHERE [user].username = ?]
[parameters: ('device1',)]
(Background on this error at: http://sqlalche.me/e/e3q8)

Due to this error, all the following queries are triggering the different error each time-
sqlalchemy.exc.StatementError: (sqlalchemy.exc.InvalidRequestError) Can't reconnect until invalid transaction is rolled back

sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('01000', '[01000] [FreeTDS][SQL Server]Unexpected EOF from the server (20017) (SQLExecDirectW)')

pyodbc.OperationalError: ('08S01', '[08S01] [FreeTDS][SQL Server]Communication link failure (0) (SQLExecDirectW)')

sqlalchemy.exc.ResourceClosedError: This result object does not return rows. It has been closed automatically.

sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('HY000', '[HY000] [FreeTDS][SQL Server]Unknown error (0) (SQLExecDirectW)')

pyodbc.OperationalError: ('08S01', '[08S01] [FreeTDS][SQL Server]Bad token from the server: Datastream processing out of sync (20020) (SQLExecDirectW)')

sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('HY008', '[HY008] [FreeTDS][SQL Server]Operation was cancelled (0) (SQLExecDirectW)')

2023-01-10 16:38:20,010 - ERROR - sqlalchemy.pool.impl.QueuePool - Exception during reset or similar
dbapi_connection.rollback()
pyodbc.Error: ('HY000', 'The driver did not supply an error!')

What is the cause of this error?
UPDATE
the sql server version - select @@verion -

Microsoft SQL Server 2017 (RTM-CU22) (KB4577467) - 14.0.3356.20 (X64)
Aug 20 2020 22:33:27
Copyright (C) 2017 Microsoft Corporation
Express Edition (64-bit) on Linux (Ubuntu 16.04.7 LTS)

",0,439,"You are using FreeTDS instead of the supported Microsoft ODBC Driver for SQL Server.  Download instructions are here.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/74461385,Select specific columns with cast using SQLAlchemy,"I'm using SQLAlchemy (Version: 1.4.44) and I'm having some unexpected results when trying to select columns and using cast on those columns.
First, most of the examples and even current documentation suggests column selection should work by passing an array to the select function like this:
s = select([table.c.col1])

However, I get the following error if I try this:
s = my_table.select([my_table.columns.user_id])

sqlalchemy.exc.ArgumentError: SQL expression for WHERE/HAVING role expected, got [Column('user_id', String(), table=&lt;my_table&gt;)].

Some examples suggest just placing the field directly in the select query.
s = select(table.c.col1)


But this seems to do nothing more than create an idle where-clause out of the field.
I eventually was able to achieve column selection with this approach:
s = my_table.select().with_only_columns(my_table.columns.created_at)


But I am not able to use cast for some reason with this approach.
s = my_table.select().with_only_columns(cast(my_table.columns.created_at, Date))

ValueError: Couldn't parse date string '2022' - value is not a string.

All help appreciated!
",0,214,"I don't think table.select() is common usage.  SQLAlchemy is in a big transition right now on its way to 2.0.  In 1.4 (and in 2) the following syntax should work, use whatever session handling you already have working I just mean the select(...):
from sqlalchemy.sql import select, cast
from sqlalchemy.dialects.postgresql import INTEGER

class User(Base):
    __tablename__ = ""users""
    id = Column(
        Integer, nullable=False, primary_key=True
    )
    name = Column(Text)

with Session(engine) as session:
    u1 = User(name=""1"")
    session.add(u1)
    session.commit()

with Session(engine) as session:
    my_table = User.__table__
    # Cast user name into integer.
    print (session.execute(select(cast(my_table.c.name, INTEGER))).all())

",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/72381857,Unable to get an id from the database after doing insert using sqlalchemy,"Assuming this model:
class Pipeline(Base):
    __tablename__ = ""pipeline""
    pipeline_id = Column(String, primary_key=True)
    pipeline_version_id = Column(Integer, primary_key=True, autoincrement=True)
    is_active = Column(Boolean, nullable=False)
    update_ts = Column(DateTime, nullable=False)
    config = Column(TEXT(), nullable=True)
    parser_version = Column(String, nullable=True)

And this insert code:
        async with self.sessionFactory() as session:
            async with session.begin():
                query = select(Pipeline).filter_by(pipeline_id=pipeline_id).filter_by(is_active=True)
                result = await session.execute(query)
                active_pipelines = result.scalars().all()
                for entry in active_pipelines:
                    entry.is_active = False
                    self.logger.info(f""Marking pipeline/version {entry.pipeline_id}/{entry.pipeline_version_id} inactive"")

                self.logger.info(f""Creating update_pipeline:  {updated_pipeline}"")

                session.add(updated_pipeline)
                pipeline_version_id = updated_pipeline.pipeline_version_id
                self.logger.info('adding updated pipeline with version id {id}'.format(id=str(pipeline_version_id)))

When I run this, it is returning a null pipeline_version_id
[Edit]
Here are other alternatives I tried:
        async with self.sessionFactory() as session:
            async with session.begin():
                query = select(Pipeline).filter_by(pipeline_id=pipeline_id).filter_by(is_active=True)
                result = await session.execute(query)
                active_pipelines = result.scalars().all()
                for entry in active_pipelines:
                    entry.is_active = False
                    self.logger.info(f""Marking pipeline/version {entry.pipeline_id}/{entry.pipeline_version_id} inactive"")
                self.logger.info(f""Creating update_pipeline:  {updated_pipeline}"")
                session.add(updated_pipeline)

        pipeline_version_id = updated_pipeline.pipeline_version_id
        self.logger.info('adding updated pipeline with version id {id}'.format(id=str(pipeline_version_id)))

Result: Failed to update DB. Error: Instance &lt;Pipeline at 0x110c31da0&gt; is not bound to a Session; attribute refresh operation cannot proceed (Background on this error at: https://sqlalche.me/e/14/bhk3)
        async with self.sessionFactory() as session:
            async with session.begin():
                query = select(Pipeline).filter_by(pipeline_id=pipeline_id).filter_by(is_active=True)
                result = await session.execute(query)
                active_pipelines = result.scalars().all()
                for entry in active_pipelines:
                    entry.is_active = False
                    self.logger.info(f""Marking pipeline/version {entry.pipeline_id}/{entry.pipeline_version_id} inactive"")
                self.logger.info(f""Creating update_pipeline:  {updated_pipeline}"")
                session.add(updated_pipeline)

            pipeline_version_id = updated_pipeline.pipeline_version_id
            self.logger.info('adding updated pipeline with version id {id}'.format(id=str(pipeline_version_id)))

Result: Failed to update DB. Error: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/14/xd2s)
How do i get the ID of the inserted record without doing another query?
These are my dependencies:
python = ""^3.7""
setuptools = ""&lt;58""
fastapi = ""^0.54.1""
gunicorn = ""&lt;20.0""
uvicorn = ""^0.11.5""
sqlalchemy = ""^1.4.25""
apache-airflow=""1.10.10""
psycopg2-binary = ""^2.9.1""

Here is my session creation:
def create_session_factory(db_conn=None):
    #
    # Using https://rogulski.it/blog/sqlalchemy-14-async-orm-with-fastapi/
    #
    if not db_conn:
        db_conn = get_default_parser_db_conn_str()
    global _engine
    if not _engine:
        _engine = create_async_engine(db_conn, **DB_ENGINE_OPTIONS)
    sessionFactory = sessionmaker(_engine, class_=AsyncSession)
    sessionFactory.configure()
    return sessionFactory

and the DB_ENGINE_OPTIONS
DB_ENGINE_OPTIONS = {
    ""poolclass"": QueuePool,
    ""pool_size"": 10,
    ""max_overflow"": 50,
    ""pool_recycle"": 3600,
    ""pool_timeout"": 30
}

Any help appreciated.
this is my first time at async DB calls with python, so I am sure i am missing something.
",0,130,"Thank you all, who answered.
Ultimately, just adding a session.flush() after the call to session.add(obj) gave me what i needed.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/71004077,"How to get average value from SQL(Superset, SQLAlchemy under PostgreSQL) timestamp","I tried to calculate the average timestamp and the query is successfully executed directly in postgresql, but when executed through superset(python's SQLAlchemy), an error occurs when converting an integer to a timestamp. How can I fix the error or do it in another way?
Query1:
select 
    to_timestamp(
        avg(
            cast(
                extract(epoch from last_checked_at) as integer)
                )::integer
        ) as datetime
from
    the_best_table

Query1 postgresql result:




datetime




2022-02-06 03:15:19+00




Query1 superset(SQLAlchemy) result:

PostgreSQL Error postgresql error: '&gt;=' not supported between
instances of 'datetime.timedelta' and 'int'
This may be triggered by: Issue 1002 - The database returned an
unexpected error. link



 

 
Also below you can see the results of the query to convert an integer to datetime. The problem is in this part of the code.
Query2:
select to_timestamp(1644117319) as datetime

Query2 postgresql result:




datetime




2022-02-06 03:15:19+00




Query2 superset(SQLAlchemy) result:

PostgreSQL Error postgresql error: '&gt;=' not supported between
instances of 'datetime.timedelta' and 'int'
This may be triggered by: Issue 1002 - The database returned an
unexpected error. link

",0,600,"I was not able to reproduce the error.
Works fine.
Setup in Postgresql Database:
postgres=&gt; create table the_best_table(last_checked_at timestamp);
CREATE TABLE
postgres=&gt; insert into the_best_table values(now());
INSERT 0 1
postgres=&gt; select
    to_timestamp(
        avg(
            cast(
                extract(epoch from last_checked_at) as integer)
                )::integer
        ) as datetime
from
    the_best_table;
        datetime
------------------------
 2022-02-07 04:54:06+09
(1 row)

The exact two queries run fine for me in SQLAlchemy:
&gt;&gt;&gt; from sqlalchemy import create_engine
&gt;&gt;&gt; engine = create_engine('postgresql+pg8000://&lt;username&gt;:&lt;passwd&gt;@192.xxx.xxx.123/postgres')
&gt;&gt;&gt;  metadata.create_all(engine)
&gt;&gt;&gt; with engine.connect() as con:
...     rs = con.execute('select to_timestamp(avg(cast(extract(epoch from last_checked_at) as integer))::integer ) as datetime from the_best_table;')
...     for row in rs:
...             print(row)
... 
(datetime.datetime(2022, 2, 7, 4, 54, 6, tzinfo=datetime.timezone(datetime.timedelta(seconds=32400))),)
&gt;&gt;&gt; with engine.connect() as con:
...     rs = con.execute('select to_timestamp(1644117319) as datetime;')
...     for row in rs:
...             print(row)
... 
(datetime.datetime(2022, 2, 6, 12, 15, 19, tzinfo=datetime.timezone(datetime.timedelta(seconds=32400))),)

",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/69447368,Connecting to MySQL using a token,"I've been trying to connect to an RDS instance using the sqlalchemy library using a token. According to the docs it should be possible via the cparams['token'] variable, but doing so I get an error of an unexpected argument.
Going deeper into the docs, there was this code where the connection could be established using the keyword argument attrs_before, but in this case such argument also does not exist.
Does anyone has any idea on how to connect to the DB using a token?
I've trying to do it via such code but with no effect:
import boto3
import struct
from sqlalchemy import create_engine, event
from sqlalchemy.engine.url import URL
from sqlalchemy.engine.url import URL
from sqlalchemy.orm import sessionmaker

SQL_COPT_SS_ACCESS_TOKEN = 1256
session = boto3.session.Session(profile_name='xxx')
rds = session.client('rds')

def get_authentication_token():
    return rds.generate_db_auth_token(DBHostname='xxxx', Port=3306, DBUsername='xxx')

engine_url = URL.create(drivername='mysql+pymysql', host='xxxx')
engine = create_engine(engine_url)

#@event.listens_for(engine, 'do_connect')
#def provide_token(dialect, conn_rec, cargs, cparams):
#    token_struct = struct.pack(f'&lt;I{len(token)}s', len(token), token)
#    cparams['attrs_before'] = {SQL_COPT_SS_ACCESS_TOKEN: token_struct}

@event.listens_for(engine, ""do_connect"")
def provide_token(dialect, conn_rec, cargs, cparams):
    cparams['token'] = get_authentication_token()

Session = sessionmaker(bind=engine)

with Session() as session:
    result = session.execute('select now()').first()


To be more precise I get the error:
TypeError: __init__() got an unexpected keyword argument 'token'
",0,758,"I had the same issue, what solved for me was replacing token with password:
@event.listens_for(engine, ""do_connect"")
def provide_token(dialect, conn_rec, cargs, cparams):
    cparams['password'] = get_authentication_token()

Another remark is that you must also pass the SSL certificate in the connection, I simply added another param:
@event.listens_for(engine, ""do_connect"")
def provide_token(dialect, conn_rec, cargs, cparams):
    cparams['password'] = get_authentication_token()
    caparam['ssl'] = {""ca"": ""rds-combined-ca-bundle.pem""}

Bear in mind that you might have to pass the full path to the rds-combined-ca-bundle.pem file. If you don't pass the SSL file and the connection param you'll still get an Access Denied even if everything else is setup correctly.
The SSL file can be downloaded from https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/66277687,Save an object using Celery and Gino,"I have the following pipeline
Event model (based on Gino 'db' object):
class Event(db.Model):
    __tablename__ = ""events""

    id = db.Column(db.BigInteger(), primary_key=True)
    request_timestamp = db.Column(db.DateTime(), nullable=False)
    service = db.Column(db.String(), nullable=False)
    url = db.Column(db.String(), nullable=False)
    status_code = db.Column(db.Integer(), nullable=False)
    response_time = db.Column(db.DateTime(), nullable=False)

FastApi app has a POST view with call to celery task to pass the data of the Event object:
@router.post(""/events/add"")
async def add_event(event: EventModel):
    event_data = {'request_timestamp': event.request_timestamp.replace(tzinfo=None),
                  'service': event.service,
                  'url': event.url,
                  'status_code': event.status_code,
                  'response_time': event.response_time.replace(tzinfo=None)
                  }

    task = celery_app.send_task(""monitoring_service.src.monitoring_service.worker.celery_worker.add_to_db"",
                                kwargs=event_data)

    return JSONResponse(content=""Event recorded successfully"", status_code=200)

And celery worker which must save an Event object to database:
@celery_app.task(acks_late=True)
async def add_to_db(request_timestamp, service, url, status_code, response_time):
    event = await Event.create(
        request_timestamp=datetime.strptime(request_timestamp, '%Y-%m-%dT%H:%M:%S.%f'),
        service=service,
        url=url,
        status_code=status_code,
        response_time=datetime.strptime(response_time, '%Y-%m-%dT%H:%M:%S.%f'),
    )

    return {""status"": True}

Now I'm receiving an error during object saving process:
Traceback (most recent call last):

File ""/usr/local/lib/python3.8/site-packages/celery/app/trace.py"", line 479, in trace_task

mark_as_done(

File ""/usr/local/lib/python3.8/site-packages/celery/backends/base.py"", line 158, in mark_as_done

self.store_result(task_id, result, state, request=request)

File ""/usr/local/lib/python3.8/site-packages/celery/backends/base.py"", line 442, in store_result

self._store_result(task_id, result, state, traceback,

File ""/usr/local/lib/python3.8/site-packages/celery/backends/database/__init__.py"", line 51, in _inner

return fun(*args, **kwargs)

File ""/usr/local/lib/python3.8/site-packages/celery/backends/database/__init__.py"", line 130, in _store_result

session.commit()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 1042, in commit

self.transaction.commit()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 504, in commit

self._prepare_impl()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 483, in _prepare_impl

self.session.flush()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2523, in flush

self._flush(objects)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2664, in _flush

transaction.rollback(_capture_exception=True)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py"", line 68, in __exit__

compat.raise_(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_

raise exception

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2624, in _flush

flush_context.execute()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 422, in execute

rec.execute(self)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 586, in execute

persistence.save_obj(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 230, in save_obj

_emit_update_statements(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 994, in _emit_update_statements

c = cached_connections[connection].execute(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1014, in execute

return meth(self, multiparams, params)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py"", line 298, in _execute_on_connection

return connection._execute_clauseelement(self, multiparams, params)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1127, in _execute_clauseelement

ret = self._execute_context(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1207, in _execute_context

self._handle_dbapi_exception(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1511, in _handle_dbapi_exception

util.raise_(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_

raise exception

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1205, in _execute_context

context = constructor(dialect, self, conn, *args)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 858, in _init_compiled

param = dict(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 861, in &lt;genexpr&gt;

processors[key](compiled_params[key])

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/sqltypes.py"", line 1689, in process

value = dumps(value, protocol)

sqlalchemy.exc.SQLAlchemyError: (builtins.TypeError) cannot pickle 'coroutine' object


warn(RuntimeWarning(

[2021-02-19 10:19:39,010: ERROR/ForkPoolWorker-1] Task monitoring_service.src.monitoring_service.worker.celery_worker.add_to_db[994ae754-f216-4fa9-b502-5fbb0221011c] raised unexpected: SQLAlchemyError(""(builtins.TypeError) cannot pickle 'coroutine' object"")

Traceback (most recent call last):

File ""/usr/local/lib/python3.8/site-packages/celery/app/trace.py"", line 479, in trace_task

mark_as_done(

File ""/usr/local/lib/python3.8/site-packages/celery/backends/base.py"", line 158, in mark_as_done

self.store_result(task_id, result, state, request=request)

File ""/usr/local/lib/python3.8/site-packages/celery/backends/base.py"", line 442, in store_result

self._store_result(task_id, result, state, traceback,

File ""/usr/local/lib/python3.8/site-packages/celery/backends/database/__init__.py"", line 51, in _inner

return fun(*args, **kwargs)

File ""/usr/local/lib/python3.8/site-packages/celery/backends/database/__init__.py"", line 130, in _store_result

session.commit()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 1042, in commit

self.transaction.commit()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 504, in commit

self._prepare_impl()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 483, in _prepare_impl

self.session.flush()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2523, in flush

self._flush(objects)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2664, in _flush

transaction.rollback(_capture_exception=True)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py"", line 68, in __exit__

compat.raise_(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_

raise exception

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2624, in _flush

flush_context.execute()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 422, in execute

rec.execute(self)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 586, in execute

persistence.save_obj(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 230, in save_obj

_emit_update_statements(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 994, in _emit_update_statements

c = cached_connections[connection].execute(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1014, in execute

return meth(self, multiparams, params)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py"", line 298, in _execute_on_connection

return connection._execute_clauseelement(self, multiparams, params)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1127, in _execute_clauseelement

ret = self._execute_context(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1207, in _execute_context

self._handle_dbapi_exception(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1511, in _handle_dbapi_exception

util.raise_(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_

raise exception

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1205, in _execute_context

context = constructor(dialect, self, conn, *args)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 858, in _init_compiled

param = dict(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 861, in &lt;genexpr&gt;

processors[key](compiled_params[key])

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/sqltypes.py"", line 1689, in process

value = dumps(value, protocol)

sqlalchemy.exc.SQLAlchemyError: (builtins.TypeError) cannot pickle 'coroutine' object

If I call event = await Event.create(...) inside a view it is successfull and saves data to db.
If I delete 'async/await' from Celery worker, this error doesn't show up, Celery task has 'SUCCESS' state  but database is empty.
Can't really understand what is not right.
",0,506,"Simple: currently, Celery is not able to handle asynchronous functions as tasks. You need to wrap it with asyncio.run (https://docs.python.org/3/library/asyncio-task.html#asyncio.run):
async def add_to_db(request_timestamp, service, url, status_code, response_time):
    event = await Event.create(
        request_timestamp=datetime.strptime(request_timestamp, '%Y-%m-%dT%H:%M:%S.%f'),
        service=service,
        url=url,
        status_code=status_code,
        response_time=datetime.strptime(response_time, '%Y-%m-%dT%H:%M:%S.%f'),
    )

@celery_app.task(acks_late=True)
def add_to_db_task(request_timestamp, service, url, status_code, response_time):
    asyncio.run(add_to_db(request_timestamp, service, url, status_code, response_time))
    return {""status"": True}

",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/66274054,Great Expectations custom expectation not ignoring nulls as requested,"Versions of the libraries we're using:
snowconn==3.7.1
snowflake-connector-python==2.3.10
snowflake-sqlalchemy==1.2.3
SQLAlchemy==1.3.23
great_expectations==0.13.10
pandas==1.1.5

Note we're grabbing data from Snowflake on our own and then feeding a dataframe of it into Great Expectations. I'm aware GE has a Snowflake data source and it's on my list to add it. But I think this setup should work even without using that data source.
We have the following Great Expectations data context config:
    data_context_config = DataContextConfig(
        datasources={
            datasource_name: DatasourceConfig(
                class_name='PandasDatasource',
                data_asset_type={
                    'module_name': 'dataqa.dataset',
                    'class_name': 'CustomPandasDataset'
                }
            )
        },
        store_backend_defaults=S3StoreBackendDefaults(
            default_bucket_name=METADATA_BUCKET,
            expectations_store_prefix=EXPECTATIONS_PATH,
            validations_store_prefix=VALIDATIONS_PATH,
            data_docs_prefix=DATA_DOCS_PATH,
        ),
        validation_operators={
            ""action_list_operator"": {
                ""class_name"": ""ActionListValidationOperator"",
                ""action_list"": [
                    {
                        ""name"": ""store_validation_result"",
                        ""action"": {""class_name"": ""StoreValidationResultAction""},
                    },
                    {
                        ""name"": ""store_evaluation_params"",
                        ""action"": {""class_name"": ""StoreEvaluationParametersAction""},
                    },
                    {
                        ""name"": ""update_data_docs"",
                        ""action"": {""class_name"": ""UpdateDataDocsAction""},
                    },
                ],
            }
        }
    )
    ge_context = BaseDataContext(project_config=data_context_config)

CustomPandasDataset is defined as:
class CustomPandasDataset(PandasDataset):
    _data_asset_type = ""CustomPandasDataset""

    @MetaPandasDataset.multicolumn_map_expectation
    def expect_column_A_equals_column_B_column_C_ratio(
        self,
        column_list,
        ignore_row_if='any_value_is_missing'
    ):
        column_a = column_list.iloc[:,0]
        column_b = column_list.iloc[:,1]
        column_c = column_list.iloc[:,2]

        return abs(column_a - (1.0 - (column_b/column_c))) &lt;= 0.001

and called like:
    cols = ['a', 'b', 'c']
    batch.expect_column_A_equals_column_B_column_C_ratio(
        cols,
        catch_exceptions=True
    )

Later on we validate the data context like so:
    return ge_context.run_validation_operator(
        ""action_list_operator"",
        assets_to_validate=batches,
        run_id=run_id)[""success""]

Often times, columns a and b are null in our data. Given I've set the ignore_row_if='any_value_is_missing' flag on the custom expectation, I'm expecting rows with null values in any of columns a, b, or c to be skipped. But Great Expectations doesn't skip them, instead adding them to the unexpected, or ""failed"" field of output:
result  
element_count   1000
missing_count   0
missing_percent 0
unexpected_count    849
unexpected_percent  84.89999999999999
unexpected_percent_total    84.89999999999999
unexpected_percent_nonmissing   84.89999999999999result 
element_count   1000
missing_count   0
missing_percent 0
unexpected_count    849
unexpected_percent  84.89999999999999
unexpected_percent_total    84.89999999999999
unexpected_percent_nonmissing   84.89999999999999


partial_unexpected_list 

0   
a   null
b   null
c   1.63

I'm unsure why this is happening. In the Great Expectations source, the multicolumn_map_expectation does:
...
            elif ignore_row_if == ""any_value_is_missing"":
                boolean_mapped_skip_values = test_df.isnull().any(axis=1)
...
            boolean_mapped_success_values = func(
                self, test_df[boolean_mapped_skip_values == False], *args, **kwargs
            )
            success_count = boolean_mapped_success_values.sum()
            nonnull_count = (~boolean_mapped_skip_values).sum()
            element_count = len(test_df)

            unexpected_list = test_df[
                (boolean_mapped_skip_values == False)
                &amp; (boolean_mapped_success_values == False)
            ]
            unexpected_index_list = list(unexpected_list.index)

            success, percent_success = self._calc_map_expectation_success(
                success_count, nonnull_count, mostly
            )

which I interpret as ignoring null-containing rows (not adding them to the unexpected list and not using them to determine percent_success). I've dropped a pdb in our code and verified that the dataframe we're calling the expectation on can be manipulated in the correct way to get ""sensible"" data (test_df.isnull().any(axis=1)), but for some reason Great Expectations is allowing those nulls to slip through. Anyone know why?
",0,2069,"I believe the poster filed a Github issue here: https://github.com/great-expectations/great_expectations/issues/2460. The progress can be tracked there.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/64046595,Unable to use amancevice/superset docker image to use Oracle DB as datasource,"I'm trying to test if superset would be adecuate for our data visualization needs. As I've no knowledge of python, I'm using a docker image of a superset installation (v0.37.1) published with all the libraries needed to connect to different databases (https://github.com/amancevice/docker-superset). When I try to set up a new database source to an Oracle database I get this error in the superset.log:
Unexpected error (cx_Oracle.DatabaseError) DPI-1047: Cannot locate a 64-bit Oracle Client library: ""libclntsh.so: cannot open shared object file: No such file or directory"". See https://oracle.github.io/odpi/doc/installation.html#linux for help
(Background on this error at: http://sqlalche.me/e/13/4xp6)

After googling I've added to the container (when I have this solved I'll investigate how to create a new image with all the changes I've performed directly to the container) the Oracle Instant Client library and modified the .bashrc file of the container (for the user superset) to add a new env variable LD_LIBRARY_PATH pointing to the directory with the Oracle Instant Client.
I've restarted the container.
To check that SQLAlchemy can connect to my OracleDB correctly I've created a test_script.py:
import sqlalchemy as sa
engine = sa.create_engine('oracle+cx_oracle://user:password@host:port/?service_name=service')
with engine.connect() as connection:
    result = connection.execute(""select * from dual"")
    for row in result:
        print(row)

And then if I connect to the container directly docker exec -it my_container bash and execute my script python test_script.py, I get the correct results and no error message, however, connecting to the superset url and trying to create a new datasource to that Oracle DB, I'm still getting the same error.
Does someone know if there's another configuration change needed in the container to make this work?
",0,952,"When you download Oracle Instant Client Basic or Basic Light ZIP packages from here, then run something like this in your Dockerfile:
RUN echo /opt/oracle/instantclient_19_8 &gt; /etc/ld.so.conf.d/oic.conf &amp;&amp; \
    ldconfig

Also see sample docker images for Linux developers and for basic Oracle Instant Client.  If you are not on an RPM based system, check out the sample Docker files in Docker for Oracle Database Applications in Node.js and Python.
The details vary with what base Docker image you are using, and whether you want to install Instant Client from ZIP files or RPMs.
Update: prebuilt containers are available from Oracle's GitHub Container Registry.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/58691132,Flask SQLAlchemy - Default filter value if no results for provided filter,"Using Flask SQLAlchemy I'm querying a MySQL database with a table called bar and looking for rows that match a filter consisting of foo and country_code:

foo_filter = 'hello'
country_code_filter = 'ES'
result = Bar.filter_by(foo=foo_filter, country_code=country_code_filter).first()


The above code will return the first row in which foo = foo_filter and country_code = country_code_filter.

However, it's possible that we might not have rows for certain country codes that match foo. In these cases (i.e, cases where the above query returns 0 results), I'd like to use a default country filter of 'RoW' as our dataset should always have an RoW value for each possible value of foo. In the unexpected occurrence that this also doesn't return any results then an error should be thrown. This is the code I have for this:

foo_filter = 'hello'
country_code_filter = 'ES'
result = Bar.filter_by(foo=foo_filter, country_code=country_code_filter).first()
if not result:
    result = Bar.filter_by(foo=foo_filter, country_code='RoW').first()
if not result:
    raise RuntimeException(f""No data for combination {foo_filter}, {country_code_filter} or {foo_filter}, RoW"")


This approach of running similar queries multiple times and checking the result each time until I get a row feels very messy/wrong but I haven't been able to find any better approaches that allow you to set an 'alternative' filter when your initial query returns 0 rows in Flask SQLAlchemy

Is there a cleaner approach to this?
",0,1239,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/58534100,"TimeoutError QueuePool limit of size x overflow xy reached, connection timed out, timeout 30","This is the error that I get:
RemoteError: Remote error: TimeoutError QueuePool limit of size x overflow xy reached, connection timed out, timeout 30 (Background on this error at: http://sqlalche.me/e/3o7r)

We have an issue with metadata agent. One compute node hosts many vms and the vms can't get ip. I've tried to restart on the dhcp metadata server both service, tried to restart our rabbitmq server as well. Also restarted neutron server, but the issue still persist and the vms are down.

Errors like these:

2019-10-24 10:34:47.794 86154 ERROR neutron.agent.metadata.agent
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent [-] Unexpected error.: RemoteError: Remote error: TimeoutError QueuePool limit of size 5 overflow 50 reached, connection timed out, timeout 30 (Background on this error at: http://sqlalche.me/e/3o7r)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent Traceback (most recent call last):
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 89, in __call__
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     instance_id, tenant_id = self._get_instance_and_tenant_id(req)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 162, in _get_instance_and_tenant_id
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     ports = self._get_ports(remote_address, network_id, router_id)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 155, in _get_ports
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return self._get_ports_for_remote_address(remote_address, networks)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/common/cache_utils.py"", line 116, in __call__
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return self.func(target_self, *args, **kwargs)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 137, in _get_ports_for_remote_address
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     ip_address=remote_address)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 106, in _get_ports_from_server
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return self.plugin_rpc.get_ports(self.context, filters)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 72, in get_ports
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return cctxt.call(context, 'get_ports', filters=filters)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/common/rpc.py"", line 150, in call
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return self._original_context.call(ctxt, method, **kwargs)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py"", line 179, in call
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     retry=self.retry)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/oslo_messaging/transport.py"", line 133, in _send
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     retry=retry)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py"", line 584, in send
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     call_monitor_timeout, retry=retry)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py"", line 575, in _send
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     raise result
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent RemoteError: Remote error: TimeoutError QueuePool limit of size 5 overflow 50 reached, connection timed out, timeout 30 (Background on this error at: http://sqlalche.me/e/3o7r)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent [u'Traceback (most recent call last):\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_messaging/rpc/server.py"", line 163, in _process_incoming\n    res = self.dispatcher.dispatch(message)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py"", line 265, in dispatch\n    return self._do_dispatch(endpoint, method, ctxt, args)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py"", line 194, in _do_dispatch\n    result = func(ctxt, **new_args)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron/api/rpc/handlers/metadata_rpc.py"", line 43, in get_ports\n    return self.plugin.get_ports(context, filters=filters)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron/db/api.py"", line 123, in wrapped\n    return method(*args, **kwargs)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron_lib/db/api.py"", line 140, in wrapped\n    setattr(e, \'_RETRY_EXCEEDED\', True)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 220, in __exit__\n    self.force_reraise()\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 196, in force_reraise\n    six.reraise(self.type_, self.value, self.tb)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron_lib/db/api.py"", line 136, in wrapped\n    return f(*args, **kwargs)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_db/api.py"", line 154, in wrapper\n    ectxt.value = e.inner_exc\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 220, in __exit__\n    self.force_reraise()\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 196, in force_reraise\n    six.reraise(self.type_, self.value, self.tb)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_db/api.py"", line 142, in wrapper\n    return f(*args, **kwargs)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron_lib/db/api.py"", line 183, in wrapped\n    LOG.debug(""Retry wrapper got retriable exception: %s"", e)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 220, in __exit__\n    self.force_reraise()\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 196, in force_reraise\n    six.reraise(self.type_, self.value, self.tb)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron_lib/db/api.py"", line 179, in wrapped\n    return f(*dup_args, **dup_kwargs)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron/db/db_base_plugin_v2.py"", line 1435, in get_ports\n    items = [self._make_port_dict(c, fields) for c in query]\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2925, in __iter__\n    return self._execute_and_instances(context)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2946, in _execute_and_instances\n    close_with_result=True)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2955, in _get_bind_args\n    **kw\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2937, in _connection_from_session\n    conn = self.session.connection(**kw)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 1035, in connection\n    execution_options=execution_options)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 1042, in _connection_for_bind\n    conn = engine.contextual_connect(**kw)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2123, in contextual_connect\n    self._wrap_pool_connect(self.pool.connect, None),\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2158, in _wrap_pool_connect\n    return fn()\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py"", line 403, in connect\n    return _ConnectionFairy._checkout(self)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py"", line 788, in _checkout\n    fairy = _ConnectionRecord.checkout(pool)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py"", line 532, in checkout\n    rec = pool._do_get()\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py"", line 1186, in _do_get\n    (self.size(), self.overflow(), self._timeout), code=""3o7r"")\n', u'TimeoutError: QueuePool limit of size 5 overflow 50 reached, connection timed out, timeout 30 (Background on this error at: http://sqlalche.me/e/3o7r)\n'].
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent
2019-10-24 10:35:04.436 86151 WARNING oslo_messaging._drivers.amqpdriver [-] Number of call queues is 21, greater than warning threshold: 20. There could be a leak. Increasing threshold to: 40
2019-10-24 10:35:04.637 86156 WARNING oslo_messaging._drivers.amqpdriver [-] Number of call queues is 21, greater than warning threshold: 20. There could be a leak. Increasing threshold to: 40
2019-10-24 10:35:05.339 86149 WARNING oslo_messaging._drivers.amqpdriver [-] Number of call queues is 21, greater than warning threshold: 20. There could be a leak. Increasing threshold to: 40


What should I do to fix it :( ?

Or how can I finetune this sqlalchemy? I have no idea to be honest what is this.
",0,797,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/56877620,SqlAlchemy Query.All() Unexpectedly Returning OperationalError,"I'm implementing search functionality using Elasticsearch in a ""Reddit clone"" web application that I'm developing. I want to support searching for threads, users, and subreddits, but when I enter a search query and search for one of the 3 above mentioned categories that does not hold any matches, I'm getting an unexpected ""OperationalError"" instead of an empty set of results.

As shown in the code I included, I attempted to use the sqlalchemy.orm.query.Query.all() function which returned the following error:

OperationalError: (sqlite3.OperationalError) near ""END"": syntax error
[SQL: SELECT user.id AS user_id, user.username AS user_username, user.email AS user_email, user.password_hash AS user_password_hash, user.last_sign_in AS user_last_sign_in 
FROM user 
WHERE 1 != 1 ORDER BY CASE user.id END]
(Background on this error at: http://sqlalche.me/e/e3q8)


I researched other StackOverflow posts and found that the first() function internally processes the database result and returns None if no results are found, but when I switched to that function, I faced this error:

OperationalError: (sqlite3.OperationalError) near ""END"": syntax error
[SQL: SELECT user.id AS user_id, user.username AS user_username, user.email AS user_email, user.password_hash AS user_password_hash, user.last_sign_in AS user_last_sign_in 
FROM user 
WHERE 1 != 1 ORDER BY CASE user.id END
 LIMIT ? OFFSET ?]
[parameters: (1, 0)]
(Background on this error at: http://sqlalche.me/e/e3q8)


Checking the documentation for SqlAlchemy, I don't see any mention of this error in either function, and reading the meaning of OperationalError, I'm concerned that my database setup is possibly incorrect. 

app/routes.py: This is the route that handles search requests made to the following URL: http://localhost:5000/search?q=&amp;index=

@app.route('/search', methods=['GET'])
def search():
    print 'Hit the /search route!'
    if not g.search_form.validate():
        return redirect(url_for('index'))
    page = request.args.get('page', 1, type=int)
    target_index = request.args.get('index', 'thread')
    if target_index == 'thread':
        results, total = Thread.search(g.search_form.q.data, page, app.config['POSTS_PER_PAGE'])
        print 'Called Thread.search(), total results = {}'.format(total['value'])
    elif target_index == 'user':
        results, total = User.search(g.search_form.q.data, page, app.config['POSTS_PER_PAGE'])
        print 'Called User.search(), total results = {}'.format(total['value'])
    elif target_index == 'subreddit':
        results, total = Subreddit.search(g.search_form.q.data, page, app.config['POSTS_PER_PAGE'])
        print 'Called Subreddit.search(), total results = {}'.format(total['value'])
    else:
        return render_template('404.html')
    try:
        results = results.all()
    except OperationalError:
        results = [None]
    total = total['value']
    next_url = url_for('search', index=target_index, q=g.search_form.q.data, page=page + 1) if total &gt; page * app.config['POSTS_PER_PAGE'] else None
    prev_url = url_for('search', index=target_index, q=g.search_form.q.data, page=page - 1) if page &gt; 1 else None
    results_list = zip(results, [None] * len(results)) # Temporarily to match expected input for template
    return render_template('search.html', title=_('Search'), results_list=results_list, next_url=next_url, prev_url=prev_url, query=g.search_form.q.data, index=target_index)


app/models.py:

class SearchableMixin(object):
    @classmethod
    def search(cls, expression, page, per_page):
        ids, total = query_index(cls.__tablename__, expression, page, per_page)
        if total == 0:
            return cls.query.filter_by(id=0), 0
        when = []
        for i in range(len(ids)):
            when.append((ids[i], i))
        return cls.query.filter(cls.id.in_(ids)).order_by(
            db.case(when, value=cls.id)), total

    @classmethod
    def before_commit(cls, session):
        session._changes = {
            'add': list(session.new),
            'update': list(session.dirty),
            'delete': list(session.deleted)
        }

    @classmethod
    def after_commit(cls, session):
        for obj in session._changes['add']:
            if isinstance(obj, SearchableMixin):
                add_to_index(obj.__tablename__, obj)
        for obj in session._changes['update']:
            if isinstance(obj, SearchableMixin):
                add_to_index(obj.__tablename__, obj)
        for obj in session._changes['delete']:
            if isinstance(obj, SearchableMixin):
                remove_from_index(obj.__tablename__, obj)
        session._changes = None

    @classmethod
    def reindex(cls):
        for obj in cls.query:
            add_to_index(cls.__tablename__, obj)

db.event.listen(db.session, 'before_commit', SearchableMixin.before_commit)
db.event.listen(db.session, 'after_commit', SearchableMixin.after_commit)

# Below is one model that implements SearchableMixin to allow searching # for users. Thread and Subreddit models follow the same logic.
class User(db.Model, UserMixin, SearchableMixin):
    __searchable__ = ['username']
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(64), index=True, unique=True)
    # &lt;Remaining User model fields here...&gt;


app/search.py: (Holds the underlying search functions to query Elasticsearch indices)

def add_to_index(index, model):
    if not app.elasticsearch:
        return
    payload = {}
    for field in model.__searchable__:
        payload[field] = getattr(model, field)
    app.elasticsearch.index(index=index, doc_type=index, id=model.id,
                                    body=payload)

def remove_from_index(index, model):
    if not app.elasticsearch:
        return
    app.elasticsearch.delete(index=index, doc_type=index, id=model.id)

def query_index(index, query, page, per_page):
    if not app.elasticsearch:
        return [], 0
    search = app.elasticsearch.search(
        index=index,
        body={'query': {'multi_match': {'query': query, 'fields': ['*']}},
              'from': (page - 1) * per_page, 'size': per_page})
    ids = [int(hit['_id']) for hit in search['hits']['hits']]
    return ids, search['hits']['total']


As my included app/routes.py shows, I made a workaround by catching the OperationalError and treating it as an indicator that no results were found, but since the all() documentation makes no mention of it, I did not expect there to be this exception being raised.
",0,371,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/28008124,How to find all columns with a particular number as an attribute?,,0,268,,,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/70104873,how to access relationships with async sqlalchemy?,,18,11578,"This is how:
from sqlalchemy.orm import selectinload

async with async_session() as session:
    result = await session.execute(select(A).order_by(A.id)
                                            .options(selectinload(A.bs)))
    a = result.scalars().first()

    print(a.bs)


key is using the selectinload method to prevent implicit IO
UPDATE
There are a few alternatives to selectinload like joinedload, lazyload. I am still trying to understand the differences.
","muons answer is correct if you want eager loading (which is better).
But if for some reason you already have loaded your model and later want to load a relationship, there is a way starting with SQLAlchemy 2.0.4:
Using session.refresh, you can tell it to load a1.bs:
await session.refresh(a1, attribute_names=[""bs""])
print(a1.bs)  # This works

From the docs:

New in version 2.0.4: Added support for AsyncSession.refresh() and the underlying Session.refresh() method to force lazy-loaded relationships to load, if they are named explicitly in the Session.refresh.attribute_names parameter.

","There is an additional way of accessing the relationship attributes in SQLAlchemy v2, for the case where you have already loaded your model and later want to load a relationship.
As the documentation says, you can use AsyncAttrs: ""when added to a specific class or more generally to the Declarative Base superclass, provides an accessor AsyncAttrs.awaitable_attrs which delivers any attribute as an awaitable"". So you can create a Declarative Base class like this:
class Base(AsyncAttrs, DeclarativeBase):
    pass

And then use awaitable_attrs to access the relationship that must be previously loaded:
a1_bs = await a1.awaitable_attrs.bs
print(a1_bs)

"
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/17198595,SQLAlchemy DELETE Error caused by having a both lazy-load AND a dynamic version of the same relationship,"Here is some example code:

users_groups = Table('users_groups', Model.metadata,
    Column('user_id', Integer, ForeignKey('users.id')),
    Column('group_id', Integer, ForeignKey('groups.id'))
)

class User(Model):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)


class Group(Model):
    __tablename__ = 'groups'
    id = Column(Integer, primary_key=True)

    users = relationship('User', secondary=users_groups, lazy='select', backref='groups')
    users_dynamic = relationship('User', secondary=users_groups, lazy='dynamic')


So what happens here is that if you add a bunch of users to a group like so:

g = Group()
g.users = [User(), User(), User()]
session.add(g)
session.commit()


and then try to delete the group

session.delete(g)
session.commit()


You will get some form of this error:

DELETE statement on table 'users_groups' expected to delete 3 row(s); Only 0 were matched.


Removing the 2nd version of the relationship (the dynamic one in my case) fixes this problem. I am not even sure where to begin in terms of understanding why this is happening. I have been using 2 versions of various relationships in many cases throughout my SQLAlchemy models in order to make it easy to use the most appropriate query-strategy given a situation. This is the first time it has caused an unexpected issue.

Any advice is welcome.
",12,6206,"both the Group.users and Group.users_dynamic relationships are attempting to reconcile the fact that the Group is being deleted along with being able to manage the User() objects they refer to; one relationship succeeds while the second one fails, as the rows in the association table were already deleted.  The most straightforward solution is to mark all but one of the identical relationships as viewonly:

class Group(Base):
    __tablename__ = 'groups'
    id = Column(Integer, primary_key=True)

    users = relationship('User', secondary=users_groups, lazy='select', backref='groups')
    users_dynamic = relationship('User', viewonly=True, secondary=users_groups, lazy='dynamic')


if you're still wanting to have both relationships handle some degree of mutations, you'd need to do this carefully as SQLAlchemy doesn't know how to coordinate among changes in two relationships at the same time, so conflicts like this can continue to happen (like double inserts, etc) if you make equivalent mutations on both relationships.   To just take care of the ""delete"" issue by itself, you can also try setting Group.users_dynamic to passive_deletes=True:

class Group(Base):
    __tablename__ = 'groups'
    id = Column(Integer, primary_key=True)

    users = relationship('User', secondary=users_groups, lazy='select', backref='groups')
    users_dynamic = relationship('User', passive_deletes=True, secondary=users_groups, lazy='dynamic')

","I just add another simple workaround.

You can delete the collections before deleting the item itself:

&gt;&gt;&gt; for user in group.users:
        group.users.remove(user)
&gt;&gt;&gt; db.session.delete(group)
&gt;&gt;&gt; db.session.commit()


Alternatively, you can also set it as an empty list:

&gt;&gt;&gt; group.users = []
&gt;&gt;&gt; db.session.commit()
&gt;&gt;&gt; db.session.delete(group)
&gt;&gt;&gt; db.session.commit()

",
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/21076105,Is this an acceptable way to make threaded SQLAlchemy queries from Twisted?,"I've been doing some reading on using SQLAlchemy's ORM in the context of a Twisted application.  It's a lot of information to digest, so I'm having a bit of trouble putting all the pieces together.  So far, I've gathered the following absolute truths:


One session implies one thread.  Always.
scoped_session, by default, provides us with a way of constraining sessions to a given thread.  In other words, I am sure that by using scoped_session, I will not pass sessions to other threads (unless I do so explicitly, which I won't).


I also gathered that there are some issues relating to lazy/eager-loading and that one possible approach is to dissociate ORM objects from a session and reattach them to another session when changing threads.  I'm quite fuzzy on the details, but I also concluded that scoped_session renders many of these points moot.

My first question is whether or not I am severely mistaken in my above conclusions.

Beyond that, I've crafted this approach, which I hope is satisfactory.

I begin by creating a scoped_session object...

Session = scoped_session(sessionmaker(bind=_my_engine))


... which I will then use from a context manager, in order to handle exceptions and clean-up gracefully:

@contextmanager
def transaction_context():
    session = Session()
    try:
        yield session
        session.commit()
    except:
        session.rollback()
        raise
    finally:
        session.remove()  # dispose of the session


Now all I need to do is to use the above context manager in a function that is deferred to a separate thread.  I've thrown together a decorator to make things a bit prettier:

def threaded(fn):
    @wraps(fn)  # functools.wraps
    def wrapper(*args, **kwargs):
        return deferToThread(fn, *args, **kwargs)  # t.i.threads.deferToThread
    return wrapper


Here is an example of how I intend to use the whole shebang.  Below is a function that performs a DB lookup using the SQLAlchemy ORM:

@threaded
def get_some_attributes(group):
    with transaction_context() as session:
        return session.query(Attribute).filter(Attribute.group == group)


My second question is whether or not this approach is viable.


Am I making any fundamentally flawed assumptions?
Are there any caveats?
Is there a better way?


Edit: Here is a related question concerning the unexpected error in my context manager.
",4,1723,"Right now I work on this exact problem, and I think I found a solution. 

Indeed, you must defer all database access functions to a thread. But in your solution, you remove the session after querying the database, so all your results ORM objects will be detached and you wont have access to their fields.

You can't use scoped_session because in Twisted we have only one MainThread (except with things that work in deferToThread). We can, however, use scoped_sesssion with scopefunc. 

In Twisted there is a great thing known as ContextTracker:


  provides a way to pass arbitrary key/value data up and down a call
  stack without passing them as parameters to the functions on that call
  stack.


In my twisted web app in method render_GET I set a uuid parameter:

call = context.call({""uuid"": str(uuid.uuid4())}, self._render, request)


and then I call the _render method to do the actual work (work with db, render html, etc).

I create the scoped_session like this: 

scopefunc = functools.partial(context.get, ""uuid"")
Session = scoped_session(session_factory, scopefunc=scopefunc)


Now within any function calls of _render I can get session with:

Session()


and at the end of _render I have to do Session.remove() to remove the session.

It worksa with my webapp and I think can work for other tasks.

This is completely standalone example, show how all it work together.

from twisted.internet import reactor, threads
from twisted.web.resource import Resource
from twisted.web.server import Site, NOT_DONE_YET
from twisted.python import context
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import sessionmaker, scoped_session
from sqlalchemy.ext.declarative import declarative_base
import uuid
import functools

engine = create_engine(
    'sqlite:///test.sql',
    connect_args={'check_same_thread': False},
    echo=False)

session_factory = sessionmaker(bind=engine)
scopefunc = functools.partial(context.get, ""uuid"")
Session = scoped_session(session_factory, scopefunc=scopefunc)
Base = declarative_base()


class User(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)
    name = Column(String)

Base.metadata.create_all(bind=engine)


class TestPage(Resource):
    isLeaf = True

    def render_GET(self, request):
        context.call({""uuid"": str(uuid.uuid4())}, self._render, request)
        return NOT_DONE_YET

    def render_POST(self, request):
        return self.render_GET(request)

    def work_with_db(self):
        user = User(name=""TestUser"")
        Session.add(user)
        Session.commit()
        return user

    def _render(self, request):
        print ""session: "", id(Session())
        d = threads.deferToThread(self.work_with_db)

        def success(result):
            html = ""added user with name - %s"" % result.name
            request.write(html.encode('UTF-8'))
            request.finish()
            Session.remove()
        call = functools.partial(context.call, {""uuid"": scopefunc()}, success)
        d.addBoth(call)
        return d

if __name__ == ""__main__"":
    reactor.listenTCP(8888, Site(TestPage()))
    reactor.run()


I print out id of session, and you can see that its different for each request. If you remove scopefunc from scoped_session constructor and do two simultaneous request(insert time.sleep to work_with_db), you will get one common session for this two requests.


  The scoped_session object by default uses threading.local() as storage, so that a single Session is maintained for all who call upon the scoped_session registry, but only within the scope of a single thread


a problem here that in twisted we have only one thread for all requests. Thats why we have to create own scopefunc, that will show the difference between requests.

An other problem, that twisted didnt pass context to callbacks and we have to wrap callback and send current context to it.

call = functools.partial(context.call, {""uuid"": scopefunc()}, success)


Still I dont know how to make it work with defer.inLineCallback, that I use everywhere in my code.
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/76107844,SqlAlchemy StaleDataError on simple update statement,"I'm trying to update a user in the database but keep running into a StaleDataError.
user = session.query(User).get(1)
user.first_name # John
user.first_name = 'Sally'
session.commit()

# &gt; sqlalchemy.orm.exc.StaleDataError: UPDATE statement on table 'user' expected to update 1 row(s);
#   -1 were matched.

From the SqlAlchemy docs on StaleDataError:

An operation encountered database state that is unaccounted for.
Conditions which cause this to happen include:
A flush may have attempted to update or delete rows and an unexpected
number of rows were matched during the UPDATE or DELETE statement.
Note that when version_id_col is used, rows in UPDATE or DELETE
statements are also matched against the current known version
identifier.
A mapped object with version_id_col was refreshed, and the version
number coming back from the database does not match that of the object
itself.
A object is detached from its parent object, however the object was
previously attached to a different parent identity which was garbage
collected, and a decision cannot be made if the new parent was really
the most recent parent.

The docs don't elaborate on version_id_col. Could this be the issue? What is this column and where can I find if it's active?
Why can't SqlAlchemy locate the row id (which is obviously there since it pulled the row just moments before) and what's wrong with my update command?
",2,578,"I found the solution in an answer by @tamersalama. I'm reposting it here because that question was hard to find in connection to the StaleDataError.
Add __mapper_args__ to the SqlAlchemy model:
class User(Base):

    __tablename__ = 'user'
    user_id = Column(Integer, primary_key=True, nullable=False)
    first_name = Column(String(20))
    last_name = Column(String(20))

    __mapper_args__ = {
        'version_id_col': user_id, # this is the id column of the model
        'version_id_generator': False
    }

",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/75758327,SQLAlchemy: method &#39;_connection_for_bind()&#39; is already in progress,"I recently updated SQLAlchemy (with [asyncio] package) to 1.4.46 and started to get the following exception when committing:

sqlalchemy.exc.IllegalStateChangeError: Method 'commit()' can't be called here; method '_connection_for_bind()' is already in progress and this would cause an unexpected state change to &lt;SessionTransactionState.CLOSED: 5&gt;

Before updating to the new version, it was working fine.
# -*- coding:utf-8 -*-

from sqlalchemy import exc, event, text
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession as SQLAlchemyAsyncSession
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.ext.asyncio import AsyncEngine
from sqlalchemy.pool import NullPool, Pool
from contextvars import ContextVar
from sanic import Sanic
import asyncio


class EngineNotInitialisedError(Exception):
    pass


class DBSessionContext:
    def __init__(self, session: Session, commit_on_exit: bool = True) -&gt; None:
        self.session = session
        self._query = None
        self.commit_on_exit = commit_on_exit
        self.token = None

    async def close(self, exc_type=None, exc_value=None, traceback=None):
        if self._query:
            if exc_value and getattr(exc_value, 'status_code', 500) &gt; 300:
                await self._query.rollback()
                self._post_processing.clear()
            else:
                await self._query.commit()
                await self.run_post_processing()

            await self._query.close()

        if self._post_processing:
            await self.run_post_processing()

    def set_token(self, token):
        self.token = token

    @property
    def query(self) -&gt; Session:
        if not self._query:
            self._query = self.session()

        return self._query


class AsyncSession(SQLAlchemyAsyncSession):
    async def execute(self, statement, **parameters):
        try:
            if isinstance(statement, str):
                # We wrap around the `text()` method automatically
                statement = text(statement)
            return await super().execute(statement, parameters)
        except exc.OperationalError as e:
            if e.orig.args[0] == 1205:
                # Lock wait timeout exceeded
                await self.rollback()
                return await super().execute(statement, parameters)

            raise e


class DBSession:
    def __init__(self):
        self.engine = None
        self.session = None
        self._session = None
        self.context = ContextVar(""context"", default=None)

    def init_app(self, app: Sanic, url: str, commit_on_exit: bool = True) -&gt; None:
        self.commit_on_exit = commit_on_exit

        engine_args = {
            'echo': app.config.get('DATABASE_ECHO', cast=bool, default=False),
            'echo_pool': app.config.get('DATABASE_ECHO_POOL', cast=bool, default=False),
            'poolclass': NullPool,  # will be used to create a connection pool instance using the connection parameters given in the URL
            # if pool_class is not NullPool:

            # the number of connections to allow in connection pool overflow
            # 'max_overflow': app.config.get('DATABASE_MAX_OVERFLOW', cast=int, default=10),
            # if True will enable the connection pool pre-ping feature that tests connections for liveness upon each checkout
            # 'pool_pre_ping': app.config.get('DATABASE_POOL_PRE_PING', cast=bool, default=True),
            # the number of connections to keep open inside the connection pool
            # 'pool_size': app.config.get('DATABASE_POOL_SIZE', cast=int, default=5),
            # this setting causes the pool to recycle connections after the given number of seconds has passed
            # 'pool_recycle': app.config.get('DATABASE_POOL_RECYCLE', cast=int, default=-1),
            # number of seconds to wait before giving up on getting a connection from the pool
            # 'pool_timeout': app.config.get('DATABASE_POOL_TIMEOUT', cast=int, default=3600),
        }

        self.engine = create_async_engine(
            url,
            **engine_args
        )

        self.session = sessionmaker(
            bind=self.engine,
            expire_on_commit=False,
            class_=AsyncSession,
            autoflush=False
        )

    async def __aenter__(self):
        if not isinstance(self.engine, AsyncEngine):
            raise EngineNotInitialisedError

        session_ctx = DBSessionContext(self.session, self.commit_on_exit)
        session_ctx.set_token(self.context.set(session_ctx))

        return session_ctx

    async def __aexit__(self, exc_type, exc_value, traceback):
        session_ctx = self.context.get()
        await asyncio.shield(session_ctx.close(exc_type, exc_value, traceback))

        self.context.reset(session_ctx.token)

    @property
    def query(self) -&gt; Session:
        return self.context.get().query


@event.listens_for(Pool, ""checkout"")
def check_connection(dbapi_con, con_record, con_proxy):
    '''Listener for Pool checkout events that pings every connection before using.
    Implements pessimistic disconnect handling strategy. See also:
    http://docs.sqlalchemy.org/en/rel_0_8/core/pooling.html#disconnect-handling-pessimistic'''

    cursor = dbapi_con.cursor()
    try:
        cursor.execute(""SELECT 1"")
    except exc.OperationalError as ex:
        if ex.args[0] in (2006,   # MySQL server has gone away
                          2013,   # Lost connection to MySQL server during query
                          2055):  # Lost connection to MySQL server at '%s', system error: %d
            raise exc.DisconnectionError()  # caught by pool, which will retry with a new connection
        else:
            raise

    cursor.close()


db = DBSession()

The code is called with the following :
async with db:
    await db.query.execute('INSERT INTO ...')

What is causing the InvalidStateChangeError I'm having? How can I avoid this issue?
",2,2652,"There is a discussion on the Github repository of SQLAlchemy, that gives a reason why the issue is occurring:
https://github.com/sqlalchemy/sqlalchemy/discussions/9312
The suggestion is that the code is calling something like
asyncio.gather(func(session), func2(session) with the two function sharing the same session, which causes the sqlalchemy.exc.IllegalStateChangeError
Removing the asyncio.gather call resolve the issue. (Or use two sessions, one for each functions).
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/77280663,What is the difference between utf8mb4_cs_0900_as_cs and utf8mb4_900_as_cs?,"I'm facing issues when building unit tests for my SQLALCHEMY api. I noticed that my MySQL DB returns an unexpected order when asking to order by a string column.
A little investigation showed that our DB is using utf8mb4_cs_0900_as_cs.
But when trying to order with utf8mb4_0900_as_cs, I get the expected order.
Can someone explain to me what are the differences between these 2 collations?
mysql&gt; SELECT
    -&gt;     column_0 AS name1
    -&gt; FROM
    -&gt;     (VALUES
    -&gt;         ROW('default_chanakah'),
    -&gt; ROW('default_aaa'),
    -&gt; ROW('default_zzz'),
    -&gt;         ROW('default_hsunan'),
    -&gt;         ROW('default_kourin1')
    -&gt;     ) AS hardcodedNames
    -&gt; ORDER BY
    -&gt; name1
    -&gt; COLLATE utf8mb4_cs_0900_as_cs;
+------------------+
| name1            |
+------------------+
| default_aaa      |
| default_hsunan   |
| default_chanakah |
| default_kourin1  |
| default_zzz      |
+------------------+
5 rows in set (0.00 sec)

mysql&gt; SELECT
    -&gt;     column_0 AS name1
    -&gt; FROM
    -&gt;     (VALUES
    -&gt;         ROW('default_chanakah'),
    -&gt; ROW('default_aaa'),
    -&gt; ROW('default_zzz'),
    -&gt;         ROW('default_hsunan'),
    -&gt;         ROW('default_kourin1')
    -&gt;     ) AS hardcodedNames
    -&gt; ORDER BY
    -&gt; name1
    -&gt; COLLATE utf8mb4_0900_as_cs;
+------------------+
| name1            |
+------------------+
| default_aaa      |
| default_chanakah |
| default_hsunan   |
| default_kourin1  |
| default_zzz      |
+------------------+
5 rows in set (0.00 sec)

",1,55,"utf8mb4_cs_0900_as_cs is Czech utf8 (first cs indicate this), this alphabet contains ""letter"" Ch and it goes after h
this is why hsusan is before chanakah
from https://en.wikipedia.org/wiki/Czech_orthography

",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/76038019,SQL Alchemy error: MissingGreenlet: greenlet_spawn has not been called,"I am building backend application with fastapi and sqlalchemy, and while trying to get a relationship between tables, I am being hit with this error.
full error:

raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/20/xd2s)

I have stablished other relationships between other tables on the same project the exact same way and had no issues whatsoever up to this point. Since all of the other relationships worked with no issues I can't tell why this one isn't working.
This is what my models for both tables looks like:
class ServiceOrderModel(Settings.DB_BASE_MODEL):
    __tablename__ = 'service_order'
    __allow_unmapped__ = True

    id = Column(Integer, primary_key=True, autoincrement=True)
    identifier = Column(String(16))
    description = Column(String(256))
    execution_value = Column(Float, nullable=True)
    charged_value = Column(Float, nullable=True)
    status_id = Column(Integer, ForeignKey('service_status.id'))
    status = relationship('ServiceStatusModel', lazy='joined', back_populates='service_orders')


class ServiceStatusModel(Settings.DB_BASE_MODEL):
    __tablename__ = 'service_status'
    __allow_unmapped__ = True

    id=Column(Integer, primary_key=True, autoincrement=True)
    status=Column(String(32))
    service_orders=relationship('ServiceOrderModel', back_populates='status', lazy='joined', uselist=True)

Can anyone help me spot why is this error happening?
For more context, this is the function that I am trying to use prior to receiving the error:
@service_order_router.post('/', status_code=status.HTTP_201_CREATED, response_model=ServiceOrderReturnSchema)
async def post(data: ServiceOrderUpdateSchema,db: AsyncSession = Depends(get_session)) -&gt; Response:

    async with db as database:
        new_service_order = ServiceOrderModel(
            identifier=data.identifier,
            description=data.description,
            execution_value=None,
            charged_value=None,
            status_id=data.status_id,
        )
        database.add(new_service_order)
        await database.commit()

        return new_service_order

",1,2605,"Seems like status is probably referenced during rendering the response but isn't loaded during creation because just an id is passed.  Can you try adding this line before returning the service order?
    await database.commit()
    # Force status to load
    await database.refresh(new_service_order, [""status""])
    return new_service_order

",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/71004077,"How to get average value from SQL(Superset, SQLAlchemy under PostgreSQL) timestamp",,0,600,"I was not able to reproduce the error.
Works fine.
Setup in Postgresql Database:
postgres=&gt; create table the_best_table(last_checked_at timestamp);
CREATE TABLE
postgres=&gt; insert into the_best_table values(now());
INSERT 0 1
postgres=&gt; select
    to_timestamp(
        avg(
            cast(
                extract(epoch from last_checked_at) as integer)
                )::integer
        ) as datetime
from
    the_best_table;
        datetime
------------------------
 2022-02-07 04:54:06+09
(1 row)

The exact two queries run fine for me in SQLAlchemy:
&gt;&gt;&gt; from sqlalchemy import create_engine
&gt;&gt;&gt; engine = create_engine('postgresql+pg8000://&lt;username&gt;:&lt;passwd&gt;@192.xxx.xxx.123/postgres')
&gt;&gt;&gt;  metadata.create_all(engine)
&gt;&gt;&gt; with engine.connect() as con:
...     rs = con.execute('select to_timestamp(avg(cast(extract(epoch from last_checked_at) as integer))::integer ) as datetime from the_best_table;')
...     for row in rs:
...             print(row)
... 
(datetime.datetime(2022, 2, 7, 4, 54, 6, tzinfo=datetime.timezone(datetime.timedelta(seconds=32400))),)
&gt;&gt;&gt; with engine.connect() as con:
...     rs = con.execute('select to_timestamp(1644117319) as datetime;')
...     for row in rs:
...             print(row)
... 
(datetime.datetime(2022, 2, 6, 12, 15, 19, tzinfo=datetime.timezone(datetime.timedelta(seconds=32400))),)

",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/77811401,Correct way of retaining database information with fastapi and sqlalchemy,"we've recently changed tech stack for our python API so it now uses fastAPI and SQLAlchemy 2. Great combo, I'm sure, but I'm having issues creating something which I'd imagine would be pretty simple. I'm trying to build a simple post endpoint which takes a request body, processes it, adds it to the database, then return the inserted database entity. Something like this:
async def create_new_shipment_with_full_data(
    session: AsyncSession, data: FullShipmentCreationRequestBody
) -&gt; Shipment:
    async with session:
        shipment_data = data.model_dump(exclude_none=True)
        cursor = await session.execute(insert(Shipment).values(**shipment_data).returning(Shipment))
        inserted_shipment = cursor.scalar()
        await session.commit()
    
        return inserted_shipment

This of course doesn't work (wouldn't be here if it did) since the shipment data seems to be lost once the session commits, yielding me this error:
MissingGreenlet(""greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place?"")
Now, I've tried solving it by expunging the instance from the session, but that doesn't seem to help in this situation. I've also fixed a similar endpoint by converting the shipment to a dictionary, yet I don't know if this is the best solution.
I'm really asking about the correct way of implementing this type of endpoints using fastapi and sqlalchemy. It feels like a very standard use case and I've been trying to get help from chatgpt which has currently shown me around 467 ways which don't work. Why does this feel like rocket science?
Thanks in advance
",0,72,"this error occurs when you try to pull up attributes synchronously, working in an asynchronous context, specifically in your case (I assume) that you have a Shipment object that has relation with any-obj object, and when you inserted something and indicated .returning, it will returns the Shipment object without any loaded relations, so after insert you have to session.refresh(inserted_shipment, attribute_names=[""any-obj""]) and its should fix your problem :)
also if you retrieve Shipment by id, for example, you have to load again any-obj attribute, it means:
async def retrieve(pk: int):
    stmt = select(Shipment).where(Shipment.id == pk).options(joinedload(Shipment.any-obj))
    result = await session.execute(stmt)
    return result.scalar()

or you may use selectinload (depends on your relations), read about it
you may read here about it
Speaking about how the greenlet pops up, Ill note that when you dont load these attributes yourself, the pydantic tries to get them synchronously
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/77383467,SQLAlchemy MissingGreenlet Error When Accessing User Object,"I'm encountering an issue in my FastAPI application when trying to access the user.password attribute. Here's a summary of the problem:
I'm using SQLAlchemy with an asynchronous MySQL database.
I've defined a User model with a password field in my application.
I'm trying to verify a user's password during the login process.
However, when I try to access user.password, I encounter the following error:
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place?
I've ensured that the user object is retrieved from the database, and I can print the user object successfully before the password verification step.
Here's the relevant code:
async def login(request: LoginSchema):
    user = await UserRepository.find_by_email(request.email)

    if user and pwd_context.verify(request.password, user.password):
        # ... (token generation and response)


My database session setup and UserRepository are as follows:
# UserRepository
class UserRepository(BaseRepo):
    model = User

    @staticmethod
    async def find_by_email(email: str):
        qry = select(User).where(User.email == email)
        return (await db.execute(qry)).scalar_one_or_none()
    
    ```

#db session
class DatabaseSession:
def init(self):
self.session = None
self.engine = None
def __getattr__(self, item):
    return getattr(self.session, item)

def init(self):
    self.engine = create_async_engine(DATABASE_URL, future=True, echo=True)
    self.session = async_sessionmaker(self.engine, expire_on_commit=False, class_=AsyncSession)()

async def create_all(self):
    async with self.engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

db = DatabaseSession()
`
I suspect that this issue may be related to SQLAlchemy's lazy loading mechanism. Could someone provide insights into what might be causing this error and how I can access the user.password attribute without triggering this exception?
Thank you in advance for your help.
",0,162,"I realized in my models, the password field used the ""deferred"" keyword, in SQLAlchemy it's used to delay the loading of certain columns, which can be useful for optimizing database queries, especially when dealing with large datasets. However, in my case, using ""deferred"" on the password column caused unexpected behavior.
When I tried to access the ""user.password"" attribute, it triggered a database query because the password column was not loaded into memory until that point.
To resolve this issue, I removed the ""deferred"" keyword from the password column definition in my User model to ensure that the password column is loaded immediately when the User object is queried from the database, eliminating the greenlet spawn error.
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/76947178,"Update: Issue connecting to PostgreSQL database using ipython-sql (psycopg2==2.9.3, sqlalchemy==2.0.2, ipython-sql==0.3.9, postgres==15.3)","I am trying to connect to an existing PostgreSQL database. I am able to connect using the psql command-line interface, as well as pgAdmin. However, when I try to connect a python environment (JupyterLab and DataSpell), I am unable to connect.
Version Information:

psycopg2 version: 2.9.3
sqlalchemy version: 2.0.2
ipython-sql version: 0.3.9
postgres version: 15.3


%load_ext sql

%env DATABASE_URL=postgresql://retail_user:retail@localhost/retail_db

%%sql
SELECT * FROM orders LIMIT 10

The above code outputs:
__init__() got an unexpected keyword argument 'bind'
Connection info needed in SQLAlchemy format, example: postgresql://username:password@hostname/dbname or an existing connection: dict_keys([])

After some investigation, I discovered that downgrading SQLAlchemy to version 1.4.22 resolved this issue.
",0,228,"The error was not due to incorrect connection details, but rather a compatibility issue between the versions of SQLAlchemy and ipython-sql I was using. I was initially using SQLAlchemy version 2.0.2, which resulted in this error.
To resolve the issue, I downgraded SQLAlchemy to version 1.4.22. After making this change, the connection worked as expected and I was able to execute SQL queries using ipython-sql without encountering the error.
For those who might face a similar problem:

Check the versions of SQLAlchemy and ipython-sql you are using.
If you are using a version of SQLAlchemy higher than 1.4.22, consider downgrading it to version 1.4.22.
Re-run your code after the downgrade to confirm that the issue is resolved.

Thank you to Adrian Klaver for suggesting the steps that ultimately led to the solution.
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/74809770,While upgrading Airflow version 2.0.0 to 2.2.5 got an error in initdb. the log is attached below,"
*While upgrading Airflow version 2.0.0 to 2.2.5 got an error in initdb. the log is attached below.
*





category=DeprecationWarning,
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361: DeprecationWarning: The logging_level option in [core] has been moved to the logging_level option in [logging] - the old setting has been used, but please update your config.
option = self._get_option_from_config_file(deprecated_key, deprecated_section, key, kwargs, section)
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The hide_sensitive_variable_fields option in [admin] has been moved to the hide_sensitive_var_conn_fields option in [core] - the old setting has been used, but please update your config.
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The base_log_folder option in [core] has been moved to the base_log_folder option in [logging] - the old setting has been used, but please update your config.
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The default_queue option in [celery] has been moved to the default_queue option in [operators] - the old setting has been used, but please update your config.
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The statsd_on option in [scheduler] has been moved to the statsd_on option in [metrics] - the old setting has been used, but please update your config.
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The default_queue option in [celery] has been moved to the default_queue option in [operators] - the old setting has been used, but please update your config.
DB: postgresql+psycopg2://airflow-dev-ipc:***@7a2dcbf5-4b02-4462-9ab2-f52f835fd961.c7e06sed0lktba7pbqj0.databases.appdomain.cloud:31604/airflow_dev_ipc
[2022-12-15 09:36:38,662] {db.py:919} INFO - Creating tables
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.s3.S3Hook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.emr.EmrHook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.s3.S3Hook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.emr.EmrHook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
INFO  [alembic.runtime.migration] Running upgrade c8ffec048a3b -&gt; a56c9515abdc, Remove dag_stat table
Traceback (most recent call last):
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context
cursor, statement, parameters, context
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 608, in do_execute
cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: table ""dag_stats"" does not exist
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
File ""/usr/local/bin/airflow"", line 8, in 
sys.exit(main())
File ""/usr/local/lib/python3.6/site-packages/airflow/main.py"", line 48, in main
args.func(args)
File ""/usr/local/lib/python3.6/site-packages/airflow/cli/cli_parser.py"", line 48, in command
return func(*args, **kwargs)
File ""/usr/local/lib/python3.6/site-packages/airflow/cli/commands/db_command.py"", line 31, in initdb
db.initdb()
File ""/usr/local/lib/python3.6/site-packages/airflow/utils/session.py"", line 70, in wrapper
return func(*args, session=session, **kwargs)
File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 592, in initdb
upgradedb(session=session)
File ""/usr/local/lib/python3.6/site-packages/airflow/utils/session.py"", line 67, in wrapper
return func(*args, **kwargs)
File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 920, in upgradedb
command.upgrade(config, 'heads')
File ""/usr/local/lib/python3.6/site-packages/alembic/command.py"", line 320, in upgrade
script.run_env()
File ""/usr/local/lib/python3.6/site-packages/alembic/script/base.py"", line 563, in run_env
util.load_python_file(self.dir, ""env.py"")
File ""/usr/local/lib/python3.6/site-packages/alembic/util/pyfiles.py"", line 92, in load_python_file
module = load_module_py(module_id, path)
File ""/usr/local/lib/python3.6/site-packages/alembic/util/pyfiles.py"", line 108, in load_module_py
spec.loader.exec_module(module)  # type: ignore
File """", line 678, in exec_module
File """", line 219, in _call_with_frames_removed
File ""/usr/local/lib/python3.6/site-packages/airflow/migrations/env.py"", line 107, in 
run_migrations_online()
File ""/usr/local/lib/python3.6/site-packages/airflow/migrations/env.py"", line 101, in run_migrations_online
context.run_migrations()
File """", line 8, in run_migrations
File ""/usr/local/lib/python3.6/site-packages/alembic/runtime/environment.py"", line 851, in run_migrations
self.get_context().run_migrations(**kw)
File ""/usr/local/lib/python3.6/site-packages/alembic/runtime/migration.py"", line 620, in run_migrations
step.migration_fn(**kw)
File ""/usr/local/lib/python3.6/site-packages/airflow/migrations/versions/a56c9515abdc_remove_dag_stat_table.py"", line 39, in upgrade
op.drop_table(""dag_stats"")
File """", line 8, in drop_table
File """", line 3, in drop_table
File ""/usr/local/lib/python3.6/site-packages/alembic/operations/ops.py"", line 1349, in drop_table
operations.invoke(op)
File ""/usr/local/lib/python3.6/site-packages/alembic/operations/base.py"", line 392, in invoke
return fn(self, operation)
File ""/usr/local/lib/python3.6/site-packages/alembic/operations/toimpl.py"", line 80, in drop_table
operation.to_table(operations.migration_context)
File ""/usr/local/lib/python3.6/site-packages/alembic/ddl/impl.py"", line 372, in drop_table
self._exec(schema.DropTable(table))
File ""/usr/local/lib/python3.6/site-packages/alembic/ddl/impl.py"", line 193, in _exec
return conn.execute(construct, multiparams)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1011, in execute
return meth(self, multiparams, params)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/sql/ddl.py"", line 72, in _execute_on_connection
return connection._execute_ddl(self, multiparams, params)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1073, in _execute_ddl
compiled,
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1317, in _execute_context
e, statement, parameters, cursor, context
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1511, in _handle_dbapi_exception
sqlalchemy_exception, with_traceback=exc_info[2], from_=e
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 182, in raise_
raise exception
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context
cursor, statement, parameters, context
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 608, in do_execute
cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) table ""dag_stats"" does not exist
[SQL:
DROP TABLE dag_stats]
(Background on this error at: http://sqlalche.me/e/13/f405)





I tried creating a table (dag_stats) in Postgres but still the issue continues.







",0,692,"You will have to upgrade the database, please refer this link for uprading Airflow version
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/67743572,Exporting data into CSV file from Flask-SQLAlchemy,"I'm looking to generate(export) a csv from a flask-sqlalchemy app i'm developing. But i'm getting some unexpected outcomes in my csv i.e. instead of the actual data from the MySQL DB table populated in the csv file, i get the declarative class model entries (placeholders??). The issue possibly could be the way i structured the query or even, the entire function.
Oddly enough - judging from the csv output (pic) - it would seem i'm on the right track since the row/column count is the same as the DB table but actual data is just not populated. I'm fairly new to SQLAlchemy ORM and Flask, so looking for some guidance here to pull through. Constructive feedback appreciated.
#class declaration with  DB object (divo)
class pearl(divo.Model):
    __tablename__ = 'users'                 
    work_id = divo.Column(divo.Integer, primary_key=True)
    user_fname = divo.Column(divo.String(length=255))
    user_lname = divo.Column(divo.String(length=255))
    user_category = divo.Column(divo.String(length=255))
    user_status = divo.Column(divo.String(length=1))
    login_id = divo.Column(divo.String(length=255))
    login_passwd = divo.Column(divo.String(length=255))

#user report function
@app.route(""/reports/users"")
def users_report():
    with open(r'C:\Users\Xxxxxxx\Projects\_repository\zzz.csv', 'w') as s_key:
        x15 = pearl.query.all()
        for i in x15:
#        x16 = tuple(x15)
            csv_out = csv.writer(s_key)
            csv_out.writerow(x15)
    flash(""Report generated. Please check designated repository."", ""green"")
    return redirect(url_for('reports_landing'))  # return redirect(url_for('other_tasks'))

#csv outcome (see attached pic)

",0,2406,"
instead of the actual data from the MySQL DB table populated in the csv file, i get the declarative class model entries (placeholders??)

Each object in the list
x15 = pearl.query.all()

represents a row in your users table.
What you're seeing in the spreadsheet are not placeholders, but string representations of each row object (See object.repr).
You could get the value of a column for a particular row object by the column name attribute, for example:
x15[0].work_id # Assumes there is at least one row object in x15

What you could do instead is something like this:
with open(r'C:\Users\Xxxxxxx\Projects\_repository\zzz.csv', 'w') as s_key:
    x15 = divo.session.query(pearl.work_id, pearl.user_fname) # Add columns to query as needed
    for i in x15:
        csv_out = csv.writer(s_key)
        csv_out.writerow(i)

i in the code above is a tuple of the form:
('work_id value', 'user_fname value')

",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/65815257,"SQLAclhemy, auroa-serverless invalid transaction issue on commit (aurora_data_api.exceptions.DatabaseError)","I'm using the sqlalchemy-aurora-data-api to connect to aurora-postgresql-serverless, with SQLalchemy as an ORM.
For the most part, this has been working fine, but I keep hitting unexpected errors from the aurora_data_api (which sqlalchemy-aurora-data-api is built upon) during commits.
I've tried to handle this in the application logic by catching the exception and re-trying, however, this is still failing:

from aurora_data_api.exceptions import DatabaseError
from botocore.exceptions import ClientError

def handle_invalid_transaction_id(func):
    retries = 3

    @wraps(func)
    def inner(*args, **kwargs):
        for i in range(retries):
            try:
                return func(*args, **kwargs)
            except (DatabaseError, ClientError):
                if i != retries:
                    # The aim here is to try and force a new transaction 
                    # If an error occurs and retry
                    db.session.close()
                else:
                    raise

    return inner


And then in my models doing something like this:
class MyModel(db.Model):
    @classmethod
    @handle_invalid_transaction_id
    def create(cls, **kwargs):
        instance = cls(**kwargs)
        db.session.add(instance)
        db.session.commit()
        db.session.close()
        return kwargs

However, I keep hitting unpredictable transaction failures:
DatabaseError: (aurora_data_api.exceptions.DatabaseError) An error occurred (BadRequestException) when calling the ExecuteStatement operation: Transaction AXwQlogMJsPZgyUXCYFg9gUq4/I9FBEUy1zjMTzdZriEuBCF44s+wMX7+aAnyyJH/6arYcHxbCLW73WE8oRYsPMN17MOrqWfUdxkZRBrM/vBUfrP8FKv6Phfr6kK6o7/0mirCtRJUxDQAQPotaeP+hHj6/IOGUCaOnodt4M3015c0dAycuqhsy4= is not found [+26ms]
It is worth noting that these are not particularly long-running transactions, so I do not think that I'm hitting the transaction expiry issue that can occur with aurora-serverless as documented here.
Is there something fundamentally wrong with my approach to this or is there a better way to handle transactions failures when they occur?
",0,534,"Just to close this off, and in case it helps anyone else, found the issue was in the transactions that were being created by in the cursor here
I can't answer the why, but we noticed that transactions were expiring despite the fact the data successfully committed. e.g:
request 1 - creates a bunch of transactions, write data, exits.
request 2 - creates a bunch of transactions, some transaction id for request 1 fails, exits.
So yeah, I don't think the issue is with the aurora-data-api, but somehow to do with transaction mgmt in general in aurora-serverless. In the end, we forked the repo and refactored so that everything is handled with ExecuteStatment calls rather than using transactions. It's been working fine so far (note we're using SQLalchemy so transactions are handled at the ORM level anyway).
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/64688620,How do I fix connection to db2 using SQLAlchemy in python?,"I'm having trouble connecting to my database BLUDB in IBM Db2 on Cloud using SQLAlchemy. Here is the code I've always used and it's always worked fine:
%sql ibm_db_sa://user:pswd@some-host.services.dal.bluemix.net:50000/BLUDB

But now I get this error:

(ibm_db_dbi.ProgrammingError) ibm_db_dbi::ProgrammingError:
Exception('[IBM][CLI Driver] SQL1042C  An unexpected system error
occurred.  SQLSTATE=58004\r SQLCODE=-1042') (Background on this error
at: http://sqlalche.me/e/13/f405) Connection info needed in SQLAlchemy
format, example: postgresql://username:password@hostname/dbname or an
existing connection: dict_keys([])

These packages are loaded as always:
import ibm_db
import ibm_db_sa
import sqlalchemy
from sqlalchemy.engine import create_engine
I looked at the python db2 documentation on ibm and the sqlalchemy error message but couldn't get anywhere.
I am working in Jupyterlab locally. I've recently reinstalled Python and Jupyterlab. That's the only thing locally that's changed.
I am able to successfully run the notebooks in the cloud at kaggle and cognitive class. I am also able to connect and query sqlite3 via python without an issue using my local notebook.
All the ibm modules and version numbers are the same before and after installation. I used requirements.txt for reinstallation.
In db2diag.log here are the last two entries:
2020-11-05-14.06.47.081000-300 I13371F372           LEVEL: Warning
PID     : 17500                TID : 7808           PROC : python.exe
INSTANCE:                      NODE : 000
HOSTNAME: DESKTOP-6FFFO2E
EDUID   : 7808
FUNCTION: DB2 UDB, bsu security, sqlexLogPluginMessage, probe:20
DATA #1 : String with size, 43 bytes
loadAuthidMapper: GetModuleHandle rc = 126
2020-11-05-14.13.49.282000-300 I13745F373           LEVEL: Warning
PID     : 3060                 TID : 12756          PROC : python.exe
INSTANCE:                      NODE : 000
HOSTNAME: DESKTOP-6FFFO2E
EDUID   : 12756
FUNCTION: DB2 UDB, bsu security, sqlexLogPluginMessage, probe:20
DATA #1 : String with size, 43 bytes
loadAuthidMapper: GetModuleHandle rc = 126
",0,1317,"I think the root of this will be down to the new version of Python and pip caching.
What version did you move from and what version are you now on. Is this a Python 2 to Python 3 change?  When changing versions, normally you would need to clean pip install all components, but pip does use a cache. Even for components that may need to be compiled, and there is a good chance that Db2 components are being compiled.
So what you will need to do is to re-install the dependancies with
pip install --no-cache-dir
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/59162110,Writing to SQL Server from Python,"This is the smallest example I can give that is a MRE

I am attempting to do the following:


Using pyodbc, read from a SQL Server instance (complete)
Then, print that data to verify it (complete)
Then, take that data, and insert it into a new table (or overwrite the table if it exists) &lt;- FAILURE


The code is below:

import pyodbc
import pandas as pd
import sqlalchemy as sa

sqlConn = pyodbc.connect(
    ""DRIVER={SQL Server};""
    ""SERVER=servername;""
    ""DATABASE=dbname;""
    ""Trusted_Connection=yes;""
)

sql = """"""
SELECT TOP (1000) [PART]
      ,[STEP]
      ,[COMPLETIONTIME]
  FROM [dbname].[dbo].[STEPS]
""""""

engine = sa.create_engine('mssql+pyodbc://servername/dbname')


df = pd.read_sql(sql, sqlConn)

df.to_sql(name = 'Test', con = engine, if_exists = 'replace', index = False)
sqlConn.commit()
sqlConn.close()


I get the following for an error:

  File ""WiP.py"", line 26, in &lt;module&gt;
    df.to_sql(name = 'Test', con = engine, if_exists = 'replace', index = False)
  File ""C:\Python367-64\lib\site-packages\pandas\core\generic.py"", line 2712, in to_sql
    method=method,
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 518, in to_sql
    method=method,
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 1319, in to_sql
    table.create()
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 641, in create
    if self.exists():
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 628, in exists
    return self.pd_sql.has_table(self.name, self.schema)
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 1344, in has_table
    self.connectable.dialect.has_table, name, schema or self.meta.schema
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2162, in run_callable
    with self._contextual_connect() as conn:
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2242, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2280, in _wrap_pool_connect
    e, dialect, self
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 152, in reraise
    raise value.with_traceback(tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2276, in _wrap_pool_connect
    return fn()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 492, in checkout
    rec = pool._do_get()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 139, in _do_get
    self._dec_overflow()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 153, in reraise
    raise value
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 136, in _do_get
    return self._create_connection()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 308, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 437, in __init__
    self.__connect(first_connect_check=True)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 639, in __connect
    connection = pool._invoke_creator(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\strategies.py"", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\default.py"", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
sqlalchemy.exc.InterfaceError: (pyodbc.InterfaceError) ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')
(Background on this error at: http://sqlalche.me/e/rvf5)


I looked that up on this site, and got the following:


  Exception raised for errors that are related to the databases
  operation and not necessarily under the control of the programmer,
  e.g. an unexpected disconnect occurs, the data source name is not
  found, a transaction could not be processed, a memory allocation error
  occurred during processing, etc.


I have also consulted:


Connecting to Microsoft SQL server using Python
List sql tables in pandas.read_sql
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql_table.html
https://github.com/mkleehammer/pyodbc/issues/300
Connecting to SQL Server 2012 using sqlalchemy and pyodbc


I also tried switching df.to_sql(name = 'Test', con = engine, if_exists = 'replace', index = False) to df.to_sql(name = 'Test', con = sqlConn, if_exists = 'replace', index = False) but got the exact same error.

What am I doing incorrectly? How can I wrote to a new table (or overwrite an existing one) from a Pandas dataframe?

UPDATE

It appears the connection is failing. The following:

import pyodbc
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

sqlConn = pyodbc.connect(
    ""DRIVER={SQL Server};""
    ""SERVER=servername;""
    ""DATABASE=dbname;""
    ""Trusted_Connection=yes;""
)

sql = """"""
SELECT TOP (1000) [ORDR_PART_NO]
      ,[OROP_ID]
      ,[COMPLETIONTIME]
  FROM [dbname].[dbo].[OpsLookup]
""""""
engine = sa.create_engine('mssql+pyodbc://servername/dbname')

cnxn = engine.connect()
result = cnxn.execute(""SELECT TOP (1000) * FROM [dbname].[dbo].[STEPS]"")
for row in result:
    print(row)
cnxn.close()


Yields:

C:\Python367-64\lib\site-packages\sqlalchemy\connectors\pyodbc.py:79: SAWarning: No driver name specified; this is expected by PyODBC when using DSN-less connections
  ""No driver name specified; ""
Traceback (most recent call last):
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2276, in _wrap_pool_connect
    return fn()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 303, in unique_connection
    return _ConnectionFairy._checkout(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 492, in checkout
    rec = pool._do_get()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 139, in _do_get
    self._dec_overflow()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 153, in reraise
    raise value
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 136, in _do_get
    return self._create_connection()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 308, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 437, in __init__
    self.__connect(first_connect_check=True)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 639, in __connect
    connection = pool._invoke_creator(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\strategies.py"", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\default.py"", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
pyodbc.InterfaceError: ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""WiP.py"", line 21, in &lt;module&gt;
    cnxn = engine.connect()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2209, in connect
    return self._connection_cls(self, **kwargs)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 103, in __init__
    else engine.raw_connection()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2307, in raw_connection
    self.pool.unique_connection, _connection
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2280, in _wrap_pool_connect
    e, dialect, self
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 152, in reraise
    raise value.with_traceback(tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2276, in _wrap_pool_connect
    return fn()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 303, in unique_connection
    return _ConnectionFairy._checkout(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 492, in checkout
    rec = pool._do_get()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 139, in _do_get
    self._dec_overflow()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 153, in reraise
    raise value
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 136, in _do_get
    return self._create_connection()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 308, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 437, in __init__
    self.__connect(first_connect_check=True)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 639, in __connect
    connection = pool._invoke_creator(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\strategies.py"", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\default.py"", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
sqlalchemy.exc.InterfaceError: (pyodbc.InterfaceError) ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')
(Background on this error at: http://sqlalche.me/e/rvf5)

",0,418,"engine = sa.create_engine('mssql+pyodbc://servername/dbname')

needed to be changed to

engine = sa.create_engine('mssql+pyodbc://servername/dbname?trusted_connection=yes&amp;driver=ODBC Driver 13 for SQL Server')

Per: https://docs.sqlalchemy.org/en/13/dialects/mssql.html#hostname-connections
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/31325860,Dynamic Datasets and SQLAlchemy,"I am refactoring some old SQLite3 SQL statements in Python into SQLAlchemy.  In our framework, we have the following SQL statements that takes in a dict with certain known keys and potentially any number of unexpected keys and values (depending what information was provided).  

import sqlite3
import sys

def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d


def Create_DB(db):
    #    Delete the database
    from os import remove
    remove(db)

#   Recreate it and format it as needed
    with sqlite3.connect(db) as conn:
        conn.row_factory = dict_factory
        conn.text_factory = str

        cursor = conn.cursor()

        cursor.execute(""CREATE TABLE [Listings] ([ID] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL UNIQUE, [timestamp] REAL NOT NULL DEFAULT(( datetime ( 'now' , 'localtime' ) )), [make] VARCHAR, [model] VARCHAR, [year] INTEGER);"")


def Add_Record(db, data):
    with sqlite3.connect(db) as conn:
        conn.row_factory = dict_factory
        conn.text_factory = str

        cursor = conn.cursor()

        #get column names already in table
        cursor.execute(""SELECT * FROM 'Listings'"")
        col_names = list(map(lambda x: x[0], cursor.description))

        #check if column doesn't exist in table, then add it
        for i in data.keys():
            if i not in col_names:
                cursor.execute(""ALTER TABLE 'Listings' ADD COLUMN '{col}' {type}"".format(col=i, type='INT' if type(data[i]) is int else 'VARCHAR'))

        #Insert record into table
        cursor.execute(""INSERT INTO Listings({cols}) VALUES({vals});"".format(cols = str(data.keys()).strip('[]'), 
                    vals=str([data[i] for i in data]).strip('[]')
                    ))

#Database filename
db = 'test.db'

Create_DB(db)

data = {'make': 'Chevy',
    'model' : 'Corvette',
    'year' : 1964,
    'price' : 50000,
    'color' : 'blue',
    'doors' : 2}
Add_Record(db, data)

data = {'make': 'Chevy',
    'model' : 'Camaro',
    'year' : 1967,
    'price' : 62500,
    'condition' : 'excellent'}
Add_Record(db, data)


This level of dynamicism is necessary because there's no way we can know what additional information will be provided, but, regardless, it's important that we store all information provided to us.  This has never been a problem because in our framework, as we've never expected an unwieldy number of columns in our tables.

While the above code works, it's obvious that it's not a clean implementation and thus why I'm trying to refactor it into SQLAlchemy's cleaner, more robust ORM paradigm.  I started going through SQLAlchemy's official tutorials and various examples and have arrived at the following code:

from sqlalchemy import Column, String, Integer
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

Base = declarative_base()

class Listing(Base):
    __tablename__ = 'Listings'
    id = Column(Integer, primary_key=True)
    make = Column(String)
    model = Column(String)
    year = Column(Integer)

engine = create_engine('sqlite:///')

session = sessionmaker()
session.configure(bind=engine)
Base.metadata.create_all(engine)

data = {'make':'Chevy',
    'model' : 'Corvette',
    'year' : 1964}

record = Listing(**data)

s = session()
s.add(record)
s.commit()
s.close()


and it works beautifully with that data dict.  Now, when I add a new keyword, such as

data = {'make':'Chevy',
'model' : 'Corvette',
'year' : 1964,
'price' : 50000}


I get a TypeError: 'price' is an invalid keyword argument for Listing error.  To try and solve the issue, I modified the class to be dynamic, too:

class Listing(Base):
    __tablename__ = 'Listings'
    id = Column(Integer, primary_key=True)
    make = Column(String)
    model = Column(String)
    year = Column(Integer)

    def __checker__(self, data):
        for i in data.keys():
            if i not in [a for a in dir(self) if not a.startswith('__')]:
                if type(i) is int:
                    setattr(self, i, Column(Integer))
                else:
                    setattr(self, i, Column(String))
            else:
                self[i] = data[i]


But I quickly realized this would not work at all for several reasons, e.g. the class was already initialized, the data dict cannot be fed into the class without reinitializing it, it's a hack more than anything, et al.).  The more I think about it, the less obvious the solution using SQLAlchemy seems to me.  So, my main question is, how do I implement this level of dynamicism using SQLAlchemy?

I've researched a bit to see if anyone has a similar issue.  The closest I've found was Dynamic Class Creation in SQLAlchemy but it only talks about the constant attributes (""tablename"" et al.).  I believe the unanswered https://stackoverflow.com/questions/29105206/sqlalchemy-dynamic-attribute-change may be asking the same question.  While Python is not my forte, I consider myself a highly skilled programmer (C++ and JavaScript are my strongest languages) in the context scientific/engineering applications, so I may not hitting the correct Python-specific keywords in my searches.

I welcome any and all help.
",0,572,"class Listing(Base):
    __tablename__ = 'Listings'
    id = Column(Integer, primary_key=True)
    make = Column(String)
    model = Column(String)
    year = Column(Integer)
    def __init__(self,**kwargs):
       for k,v in kwargs.items():
           if hasattr(self,k):
              setattr(self,k,v)
           else:
              engine.execute(""ALTER TABLE %s AD COLUMN %s""%(self.__tablename__,k)
              setattr(self.__class__,Column(k, String))
              setattr(self,k,v)


might work ... maybe ... I am not entirely sure I did not test it

a better solution would be to use a relational table

class Attribs(Base):
    listing_id = Column(Integer,ForeignKey(""Listing""))
    name = Column(String)
    val = Column(String)

class Listing(Base):
    id = Column(Integer,primary_key = True)
    attributes = relationship(""Attribs"",backref=""listing"")
    def __init__(self,**kwargs):
        for k,v in kwargs.items():
            Attribs(listing_id=self.id,name=k,value=v)
    def __str__(self):
        return ""\n"".join([""A LISTING"",] + [""%s:%s""%(a.name,a.val) for a in self.attribs])


another solution would be to store json

class Listing(Base):
    __tablename__ = 'Listings'
    id = Column(Integer, primary_key=True)
    data = Column(String)
    def __init__(self,**kwargs):
       self.data = json.dumps(kwargs)
       self.data_dict = kwargs


the best solution would be to use a no-sql key,value store (maybe even just a simple json file? or perhaps shelve? or even pickle I guess)
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/4253176,Issue with SqlAlchemy - &quot;Parent instance &lt;SomeClass&gt; is not bound to a Session; lazy load operation...&quot;,"I have a small thrift server in python that I use do some fast lookups. The server queries mysql via SqlAlchemy on the first request and shoves all returned objects into a dictionary so on subsequent requests no DB call is needed. I just get the object from the dict and then call some of the object methods needed to give the proper response.

Initially, everything is fine. However, after the server runs a while, I am getting this exception when accessing the sqlalchemy object methods:


  
    Parent instance  is not bound to a Session; lazy load operation of attribute 'rate' cannot proceed.
  


Strange, because I set eagerload('rate').

I cannot really see a pattern to this behavior, it only affects some objects. However, once it does affect an object it will continue to do so on each request until I restart my python server.

Any ideas?
",9,11256,"You probably cache objects between the requests, and when the commit happens, session object is getting cleared, invalidating your objects. If you start your server via some multithreaded web server that starts workers as needed, that explains why there's no pattern.
If you dont want to get the bottom of this and just need a quick fix, this will always work:  

if obj not in session:
    obj = session.query(ObjClass).get(obj.id)


The proper solution would be to make sure you don't cache objects between requests.
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/38357352,Convert datetime to unix timestamp in SQLAlchemy model before executing query?,"I am using SQLAlchemy to work with a remote database that uses a strange timestamp format--it stores timestamps as double-precision milliseconds since epoch. I'd like to work with python datetime objects, so I wrote getter/setter methods in my model, following this gist:

from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import synonym
from sqlalchemy.dialects.mysql import DOUBLE
import datetime

Base = declarative_base()
class Table(Base):
    __tablename__ = ""table""

    id = Column(Integer, primary_key=True)
    _timestamp = Column(""timestamp"", DOUBLE(asdecimal=False))

    @property
    def timestamp(self):
        return datetime.datetime.utcfromtimestamp(float(self._timestamp)/1000.)

    @timestamp.setter
    def timestamp(self, dt):
        self._timestamp = float(dt.strftime(""%s""))*1000.

    timestamp = synonym('_timestamp', descriptor=timestamp)


This works great for inserting new rows into the table and working with objects from the table:

&gt;&gt;&gt; table = session.query(Table).first()
&lt;Table id=1&gt;
&gt;&gt;&gt; table.timestamp
datetime.datetime(2016, 6, 27, 16, 9, 3, 320000)
&gt;&gt;&gt; table._timestamp
1467043743320.0


However, it breaks down when I try to use a datetime in a filter expression:

&gt;&gt;&gt; july = datetime.datetime(2016, 7, 1)
&gt;&gt;&gt; old = session.query(Table).filter(Table.timestamp &lt; july).first()
/lib/python2.7/site-packages/sqlalchemy/engine/default.py:450: Warning: Truncated incorrect DOUBLE value: '2016-07-01 00:00:00'
&gt;&gt;&gt; july_flt = float(july.strftime(""%s""))*1000.
&gt;&gt;&gt; old = session.query(Table).filter(Table.timestamp &lt; july_flt).first()
&lt;Table id=1&gt;


I assume this is because my getter/setter methods apply to instances of the table class, but don't change the behavior of the class itself. I've tried rewriting using a hybrid property instead of a synonym:

from sqlalchemy.ext.hybrid import hybrid_property

class Table(Base):
    __tablename__ = ""table""

    id = Column(Integer, primary_key=True)
    _timestamp = Column(""timestamp"", DOUBLE(asdecimal=False))

    @hybrid_property
    def timestamp(self):
        return datetime.datetime.utcfromtimestamp(float(self._timestamp)/1000.)

    @timestamp.setter
    def timestamp(self, dt):
        self._timestamp = float(dt.strftime(""%s""))*1000.


Again, this works with Table instances, but fails on a query--now it's hitting my getter method when I run the query:

&gt;&gt;&gt; july = datetime.datetime(2016, 7, 1)
&gt;&gt;&gt; old = session.query(Table).filter(Table.timestamp &lt; july).first()
Traceback:
  File ""models.py"", line 42, in timestamp
    return datetime.datetime.utcfromtimestamp(float(self._timestamp)/1000.)
TypeError: float() argument must be a string or a number


With the debugger, I can see that the getter is receiving the Table._timestamp class (not a specific Table._timestamp, and not 'july').

I see that I could use the hybrid_property.expression decorator to define a SQL expression for converting timestamps into datetime, but what I'd really like is to convert the datetime into a timestamp on the python side, then run the query using timestamps. In other words, I'd like to use datetimes everywhere (including in queries), but have everything done with the microsecond timestamps on the SQL side. How can I do this?
",9,15018,"You have to use a custom type, which isn't as scary as it sounds.

from sqlalchemy.types import TypeDecorator


class DoubleTimestamp(TypeDecorator):
    impl = DOUBLE

    def __init__(self):
        TypeDecorator.__init__(self, as_decimal=False)

    def process_bind_param(self, value, dialect):
        return value.replace(tzinfo=datetime.timezone.utc).timestamp() * 1000

    def process_result_value(self, value, dialect):
        return datetime.datetime.utcfromtimestamp(value / 1000)


Then Table becomes:

class Table(Base):
    __tablename__ = ""table""

    id = Column(Integer, primary_key=True)
    timestamp = Column(DoubleTimestamp)


And then everything you mention works. You insert, select and compare with datetimes but it's stored as a DOUBLE.

Here I've used different logic for converting between timestamps since strftime('%s') isn't the correct solution. It's a different question which has been answered correctly here. Oh and I noticed you said microseconds but only convert to milliseconds in the code you posted, unless it was a slip of the tongue .
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/5675654,How to correctly achieve test isolation with a stateful Python module?,"The project I'm working on is a business logic software wrapped up as a Python package. The idea is that various script or application will import it, initialize it, then use it.

It currently has a top level init() method that does the initialization and sets up various things, a good example is that it sets up SQLAlchemy with a db connection and stores the SA session for later access. It is being stored in a subpackage of my project (namely myproj.model.Session, so other code could get a working SA session after import'ing the model).

Long story short, this makes my package a stateful one. I'm writing unit tests for the project and this stafeful behaviour poses some problems:


tests should be isolated, but the internal state of my package breaks this isolation
I cannot test the main init() method since its behavior depends on the state
future tests will need to be run against the (not yet written) controller part with a well known model state (eg. a pre-populated sqlite in-memory db)


Should I somehow refactor my package because the current structure is not the Best (possible) Practice(tm)? :)

Should I leave it at that and setup/teardown the whole thing every time? If I'm going to achieve complete isolation that'd mean fully erasing and re-populating the db at every single test, isn't that overkill?

This question is really on the overall code &amp; tests structure, but for what it's worth I'm using nose-1.0 for my tests. I know the Isolate plugin could probably help me but I'd like to get the code right before doing strange things in the test suite.
",8,1778,"You have a few options:

Mock the database

There are a few trade offs to be aware of.

Your tests will become more complex as you will have to do the setup, teardown and mocking of the connection.  You may also want to do verification of the SQL/commands sent.  It also tends to create an odd sort of tight coupling which may cause you to spend additonal time maintaining/updating tests when the schema or SQL changes.

This is usually the purest for of test isolation because it reduces a potentially large dependency from testing.  It also tends to make tests faster and reduces the overhead to automating the test suite in say a continuous integration environment.

Recreate the DB with each Test

Trade offs to be aware of.

This can make your test very slow depending on how much time it actually takes to recreate your database.  If the dev database server is a shared resource there will have to be additional initial investment in making sure each dev has their own db on the server.  The server may become impacted depending on how often tests get runs.  There is additional overhead to running your test suite in a continuous integration environment because it will need at least, possibly more dbs (depending on how many branches are being built simultaneously).

The benefit has to do with actually running through the same code paths and similar resources that will be used in production.  This usually helps to reveal bugs earlier which is always a very good thing.

ORM DB swap

If your using an ORM like SQLAlchemy their is a possibility that you can swap the underlying database with a potentially faster in-memory database.  This allows you to mitigate some of the negatives of both the previous options.

It's not quite the same database as will be used in production, but the ORM should help mitigate the risk that obscures a bug.  Typically the time to setup an in-memory database is much shorter that one which is file-backed.  It also has the benefit of being isolated to the current test run so you don't have to worry about shared resource management or final teardown/cleanup.
","Working on a project with a relatively expensive setup (IPython), I've seen an approach used where we call a get_ipython function, which sets up and returns an instance, while replacing itself with a function which returns a reference to the existing instance. Then every test can call the same function, but it only does the setup for the first one.

That saves doing a long setup procedure for every test, but occasionally it creates odd cases where a test fails or passes depending on what tests were run before. We have ways of dealing with that - a lot of the tests should do the same thing regardless of the state, and we can try to reset the object's state before certain tests. You might find a similar trade-off works for you.
","Mock is a simple and powerfull tool to achieve some isolation. There is a nice video from Pycon2011 which shows how to use it. I recommend to use it together with py.test which reduces the amount of code required to define tests and is still very, very powerfull.
"
SQLAlchemy strange behavior,https://stackoverflow.com/questions/7987981,strange python regex behavior - maybe connected to unicode or sqlalchemy,"I'm trying to search for a pattern in sqlalchemy results (actually filter by a 'like' or 'op'('regexp')(pattern) which I believe is implanted with regex somewhere) - the string and the search string are both in hebrew, and presumably (maybe I'm wrong-)-unicode
where r = u'' and c = u', ,    '
when I do re.search(r,c) I get the SRE.match object
but when I query the db like:

f = session.query(classname)
c = f[0].color


and c gives me:

'\xd7\x9c\xd7\x91\xd7\x9f,\xd7\x95\xd7\xa8\xd7\x95\xd7\x93,'


or print (c):

,,


practicaly the same but running re.search(r,c) gives me no match object.

Since I suspected a unicode issue I tried to transform to unicode with unicode(c)
and I get an 'UnicodeDecodeError: 'ascii' codec can't decode byte 0xd7 in position 0: ordinal' which I guess means this is already unicode string - so where's the catch here?
I would prefer using the sqlalchemy 'like' but I get the same problem there = where I know for sure (as I showed in my example that the data contains the string)

Should I transform the search string,pattern somehow? is this related to unicode? something else?

The db table (which I'm quering) collation is utf8_unicode_ci
",3,411,"c = f[0].color


is not returning a Unicode string (or its repr() would show a u'...' kind of string), but a UTF-8 encoded string.

Try 

c = f[0].color.decode(""utf-8"")


which results in

u'\u05dc\u05d1\u05df,\u05d5\u05e8\u05d5\u05d3,'


or

u',,'


if your console can display Hebrew characters.
","'\xd7\x9c\xd7\x91\xd7\x9f,\xd7\x95\xd7\xa8\xd7\x95\xd7\x93, is encoded representation of string u', , '. So in the second example you should write re.search(r,c.decode('utf-8'))
You're trying to do almost the same except setting encoding parameter. It makes python try ascii encoding
",
SQLAlchemy strange behavior,https://stackoverflow.com/questions/20933018,Random errors with SQLAlchemy,"I'm using a setup with nginx, uwsgi and SQLAlchemy. I recently switched from SQLObject and I'm now seeing strange random errors with SQLAlchemy. For instance:

sqlalchemy.exc.ResourceClosedError: This result object does not return rows. It has been closed automatically.


or:

sqlalchemy.exc.NoSuchColumnError: ""Could not locate column in row for column 'module.id'""


Is this some kind of behavior in SQLAlchemy which I'm not aware of? Can it be related to multiple processes/threads in uwsgi?

My uwsgi config file looks like this:

[uwsgi]
plugins=python
socket = 127.0.0.1:9002
wsgi-file = /thesystem/code/api.py
master = True
processes  = 4
threads = 2
daemonize = /thesystem/logs/uwsgi.log
pidfile = /thesystem/uwsgi.pid

",3,847,"Very probably you are opening connections in /thesystem/code/api.py entry point.

That means your file descriptors will be inherited in workers and this does not work with sqlalchemy.

Add --lazy-apps (lazy-apps = true in your ini config) to load /thesystem/code/api.py in each worker instead of loading it in the master and then calling fork()
","In addition to the accepted answer, if you do not want to (or cannot) change preforking for lazy-apps, because of the increase in memory usage, for instance, or the changes in your uwsgi reload strategy, you can simply reconnect to the database after forking:

import uwsgi
def setup_db():
    """""" routine that sets up the connection to your database """"""
    ...

uwsgi.post_fork_hook = setup_db

",
SQLAlchemy strange behavior,https://stackoverflow.com/questions/39298822,SQLAlchemy secondary join model fails under strange conditions,"I have a strange issue that I simply cannot resolve. Essentially, I have a model and system which works perfectly - except under a very specific (and seemingly arbitrary) set of circumstances.

I'll paste the model in a second but here's the idea. I want certain tables to be versioned. That means for a given table, I break it into two tables, the Master part which has the natural keys for the object, and the Version table which has all the associated data which may change. Then some of my models of course have a relationship, so I create a join table that links versions.

Here are the models:

class Versioned(object):

    def __init__(self, **kwargs):

        super(Versioned, self).__init__(**kwargs)

        self.active = True
        self.created_on = datetime.datetime.now()

    active = Column(BOOLEAN)
    created_on = Column(TIMESTAMP, server_default=func.now())

    def __eq__(self, other):

        return self.__class__ == other.__class__ and \
            all([getattr(self, key) == getattr(other, key)
                for key in self.comparison_keys
                ])

    def __ne__(self, other):

        return not self.__eq__(other)

    comparison_keys = []

class Parent(Base):

    __tablename__ = 'parent'

    id = Column(INTEGER, primary_key=True)

    name = Column(TEXT)

    versions = relationship(""ParentVersion"", back_populates=""master"")

    children = relationship(""Child"", back_populates=""parent"")

    @property
    def current_version(self):
        active_versions = [v for v in self.versions if v.active==True]

        return active_versions[0] if active_versions else None

class ParentVersion(Versioned, Base):

    __tablename__ = 'parent_version'

    id = Column(INTEGER, primary_key=True)

    master_id = Column(INTEGER, ForeignKey(Parent.id))

    address = Column(TEXT)

    master = relationship(""Parent"", back_populates=""versions"")

    children = relationship(""ChildVersion"",
        secondary=lambda : Parent_Child.__table__
    )

class Child(Base):

    __tablename__ = 'child'

    id = Column(INTEGER, primary_key=True)

    parent_id = Column(INTEGER, ForeignKey(Parent.id))

    name = Column(TEXT)

    versions = relationship(""ChildVersion"", back_populates=""master"")

    parent = relationship(""Parent"", back_populates=""children"")

    @property
    def current_version(self):
        active_versions = [v for v in self.versions if v.active==True]

        return active_versions[0] if active_versions else None


class ChildVersion(Versioned, Base):

    __tablename__ = 'child_version'

    id = Column(INTEGER, primary_key=True)

    master_id = Column(INTEGER, ForeignKey(Child.id))

    age = Column(INTEGER)

    fav_toy = Column(TEXT)

    master = relationship(""Child"", back_populates=""versions"")

    parents = relationship(""ParentVersion"",
        secondary=lambda: Parent_Child.__table__,
    )

    comparison_keys = [
        'age',
        'fav_toy',
    ]

class Parent_Child(Base):

    __tablename__ = 'parent_child'

    id = Column(INTEGER, primary_key=True)

    parent_id = Column(INTEGER, ForeignKey(ParentVersion.id))
    child_id = Column(INTEGER, ForeignKey(ChildVersion.id))


Okay, so I know the more recent SQLAlchemy models have some idea of versioning, it's possible that I'm doing this the wrong way. But this fits my use case very well. So humor me and let's assume the model is okay (in the general sense - if there's a minor detail causing the bug that would be good to fix)

Now suppose I want to insert data. I have data from some source, I take it in and build models. Ie, split things into Master/Version, assign the child relationships, assign the version relationships. Now I want to compare it against the data already in my database. For each master object, if I find it, I compare the versions. If the versions are different, you create a new version. The tricky part becomes, if a Child version is different, I want to insert a new Parent version, and update all its relationships. Maybe the code makes more sense to explain this part. search_parent is the object I have created in my pre-parsing stage. It has a version, and children objects, which also have versions.

parent_conds = [
    getattr(search_parent.__class__, name) == getattr(search_parent, name)
    for name, column in search_parent.__class__.__mapper__.columns.items()
    if not column.primary_key
]

parent_match = session.query(Parent).filter(*parent_conds).first()

# We are going to make a new version
parent_match.current_version.active=False
parent_match.versions.append(search_parent.current_version)

for search_child in search_parent.children[:]:

    search_child.parent_id = parent_match.id

    search_conds = [
        getattr(search_child.__class__, name) == getattr(search_child, name)
        for name, column in search_child.__class__.__mapper__.columns.items()
        if not column.primary_key
    ]

    child_match = session.query(Child).filter(*search_conds).first()

    if child_match.current_version != search_child.current_version:
        # create a new version: deactivate the old one, insert the new
        child_match.current_version.active=False
        child_match.versions.append(search_child.current_version)

    else:
        # copy the old version to point to the new parent version
        children = parent_match.current_version.children

        children.append(child_match.current_version)
        children.remove(search_child.current_version)
        session.expunge(search_child.current_version)

    session.expunge(search_child)

session.expunge(search_parent)

session.add(parent_match)

session.commit()


Okay, so once again, this might not be the perfect or even best approach. But it does work. EXCEPT, and this is what I can't figure out. It doesn't work if I'm updating the child's age attribute to the integer value zero. If the child objects start with age 0, and I change it to something else, this works beautifully. If I start with some non-zero integer, and update the age to 0, I get this warning:

SAWarning: Object of type &lt;ChildVersion&gt; not in session, add operation   along 'ParentVersion.children' won't proceed (mapperutil.state_class_str(child), operation, self.prop))


The updated version is inserted, however the insert into the parent_child join table doesn't happen. And it's not that it fails, it's that SQLAlchemy has determined the child object doesn't exist and can't create the join. But it does exist, I know it gets inserted.

Again, this only happens if I'm inserting a new version with age=0. If I'm inserting a new version with any other age, this works exactly as I want it to.

There are other odd things about the bug - it doesn't happen if you don't insert enough children (seems to be around 12 triggers the bug), it doesn't happen depending on other attributes sometimes. I don't think I fully understand the surface area of what causes it.

Thanks for taking the time to read this far. I have a fully working demo complete with source data I'd be happy to share, it just requires some setup so I didn't know if it was appropriate in this post. I hope someone has ideas for what to look at because at this point I'm totally out.

edit: Here is the full stack trace leading to the warning.

  File ""repro.py"", line 313, in &lt;module&gt;
  load_data(session, second_run)
File ""repro.py"", line 293, in load_data
  session.commit()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 801, in commit
  self.transaction.commit()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 392, in commit
  self._prepare_impl()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 372, in _prepare_impl
  self.session.flush()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 2019, in flush
  self._flush(objects)
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 2101, in _flush
  flush_context.execute()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py"", line 373, in execute
  rec.execute(self)
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py"", line 487, in execute
  self.dependency_processor.process_saves(uow, states)
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/dependency.py"", line 1053, in process_saves
  False, uowcommit, ""add""):
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/dependency.py"", line 1154, in _synchronize
  (mapperutil.state_class_str(child), operation, self.prop))
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 1297, in warn
  warnings.warn(msg, exc.SAWarning, stacklevel=2)
File ""repro.py"", line 10, in warn_with_traceback
  traceback.print_stack()
/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/dependency.py:1154: SAWarning: Object of type &lt;ChildVersion&gt; not in session, add operation along 'ParentVersion.children' won't proceed
(mapperutil.state_class_str(child), operation, self.prop))


edit2:
Here is a gist with a python file you can run to see the strange behavior.
https://gist.github.com/jbouricius/2ede420fb1f7a2deec9f557c76ced7f9
",3,505,"The reason you get this error is that you've inadvertently added objects into the session.

Here is the MVCE:

engine = create_engine(""sqlite://"", echo=False)


def get_data():
    children = [
        Child(name=""Carol"", versions=[ChildVersion(age=0, fav_toy=""med"")]),
        Child(name=""Timmy"", versions=[ChildVersion(age=0, fav_toy=""med"")]),
    ]
    return Parent(
        name=""Zane"", children=children,
        versions=[
            ParentVersion(
                address=""123 Fake St"",
                children=[v for child in children for v in child.versions]
            )
        ]
    )


def main():
    Base.metadata.create_all(engine)

    session = Session(engine)
    parent_match = get_data()
    session.add(parent_match)
    session.commit()

    with session.no_autoflush:
        search_parent = get_data()

        parent_match.versions.append(search_parent.current_version)
        for search_child in search_parent.children[:]:
            child_match = next(c for c in parent_match.children if c.name == search_child.name)

            if child_match.current_version != search_child.current_version:
                child_match.versions.append(search_child.current_version)
            else:
                session.expunge(search_child.current_version)

            session.expunge(search_child)

        session.expunge(search_parent)
        session.commit()


Aside: this is what you needed to provide in the question itself. Providing a tarball with instructions is not the best way to get answers.

The line

parent_match.versions.append(search_parent.current_version)


not only adds search_parent.current_version, it also adds search_parent, which in turn adds all related objects, including the child versions of other children. Judging by the fact that you later expunge other related objects to prevent them from being added to the session, I conclude that you only want to add search_parent.current_version without adding other related objects. Due to the circular nature of your relationships you need to take care to lift only the objects you want out of search_parent before you add them. Here is the fixed MVCE:

with session.no_autoflush:
    search_parent = get_data()

    current_parent_version = search_parent.current_version
    search_parent.versions.remove(current_parent_version)
    current_parent_version.children = []  # &lt;--- this is key
    for search_child in search_parent.children[:]:
        child_match = next(c for c in parent_match.children if c.name == search_child.name)

        if child_match.current_version != search_child.current_version:
            current_child_version = search_child.current_version
            search_child.versions.remove(current_child_version)
            child_match.versions.append(current_child_version)
            current_parent_version.children.append(current_child_version)

    parent_match.versions.append(current_parent_version)

    session.commit()

",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/66896139,Flask-Executor and Flask-SQLAlchemy: Can&#39;t get updated data from DB inside of executor function,"I'm adding a Flask-based API to my web application to control the start and stop of some network automation functions. I've ran into a strange behavior where functions called by a Flask-Executor .submit() method are seemingly unable to get new or updated data from the database.
I know this question is very involved, so thank you to anyone who shares their time and input. See the end of this question for an overview of my project structure.
The flask-executor documentation says:

When calling submit() or map() Flask-Executor will wrap ThreadPoolExecutor callables with a copy of both the current application context and current request context

I don't quite fully understand what it means by context, but I feel that it might be a good hint about why this should or shouldn't work. (I am using the ThreadPoolExecutor, by the way). I assume that the db SQLAlchemy object is part of the application context, and as such a copy of db should be made available in the executor function. This didn't seem to be the case because I still had to import db in the file containing the function called by the executor, as you'll see later on in this post.
My front end has simple start and stop buttons which send a POST to the following API route:

file: app/api.py

from flask import request
from flask_login import login_required
from app import app, db, executor
from app.models import Project
from datetime import datetime
from automation.Staging import control

@app.route('/api/staging/control', methods=['POST'])
@login_required
def staging_control():
    data = request.json
    project_id = data['project-id']
    action = data['staging-control']

    project = Project.query.get(project_id)
    sp = project.staging_profile
    current_status = sp.status

    if action == 'start':
        if current_status == 'STARTED':
            return {'response': 200, 'message': 'Job already running!'}
        else:
            sp.status = 'STARTED'
            db.session.commit()
            # The executor only spawns the thread if the task status was not already started.
            executor.submit(control.start_staging, project_id)
        
    
    elif action == 'stop':
        if current_status == 'STARTED':
            sp.status = 'STOPPED'
            db.session.commit()

    return {'response' : 200, 'message': 'OK'}

Background
The status of the job is stored in a DB model.  If a start action is POSTed, the DB model's status column is updated. Likewise, if a stop action is POSTed, the DB model's status is updated.
The executor's function call to control.start_staging spawns a thread that begins an infinite loop which does some work and then sleeps for X seconds.  At the start of each time through the loop, I am trying to check the DB model's status column to determine whether or not to break from the loop and close the thread.
Starting the thread works just fine. The database model gets updated, the executor spawns the thread, and my while loop begins.
Sending the stop action from my frontend works just fine too. The status in the DB is set to STOPPED, and I can see this with manual queries to in my DB shell.
However, the control.start_staging function originally started by the executor still thinks the status is set to STARTED, even though it will actually be updated to STOPPED at some time during the thread's operation. I have attempted to get the updated value as many ways as I can think of from inside the thread. I've seen this similar question.
Here is the control.start_staging function. I've shared a few of the different ways that I've tried to get the updated status in the excerpt below as comments:

file: automation/Staging/control.py

from app import db
from app.models import Project, Staging_Profile
from app.config import STAGING_DURATION_MINS
from datetime import datetime, timedelta
from time import sleep


def start_staging(project_id):    
    project = Project.query.get(project_id)
    print(f""Received START for project {project.project_name}"")
    sp = project.staging_profile
    sp.last_activity = datetime.utcnow()
    db.session.commit()
    status = sp.status

    # Staging Loop Start
    while True:

        # This just serves as a force-stop if the job runs for more than STAGING_DURATION_MINUTES minutes.
        if sp.last_activity + timedelta(minutes=STAGING_DURATION_MINS) &gt; datetime.utcnow():
            print(f""Status is: {sp.status}"")

            # ATTEMPT 1: does not get updated data
            # status = sp.status

            # ATTEMPT 2: does not get updated data
            # status = Staging_Profile.query.get(project.staging_profile_id).status

            # ATTEMPT 3: does not get updated data
            all_profiles = db.session.query(Staging_Profile).all()
            this_profile = [profile for profile in all_profiles if profile.id == sp.id][0]

            if this_profile.status == 'STOPPED':
                print(""Status is STOPPED. Returning"")
                break
            
            else:
                print(f""Status is {this_profile.status}"")

            # Do work
            do_some_stuff()

        else:
            break
        sleep(5)

    return

Now, what's really puzzling is that I can write data to the database from inside the executor function. The line sp.last_activity = datetime.utcnow() followed by db.session.commit() successfully writes the current time when the thread is started.
My Suspicions
I have built this application in a very modular style approach, and I feel that perhaps this is the source of the issue.
Here is an overview of the relevant parts of my application structure:
app/
 __init__.py   # This is where my db &amp; executor are instantiated
 api.py        # This is where the /api/staging/control route lives
 models.py     # This holds my SQLAlchemy DB classes
 routes.py     # This holds my regular front-end routes
 config.py     # General config parameters

automation/
 Staging/
   control.py    # This is where the function passed to the executor is defined
   __init__.py   # Empty
 __init__.py      # Empty

Thanks again. I will post a resolution or a workaround to this issue when I find one.
",3,437,"use update
Project.query.filter_by(id=project_id).update({
 'last_activity': datetime.utcnow()
})

",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/46440663,Strange filter behavior in flask-sqlalchemy,"I have a query in flask-sqlalchemy and filter is behaving strange:

q.filter(Transaction.transaction_id == ReconciledTransaction.safe_withdraw_id).all()


It works fine, but:

q.filter(Transaction.transaction_id != ReconciledTransaction.safe_withdraw_id).all()


Doesn't work correctly! What seems to be the problem?

UPD
My models:
Reconciled transaction model:

class ReconciledTransactionModel(db.Model):
    """"""Reconciled Transaction model""""""

    __tablename__ = 'ReconciledTransaction'

    id = db.Column('id', db.Integer, primary_key=True, nullable=False)
    balance_entry_id = db.Column('BalanceEntry_id', db.Integer, db.ForeignKey(""BalanceEntry.id""), nullable=False)
    safe_withdraw_id = db.Column('Transaction_id', db.String, nullable=False)
    datetime = db.Column('datetime', db.Date(), nullable=False)
    balance_entry_amount = db.Column('BalanceEntry_amount', db.Float)
    reconciled_amount = db.Column('ReconciledAmount', db.Float)
    currency = db.Column('currency', db.String)
    reconciliation_status = db.Column('reconciliation_status', db.String, nullable=False)
    status_code = db.Column('status_code', db.Integer, nullable=False)


Transaction Model:

class TransactionModel(db.Model):
    """"""Transaction SA model.""""""

    __tablename__ = 'Transaction'

    id = db.Column('id', db.Integer, primary_key=True)
    till_id = db.Column('Till_id', db.Integer, db.ForeignKey(""Till.id""),
                        nullable=False)
    till = relationship(""Till"", foreign_keys=[till_id], backref=""transactions"", enable_typechecks=False)
    establishment_id = db.Column('Establishment_id', db.Integer,
                                 db.ForeignKey(""Establishment.id""),
                                 nullable=False)
    establishment = relationship(""Establishment"",
                                 foreign_keys=[establishment_id],
                                 backref=""transactions"",
                                 enable_typechecks=False)
    employee_id = db.Column('Employee_id', db.Integer,
                            db.ForeignKey(""Employee.id""),
                            nullable=False)
    employee = relationship(""Employee"",
                            foreign_keys=[employee_id],
                            backref=""transactions"",
                            enable_typechecks=False)
    local_time = db.Column('local_time', db.DateTime, nullable=False)
    create_time = db.Column('create_time', db.TIMESTAMP(timezone=True),
                            nullable=False)
    send_time = db.Column('send_time', db.TIMESTAMP(timezone=True),
                          nullable=False)
    receive_time = db.Column('receive_time', db.TIMESTAMP(timezone=True),
                             nullable=False)
    total_value = db.Column('total_value', db.Integer, nullable=False)
    amount = db.Column('amount', db.Float, nullable=False)
    discrepancy = db.Column('discrepancy', db.Float, nullable=False)
    type = db.Column('type', db.Enum('shift',
                                     'payment',
                                     'skimming',
                                     'withdraw',
                                     'refund',
                                     'till',
                                     'till_deposit',
                                     'safe_deposit',
                                     'safe_withdraw',
                                     'till_reset',
                                     name='transaction_type'),
                     nullable=False)
    status = db.Column('status',
                       db.Enum('start', 'end', name='transaction_status'),
                       nullable=False)
    receipt_id = db.Column('receipt_id', db.String(32), server_default=None)
    transaction_id = db.Column('transaction_id', db.String(32),
                               server_default=None)
    parent_transaction = db.Column('parent_transaction', db.String(32),
                                   server_default=None)
    discrepancy_reason = db.Column('discrepancy_reason', db.String(1024))
    resolve_discrepancy_reason = db.Column('resolve_discrepancy_reason',
                                           db.String(1024))
    accounted = db.Column('accounted', db.Boolean, default=False)


And here is my query:

_transactions = db.session.query(Transaction,
                                 status_sq.c.count,
                                 end_transaction_sq.c.discrepancy,
                                 end_transaction_sq.c.discrepancy_reason,
                                 end_transaction_sq.c.resolve_discrepancy_reason,
                                 end_transaction_sq.c.amount,
                                 ). \
    filter(Transaction.establishment_id.in_(store_ids)). \
    filter(Transaction.amount != 0). \
    filter_by(status='start')

transactions = _transactions. \
    filter(Transaction.type.in_(transaction_types)). \
    outerjoin(status_sq,
              Transaction.transaction_id == status_sq.c.transaction_id). \
    outerjoin(end_transaction_sq,
              Transaction.transaction_id == end_transaction_sq.c.transaction_id)

# check possible values for sorting and pages
if sort_field not in allowed_sort_fields:
    sort_field = Transaction.default_sort_field
if sort_dir not in (ASCENDING, DESCENDING):
    sort_dir = Transaction.default_sort_dir
if per_page &gt; 100:  # hard limit
    per_page = Transaction.default_per_page

if sort_dir == ASCENDING:
    order = allowed_sort_fields[sort_field].desc()
else:
    order = allowed_sort_fields[sort_field].desc()

q = transactions.\
    join(Establishment).\
    join(Employee, Transaction.employee_id == Employee.id). \
    outerjoin(Currency). \
    group_by(Transaction,
             status_sq.c.count,
             end_transaction_sq.c.discrepancy,
             end_transaction_sq.c.discrepancy_reason,
             end_transaction_sq.c.resolve_discrepancy_reason,
             end_transaction_sq.c.amount,
             allowed_sort_fields[sort_field]).\
    order_by(order)
items = q.filter(Transaction.transaction_id == ReconciledTransaction.safe_withdraw_id).limit(per_page).offset((page - 1) * per_page).all()


'Doesn't work correctly' means that in second case(when I place !=, and wanna take transactions only, which are not in ReconciledTransaction table) filter gets ignored, but when filter contains ==, all works correctly(I have only matched transactions).
",2,211,"When you use query like this:

q = db.session.query(Transaction). \
    filter(Transaction.transaction_id != ReconciledTransaction.safe_withdraw_id)


it transforms into SQL query:

SELECT Transaction.* FROM Transaction, ReconciledTransaction
WHERE Transaction.transaction_id != ReconciledTransaction.safe_withdraw_id


which means you will get all Transaction rows with all ReconciledTransaction rows except those with matching ids.

If you need to get all Transaction objects which are not in ReconciledTransaction table you can first get all ReconciledTransaction ids:

r_query = db.session.query(ReconciledTransaction.safe_withdraw_id). \
    group_by(ReconciledTransaction.safe_withdraw_id)
r_ids = [x[0] for x in r_query]


and then use NOT IN filter in your Transaction query:

q = q.filter(Transaction.transaction_id.notin_(r_ids))


Or your can use subquery:

q = q.filter(Transaction.transaction_id.notin_(
    db.session.query(ReconciledTransaction.safe_withdraw_id)
))


Edit: as Ilja Everil stated NOT EXISTS operator performance might be better than NOT IN. SQLAlchemy query will look like this:

q = q.filter(~session.query(ReconciledTransaction). \
    filter(ReconciledTransaction.safe_withdraw_id == Transaction.id).exists())

",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/56317578,SQLAlchemy does not update/expire model instances with external changes,"Recently I came across strange behavior of SQLAlchemy regarding refreshing/populating model instances with the the changes that were made outside of the current session. I created the following minimal working example and was able to reproduce problem with it.


from time import sleep

from sqlalchemy import orm, create_engine, Column, BigInteger, Integer
from sqlalchemy.ext.declarative import declarative_base

DATABASE_URI = ""postgresql://{user}:{password}@{host}:{port}/{name}"".format(
    user=""postgres"",
    password=""postgres"",
    host=""127.0.0.1"",
    name=""so_sqlalchemy"",
    port=""5432"",
)


class SQLAlchemy:
    def __init__(self, db_url, autocommit=False, autoflush=True):
        self.engine = create_engine(db_url)
        self.session = None

        self.autocommit = autocommit
        self.autoflush = autoflush

    def connect(self):
        session_maker = orm.sessionmaker(
            bind=self.engine,
            autocommit=self.autocommit,
            autoflush=self.autoflush,
            expire_on_commit=True
        )
        self.session = orm.scoped_session(session_maker)

    def disconnect(self):
        self.session.flush()
        self.session.close()
        self.session.remove()
        self.session = None


BaseModel = declarative_base()


class TestModel(BaseModel):
    __tablename__ = ""test_models""

    id = Column(BigInteger, primary_key=True, nullable=False)
    field = Column(Integer, nullable=False)


def loop(db):
    while True:
        with db.session.begin():
            t = db.session.query(TestModel).with_for_update().get(1)
            if t is None:
                print(""No entry in db, creating..."")
                t = TestModel(id=1, field=0)
                db.session.add(t)
                db.session.flush()

            print(f""t.field value is {t.field}"")
            t.field += 1
            print(f""t.field value before flush is {t.field}"")
            db.session.flush()
            print(f""t.field value after flush is {t.field}"")

        print(f""t.field value after transaction is {t.field}"")
        print(""Sleeping for 2 seconds."")
        sleep(2.0)


def main():
    db = SQLAlchemy(DATABASE_URI, autocommit=True, autoflush=True)
    db.connect()
    try:
        loop(db)
    except KeyboardInterrupt:
        print(""Canceled"")


if __name__ == '__main__':
    main()



My requirements.txt file looks like this:

alembic==1.0.10
psycopg2-binary==2.8.2
sqlalchemy==1.3.3


If I run the script (I use Python 3.7.3 on my laptop running Ubuntu 16.04), it will nicely increment a value every two seconds as expected:

t.field value is 0
t.field value before flush is 1
t.field value after flush is 1
t.field value after transaction is 1
Sleeping for 2 seconds.
t.field value is 1
t.field value before flush is 2
t.field value after flush is 2
t.field value after transaction is 2
Sleeping for 2 seconds.
...


Now at some point I open postgres database shell and begin another transaction:

so_sqlalchemy=# BEGIN;
BEGIN
so_sqlalchemy=# UPDATE test_models SET field=100 WHERE id=1;
UPDATE 1
so_sqlalchemy=# COMMIT;
COMMIT


As soon as I press Enter after the UPDATE query, the script blocks as expected, as I'm issuing SELECT ... FOR UPDATE query there. However, when I commit the transaction in the database shell, script continues from the previous value (say, 27) and does not detect that external transaction has changed the value of field in database to 100.

My question is, why does this happen at all? There are several factors that seem to contradict the current behavior:


I'm using expire_on_commit setting set to True, which seems to imply that every model instance that has been used in transaction will be marked as expired after the transaction has been committed. (Quoting documentation, ""When True, all instances will be fully expired after each commit(), so that all attribute/object access subsequent to a completed transaction will load from the most recent database state."").
I'm not accessing some old model instance but rather issue completely new query every time. As far as I understand, this should lead to direct query to the database and not access cached instance. I can confirm that this is indeed the case if I turn sqlalchemy debug log on.


The quick and dirty fix for this problem is to call db.session.expire_all() right after the transaction has begun, but this seems very inelegant and counter-intuitive. I would be very glad to understand what's wrong with the way I'm working with sqlalchemy here.
",2,1378,"I ran into a very similar situation with MySQL. I needed to ""see"" changes to the table that were coming from external sources in the middle of my code's database operations. I ended up having to set autocommit=True in my session call and use the begin() / commit() methods of the session to ""see"" data that was updated externally.

The SQLAlchemy docs say this is a legacy configuration:


  Warning
  
  autocommit mode is a legacy mode of use and should not be considered for new projects.


but also say in the next paragraph:


  Modern usage of autocommit mode tends to be for framework integrations that wish to control specifically when the begin state occurs


So it doesn't seem to be clear which statement is correct.
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/17315422,Unable to store datetime.datetime.max using SQLAlchemy==0.8.1 with the mysql-python==1.2.4 driver,"I've noticed a change in behavior for storing datetime.datetime.max via SQLAlchemy==SQLAlchemy==0.8.1 and going from mysql-python==1.2.3 to mysql-python==1.2.4. By only changing the driver from 1.2.3 to 1.2.4 I go from being able to store to being unable to store it.

Where do I turn to for help in this matter? SQLAlchemy or mysql-python? Is this expected behaviour or a bug or do I have a bad setup? I fear that a change like this will break a lot of systems out there.

This is my SQLAlchemy setup:

from sqlalchemy import create_engine, Integer, DateTime, Column
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base
from datetime import datetime

engine = create_engine('mysql://root@localhost/test_database', echo=True)
Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)
    age = Column(DateTime, default=datetime.max)

Base.metadata.create_all(engine)
session = sessionmaker(bind=engine)()
u = User()
session.add(u)
session.commit()


I also have a virtualenv called test. This is what happens when I run the code above.

(test)  ~  pip install MySQL-python==1.2.3
(test)  ~  python test.py
2013-06-26 10:29:18,885 INFO sqlalchemy.engine.base.Engine SELECT DATABASE()
2013-06-26 10:29:18,885 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:29:18,887 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'character_set%%'
2013-06-26 10:29:18,887 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:29:18,891 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'sql_mode'
2013-06-26 10:29:18,891 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:29:18,896 INFO sqlalchemy.engine.base.Engine DESCRIBE `users`
2013-06-26 10:29:18,896 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:29:18,904 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
2013-06-26 10:29:18,905 INFO sqlalchemy.engine.base.Engine INSERT INTO users (age) VALUES (%s)
2013-06-26 10:29:18,905 INFO sqlalchemy.engine.base.Engine (datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),)
2013-06-26 10:29:18,908 INFO sqlalchemy.engine.base.Engine COMMIT


And the database (test_database) looks like this:

mysql&gt; select * from users;
+----+---------------------+
| id | age                 |
+----+---------------------+
|  1 | 9999-12-31 23:59:59 |
+----+---------------------+
1 row in set (0.00 sec)


This is my expected result so nothing strange here.

However, by simply switching the driver to mysql-python==1.2.4 I get this result.

(test)  ~  pip install MySQL-python==1.2.4
(test)  ~  python test.py
2013-06-26 10:33:39,544 INFO sqlalchemy.engine.base.Engine SELECT DATABASE()
2013-06-26 10:33:39,544 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:33:39,546 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'character_set%%'
2013-06-26 10:33:39,546 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:33:39,546 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'sql_mode'
2013-06-26 10:33:39,546 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:33:39,547 INFO sqlalchemy.engine.base.Engine DESCRIBE `users`
2013-06-26 10:33:39,547 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:33:39,551 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
2013-06-26 10:33:39,552 INFO sqlalchemy.engine.base.Engine INSERT INTO users (age) VALUES (%s)
2013-06-26 10:33:39,552 INFO sqlalchemy.engine.base.Engine (datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),)
/Users/pelle/.virtualenvs/test/lib/python2.7/site-packages/sqlalchemy/engine/default.py:324: Warning: Datetime function: datetime field overflow
  cursor.execute(statement, parameters)
/Users/pelle/.virtualenvs/test/lib/python2.7/site-packages/sqlalchemy/engine/default.py:324: Warning: Out of range value for column 'age' at row 1
  cursor.execute(statement, parameters)
2013-06-26 10:33:39,553 INFO sqlalchemy.engine.base.Engine COMMIT


And the database looks like this.

mysql&gt; select * from users;
+----+---------------------+
| id | age                 |
+----+---------------------+
|  1 | 0000-00-00 00:00:00 |
+----+---------------------+
1 row in set (0.00 sec)


So now all of the sudden I receive a warning Warning: Datetime function: datetime field overflow and I end up with a nullable value in my database.
",1,832,"This is a reported bug in the new version of the MySQL server (5.6.X) to do with the rounding of fractional seconds.

See this link for more information:
http://bugs.mysql.com/bug.php?id=68760

The way round this is to round out the milliseconds.
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/20025324,Why does SQLAlchemy insert expression values() method function differently when executing in one statement or two?,"I'm working through the SQLAlchemy core tutorial (http://docs.sqlalchemy.org/en/rel_0_8/core/tutorial.html) and found a strange behavior. In the Insert Expressions section they first create a basic insert expression and print it.

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; str(ins)
'INSERT INTO users (id, name, fullname) VALUES (:id, :name, :fullname)'


They then perform the same operation, adding values to specific columns, which limits the number of columns in the expression to those listed in the values() call.

&gt;&gt;&gt; ins = users.insert().values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; str(ins)
'INSERT INTO users (name, fullname) VALUES (:name, :fullname)'


Why is it that if I take the second version and perform it in 2 lines instead of 1, the values() call doesn't take?

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; ins.values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; str(ins)
'INSERT INTO users (id, name, fullname) VALUES (:id, :name, :fullname)'


I know the values() call is truly not doing anything because the I tested the params value from a ins.compile().params call and they are all None in the version in 2 lines.

&gt;&gt;&gt; ins = users.insert().values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; ins.compile().params
{'fullname': 'Jack Jones', 'name': 'jack'}

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; ins.values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; ins.compile().params
{'fullname': None, 'password': None, 'id': None, 'name': None}

",1,197,"The difference between these two:

&gt;&gt;&gt; ins = users.insert().values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; ins.compile().params
{'fullname': 'Jack Jones', 'name': 'jack'}

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; ins.values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; ins.compile().params
{'fullname': None, 'password': None, 'id': None, 'name': None}


is that you are not saving the returned value of ins in the second case.
I haven't tested, but to make them equivalent, it should be something like:

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; ins = ins.values(name='jack', fullname='Jack Jones')


Otherwise, what you do with ins.values just vanishes
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/32149305,MySQL data not updating in web app - Python/Flask,"I'm building a Python/Flask Application using a MySQL backend. Many of the views have forms that submit data to a route, and then the route adds the data via db.session.commit(). I can confirm that the MySQL db updates with every commit, but then if I subsequently refresh the page multiple times, the data changes every time (i.e. most recently added items will disappear, reappear).

I've done a couple things to try to fix:

HTML Headers
I know it's not best to add a ton of headers and hope for the best, but I added these headers to fight caching:

&lt;meta http-equiv=""cache-control"" content=""max-age=0"" /&gt;
&lt;meta http-equiv=""cache-control"" content=""private, proxy-revalidate, s-maxage=0, no-cache, no-store, must-revalidate"" /&gt;
&lt;meta http-equiv=""expires"" content=""0"" /&gt;
&lt;meta http-equiv=""expires"" content=""Tue, 01 Jan 1980 1:00:00 GMT"" /&gt;
&lt;meta http-equiv=""pragma"" content=""no-cache"" /&gt;


Flask Headers
I added headers to the Flask app itself:

response.headers[""Cache-Control""] = ""no-cache, no-store, must-revalidate"" # HTTP 1.1.
response.headers[""Pragma""] = ""no-cache"" # HTTP 1.0.
response.headers[""Expires""] = ""0"" # Proxies.


Disable MySQL Caching

SET SESSION query_cache_type=OFF;


Apache2 Disable Caching

&lt;filesMatch ""\.(html|htm|js|css)$""&gt;
    FileETag None
    &lt;ifModule mod_headers.c&gt;
            Header unset ETag
            Header set Cache-Control ""max-age=0, no-cache, no-store, must-revalidate""
            Header set Pragma ""no-cache""
            Header set Expires ""Wed, 11 Jan 1984 05:00:00 GMT""
    &lt;/ifModule&gt;
&lt;/filesMatch&gt;


Some behaviors based on just prying around and facts:


Whenever I execute sudo service apache2 restart, following it everything works fine. Some other posts mentioned that there may be processes that are started that are causing this?
I'm using SQLAlchemy as my ORM, so at one point I thought that maybe it was SQLAlchemy caching information, so following every commit I tried db.session.close() so that the connection would be fresh every time I queried.


Can anyone give me a hand? Like many folks learning Flask, I'm a beginner at web application development.

Many thanks!

EDIT: 


When I run the application by just doing python init.py, it works perfectly fine.
I receive intermittent HTTP 500 errors when refreshing the page at a quick pace.


EDIT 2:
Here's a sample of a route that I'm POSTing to, following which I'm redirecting to itself and displaying the GET option to the user. I would expect that after committing data, the subsequent query should include it, but many times the web application doesn't show the most recently added data. On multiple refreshes, it'll sometimes show and sometimes disappear. Very strange:

@app.route('/projects', methods=['GET','POST'])
@login_required
def projects():
    user = return_current_user()
    if request.method == 'POST':
            if request.form['title'] and request.form['description']:
                    project = Projects(request.form['title'], request.form['description'])
                    if project.title != None and project.description != None:
                            user.projects.append(project)
                            db.session.commit()

            return redirect('/projects')
    elif request.method == 'GET':
            if 'clear-page' in request.args:
                    return redirect('/projects')
            elif 'query' in request.args:
                    projects = Projects.query.filter(Projects.title.like('%'+request.args['query']+'%')).order_by(Projects.timestamp.desc()).all()

            else:
                    projects = Projects.query.order_by(Projects.timestamp.desc()).all()

            return render_template('projects.html', title=""Projects"", user=user, projects=projects)

",1,2368,"I actually figured this out - a solve was to add db.session.commit() before the query. Not sure if it's the ""correct"" way to approach but it does the trick for my small-scale application!
","You might try switching off all the special changes you have made on the apparent assumption that some caching mechanism is responsible for the observed results, and show some of the code of your flask site. Then start to gather documented evidence of what is actually happening.
",
SQLAlchemy strange behavior,https://stackoverflow.com/questions/53164153,why should we set the local_infile=1 in sqlalchemy to load local file? Load file not allowed issue in sqlalchemy,"I am using sqlalchemy to connect to MySQL database and found a strange behavior.
If I query 

LOAD DATA LOCAL INFILE 
'C:\\\\Temp\\\\JaydenW\\\\iata_processing\\\\icer\\\\rename\\\\ICER_2017-10- 
12T09033
7Z023870.csv    


It pops an error:

sqlalchemy.exc.InternalError: (pymysql.err.InternalError) (1148, u'The used 
command is not allowed with this MySQL versi
on') [SQL: u""LOAD DATA LOCAL INFILE 
'C:\\\\Temp\\\\JaydenW\\\\iata_processing\\\\icer\\\\rename\\\\ICER_2017-10- 
12T090337Z023870.csv' INTO TABLE genie_etl.iata_icer_etl LINES TERMINATED BY 
'\\n' 
IGNORE 1 Lines   (rtxt);""] (Background on this error at: 
http://sqlalche.me/e/2j85)


And I find the reason is that:
I need to set the parameter as

args = ""mysql+pymysql://""+username+"":""+password+""@""+hostname+""/""+database+""? 
local_infile=1""


If I use MySQL official connection library. I do not need to do so.

myConnection = MySQLdb.connect(host=hostname, user=username, passwd=password, db=database)


Can anyone help me to understand the difference between the two mechanisms?
",1,1080,"The reason is that the mechanisms use different drivers.
In SQLAlchemy you appear to be using the pymysql engine, which uses the PyMySQL Connection class to create the DB connection. That one requires the user to explicitly pass the local_infile parameter if they want to use the LOAD DATA LOCAL command.

The other example uses MySQLdb, which is basically a wrapper around the MySQL C API (and to my knowledge not the official connection library; that would be MySQL Connector Python, which is also available on SQLAlchemy as mysqlconnector). This one apparently creates the connection in a way that the LOAD DATA LOCAL is enabled by default.
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/64855545,SQLalchemy rowcount always -1 for statements,"I was playing around with SQLalchemy and Microsoft SQL Server to get a hang of the functions when I came across a strange behavior. I was taught that the attribute rowcount on the result proxy object will tell how many rows were effected by executing a statement. However, when I select or insert single or multiple rows in my test database, I always get -1. How could this be and how can I fix this to reflect the reality?
connection = engine.connect()
metadata = MetaData()

# Ex1: select statement for all values
student = Table('student', metadata, autoload=True, autoload_with=engine)
stmt = select([student])
result_proxy = connection.execute(stmt)
results = result_proxy.fetchall()
print(result_proxy.rowcount)

# Ex2: inserting single values
stmt = insert(student).values(firstname='Severus', lastname='Snape')
result_proxy = connection.execute(stmt)
print(result_proxy.rowcout)
 
# Ex3: inserting multiple values 
stmt = insert(student)
values_list = [{'firstname': 'Rubius', 'lastname': 'Hagrid'},
               {'firstname': 'Minerva', 'lastname': 'McGonogall'}]
result_proxy = connection.execute(stmt, values_list)
print(result_proxy.rowcount)

The print function for each block seperately run example code prints -1. The Ex1 successfully fetches all rows and both insert statements successfully write the data to the database.
According to the following issue, the rowcount attribute isn't always to be trusted. Is that true here as well? And when, how can I compensate with a Count statement in a SQLalcehmy transaction?
PDO::rowCount() returning -1
",1,1388,"The single-row INSERT  VALUES (  ) is trivial: If the statement succeeds then one row was affected, and if it fails (throws an error) then zero rows were affected.
For a multi-row INSERT simply perform it inside a transaction and rollback if an error occurs. Then the number of rows affected will either be zero or len(values_list).
To get the number of rows that a SELECT will return, wrap the select query in a SELECT count(*) query and run that first, for example:
select_stmt = sa.select([Parent])
count_stmt = sa.select([sa.func.count(sa.text(""*""))]).select_from(
    select_stmt.alias(""s"")
)
with engine.connect() as conn:
    conn.execution_options(isolation_level=""SERIALIZABLE"")
    rows_found = conn.execute(count_stmt).scalar()
    print(f""{rows_found} row(s) found"")
    results = conn.execute(select_stmt).fetchall()
for item in results:
    print(item.id)

",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/65100066,SQLAlchemy loads unrelated cached objects when using contains_eager,"I'm using the contains_eager functionality of SQLAlchemy and I'm seeing strange behavior when objects are already loaded in an existing session. Specifically, it seems that those objects are not filtered out of the relationship collection as they would be when loading data fresh.
Here is a minimal example. The steps are

Create a parent-child relationship.
Add a parent and two children with different values.
Perform a joined, filtered query, using contains_eager to load matching children for the parent. Note that the filter should exclude one of the two children.
Observe that both children have been populated on the children property of the resulting object.
The correct results can be obtained by using a new session, or even by calling session.expire_all(), which indicates that the issue is that the children already exist in the current session.

Is this the expected behavior? And if so, is calling expire_all the right thing to do to avoid this?
More generally, should contains_eager be avoided because of this? It seems like a break in the abstraction if one has to keep track of whether or not a child object already exists before issuing a query. But maybe I am missing something.
from sqlalchemy import and_, Column, create_engine, DateTime, ForeignKey, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import contains_eager, relationship, sessionmaker


create_statements = [""""""
    DROP TABLE IF EXISTS child;
    """""", """"""
    DROP TABLE IF EXISTS parent;
    """""", """"""
    CREATE TABLE parent
    (
        id INTEGER NOT NULL PRIMARY KEY,
        name VARCHAR
    );
    """""", """"""
    CREATE TABLE child
    (
        id INTEGER NOT NULL PRIMARY KEY,
        parent_id INTEGER REFERENCES parent(id),
        value INTEGER
    );
    """"""
]

Base = declarative_base()


class Parent(Base):
    __tablename__ = ""parent""
    __table_args__ = {'implicit_returning': False}
    id = Column(Integer, primary_key=True)
    name = Column(String)

    children = relationship(""Child"", back_populates=""parent"")


class Child(Base):
    __tablename__ = ""child""
    __table_args__ = {'implicit_returning': False}
    id = Column(Integer, primary_key=True)
    parent_id = Column(Integer, ForeignKey(Parent.id))
    value = Column(Integer)

    parent = relationship(Parent, back_populates=""children"")


if __name__ == ""__main__"":
    engine = create_engine(f""sqlite:///"")
    session = sessionmaker(bind=engine)()

    for statement in create_statements:
        session.execute(statement)

    p1 = Parent(id=1, name=""A"")
    c1 = Child(id=1, parent=p1, value=10)
    c2 = Child(id=2, parent=p1, value=20)
    session.add_all([p1, c1, c2])
    session.flush()
    # session.expire_all()  # Uncommenting this makes the below work as expected.

    results = session \
        .query(Parent) \
        .join(Child, Parent.id == Child.parent_id) \
        .options(
            contains_eager(Parent.children)
        ).filter(Child.value &lt; 15) \
        .order_by(Parent.id) \
        .all()

    print(len(results[0].children))  # We should only have 1 child.
    print(all(c.value &lt; 15 for c in results[0].children))  # All children should match the above filter condition.

",1,551,"I asked this question on the SQLAlchemy GitHub page. The solution is to use populate_existing on any query that uses contains_eager and filter. In my specific example, this query does the right thing
session \
        .query(Parent) \
        .join(Child, Parent.id == Child.parent_id) \
        .options(
            contains_eager(Parent.children)
        ).filter(Child.value &lt; 15) \
        .order_by(Parent.id) \
        .populate_existing() \
        .all()

",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/71445447,"How can I access the joined results of lazy=joined, without executing a second SQL statement, or changing the parent query","Sqlalchemy lazy=joined performs a join for a simple parent query, but does not seem to remember what was joined.
I have this simple one-to-one relationship defined:
class User(Base):
    __tablename__ = 'user'

    email = db.Column(db.Unicode(255), nullable=False, server_default=u'', unique=True)
    password = db.Column(db.String(255), nullable=False, server_default='')
    ...
    userprofile = db.relationship(""Userprofile"",  
                                    uselist=False,
                                    backref=db.backref('userprofile', lazy='joined', innerjoin=True), 
                                    passive_deletes=True)   


class Userprofile(Base):
    __tablename__ = 'userprofile'
    user_id = db.Column(db.Integer, db.ForeignKey('user.id', ondelete='CASCADE'))
    first_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')
    last_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')
    ...
    user = db.relationship(""User"", uselist=False, backref=db.backref('user', lazy='joined', innerjoin=True))

I know the userprofile relationship may have too many options specified but I have tried everything I can think of.
As part of a 3rd party user management package, this query is executed for each web page, in order to get the user making the request, resulting in this SQL:
user = User.query.filter(User.id == user_id).one()

sqlalchemy.engine.Engine - INFO - SELECT  user.id AS user_id, user.email AS user_email, user.password AS user_password, userprofile_1.id AS userprofile_1_id, userprofile_1.user_id AS userprofile_1_user_id, userprofile_1.first_name AS userprofile_1_first_name, userprofile_1.last_name AS userprofile_1_last_name ...
FROM user JOIN userprofile AS userprofile_1 ON user.id = userprofile_1.user_id 
WHERE user.id = ?


Then, in the same view, when I want to access a field from user profile, this SQL is executed:
user.userprofile.first_name
  
sqlalchemy.engine.Engine - INFO - SELECT userprofile.id AS userprofile_id,  userprofile.user_id AS userprofile_user_id, userprofile.first_name AS userprofile_first_name, userprofile.last_name AS userprofile_last_name  ...
FROM userprofile 
WHERE ? = userprofile.user_id


Which to me is very strange.  The first query has the userprofile fields already, so why the second SQL statement?
I cant really change the user query to add something like contains_eager to the query, so that approach is not an option.  Also, sqlalchemy complains if I try to use lazy='dynamic' for a one-to-one relationship.
I have 2 questions then:

what can I do to the table definitions, if anything, to eliminate the second SQL statement?  Again, changing the User query is not an option.

Any idea why contains_eager is not the default behavior for lazy=joined?  It seems like that should be the default.  Or is there an option in the db.relationsip function to request eager loading?


",1,1362,"Gord Thompson solved it for me.
Since I posted the original code, I thought it might be helpful to post the modified code that worked.
class User(Base, UserMixin):
    __tablename__ = 'user'

    email = db.Column(db.Unicode(255), nullable=False, server_default=u'', unique=True)

    userprofile = db.relationship(
        ""Userprofile"",
        back_populates=""user"",
        uselist=False,
        lazy=""joined"",
        innerjoin=True,
        passive_deletes=True)   

class Userprofile(Base):
    __tablename__ = 'userprofile'
    user_id = db.Column(db.Integer, db.ForeignKey('user.id', ondelete='CASCADE'))
    first_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')
    last_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')

user = db.relationship(""User"", back_populates=""userprofile"", uselist=False)



",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/19970809,SQLAlchemy: Unable to get the first item of query,"I'm currently learning SQLAlchemy, and i found this strange thing. I was experimenting with a table which stores a person's name and address, and to get them i use this:

session.query(User)


And to get the first item, i tried:

session.query(User).first()


Which throws a DatabaseError:

Traceback (most recent call last):
  File ""&lt;pyshell#4&gt;"", line 1, in &lt;module&gt;
    session.query(User).first()
  File ""build\bdist.win32\egg\sqlalchemy\orm\query.py"", line 2275, in first
    ret = list(self[0:1])
  File ""build\bdist.win32\egg\sqlalchemy\orm\query.py"", line 2142, in __getitem__
    return list(res)
  File ""build\bdist.win32\egg\sqlalchemy\orm\query.py"", line 2346, in __iter__
    return self._execute_and_instances(context)
  File ""build\bdist.win32\egg\sqlalchemy\orm\query.py"", line 2361, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 664, in execute
    return meth(self, multiparams, params)
  File ""build\bdist.win32\egg\sqlalchemy\sql\elements.py"", line 272, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 761, in _execute_clauseelement
    compiled_sql, distilled_params
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 874, in _execute_context
    context)
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 1023, in _handle_dbapi_exception
    exc_info
  File ""build\bdist.win32\egg\sqlalchemy\util\compat.py"", line 185, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb)
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 867, in _execute_context
    context)
  File ""build\bdist.win32\egg\sqlalchemy\engine\default.py"", line 376, in do_execute
    cursor.execute(statement, parameters)
DatabaseError: (DatabaseError) ORA-01036: illegal variable name/number
 'SELECT test_user_uid, test_user_name, test_user_address \nFROM (SELECT test_user.""uid"" AS test_user_uid, test_user.name AS test_user_name, test_user.address AS test_user_address \nFROM test_user) \nWHERE ROWNUM &lt;= :ROWNUM_1' {'ROWNUM_1': 1}


However, i was able to retrieve what i wanted if i select all the rows, and loop through the query object:

users = [user for user in session.query(User)]
user1 = users[0]


That's all, i thought it's strange. Here's my mapping class:

class User(Base):
    __tablename__ = 'test_user'

    uid = Column(Integer, primary_key = True)
    name = Column(String(50))
    address = Column(String(100))

    def __repr__(self):
        return ""&lt;User (%s, %s)""%(self.name, self.address)


My best guess is that Session.query().first() is looking for the first row, with the generated query. However, the working method retrieves all the rows, and select the first one in Python. The problem is clearly from the generated query (invalid query). The main question is, what caused SQLAlchemy to create an invalid query?

Also, i noticed that SQLAlchemy makes things more difficult by making a query with sub-query. Is that behavior intended?

I hope i can get a satisfying answer, thanks!
",0,708,"Well, it didn't take me long to realize this. It turns out that it's a version problem, i was previously using cx_Oracle version 5.0.2 10g, i tried to upgrade it to version 5.1.2 10g, and things works fine. 

This is probably an undocumented bug in SQLAlchemy, i can't find a place where they mention it. 

Conclusion: If you want to use the latest version of SQLAlchemy (0.9.0b1) with Oracle 10g, you shouldn't use cx_Oracle older than version 5.1.2 10g.

Hope this helps, and thanks for reading the question!
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/51361689,FLASK After commit causes odd IntegrityError violates unique constraint,"UPDATE / CLARIFICATION

I confirmed that this strange behavior only occurs on the macOS machine, moving everything to a windows machine (using sqlite and doing a fresh init and migrate) doesn't cause the error... doing the same on my High Sierra box does cause the odd error. 

Is anyone familiar with some known difference between sqlalchemy on Windows and macOS that might help?



Short version... I'm getting an integrity error (unique constraint) after I try to commit ANY entry to the DB, even if there are NO EXISTING entries at all in the table... why?

DETAILS

I've built a FLASK project (roughly based on the Miguel Grinberg Flask Maga Tutorial) using postgresql and sqlalchemy, the front-end has a page to register a user with a confirmation email (which works fine)...  to save time I've written a route (see below) which pre-loads a confirmed user to the Users database, this user is the ONLY user in the Users table and I only visit the route ONE TIME. 

After a successful commit I get an IntegrityError ""duplicate key value violates unique constraint"". This route only adds ONE user to an existing EMPTY Users table. The data IS successfully saved to the DB, the user can log in, but an error gets thrown. I get a similar error (see below) but am focusing on this route as an example because it is shorter than other views I've written.

EXAMPLE OF ROUTE CAUSING UNIQUE CONSTRAINT ERROR 

@main.route('/popme')
#@login_required
def popme():
    ## add user
    u1 = User()
    u1.email = 'user@domain.com'
    u1.username = 'someuser'
    u1.password_hash = 'REMOVED'
    u1.confirmed = '1'
    u1.role_id = 3
    u1.name = 'Some User'
    db.session.add(u1)
    db.session.commit()
    flash('User someuser can now login!')
    return redirect(url_for('main.index'))


I only started getting this error after moving the entire project from a Windows machine to a MacOS machine. I'm running Python 3.6 in a virtual environment, this error occurs if I'm using sqlite3 or postgresql.

I've written a much longer route which pre-fills in about 20 other tables successfully (does on commit() at the end, all data IS stored in the DB), however I get an IntegrityError ""duplicate key value violates unique constraint"" every time for a seemingly random entry. I've destroyed the DB, done an init, migrated... each time when the commit() is called a IntegrityError is thrown, each time on a different table, there is no apparent reasoning. 

BELOW IS USER MODEL

class User(UserMixin, db.Model):
    __tablename__ = 'users'
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(64), unique=True, index=True)
    username = db.Column(db.String(64), unique=True, index=True)
    password_hash = db.Column(db.String(128))
    confirmed = db.Column(db.Boolean, default=False)
    role_id = db.Column(db.Integer, db.ForeignKey('roles.id'))
    name = db.Column(db.String(64))
    last_seen = db.Column(db.DateTime(), default=datetime.utcnow)

    def ping(self):
        self.last_seen = datetime.utcnow()
        db.session.add(self)

    @property
    def password(self):
        raise AttributeError('password is not a readable attribute')

    @password.setter
    def password(self, password):
        self.password_hash = generate_password_hash(password)

    def verify_password(self, password):
        return check_password_hash(self.password_hash, password)

    def generate_confirmation_token(self, expiration=3600):
        s = Serializer(current_app.config['SECRET_KEY'], expiration)
        return s.dumps({'confirm': self.id})

    def confirm(self, token):
        s = Serializer(current_app.config['SECRET_KEY'])
        try:
            data = s.loads(token)
        except:
            return False
        if data.get('confirm') != self.id:
            return False
        self.confirmed = True
        db.session.add(self)
        return True

    def generate_reset_token(self, expiration=3600):
        s = Serializer(current_app.config['SECRET_KEY'], expiration)
        return s.dumps({'reset': self.id})

    def reset_password(self, token, new_password):
        s = Serializer(current_app.config['SECRET_KEY'])
        try:
            data = s.loads(token)
        except:
            return False
        if data.get('reset') != self.id:
            return False
        self.password = new_password
        db.session.add(self)
        return True

    def generate_email_change_token(self, new_email, expiration=3600):
        s = Serializer(current_app.config['SECRET_KEY'], expiration)
        return s.dumps({'change_email': self.id, 'new_email': new_email})

    def change_email(self, token):
        s = Serializer(current_app.config['SECRET_KEY'])
        try:
            data = s.loads(token)
        except:
            return False
        if data.get('change_email') != self.id:
            return False
        new_email = data.get('new_email')
        if new_email is None:
            return False
        if self.query.filter_by(email=new_email).first() is not None:
            return False
        self.email = new_email
        db.session.add(self)
        return True

    def can(self, permissions):
        return self.role is not None and (self.role.permissions &amp; permissions) == permissions

    def is_administrator(self):
        return self.can(Permission.ADMINISTRATOR)

    def __init__(self, **kwargs):
        super(User, self).__init__(**kwargs)
        if self.role is None:
            if self.email == current_app.config['FLASKY_ADMIN']:
                self.role = Role.query.filter_by(permissions=0xff).first()
            if self.role is None:
                self.role = Role.query.filter_by(default=True).first()

    def __repr__(self):
        return '&lt;User %r&gt;' % self.username


I've tried Sql-alchemy Integrity error but its my understanding that sqlalchemy does auto-increment primary keys. 

UPDATED INTEGRITY ERROR

sqlalchemy.exc.IntegrityError: (psycopg2.IntegrityError) duplicate key value violates unique constraint ""ix_users_email""
DETAIL:  Key (email)=(worldbmd@gmail.com) already exists.
 [SQL: 'INSERT INTO users (email, username, password_hash, confirmed, role_id, name, last_seen) VALUES (%(email)s, %(username)s, %(password_hash)s, %(confirmed)s, %(role_id)s, %(name)s, %(last_seen)s) RETURNING users.id'] [parameters: {'email': 'user@domain.com', 'username': 'someuser', 'password_hash': 'REMOVED', 'confirmed': '1', 'role_id': 1, 'name': 'Some User', 'last_seen': datetime.datetime(2018, 7, 16, 17, 27, 13, 451593)}]

",0,867,"I also got into the same problem..as Joost said flask app runs twice. So we need to set it to run only once. We can achieve it by adding use_reloader=False like:
if __name__ = ""__main__"":
    app.run(debug=True, use_reloader=False)

or
we can directly set debug=False,
if __name__ = ""__main__"":
    app.run(debug=False)

","The integrity error is caused by trying to add a blister to the blisters table with a non unique property. I think your model looks something like this:

class Blister(db.Model):
    __tablename__ = 'blisters'
    id = db.Column(db.Integer, primary_key=True)
    name= db.Column(db.String(64), unique=True)
    notes= db.Column(db.String(64))
    cost= db.Column(db.Float)


And you're trying to add a blister with a name Small round dome which is already in the blisters table in the database, therefore causing the Integrity Error.
",
SQLAlchemy strange behavior,https://stackoverflow.com/questions/61943730,Fields of SQLAlchemy models filled after creating new object,"I am creating a Python application using the Flask framework with SQLAlchemy and now struggling with a strange behavior on creating new model objects.

Lets say I have a model Offer which has some fields, a private property __validation_errors and a public method validate:

from app import db

class Offer(db.Model):
    currency = db.Column(db.String(255))
    price = db.Column(db.Numeric(20, 5))
    __validation_errors = []

    def validate(self, errors: List[Exception]) -&gt; bool:
        self.validate_not_empty()
        self.validate_price()
        for err in self.__validation_errors:
            errors.append(err)
        if len(self.__validation_errors) &gt; 0:
            return False
        return True


the object itself is created in a service class by calling

offer = Offer(currency, price)


and providing data received with a POST request in a router.
If the validation fails, an error message is returned to the user and the object is not written into DB.

And here begins the strange part. If a request with invalid data is received and another request with correct data is sent, the values in the __validation_errors list don't disappear in the offer object, like if the object is reused and not reset. If another invalid request is made, the errors are just appended to the already existing in the list. Of course, it can be fixed by setting __validation_errors = [] every time the validate() method is called, but I would like to understand what is happening here. Do I miss some SQLAlchemy specific features? The app.db object and the Session are created on app and exist all the time.
",0,495,"So the trouble here is that __validation_errors is bound to the class Offer, rather than an instance of Offer, which means that unless each instance sets its own self.__validation_errors, self.__validation_errors just maps to Offer.__validation_errors for every instance (kind of like accessing an attribute/function of a subclass that only exists on the parent class).

Take for example:

&gt;&gt;&gt; class SomeClass:
...     some_list = []
... 
&gt;&gt;&gt; a = SomeClass()
&gt;&gt;&gt; b = SomeClass()
&gt;&gt;&gt; a.some_list.append(5)
&gt;&gt;&gt; a.some_list
[5]
&gt;&gt;&gt; b.some_list
[5]
&gt;&gt;&gt; a.some_list is b.some_list
True


This is because anything in the class body is an attribute of the class itself (this makes a little more sense considering the fact that methods are also class attributes - unlike in prototype-based languages), rather than a value each instance is assigned after it is created

&gt;&gt;&gt; class SomeClass:
...     x = [1, 2, 3, 4, 5]
...
&gt;&gt;&gt; SomeClass.x
[1, 2, 3, 4, 5]
&gt;&gt;&gt; SomeClass().x
[1, 2, 3, 4, 5]
&gt;&gt;&gt; SomeClass.x is SomeClass().x
True


If you want to bind the attribute to the instance rather than the class, you attach it to self, preferably in __init__

class SomeClass:
    def __init__(self):
        self.x = 5




PS, it's worth noting that this is only really noticeable when you're working with mutable attributes. With immutable attributes, any ""change"" will create a new value on the object, and since &lt;instance&gt;.&lt;attribute&gt; is retrieved instead of &lt;instance&gt;.__class__.&lt;attribute&gt; whenever it exists, it will appear that the instance was separate from the class to begin with.



So, back to your example, I'd suggest initializing __validation_errors at the same time you __init__ialize the rest of the object:

class Offer(db.Model):
    ...
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.__validation_errors = []

    def validate(self, errors: List[Exception]) -&gt; bool:
        ...

",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/64770459,"In Python/Flask, why would &quot;from models import Result&quot; cause a circular import error, while &quot;from models import *&quot; and &quot;import models&quot; both work?","I'm working through the Flask By Example tutorial, and I'm running into a circular import error when setting up the database through SQLAlchemy. When other people run into this problem, it seems to be because of a misplaced import statement in app.py. In my case, the error seems to depend on the way I'm importing the database models instead, which I don't understand. import models and from models import * both work, but from models import Result fails with the following error message:
Traceback (most recent call last):
  File ""app.py"", line 13, in &lt;module&gt;
    from models import Result
  File ""/mnt/c/users/power/desktop/projects/fbe/models.py"", line 1, in &lt;module&gt;
    from app import db
  File ""/mnt/c/users/power/desktop/projects/fbe/app.py"", line 13, in &lt;module&gt;
    from models import Result
ImportError: cannot import name 'Result' from partially initialized module 'models' (most likely due to a circular import) (/mnt/c/users/power/desktop/projects/fbe/models.py)

Any ideas why this might be happening? It seems like there may be some nuances to how Python does imports that I'm unaware of. It's also strange that my code is pretty much identical to the provided code, and and yet I still get an error. Could the behavior with newer versions of Python have changed? I highly doubt it, but for what it's worth I'm using 3.8.5.
My code is below. All files live in the main app directory.
app.py
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate

app = Flask(__name__)
app.config.from_pyfile('config.py')
app.config.from_pyfile('instance/prod-config.py', silent=True)
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

db = SQLAlchemy(app)
migrate = Migrate(app, db)

### this works ###
import models

### this works too ###
# from models import *

### this fails with the above error ###
# from models import Result

@app.route('/')
def hello():
    return 'Hello world!'

@app.route('/&lt;name&gt;')
def hello_personalized(name):
    return ('Hello ' + name)

if __name__ == '__main__':
    app.run()

models.py
from app import db
from sqlalchemy.dialects.postgresql import JSON

class Result(db.Model):
    __tablename__ = 'results'

    id = db.Column(db.Integer, primary_key=True)
    url = db.Column(db.String())
    result_all = db.Column(JSON)
    result_no_stop_words = db.Column(JSON)

    def __init__(self, url, result_all, result_no_stop_words):
        self.url = url
        self.result_all = result_all
        self.result_no_stop_words = result_no_stop_words

    def __repr__(self):
        return '&lt;id {}&gt;'.format(self.id)

config.py
DEBUG = True
TESTING = True
CSRF_ENABLED = True
SECRET_KEY = 'needs-to-be-changed'
SQLALCHEMY_DATABASE_URI = 'postgresql://username:password@localhost/fbe'

",0,626,"Tbh, neither of those examples should work. You clearly have a circular import between those files: from app import db and from models import Result.
My guess here is that when you're using import models or even from models import *, you're importing from another package called models that exists on your python environment.
My solution would be to use separation of concerns and remove db initialization from your Flask view declarations. You can take a look on one of the many possible solutions here.
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/65742509,How to get list of objects from multi-value field with SqlAlchemy using ORM?,"I have MS Access DB file (.accdb) from my client and need to describe tables and columns with declarative_base class. As I can see in table constructor - one of column has Integer value and has relationship ""one-to-many"" with another column in some another table (foreign key).
But actually in this foreign key stored not single Integer value, but string with number values separated with semicolons. This technique called as ""multi-value fields"". In fact this is ""many-to-many"" relationship without associative tables.
Very simplified scheme:
Persons
-------------
id - Integer
name - String
vacancy_id - Integer (multi-value, Foreign key of Vacancies.id)

Vacancies
-------------
id - Integer
vacancy_name - String

I tried to map classes to tables using declarative_base parent class. But can't find how to declare ""many-to-many"" relationship without associative table. Now I have such code.
Base = declarative_base()


class Vacancy(Base):
    __tablename__ = 'Vacancies'
    id = sa.Column(sa.Integer, name='id', primary_key=True, autoincrement=True)
    vacancy_name = sa.Column(sa.String(255), name='vacancy_name')


class Person(Base):
    __tablename__ = 'Persons'
    id = sa.Column(sa.Integer, name='id', primary_key=True, autoincrement=True)
    name = sa.Column(sa.String(255), name='name')
    vacancy_id = sa.Column(sa.Integer, ForeignKey(Vacancy.id), name='vacancy_id')
    vacancies = relationship(Vacancy)

During request Person I have strange behavior:

If vacancy_id not specified, I get Person.vacancies as None.
If vacancy_id specified as single value (i.e. ""1""), in Person.vacancies I get single object of Vacancy class.
If vacancy_id specified as multiple value (i.e. ""1;2;3""), in Person.vacancies I also get None.

Of course I can request raw Person.vacancy_id, split it with semicolon, and make request to get Vacancies with list of ID's.
But I wonder - if SqlAlchemy can process ""multi-value fields""? And what the best way to work with such fileds?
UPDATE
At present I made following workaround to automatically parse multi-value columns. This should be added to Persons class:
@orm.reconstructor
def load_on_init(self):
    if self.vacancy_id:
        ids = self.vacancy_id.split(';')
        self.vacancies = [x for x in Vacancy.query.filter(Vacancy.id.in_(ids)).all()]
    else:
        self.vacancies = []

Vacancies class should have fllowing attribute:
query = DBSession.query_property()

Finally we have to prepare session for in-class usage:
engine = create_engine(CONNECTION_URI)
DBSession = scoped_session(sessionmaker(bind=engine))
Base = declarative_base()

",0,519,"Access ODBC provides very limited support for multi-value lookup fields. Such fields are actually implemented using a hidden association table (with a name like f_1BC9E55B5578456EB5ACABC99BB2FF0B_vacancies) but those tables are not accessible from SQL statements:
SELECT * from f_1BC9E55B5578456EB5ACABC99BB2FF0B_vacancies

results in the error

The Microsoft Access database engine cannot find the input table or query ''. Make sure it exists and that its name is spelled correctly.

As you have discovered, Access ODBC will read the key values of the multiple entries and present them as a semicolon-separated list that we can parse, but we cannot update those values
UPDATE Persons SET vacancies = '1;2' WHERE id = 1

fails with

An UPDATE or DELETE query cannot contain a multi-valued field. (-3209)

So, TL;DR, if you only need to read from the database then your workaround may be sufficient, but if you need to modify those multi-valued fields then Access ODBC is not going to get the job done for you.
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/71630879,Strange bevaiour of session.add in sqhalchemy (duplicate key value violates unique constraint in session.add)),"I encountered strange behaviour of SQLAlchemy
I tried to add a new object to the DB. This is done in the function add_tag()
class News(Base):
    __tablename__ = ""news""

    id = Column(Integer, primary_key=True)

    title = Column(String)
    description = Column(String)
    
    def add_tag(self, tag_name, session):
        logging.debug(""adding news tags   "" + tag_name)
        tag = session.query(Tag).filter_by(name=tag_name).first()
        logging.debug(tag)
        if tag:
            nt = NewsTags()
            nt.tag_id = tag.id
           self.tags.append(nt)
        else:
            new_tag = Tag()
            logging.debug('Creating a new tag:  ' + tag_name)
            new_tag.name = tag_name
            session.add(new_tag)
            session.commit()
            nt = NewsTags()
            nt.tag_id = new_tag.id
            self.tags.append(nt)


class NewsTags(Base):
    __tablename__ = ""news_tags""

    news_id = Column(ForeignKey(""news.id""), primary_key=True)
    tag_id = Column(ForeignKey(""tag.id""), primary_key=True)
    news = relationship(""News"", backref=""tags"")



class Tag(Base):

    __tablename__ = ""tag""
    id = Column(Integer, primary_key=True)
    name = Column(String(30))

    news = relationship(NewsTags, backref=""tag"", post_update=True)
 



I first check if the object already exists. If it does not, I try to create a new one and add it to the session (add it to the DB).
I already had some objects in the table (create by other means).
See a copy paste of values in table tag:
id, Name
1   Sports
2   Business
3   Finance
4   World
5   US
6   UK
7   Technology
8   Science
9   Health
10  Video Games
11  IT
12  Startups
13  Europe
14  Apps
15  Space

I think I might have added some the data to the DB manually (via SQL insert), but some of it was created through SQLAlchemy.
I came across very strange behavior when trying to add a new object.
First it tried to create an object with id=1 and failed as it was a duplicate. I did not save the stack trace, but I ran it again, it tried to add the object with id=2, here's the stack trace.
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint ""tag_pkey""
DETAIL:  Key (id)=(2) already exists.

Full stack trace:
DEBUG:root:adding news tags   AI
2022-03-28 06:52:33,188 INFO sqlalchemy.engine.Engine SELECT tag.id AS tag_id, tag.name AS tag_name 
FROM tag 
WHERE tag.name = %(name_1)s 
 LIMIT %(param_1)s
INFO:sqlalchemy.engine.Engine:SELECT tag.id AS tag_id, tag.name AS tag_name 
FROM tag 
WHERE tag.name = %(name_1)s 
 LIMIT %(param_1)s
2022-03-28 06:52:33,188 INFO sqlalchemy.engine.Engine [cached since 0.005467s ago] {'name_1': 'AI', 'param_1': 1}
INFO:sqlalchemy.engine.Engine:[cached since 0.005467s ago] {'name_1': 'AI', 'param_1': 1}
DEBUG:root:None
DEBUG:root:Creating a new tag:  AI
2022-03-28 06:52:33,191 INFO sqlalchemy.engine.Engine INSERT INTO tag (name) VALUES (%(name)s) RETURNING tag.id
INFO:sqlalchemy.engine.Engine:INSERT INTO tag (name) VALUES (%(name)s) RETURNING tag.id
2022-03-28 06:52:33,191 INFO sqlalchemy.engine.Engine [generated in 0.00015s] {'name': 'AI'}
INFO:sqlalchemy.engine.Engine:[generated in 0.00015s] {'name': 'AI'}
2022-03-28 06:52:33,192 INFO sqlalchemy.engine.Engine ROLLBACK
INFO:sqlalchemy.engine.Engine:ROLLBACK
Traceback (most recent call last):
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1808, in _execute_context
    self.dialect.do_execute(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/default.py"", line 732, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint ""tag_pkey""
DETAIL:  Key (id)=(2) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/app/assign.py"", line 59, in &lt;module&gt;
    main()
  File ""/app/assign.py"", line 46, in main
    parse_feeds(session)
  File ""/app/models/VitalNewsFeed.py"", line 77, in parse_feeds
    feed.parse_feed(session)
  File ""/app/models/VitalNewsFeed.py"", line 63, in parse_feed
    new_news = News(entry=e, source=self.source, tags=self.tags, session=session)
  File ""&lt;string&gt;"", line 4, in __init__
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/state.py"", line 480, in _initialize_instance
    manager.dispatch.init_failure(self, args, kwargs)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
    raise exception
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/state.py"", line 477, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File ""/app/models/VitalNews.py"", line 178, in __init__
    self.add_tag(tag, session)
  File ""/app/models/VitalNews.py"", line 121, in add_tag
    session.commit()
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 1431, in commit
    self._transaction.commit(_to_root=self.future)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 829, in commit
    self._prepare_impl()
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 808, in _prepare_impl
    self.session.flush()
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 3363, in flush
    self._flush(objects)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 3503, in _flush
    transaction.rollback(_capture_exception=True)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
    raise exception
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 3463, in _flush
    flush_context.execute()
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/persistence.py"", line 244, in save_obj
    _emit_insert_statements(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/persistence.py"", line 1237, in _emit_insert_statements
    result = connection._execute_20(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1620, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/sql/elements.py"", line 325, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1487, in _execute_clauseelement
    ret = self._execute_context(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1851, in _execute_context
    self._handle_dbapi_exception(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 2032, in _handle_dbapi_exception
    util.raise_(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
    raise exception
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1808, in _execute_context
    self.dialect.do_execute(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/default.py"", line 732, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""tag_pkey""
DETAIL:  Key (id)=(2) already exists.

[SQL: INSERT INTO tag (name) VALUES (%(name)s) RETURNING tag.id]
[parameters: {'name': 'AI'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)

I tried running the script again. Now it tired to create with id=3
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""tag_pkey""
DETAIL:  Key (id)=(3) already exists.

[SQL: INSERT INTO tag (name) VALUES (%(name)s) RETURNING tag.id]
[parameters: {'name': 'AI'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)

id=4
and so on. Every time it failed and I had to run the script again. As if it did not know the last value of id in the table of Tags. After a few fails it created with id=9, which was not a duplicate and the script continued working fine. But the error was strange. Shouldn't it check if primary key is available before creating a new object?
Now the error does not reproduce on dev. But when I pushed my code to prod the same thing happens again.
I am using PostgreSQL with SQLAlchemy.
",0,1051,"This was not SQLAlchemy's fault. It was PostgresSQL and me inserting into the table with a PostgreSQL client Postico without auto-increment which broke the sequence.
Running SQL command of ""SELECT setval('tag_id_seq', (SELECT MAX(id) FROM tag), true);"" fixed it.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/25768428,SQLAlchemy connection errors,"I'm experiencing some strange bugs which seem to be caused by connections used by Sqlalchemy, which i can't pin down exactly.. i was hoping someone has a clue whats going on here.

We're working on a Pyramid (version 1.5b1) and use Sqlalchemy (version 0.9.6) for all our database connectivity. Sometimes we get errors related to the db connection or session, most of the time this would be a cursor already closed or This Connection is closed error, but we get other related exceptions too:

(OperationalError) connection pointer is NULL
(InterfaceError) cursor already closed
Parent instance &lt;...&gt; is not bound to a Session, and no contextual session is established; lazy load operation of attribute '...' cannot proceed

A conflicting state is already present in the identity map for key (&lt;class '...'&gt;, (1001L,))
This Connection is closed (original cause: ResourceClosedError: This Connection is closed)
(InterfaceError) cursor already closed
Parent instance &lt;...&gt; is not bound to a Session; lazy load operation of attribute '...' cannot proceed
Parent instance &lt;...&gt; is not bound to a Session, and no contextual session is established; lazy load operation of attribute '...' cannot proceed
'NoneType' object has no attribute 'twophase'
(OperationalError) connection pointer is NULL
This session is in 'prepared' state; no further


There is no silver bullet to reproduce them, only by refreshing many times they are bound to happen one at some point. So i made a script using multi-mechanize to spam different urls concurrently and see where and when it happens.

It appears the url triggered doesn't really matter, the errors happen when there are concurrent requests that span a longer time (and other requests get served in between). This seems to indicate there is some kind of threading problem; that either the session or connection is shared among different threads.

After googling for these issues I found a lot of topics, most of them tell to use scoped sessions, but the thing is we do use them already:

db_session = scoped_session(sessionmaker(extension=ZopeTransactionExtension(), autocommit=False, autoflush=False))
db_meta = MetaData()



We have a BaseModel for all our orm objects:

BaseModel = declarative_base(cls=BaseModelObj, metaclass=BaseMeta, metadata=db_meta)
We use the pyramid_tm tween to handle transactions during the request
We hook db_session.remove() to the pyramid NewResponse event (which is fired after everything has run). I also tried putting it in a seperate tween running after pyramid_tm or even not doing it at all, none of these seem to have effect, so the response event seemed like the most clean place to put it.
We create the engine in our main entrypoint of our pyramid project and use a NullPool and leave connection pooling to pgbouncer. We also configure the session and the bind for our BaseModel here:

engine = engine_from_config(config.registry.settings, 'sqlalchemy.', poolclass=NullPool)
db_session.configure(bind=engine, query_cls=FilterQuery)
BaseModel.metadata.bind = engine
config.add_subscriber(cleanup_db_session, NewResponse)
return config.make_wsgi_app()
In our app we access all db operation using:

from project.db import db_session
...
db_session.query(MyModel).filter(...)
db_session.execute(...)
We use psycopg2==2.5.2 to handle the connection to postgres with pgbouncer in between
I made sure no references to db_session or connections are saved anywhere (which could result in other threads reusing them)


I also tried the spamming test using different webservers, using waitress and cogen i got the errors very easily, using wsgiref we unsurprisingly have no errors (which is singlethreaded). Using uwsgi and gunicorn (4 workers, gevent) i didn't get any errors.

Given the differences in the webserver used, I thought it either has to do with some webservers handling requests in threads and some using new processes (maybe a forking problem)? To complicate matters even more, when time went on and i did some new tests, the problem had gone away in waitress but now happened with gunicorn (when using gevent)! I have no clue on how to go debugging this...

Finally, to test what happens to the connection, i attached an attribute to the connection at the start of the cursor execute and tried to read the attribute out at the end of the execute:

@event.listens_for(Engine, ""before_cursor_execute"")
def _before_cursor_execute(conn, cursor, stmt, params, context, execmany):
  conn.pdtb_start_timer = time.time()

@event.listens_for(Engine, ""after_cursor_execute"")
def _after_cursor_execute(conn, cursor, stmt, params, context, execmany):
  print conn.pdtb_start_timer


Surprisingly this sometimes raised an exception: 'Connection' object has no attribute 'pdtb_start_timer'

Which struck me as very strange.. I found one discussion about something similar: https://groups.google.com/d/msg/sqlalchemy/GQZSjHAGkWM/rDflJvuyWnEJ
And tried adding strategy='threadlocal' to the engine, which from what i understand should force 1 connection for the tread. But it didn't have any effect on the errors im seeing.. (besides some unittests failing because i need two different sessions/connections for some tests and this forces 1 connection to be associated)

Does anyone have any idea what might go on here or have some more pointers on how to attack this problem?

Thanks in advance!

Matthijs Blaas
",9,5967,"Update: The errors where caused by multiple commands that where send in one prepared sql statement. Psycopg2 seems to allow this, but apparently it can cause strange issues. The PG8000 connector is more strict and bailed out on the multiple commands, sending one command fixed the issue!
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/14470688,SQLAlchemy Bidirectional Relationship association proxy,"Update:

For anyone having this issue, with the very latest SQLAlchemy this behaviour has been fixed.

Original issue:

I am having a problem with getting association proxies to update correctly.

Using the example models here: http://docs.sqlalchemy.org/en/rel_0_7/orm/extensions/associationproxy.html#simplifying-association-objects

But changing UserKeyword with this line:

keyword = relationship(""Keyword"", backref=backref(""user_keywords"", cascade=""all, delete-orphan""))


and adding this to Keyword:

users = association_proxy('user_keywords', 'user')


So a keyword instance has a list of users.

The following works as expected:

&gt;&gt;&gt; rory = User(""rory"")
&gt;&gt;&gt; session.add(rory)
&gt;&gt;&gt; chicken = Keyword('chicken')
&gt;&gt;&gt; session.add(chicken)
&gt;&gt;&gt; rory.keywords.append(chicken)
&gt;&gt;&gt; chicken.users
[&lt;__main__.User object at 0x1f1c0d0&gt;]
&gt;&gt;&gt; chicken.user_keywords
[&lt;__main__.UserKeyword object at 0x1f1c450&gt;]


But removals do strange things. Removing from the association proxy lists like so:

&gt;&gt;&gt; rory.keywords.remove(chicken)


Causes an integrity error as SA tries to set one of the foreign key columns to null.

Doing this:

&gt;&gt;&gt; rory.user_keywords.remove(rory.user_keywords[0])


Results in this:

&gt;&gt;&gt; chicken.users
[None]


I have missed something obvious haven't I?
",8,2495,"UserKeyword requires that it be associated with both a Keyword and User at the same time in order to be persisted.   When you associate it with a User and Keyword, but then remove it from the User.user_keywords collection, it's still associated with the Keyword.   

&gt;&gt;&gt; rory.keywords.remove(chicken)

# empty as we expect
&gt;&gt;&gt; rory.user_keywords
[]   

# but the other side, still populated.  UserKeyword 
# has no User, but still has Keyword
&gt;&gt;&gt; chicken.user_keywords
[&lt;__main__.UserKeyword object at 0x101748d10&gt;]

# but the User on that UserKeyword is None
&gt;&gt;&gt; chicken.user_keywords[0].user is None
True

# hence accessing the ""association"" gives us None
# as well
&gt;&gt;&gt; chicken.users
[None]


So if we were to flush() this right now, you've got a UserKeyword object ready to go but it has no User on it, so you get that NULL error.    At INSERT time, the object is not considered to be an ""orphan"" unless it is not associated with any Keyword.user_keywords or User.user_keywords collections.   Only if you were to say, del chicken.user_keywords[0] or equivalent, would you see that no INSERT is generated and the UserKeyword object is forgotten.

If you were to flush the object to the database before removing it from ""rory"", then things change.  The UserKeyword is now persistent, and when you remove ""chicken"" from ""rory.keywords"", a ""delete-orphan"" event fires off which does delete the UserKeyword, even though it still is associated with the Keyword object:

rory.keywords.append(chicken)

session.flush()

rory.keywords.remove(chicken)

session.flush()


you see the SQL:

INSERT INTO ""user"" (name) VALUES (%(name)s) RETURNING ""user"".id
{'name': 'rory'}

INSERT INTO keyword (keyword) VALUES (%(keyword)s) RETURNING keyword.id
{'keyword': 'chicken'}

INSERT INTO user_keyword (user_id, keyword_id, special_key) VALUES (%(user_id)s, %(keyword_id)s, %(special_key)s)
{'keyword_id': 1, 'special_key': None, 'user_id': 1}

DELETE FROM user_keyword WHERE user_keyword.user_id = %(user_id)s AND user_keyword.keyword_id = %(keyword_id)s
{'keyword_id': 1, 'user_id': 1}


Now a reasonable person would ask, ""isn't that inconsistent?""   And at the moment I'd say, ""absolutely"".   I need to look into the test cases to see what the rationale is for this difference in behavior, I've identified in the code why it occurs in this way and I'm pretty sure this difference in how an ""orphan"" is considered for ""pending"" versus ""persistent"" objects is intentional, but in this particular permutation obviously produces a weird result.   I might make a change in 0.8 for this if I can find one that is feasible.

edit: http://www.sqlalchemy.org/trac/ticket/2655 summarizes the issue which I'm going to have to think about.   There is a test for this behavior specifically, need to trace that back to its origin.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/24231535,Flask-SQLAlchemy Can&#39;t Create Relationship - SAWarning,"I used fbone to start this project off and I'm using its implementation of a Flask-SQLAlchemy column extension named DenormalizedText. I get the concept of this and how it works (not my issue) but my implementation of the add method is having odd results.

DenormalizedText

class DenormalizedText(Mutable, types.TypeDecorator):
    """"""
    Stores denormalized primary keys that can be
    accessed as a set.

    :param coerce: coercion function that ensures correct
               type is returned

    :param separator: separator character
    """"""

    impl = types.Text

    def __init__(self, coerce=int, separator="" "", **kwargs):

        self.coerce = coerce
        self.separator = separator

    super(DenormalizedText, self).__init__(**kwargs)

    def process_bind_param(self, value, dialect):
        if value is not None:
            items = [str(item).strip() for item in value]
            value = self.separator.join(item for item in items if item)
        return value

    def process_result_value(self, value, dialect):
        if not value:
            return set()
        return set(self.coerce(item) for item in value.split(self.separator))

    def copy_value(self, value):
        return set(value)


My class Person has a DenormalizedText parameter named family

family = Column(DenormalizedText)

and here's the add method

# just adds each object to the other's family relationship
def add_family(self, person):
    self.family.add(person.id)
    person.family.add(self.id)


So here's the strange bits:


I have another relationship for Person done the exact same way for a different class called Residence. This works fine. So I think for a minute maybe there's an issue with a self-referencing implementation. But fbone does this with their provided User class and that works?!
So I wrote a test... and it passed!
Everything leading up to the use of this method works fine. Both Person objects are in session and committed, I double check the ""family member"" before trying to add (make sure they were saved to the db and have an id).


Occasionally I get this rare error:
SAWarning: The IN-predicate on ""persons.id"" was invoked with an empty sequence. This results in a contradiction, which nonetheless can be expensive to evaluate.  Consider alternative strategies for improved performance.



Like I said this works fine for the residences relationship with Person but here's the relevant code from the view and form handler in case:

def create_step2_family():

    client = Person()
    spouse = Person()
    spouse_form = PersonalForm()

    # default action
    try:
        client = Person.get_by_id(session['working_client'])
    except KeyError:
        flash('Please complete this section first.', 'warning')
        return redirect(url_for('client.create_step1_personal'))
    else:

        # spouse form action
        if request.method == 'POST' and spouse_form.validate_on_submit():
            spouse_form.populate_obj(spouse)
            spouse.type_code = SPOUSE

            db.session.add(spouse)
            db.session.commit()

            if Person.get_by_id(spouse.id):
                client.add_family(spouse)
                db.session.add(client)
                db.session.commit()

    return render_template('clients/wizard/step2/family.html', active=""Create"",
    client=client, spouse=spouse, spouse_form=spouse_form, active_tab=""family"")

",3,650,"I need a vacation

The error ""SAWarning: The IN-predicate on ""persons.id"" was invoked with an empty sequence. This results in a contradiction, which nonetheless can be expensive to evaluate.  Consider alternative strategies for improved performance."" was because I was calling a query on the column that returned nothing, or the list of persons.ids was empty because, well, nothing was being added.

And nothing was being added because I forgot these little lines at the bottom of my add method:

def add_family(self, person):
    self.family.add(person.id)
    person.family.add(self.id)
    self.family=set(self.family)
    person.family=set(person.family)


I'm a mess. Anyway, I won't accept my answer for this if someone can provide a better explanation or improvement to the code.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/7017058,SQLAlchemy intricacies with relationships and uncommitted objects,"I am using SQLAlchemy via Flask, and I want to add simple personal messaging to my webapp.  The model has a User class, a PersonalMessage class and a PersonalMessageUser association class, with the latter setting up relationships to the former twonothing fancy.  Here is a stripped down version:

import collections
import datetime

from flaskext.sqlalchemy import SQLAlchemy
from . import app

db = SQLAlchemy(app)

def current_ts():
    return datetime.datetime.utcnow()

class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(127), nullable=False, unique=True)

    def __repr__(self):
        return '&lt;User {0.username!r} (#{0.id})&gt;'.format(self)

class PersonalMessage(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    subject = db.Column(db.String(127), nullable=False)
    body = db.Column(db.Text, nullable=False)
    date = db.Column(db.DateTime, nullable=False, default=current_ts)

    def __repr__(self):
        return '&lt;PersonalMessage {0.subject!r} (#{0.id})&gt;'.format(self)

    def __init__(self, subject, body, from_, to=None, cc=None, bcc=None):
        self.subject = subject
        self.body = body
        if not to and not cc and not bcc:
            raise ValueError, 'No recipients defined'
        self._pm_users.append(PersonalMessageUser(
            message=self, user_type='From', user=from_,
        ))
        for type, values in {'To': to, 'CC': cc, 'BCC': bcc}.items():
            if values is None:
                continue
            if not isinstance(values, collections.Iterable):
                values = [values]
            for value in values:
                self._pm_users.append(PersonalMessageUser(
                    message=self, user=value, user_type=type,
                ))

class PersonalMessageUser(db.Model):
    pm_id = db.Column(db.ForeignKey(PersonalMessage.id), nullable=False,
                      primary_key=True)
    message = db.relationship(PersonalMessage, backref='_pm_users',
                              lazy='subquery')
    user_id = db.Column(db.ForeignKey(User.id), nullable=False,
                        primary_key=True)
    user = db.relationship(User, backref='_personal_messages')
    user_type = db.Column(
        db.Enum('From', 'To', 'CC', 'BCC', name='user_type'),
        nullable=False, default='To', primary_key=True,
    )

    def __repr__(self):
        return (
            '&lt;PersonalMessageUser '
            '{0.user_type}: {0.user.username!r} '
            '(PM #{0.pm_id}: {0.message.subject!r})&gt;'
        ).format(self)


Everything works fine basically, but I noticed something strange when I played around with it in the Python interpreter: when I create a new PersonalMessage with one sender and one recipient, the _pm_users backref actually lists each user twice.  Once the object has been committed to the database, it looks okay, though.  See the following session as an example:

&gt;&gt;&gt; al = User(username='al')
&gt;&gt;&gt; db.session.add(al)
&gt;&gt;&gt; steve = User(username='steve')
&gt;&gt;&gt; db.session.add(steve)
&gt;&gt;&gt; db.session.commit()
BEGIN (implicit)
INSERT INTO user (username) VALUES (?)
('al',)
INSERT INTO user (username) VALUES (?)
('steve',)
COMMIT
&gt;&gt;&gt; pm = PersonalMessage('subject', 'body', from_=al, to=steve)
&gt;&gt;&gt; pm._pm_users
BEGIN (implicit)
SELECT user.id AS user_id, user.username AS user_username 
FROM user 
WHERE user.id = ?
(1,)
SELECT user.id AS user_id, user.username AS user_username 
FROM user 
WHERE user.id = ?
(2,)
[&lt;PersonalMessageUser From: u'al' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser From: u'al' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #None: 'subject')&gt;]
&gt;&gt;&gt; len(pm._pm_users)
4
&gt;&gt;&gt; db.session.add(pm)
&gt;&gt;&gt; pm._pm_users
[&lt;PersonalMessageUser From: u'al' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser From: u'al' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #None: 'subject')&gt;]
&gt;&gt;&gt; db.session.commit()
INSERT INTO personal_message (subject, body, date) VALUES (?, ?, ?)
('subject', 'body', '2011-08-10 19:48:15.641249')
INSERT INTO personal_message_user (pm_id, user_id, user_type) VALUES (?, ?, ?)
((1, 1, 'From'), (1, 2, 'To'))
COMMIT
&gt;&gt;&gt; pm._pm_users
BEGIN (implicit)
SELECT personal_message.id AS personal_message_id,
    personal_message.subject AS personal_message_subject,
    personal_message.body AS personal_message_body,
    personal_message.date AS personal_message_date 
FROM personal_message 
WHERE personal_message.id = ?
(1,)
SELECT personal_message_user.pm_id AS personal_message_user_pm_id,
    personal_message_user.user_id AS personal_message_user_user_id,
    personal_message_user.user_type AS personal_message_user_user_type 
FROM personal_message_user 
WHERE ? = personal_message_user.pm_id
(1,)
SELECT user.id AS user_id, user.username AS user_username 
FROM user 
WHERE user.id = ?
(1,)
SELECT user.id AS user_id, user.username AS user_username 
FROM user 
WHERE user.id = ?
(2,)
[&lt;PersonalMessageUser From: u'al' (PM #1: u'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #1: u'subject')&gt;]


At least the final result is what I expect it to be, but each user showing up twice before committing makes me feel uncomfortable; Id like to understand whats going on there.  Do I miss something in my relationship/backref setup, or shall I just ignore this?
",3,1390,"When you call

self._pm_users.append(PersonalMessageUser(
    message=self, user_type='From', user=from_,
))


you have twice append object to _pm_users list

This should work for you:

PersonalMessageUser(
    message=self, user_type='From', user=from_,
)


or

self._pm_users.append(
    PersonalMessageUser(user_type='From', user=from_,)
)       


When set relationship property, sqlalchemy associate objects for you
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/63081680,Error &#39;str&#39; object has no attribute &#39;toordinal&#39; in asyncpg,"My queries were giving strange results so i debugged a little bit, i change my String, date object to sqlalchemy Date so it raised this error
asyncpg.exceptions.DataError: invalid input for query argument $2: '2020-03-11'
('str' object has no attribute 'toordinal')

Here is my sqlalchemyTable
db = sqlalchemy.Table(
""db"",
metadata,
sqlalchemy.Column(""date"", Date),
sqlalchemy.Column(""data"", JSONB),
)

how i insert values:
query = db.insert().values(
    date=datetime.strptime(key,""%d/%m/%Y"").strftime(""%Y-%m-%d""),
    data=value,
)
try:
    await database.execute(query)
except UniqueViolationError:
    pass

Why did i change the type String to Date,because when i ran a query like
query = f""""""SELECT * FROM db WHERE date BETWEEN SYMMETRIC '{start_at}' AND '{end_at}'""""""
return await database.execute(query)

It was only returning one row and one column like 2020-03-11
",2,2635,"I would say your issue is here:
date=datetime.strptime(key,""%d/%m/%Y"").strftime(""%Y-%m-%d""),
You are passing a string to a date field.  SQLAlchemy is looking for a date object to be passed in, hence the ('str' object has no attribute 'toordinal') error. toordinal being an attribute of a date object. Remove the .strftime(""%Y-%m-%d"") and it should work.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/47276146,How to add a CASE column to SQLAlchemy output?,"So far I've got basically the following:

MyTable.query
    .join()
    .filter()


The filter has a complicated case insensitive prefix check:

or_(*[MyTable.name.ilike(""{}%"".format(prefix)) for prefix in prefixes])


Now I have to retrieve the matching prefix in the result. In pseudo-SQL:

SELECT CASE
           WHEN strpos(lower(my_table.foo), 'prefix1') = 0 THEN 'prefix1'
           WHEN 
       END AS prefix
 WHERE prefix IS NOT NULL;


The SQLAlchemy documentation demonstrates how to use CASE within WHERE, which seems like a strange edge case, and not how to use it to get a new column based on an existing one.

The goal is to avoid duplicating the logic and prefix names anywhere.
",2,1710,"You know that case is an expression. You can use it in any place where SQL allows it, and link to docs you have provided show how to construct the case expression.

from sqlalchemy import case, literal
from sqlalchemy.orm import Query

q = Query(case([
    (literal(1) == literal(1), True),
], else_ = False))

print(q)

SELECT CASE WHEN (:param_1 = :param_2) THEN :param_3 ELSE :param_4 END AS anon_1

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/17315422,Unable to store datetime.datetime.max using SQLAlchemy==0.8.1 with the mysql-python==1.2.4 driver,,1,832,"This is a reported bug in the new version of the MySQL server (5.6.X) to do with the rounding of fractional seconds.

See this link for more information:
http://bugs.mysql.com/bug.php?id=68760

The way round this is to round out the milliseconds.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/64855545,SQLalchemy rowcount always -1 for statements,,1,1388,"The single-row INSERT  VALUES (  ) is trivial: If the statement succeeds then one row was affected, and if it fails (throws an error) then zero rows were affected.
For a multi-row INSERT simply perform it inside a transaction and rollback if an error occurs. Then the number of rows affected will either be zero or len(values_list).
To get the number of rows that a SELECT will return, wrap the select query in a SELECT count(*) query and run that first, for example:
select_stmt = sa.select([Parent])
count_stmt = sa.select([sa.func.count(sa.text(""*""))]).select_from(
    select_stmt.alias(""s"")
)
with engine.connect() as conn:
    conn.execution_options(isolation_level=""SERIALIZABLE"")
    rows_found = conn.execute(count_stmt).scalar()
    print(f""{rows_found} row(s) found"")
    results = conn.execute(select_stmt).fetchall()
for item in results:
    print(item.id)

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/71445447,"How can I access the joined results of lazy=joined, without executing a second SQL statement, or changing the parent query",,1,1362,"Gord Thompson solved it for me.
Since I posted the original code, I thought it might be helpful to post the modified code that worked.
class User(Base, UserMixin):
    __tablename__ = 'user'

    email = db.Column(db.Unicode(255), nullable=False, server_default=u'', unique=True)

    userprofile = db.relationship(
        ""Userprofile"",
        back_populates=""user"",
        uselist=False,
        lazy=""joined"",
        innerjoin=True,
        passive_deletes=True)   

class Userprofile(Base):
    __tablename__ = 'userprofile'
    user_id = db.Column(db.Integer, db.ForeignKey('user.id', ondelete='CASCADE'))
    first_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')
    last_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')

user = db.relationship(""User"", back_populates=""userprofile"", uselist=False)



",,
SQLAlchemy strange result,https://stackoverflow.com/questions/65100066,SQLAlchemy loads unrelated cached objects when using contains_eager,,1,551,"I asked this question on the SQLAlchemy GitHub page. The solution is to use populate_existing on any query that uses contains_eager and filter. In my specific example, this query does the right thing
session \
        .query(Parent) \
        .join(Child, Parent.id == Child.parent_id) \
        .options(
            contains_eager(Parent.children)
        ).filter(Child.value &lt; 15) \
        .order_by(Parent.id) \
        .populate_existing() \
        .all()

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/67975069,Phantom Queries in PyMSQL + SQLAlchemy on Lambda,"I'm trying to debug an issue that one of our developers recently revealed.
We're using AWS Lambda functions to periodically query our MySQL DB.
These functions are written in Python 3.7, with a Lambda layer added to support using PyMSQL, SQLAlchemy, and Pandas.
The (summarized and redacted) code looks like this:
import pymysql
from sqlalchemy import create_engine
import pandas as pd

mysql_connector = create_engine('mysql+pymysql://' + oltp_username + ':' + oltp_password + '@' + host + '/' + db_name)
query = """"""
        SELECT sum(field) + """"""
        FROM schema.table;
        """"""
df = pd.read_sql(query, mysql_connector)

The works just fine, and returns the expected results, however there is a strange side-effect. Our query-logging software indicates that two queries are being received from this function: the expected query in its proper format, along with a mysterious query that looks like this:
DESCRIBE `
        SELECT sum(field)
        FROM schema.table;
        `

In MySQL this is obviously a malformed query, since DESCRIBE operates on tables. Our query logging software indicates that this error appears for every single query that is run from this Lambda function.
Does anybody know where these phantom queries might be coming from? I presume it's some option in either PyMYSQL or SQLAlchemy, but I can't find anything in the documentation. Also, why are these being sent? Including a DESCRIBE makes no sense. I suspect it's just sending the raw query string to the DESCRIBE function, due to an expression like DATE_FORMAT(datefield,""%%Y-%%m-%%d %%H:%%i:00"") appearing with the consecutive %, instead of being properly escaped, as it is in the correct query.
",1,161,"snakecharmerb and Ilja Everil provided a few answers in the comments, all of which work. Thanks!

Upgrade SQLAlchemy to version 1.4
Don't use pandas to execute the SQL.
Use pandas read_sql_query function instead of read_sql.

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/59093282,Why is Oracle Pivot producing non-existent results?,"I manage a database holding a large amount of climate data collected from various stations. It's an Oracle 12.2 DB, and here's a synopsis of the relevant tables:

FACT = individual measurements at a particular time


UTC_START = time in UTC at which the measurement began
LST_START = time in local standard time (to the particular station) at which the measurement began
SERIES_ID = ID of the series to which the measurement belongs (FK to SERIES)
STATION_ID = ID of the station at which the measurement occurred (FK to STATION)
VALUE = value of the measurement


Note that UTC_START and LST_START always have a constant difference per station (the LST offset from UTC). I have confirmed that there are no instances where the difference between UTC_START and LST_START is anything other than what is expected.

SERIES = descriptive data for series of data


SERIES_ID = ID of the series (PK)
NAME = text name of the series (e.g. Temperature)


STATION = descriptive data for stations


STATION_ID = ID of the station (PK)
SITE_ID = ID of the site at which a station is located (most sites have one station, but a handful have 2)
SITE_RANK = rank of the station within the site if there are more than 1 stations.
EXT_ID = external ID for a site (provided to us)


The EXT_ID of a site applies to all stations at that site (but may not be populated unless SITE_RANK == 1, not ideal, I know, but not the issue here), and data from lower ranked stations is preferred. To organize this data into a consumable format, we're using a PIVOT to collect measurements occurring at the same site/time into rows.

Here's the query:

WITH
    primaries AS (
        SELECT site_id, ext_id
        FROM station
        WHERE site_rank = 1
    ),

    data as (
        SELECT d.site_id, d.utc_start, d.lst_start, s.name, d.value FROM (
            SELECT s.site_id, f.utc_start, f.lst_start, f.series_id, f.value,
                 ROW_NUMBER() over (PARTITION BY s.site_id, f.utc_start, f.series_id ORDER BY s.site_rank) as ORDINAL
                 FROM fact f
                      JOIN station s on f.station_id = s.station_id
        ) d
            JOIN series s ON d.series_id = s.series_id
            WHERE d.ordinal = 1
                AND d.site_id = ?
                AND d.utc_start &gt;= ?
                AND d.utc_start &lt; ?
    )

    records as (

        SELECT * FROM data
        PIVOT (
               MAX(VALUE) AS VALUE
               FOR NAME IN (
                   -- these are a few series that we would want to collect by UTC_START
                   't5' as t5,
                   'p5' as p5,
                   'solrad' as solrad,
                   'str' as str,
                   'stc_05' as stc_05,
                   'rh' as rh,
                   'smv005_05' as smv005_05,
                   'st005_05' as st005_05,
                   'wind' as wind,
                   'wet1' as wet1
                   )
                )
    )

SELECT r.*, p.ext_id
FROM records r JOIN primaries p on r.site_id = p.site_id


Here's where things get odd. This query works perfectly in SQLAlchemy, IntelliJ (using OJDBC thin), and Orcale SQL Developer. But when it's run from within our Java program (same JDBC urls, and credentials, using plain old JDBC statments and result sets), it gives results that don't make sense. Specifically for the same station, it will return 2 rows with the same UTC_START, but different LST_START (recall that I have verified that this 100% does not occur anywhere in the FACT table). Just to ensure there was no weird parameter handling going on, we tested hard-coding values in for the placeholders, and copy-and-pasted the exact same query between various clients, and the only one that returns these strange results is the Java program (which is using the exact same OJDBC jar as IntelliJ).

If anyone has any insight or possible causes, it would be greatly appreciated. We're at a bit of a loss right now.
",1,72,"It turns out that Nathan's comment was correct. Though it seems counter-intuitive (to me, at least), it appears that calling ResultSet.getString on a DATE column will in fact convert to Timestamp first. Timestamp has the unfortunate default behavior of using the system default timezone unless you specify otherwise explicitly.

This default behavior meant that daylight saving's time was taken into account when we didn't intend it to be, leading to the odd behavior described.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/49360525,SqlAlchemy like filter case insensitive but it should be case sensitive,"I am using SqlAlchemy for a database, but I have a problem when using like function in queries.

My database is SQLite.

My request is like this: 

self.session.query(Value.scan).filter(Value.current_value.like(""%"" + search + ""%""), Value.tag == Tag.tag, Tag.visible == True).distinct().all()


The column Value.current_value is a String, you can see the declaration here:

class Value(Base):
    current_value = Column(String, nullable=False)


search variable is a str coming from a rapid search bar, and is case sensitive (I never call lower or upper on it).

I want to do a case sensitive search, but the results are case insensitive.

I did some research and like should be case sensitive, and ilike case insensitive, so I don't understand why it's case insensitive.

Should I choose another type for my column that has to be case sensitive?

Another strange thing is that I have the same problem when using the function contains on the same column (case insensitive result), but not when using operators like ==, !=, &lt;, or &gt; (case sensitive result)

Does semeone knows why it's case sensitive with operators, but not with like and contains functions?

Best regards

Lucie
",1,4581,"In SQLite, LIKE is by default case insensitive.

What I had to do is activating the case_sensitive_like pragma.

I created a class to activate the pragma like this:

class ForeignKeysListener(PoolListener):
    """"""
    Class to activate the pragma case_sensitive_like, that makes the
    like and contains functions case sensitive
    """"""
    def connect(self, dbapi_con, con_record):
        db_cursor = dbapi_con.execute('pragma case_sensitive_like=ON')


When you create the engine, you just have to add the listener like this:

engine = create_engine(
    'sqlite:///' + os.path.join(self.folder, 'database', 'mia2.db'),
    listeners=[ForeignKeysListener()])

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/75913528,AWS lambda does not update rds postgres table but the execution is successful,"I have a postgres DB in RDS. When I connect it from my local and insert into it via sqlalchemy, I can see the updated results but when I run the same code from Lambda, the execution is shown as completed (i.e code runs correctly) but the results are not updated in the DB. In the same code when I print out result of `SELECT * table_name*, I can see the new row addition but when I check from local (using both pgadmin and through code) whether the row was really added or not, it it is missing.
One strange thing I observed is that I have a column 'ID' which is a primary key. When I add the row with ID 25 from lambda followed by adding another row from local, the row added from local is assigned row 27 (instead of 26 which was assigned to row added from lambda, however I can't see the row with id 26).
The following is my code:
from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, text
engine = create_engine(""postgresql://%s:%s@%s:%s/%s"" % (username, password, host, port, database))
conn = engine.connect()
print(""CONNECTED TO RDS"")

query = 'select * from predictions'
result = conn.execute(text(query)).fetchall()
print(""THE QUERY RESULT IS"", '\n', result)
upload_query = ""INSERT INTO predictions (image_url, image_class) VALUES ('LOCAL_ADD', 'LOCAL_ADD');""
conn.execute(text(upload_query))
query = 'select * from predictions'
result = conn.execute(text(query)).fetchall()
print(""THE QUERY RESULT IS"", '\n', result)

conn.close()

My lambda has no VPC to it. However my RDS DB has 4 security groups with two of them being inbound/outbound rules set to 'All Traffic' with '0.0.0.0'
",0,563,"You've executed the changes with conn.execute, but if autocommit is not set within the lambda session then the changes won't be issued to the database until you commit them with conn.commit.
So add the line conn.commit() like so:
conn.execute(text(upload_query))
conn.commit()

","Enable autocommit or explicitly COMMIT the transacion in the Lambda function.
Sequence values are updated whenever the next value is requested. This occurs outside of the transaction. It can be viewed as occurring in an independent transaction. This is necessary so that sequences can be used by concurrent transactions without introducing waits or deadlocks.
",
SQLAlchemy strange result,https://stackoverflow.com/questions/73440112,MySql: Insert record doesn&#39;t exists or actually never being created?,"Recently I've encountered a strange problem which I couldn't see any ideas based on my current knowledge.
Backend:  Python3, Sqlalchemy,
MySQL Config: read-committed, id auto-increment, cluster with 3 nodes.
Query: insert into xxx values(xxx...) and then db.session.commit()
Expect Result: New record id returned and mysql successfully create one record.
Actually Result: New record id returned and no mysql record created and no binlog found.
I wonder: if something panic, the transcation should've rollback and no id should returned. What I missed?
",0,27,"Possibly one is overwriting the session with child processes,
Connection problems with SQLAlchemy and multiple processes
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/72293864,Getting sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) NOT NULL constraint failed: user.image_file,"I am new to learning Flask. following youtube tutorial, I did the same as the trainer did. But I am getting the Integrity error.
Flask_Blog.Py:
from datetime import datetime
from flask import Flask, render_template, url_for, flash, redirect
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy.com import dynamic

from forms import RegistrationForm, LoginForm

app = Flask(__name__)
app.config['SECRET_KEY'] = '5791628bb0b13ce0c676dfde280ba245'
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///site.db'
db = SQLAlchemy(app)


class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(20), unique=True, nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)
    image_file = db.Column(db.String(20), nullable=False)
    password = db.Column(db.String(60), nullable=False)
    posts = db.relationship('Post', backref='author', lazy=True)

    def __repr__(self):
        return f""User('{self.username}', '{self.email}', '{self.image_file}')""


class Post(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    title = db.Column(db.String(100), nullable=False)
    date_posted = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)
    content = db.Column(db.Text, nullable=False)
    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)

    def __repr__(self):
        return f""Post('{self.title}', '{self.date_posted}')""


posts = [
    {
        'author': 'Corey Schafer',
        'title': 'Blog Post 1',
        'content': 'First post content',
        'date_posted': 'April 20, 2018'
    },
    {
        'author': 'Jane Doe',
        'title': 'Blog Post 2',
        'content': 'Second post content',
        'date_posted': 'April 21, 2018'
    }
]


@app.route(""/"")
@app.route(""/home"")
def home():
    return render_template('home.html', posts=posts)


@app.route(""/about"")
def about():
    return render_template('about.html', title='About')


@app.route(""/register"", methods=['GET', 'POST'])
def register():
    form = RegistrationForm()
    if form.validate_on_submit():
        flash(f'Account created for {form.username.data}!', 'success')
        return redirect(url_for('home'))
    return render_template('register.html', title='Register', form=form)


@app.route(""/login"", methods=['GET', 'POST'])
def login():
    form = LoginForm()
    if form.validate_on_submit():
        if form.email.data == 'admin@blog.com' and form.password.data == 'password':
            flash('You have been logged in!', 'success')
            return redirect(url_for('home'))
        else:
            flash('Login Unsuccessful. Please check username and password', 'danger')
    return render_template('login.html', title='Login', form=form)

 if __name__ == '__main__':
        app.run(debug=True) 

This the commands I am trying to execute.
Commands:
**from Flask_Blog import db
db.create_all()
from Flask_Blog import User, Post
user_1 = User(username='Strange', email='doctor@strange.com', password='password')
db.session.add(user_1)
db.session.commit()**

After executing commit I am getting below error.

Traceback (most recent call last):
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1819, in _execute_context
self.dialect.do_execute(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 732, in do_execute
cursor.execute(statement, parameters)
sqlite3.IntegrityError: NOT NULL constraint failed: user.image_file
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
File ""C:\Users\darjunku\AppData\Local\Programs\Python\Python310\lib\code.py"", line 90, in runcode
exec(code, self.locals)
File """", line 1, in 
File """", line 2, in commit
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1435, in commit
self._transaction.commit(_to_root=self.future)
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 829, in commit
self._prepare_impl()
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 808, in _prepare_impl
self.session.flush()
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3367, in flush
self.flush(objects)
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3506, in flush
with util.safe_reraise():
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in exit
compat.raise(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise
raise exception
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3467, in _flush
flush_context.execute()
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 456, in execute
rec.execute(self)
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 630, in execute
util.preloaded.orm_persistence.save_obj(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 245, in save_obj
_emit_insert_statements(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 1238, in _emit_insert_statements
result = connection._execute_20(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1631, in _execute_20
return meth(self, args_10style, kwargs_10style, execution_options)
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\sql\elements.py"", line 325, in _execute_on_connection
return connection._execute_clauseelement(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1498, in _execute_clauseelement
ret = self._execute_context(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1862, in _execute_context
self.handle_dbapi_exception(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 2043, in handle_dbapi_exception
util.raise(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise
raise exception
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1819, in _execute_context
self.dialect.do_execute(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 732, in do_execute
cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) NOT NULL constraint failed: user.image_file
[SQL: INSERT INTO user (username, email, image_file, password) VALUES (?, ?, ?, ?)]
[parameters: ('Strange', 'doctor@strange.com', None, 'password')]
(Background on this error at: https://sqlalche.me/e/14/gkpj)

Traceback (most recent call last):
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1819, in _execute_context
    self.dialect.do_execute(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 732, in do_execute
    cursor.execute(statement, parameters)
sqlite3.IntegrityError: NOT NULL constraint failed: user.image_file
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""C:\Users\darjunku\AppData\Local\Programs\Python\Python310\lib\code.py"", line 90, in runcode
    exec(code, self.locals)
  File ""&lt;input&gt;"", line 1, in &lt;module&gt;
  File ""&lt;string&gt;"", line 2, in commit
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1435, in commit
    self._transaction.commit(_to_root=self.future)
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 829, in commit
    self._prepare_impl()
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 808, in _prepare_impl
    self.session.flush()
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3367, in flush
    self._flush(objects)
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3506, in _flush
    with util.safe_reraise():
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3467, in _flush
    flush_context.execute()
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 245, in save_obj
    _emit_insert_statements(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 1238, in _emit_insert_statements
    result = connection._execute_20(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1631, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\sql\elements.py"", line 325, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1498, in _execute_clauseelement
    ret = self._execute_context(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1862, in _execute_context
    self._handle_dbapi_exception(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 2043, in _handle_dbapi_exception
    util.raise_(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1819, in _execute_context
    self.dialect.do_execute(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 732, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) NOT NULL constraint failed: user.image_file
[SQL: INSERT INTO user (username, email, image_file, password) VALUES (?, ?, ?, ?)]
[parameters: ('Corey', 'C@demo.com', None, 'password')]
(Background on this error at: https://sqlalche.me/e/14/gkpj)*

",0,853,"Try:
image_file = db.Column(db.String(20), nullable=True)

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/71451982,How to make alembic or flask migrate name foreign keys when autogenerating migrations?,"I've been struggling with this issue on and off for quite some time, and strangely could not find a straightforward question/answer combo on this on SO. Related questions here and here. I finally found a solution so I will ask and answer my own question.
In Flask SQLAlchemy (and regular SQLAlchemy), you can have a column like this:
class Character(db.model):
  background_id = db.Column(db.Integer, db.ForeignKey('backgrounds.id'))

When you run flask db migrate, or alembic revision --autogenerate, this will result in an operation that looks like this:
def upgrade():
  op.create_foreign_key(None, 'characters', 'backgrounds', ['background_id'], ['id'])

def downgrade():
  op.drop_constraint(None, 'characters', type_='foreignkey')

The None here is bad. In fact, if you try to downgrade later, this will always fail, because drop_constraint needs the name of the constraint.
You can change this every time you generate a migration, like this:
def upgrade():
  op.create_foreign_key('fk_characters_backgrounds', 'characters', 'backgrounds', ['background_id'], ['id'])

def downgrade():
  op.drop_constraint('fk_characters_backgrounds', 'characters', type_='foreignkey')

Which works!
But if you're like me, you don't want to have to remember to do this every time you autogenerate a revision with a foreign key.
So the question is, how can we make this automatic?
",0,1258,"There is an answer to this question in the best practices suggested here, at the end of the section on The Importance of Naming Conventions. The solution is to add a naming_convention to your sqlalchemy metadata, like this:
convention = {
  ""ix"": ""ix_%(column_0_label)s"",
  ""uq"": ""uq_%(table_name)s_%(column_0_name)s"",
  ""ck"": ""ck_%(table_name)s_%(constraint_name)s"",
  ""fk"": ""fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s"",
  ""pk"": ""pk_%(table_name)s""
}

metadata = MetaData(naming_convention=convention)

More specifically, with Flask-SQLAlchemy, do this when initializing your db:
from sqlalchemy import MetaData

convention = {
  ""ix"": ""ix_%(column_0_label)s"",
  ""uq"": ""uq_%(table_name)s_%(column_0_name)s"",
  ""ck"": ""ck_%(table_name)s_%(constraint_name)s"",
  ""fk"": ""fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s"",
  ""pk"": ""pk_%(table_name)s""
}

db = SQLAlchemy(metadata=MetaData(naming_convention=convention))

And voila! If you run autogenerate, you'll get this:
def upgrade():
  op.create_foreign_key(op.f('fk_characters_background_id_backgrounds'), 'characters', 'backgrounds', ['background_id'], ['id'])

def downgrade():
  op.drop_constraint(op.f('fk_characters_background_id_backgrounds'), 'characters', type_='foreignkey')

Thanks (unsurprisingly) to  Miguel Grinberg, creator of Flask Migrate, for having linked to the correct page in the Alembic docs that finally allowed me to solve this problem! Someone had asked about this in an issue on Flask Migrate GitHub, and Miguel correctly pointed out that this was an Alembic issue, not a Flask Migrate issue.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/69566226,&quot;MySQLInterfaceError: Python type list cannot be converted&quot; on all sqlalchemy.orm.Query methods,"I am running a Python 3.9.0 server through uvicorn with sqlalchemy connecting to a mysql server.
I'm trying to make queries with a db (session) object that I construct as such, based on examples I've pulled from the docs and web:
database.py
from sqlalchemy.orm import sessionmaker
...
engine = create_engine(
    SQLALCHEMY_DATABASE_URL,
    connect_args={'auth_plugin': 'mysql_native_password'}
)
...
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

main.py
from sqlalchemy.orm import Session
from .database import SessionLocal, engine, Base

def get_db():
    try:
        db = SessionLocal()
        yield db
    finally:
        db.close()

Per the docs, I'm trying to invoke queries as follows:
main.py
db.query(models.OriginalSong).filter_by(original_song_title=original_song_title).first()

However, methods such as first() and all() are throwing the same error:
_mysql_connector.MySQLInterfaceError: Python type list cannot be converted

pointing to the line where the all() or first() method was invoked.
I checked the type of the invocation and they appear to be correct:
print(type(db.query(models.OriginalSong).filter_by(original_song_title=original_song_title)))

&lt;class 'sqlalchemy.orm.query.Query'&gt;

I don't understand what I'm doing differently from what the docs on the query type or the tutorial. There was one stackoverflow question I found with the same error, but trying to join the ""list"" just resulted in the same error. I'm not even sure what the return type of first() or one() are as it errors before I can try to print the type.
How can I access the result of my query without an error?
(I tried to include only the necessary code, but the full repo can be seen on github)
EDIT: Strangely, a prior query works:
query = db.query(models.RemixArtist).filter_by(remix_artist_name=remix_artist_name).first()

I don't see any difference between the two.
",0,2078,"The issue wasn't with the return of the query, the issue was with the parameter passed to the filter_by. Without realizing it, I was passing it a list, when it expected a string. I had to change the argument's value for parameter original_song_title to be original_song_title[0].
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/67858384,SQLAlchemy: How do I avoid this inconsistency between ORM cache and DB,"Here's a simplified version of my problem. I have a program query.py:
import time
from models import Ball, session

time.sleep(1)
r = session.query(Ball).filter(Ball.color=='red').first()
print(f'Red ball color is {r.color}')
time.sleep(2)
b = session.query(Ball).filter(Ball.color=='blue').first()
print(f'Blue ball color is {b.color}')
print(f'Red ball id is {r.id}, blue ball id is {b.id}')

When I run query.py at the same time as modify.py (included below), I get the following output:
$ python modify.py &amp;! python query.py
Red ball color is red                                                                                                                                                   
Blue ball color is red                                                                                                                                                  
Red ball id is 1, blue ball id is 1                                                                                                                  

The problem is that the blue ball is red!
Here is the content of models.py:
import sqlalchemy as sa
import sqlalchemy.orm as sao
import sqlalchemy.ext.declarative as saed

Base = saed.declarative_base()

class Ball(Base):
    __tablename__ = 'ball'
    id = sa.Column(sa.Integer, primary_key=True)
    color = sa.Column(sa.String)

engine = sa.create_engine('sqlite:///test.db')
Base.metadata.create_all(engine)
session = sao.Session(engine)

And here is modify.py:
import time
from models import Ball, session

session.query(Ball).delete()
b = Ball(color='red')
session.add(b)
session.commit()
time.sleep(2)
b.color = 'blue'
session.add(b)
session.commit()

I find it very strange that I get an inconsistency here between my DB query (that sees the latest DB state) and the object returned via the SQLAlchemy identiy map for my DB query (which is stale, reflecting the DB state the first time the row in question was read). I know that restarting my transaction in the query.py process before each query will invalidate the cached objects in the identity map and result in the blue ball being blue here, but that's a non-starter.
I'd be happy if the blue ball were blue -- i.e. if the DB query and object it returned agreed -- or if the blue ball query returned None -- i.e. if the concurrent DB modification was not visible in the query transaction. But I seem to be stuck in the middle.
",0,178,"It seems the underlying issue is that SQLite support is buggy by default in Python, and SQLAlchemy knowingly inherits this buggy behavior. I eventually figured out how to get both possible correct behaviors, i.e. either make the blue ball blue, by invalidating the identity map/cache without cancelling the current transaction, or make the query for blue balls return None, by running the query.py in a properly isolated transaction.
Achieving isolation / making the blue ball query return None
I found that setting the isolation level, by passing isolation_level=&lt;level&gt; to create_engine had no effect, i.e. this didn't provide a way to make the query.py run in a transaction that is isolated from the writes by modify.py, where the query for blue balls would return None. After reading about isolation levels in SQLite it seems that setting the isolation level to SERIALIZABLE should accomplish this, but it does not. However, the SQLAlchemy documentation warns that by default SQLite transactions are broken:

In the section Database Locking Behavior / Concurrency, we refer to the pysqlite drivers assortment of issues that prevent several features of SQLite from working correctly. The pysqlite DBAPI driver has several long-standing bugs which impact the correctness of its transactional behavior. In its default mode of operation, SQLite features such as SERIALIZABLE isolation, transactional DDL, and SAVEPOINT support are non-functional, and in order to use these features, workarounds must be taken.

That page goes on to suggest workarounds to get functioning transactions, and those workarounds work for me. Namely, adding the following to the bottom of models.py achieves the isolation behavior where the blue balls query returns None:
@sa.event.listens_for(engine, ""connect"")
def do_connect(dbapi_connection, connection_record):
    # disable pysqlite's emitting of the BEGIN statement entirely.
    # also stops it from emitting COMMIT before any DDL.
    dbapi_connection.isolation_level = None

@sa.event.listens_for(engine, ""begin"")
def do_begin(conn):
    # emit our own BEGIN
    conn.exec_driver_sql(""BEGIN"")

With this change, the output becomes
$ python modify.py &amp;! python query.py
Red ball color is red                                                                                                                                                   
Traceback (most recent call last):                                                                                                                                      
  File ""query.py"", line 10, in &lt;module&gt;
    print(f'Blue ball color is {b.color}')
AttributeError: 'NoneType' object has no attribute 'color'

I.e. the DB write in modify.py that makes the ball blue is not visible in the (implicit) transaction in query.py.
Making the query and returned object agree / making the blue ball blue
On the other hand, to get the behavior where the the blue ball is blue, it's enough to invalidate the cache/identity map before each query, using session.expire_all(). I.e., changing query.py to the following works:
import time
from models import Ball, session

time.sleep(1)
r = session.query(Ball).filter(Ball.color=='red').first()
print(f'Red ball color is {r.color}')
time.sleep(2)
# Added this line:
session.expire_all()
b = session.query(Ball).filter(Ball.color=='blue').first()
print(f'Blue ball color is {b.color}')
print(f'Red ball id is {r.id}, blue ball id is {b.id}')

With this change, the output becomes
$ python modify.py &amp;! python query.py                                                                                                                                
Red ball color is red                                                                                                                                                   
Blue ball color is blue                                                                                                                                                 
Red ball id is 1, blue ball id is 1

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/66004467,MySQL BIGINT inconsistent for inserts?,"On ubuntu.. running MySQL v 5.6.
created a python program that performs all my operations.
my app creates tables dynamically. there are many. a few are very similar.. for example, here are two:
create table tst.intgn_party_test_load (
  party_id bigint unsigned NOT NULL,
  party_supertype varchar(15) NOT NULL,
  carrier_party_id bigint unsigned NOT NULL,
  full_name varchar(500),
  lda_actv_ind integer,
  lda_file_id integer,
  lda_created_by varchar(100),
  lda_created_on datetime,
  lda_updated_by varchar(100),
  lda_updated_on datetime, 
  PRIMARY KEY(party_id,party_supertype,carrier_party_id)
) 

and
create table tst.intgn_party_relationship (
  parent_party_id bigint unsigned NOT NULL,
  child_party_id bigint unsigned NOT NULL,
  relationship_type varchar(10),
  lda_actv_ind integer,
  lda_file_id integer,
  lda_created_by varchar(100),
  lda_created_on datetime,
  lda_updated_by varchar(100),
  lda_updated_on datetime, 
  PRIMARY KEY(parent_party_id,child_party_id,relationship_type)
) 

My program also dynamically populates the tables. I construct the party id fields using source data converted to an BIGINT.
For example, the insert it constructs for the first table is:
INSERT INTO intgn_party_test_load (
  party_supertype, 
  carrier_party_id, 
  party_id, 
  full_name, 
  lda_actv_ind, 
  lda_file_id) 
SELECT  
  'Agency' as s0,
  0 as s1,
  CONV(SUBSTRING(CAST(SHA(CONCAT(full_name,ga)) AS CHAR), 1, 16), 16, 10) as s2,
  CONCAT(full_name,'-',ga) as s3, 
  lda_actv_ind, 
  lda_file_id 
FROM tst.raw_listing_20210118175114 
ON DUPLICATE KEY 
UPDATE  
  full_name = VALUES(full_name), 
  lda_actv_ind = VALUES(lda_actv_ind), 
  lda_file_id = VALUES(lda_file_id) ;

and for the second table the insert constructed looks very similar, and is based on the exact same source data:
INSERT INTO tst.intgn_party_relationship (
  parent_party_id,
  relationship_type,
  child_party_id, 
  lda_actv_ind, 
  lda_file_id) 
SELECT (Select party_id 
        from intgn_party 
        where full_name = 'xxx') as s0,
       'Location' as s1,
       CONV(SUBSTRING(CAST(SHA(CONCAT(full_name,ga)) AS CHAR), 1, 16), 16, 10) as s2, 
       lda_actv_ind, 
       lda_file_id 
FROM tst.raw_listing_20210118175114 
ON DUPLICATE KEY 
UPDATE  
  lda_actv_ind = VALUES(lda_actv_ind), 
  lda_file_id = VALUES(lda_file_id) 

Now... the first table (intgn_party_test_load) is the issue. I can drop it, recreate it manually even.. no matter what i do, the data inserted into it via python has the BIGINT party_id truncated to just 16 digits.
EVERY OTHER TABLE that uses the exact same formula to populate the party_id, creates BIGINT numbers that are between 18 and 20 digits long. I can see all the same source records loaded in the tables, and i see the truncated values in the first table (intgn_party_test_load). for example, the first table has a record with party id = 7129232523783260.  the second table (and many others) has the same record loaded with [child]party id  = 7129232523783260081.
The exact same formula, executed the exact same way from python.. but this table gets shorter BIGINTs.
Interestingly, I tried manually running the insert into this table (not using the python program), and it inserts the full BIGINT values.
So I'm confused why the python program has 'chosen' this table to not work correctly, while it works fine on all other tables.
Is there some strange scenario where values get truncated?
BTW, my python program utilizes sqlalchemy to run the creations/inserts. Since it works manually, I have to assume its related to sqlalchemy.. but no idea why it works on all but this table..
[edit]
to add, the sql commands through sqlalchemy are executed using db_connection.execute(sql)
[edit - adding more code detail]
from sqlalchemy import create_engine, exc

engine = create_engine(
            connection_string,
            pool_size=6, max_overflow=10, encoding='latin1', isolation_level='AUTOCOMMIT'
        )
        connection = engine.connect()

sql = ""INSERT INTO intgn_party_test_load (
  party_supertype, 
  carrier_party_id, 
  party_id, 
  full_name, 
  lda_actv_ind, 
  lda_file_id) 
SELECT  
  'Agency' as s0,
  0 as s1,
  CONV(SUBSTRING(CAST(SHA(CONCAT(full_name,ga)) AS CHAR), 1, 16), 16, 10) as s2,
  CONCAT(full_name,'-',ga) as s3, 
  lda_actv_ind, 
  lda_file_id 
FROM tst.raw_listing_20210118175114 
ON DUPLICATE KEY 
UPDATE  
  full_name = VALUES(full_name), 
  lda_actv_ind = VALUES(lda_actv_ind), 
  lda_file_id = VALUES(lda_file_id) ;""

        result = db_connection.execute(sql)

Thats as best i can reduce it too (the code is much more complicated as it dynamically creates the statement amoungst other things).. but from my logging, i see the exact statement it is executing (As above), and i see the result in the BIGINT columns after.  all tables but this one. And only when through the app.
so it doesn't happen to the other tables even through the app..
very confusing.. was hoping someone just knew a bug in mySQL 5.6 around BIGINTs as it pertains to maybe the destination table's key construct or total length of records.. or some other crazy reason.  I do see that interestingly, if i do a distinct on BIGINT column that has &gt;18 digit lengths, it comes back as 16 digits - guess the distinct function doesn't support BIGINT..
was kinda hoping this hints at an issue, but i don't get why the other tables would work fine...
[EDIT - adding some of the things i see sqlalchemy running apparently, around the actual run of my query.. just in the crazy case they impact anything - for the one table?? ]
SET AUTOCOMMIT = 0
SET AUTOCOMMIT = 1
SET NAMES utf8mb4
SHOW VARIABLES LIKE 'sql_mode'
SHOW VARIABLES LIKE 'lower_case_table_names'
SELECT VERSION()
SELECT DATABASE()
SELECT @@tx_isolation
show collation where `Charset` = 'utf8mb4' and `Collation` = 'utf8mb4_bin'
SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1
SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1
SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8mb4) COLLATE utf8mb4_bin AS anon_1
ROLLBACK
SET NAMES utf8mb4

hard to say the order or anything like that.. there are a ton that get run at the same microsecond.
",0,326,"after racking my brains for days.. coming at it from all angles, i could not figure out why 1 table out of many, had issues with truncating the SHA'd value.
In the end, i have redesigned how i hold my Ids, and i no longer bother converting to BIGINT. it all works fine when i leave it as CHAR.
CAST(SHA(CONCAT(full_name,ga)) AS CHAR)

So changed all my Id columns to varchar(40) and use the above style.  All good now. Joins will use varchar instead of bigint - which i'm ok with.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/61657802,Flask SQLAlchemy Sessions commit() not working &quot;sometimes&quot;,"I've been working in a web app for a while and this is the first time I realize this problem, I think it could be related with how SQLAlchemy sessions are handled, so some clarification in simple term would be helpful.
My configuration for work with flask sqlAlchemy is:

from flask_sqlalchemy import SQLAlchemy
db = SQLAlchemy(app)


My problem: db.session.commit() sometimes doesn't save changes. I wrote some flask endpoints which are reached via the front end requests in the user browser.
In this particular case, I'm editing a hotel ""Booking"" object altering the ""Rooms"" columns which is a Text field. 

the function does the following:

1-Query the Booking object from the dates in the request

2- Edit the Rooms column of this Booking object

3- Commit the changes ""db.session.commit()""

4- If a user has X functionality active, I make some checks calling a second function:

4.1- This functions make some checks and query and edit another object in the database different from the ""Booking"" object I edited previously.

4.2- At the end of this secondary function I call db.session.commit() ""Note this changes always got saved correctly in the database""

4.3- Return the results to the previous function

5- Return results to the front end (""just before this return, I print the Booking.Rooms to make sure it looks as it should, and it does... I even tried to make a second commit after the print but before the return... But after this, sometimes Booking.Rooms are updated as expected but some other times it doesn't... I noted if repeat the action many times it finally works, but given the intermediate function ""described in point 4"" saves all his changes correctly, this causes an inconsistency in the data and drives me mad because if I repeat the action and procedure in the function of point 4 worked correctly, I can't repeat the mod Rooms action...

So, I'm now really confused if this is something I don't understand from flask sessions, for what I understand, whenever I make a new request to flask, it's an isolated session, right?
I mean, if 2 concurrent users are storing some changes in the database, a db.session.commit() from one of the users won't commit the changes from the other one, right?

Same way, if I call db.session.commit() in one request, that changes are stored in the database, and if after that ""in the same request"", I keep modding things, it's like another session, right? And the committed changes are there already safely stored? And I can still use previous objects for further modifications 

Anyway, all of this shouldn't be a problem because after the commit() I print the Booking.Rooms and looks as expected... And some times it works getting stored correctly and some times it doesn't...

Also note: When I return this result to the client, the client makes instantly a second request to the server to request updated Booking data, and then the data is returned without this expected changes committed... I suppose flask handled all the commit() before it gets the second request ""other way it wouldn't have returned the result previously..."" 

Can this be a limitation of the flask development server which can't handle correctly many requests and that when deployed with gunicorn it doesn't happen?

Any hint or clarification about Sessions would be nice, because this is pretty strange behaviour, especially that sometimes works and others don't...

And as requested here is the code, I know is not possible to reproduce, have a lot of setup behind and would need a lot of data to works as intended under same circumstances as in my case, but this should provide an overview of how the functions looks like and where are the commits I mention above. Any ideas of where can be the problem is very helpful.

#Main function hit by the frontend
@users.route('/url_endpoint1', methods=['POST'], strict_slashes=False)
@login_required
def add_room_to_booking_Api():
    try:
        bookingData = request.get_json()
        roomURL=bookingData[""roomSafeURL""]
        targetBooking = bookingData[""bookingURL""]
        startDate = bookingData[""checkInDate""]
        endDate = bookingData[""checkOutDate""]
        roomPrices=bookingData[""roomPrices""]

        booking = Bookings.query.filter_by(SafeURL=targetBooking).first() 
        alojamiento = Alojamientos.query.filter_by(id=reserva.CodigoAlojamiento).first() #owner of the booking
        room=Rooms.query.filter_by(SafeURL=roomURL).first()
        roomsInBooking=ast.literal_eval(reserva.Habitaciones) #I know, should be json.loads() and json.dumps() for better performance probably...

        #if room is available for given days add it to the booking
        if CheckIfRoomIsAvailableForBooking(alojamiento.id, room, startDate, endDate, booking) == ""OK"":

            roomsInBooking.append({""id"": room.id, ""Prices"": roomPrices, ""Guests"":[]}) #add the new room the Rooms column of the booking
            booking.Habitaciones = str(roomsInBooking)#save the new rooms data
            print(booking.Habitaciones) # check changes applied
            room.ReservaAsociada = booking.id  # associate booking and room
            for ocupante in room.Ocupantes: #associate people in the room with the booking
                ocupante.Reserva = reserva.id

            #db.session.refresh(reserva) # test I made to check if something changes but didn't worked
            if some_X_function() == True: #if user have some functionality enabled
                #db.session.begin() #another test which didn't worked
                RType = WuBook_Rooms.query.filter_by(ParentType=room.Tipo).first()
                RType=[RType] #convert to list because I resuse the function in cases with multiple types
                resultAdd = function4(RType, booking.Entrada.replace(hour=0, minute=0, second=0), booking.Salida.replace(hour=0, minute=0, second=0))
                if resultAdd[""resultado""] == True:  # ""resultado"":error, ""casos"":casos
                    return (jsonify({""resultado"": ""Error"", ""mensaje"": resultAdd[""casos""]}))

            print(booking.Habitaciones) #here I still get expected result
            db.session.commit()
            #I get this return of averything correct in my frontend but not really stored in the database
            return jsonify({""resultado"": ""Ok"", ""mensaje"": ""Room "" + str(room.Identificador) + "" added to the booking""})

        else:
            return (jsonify({""resultado"": ""Error"", ""mensaje"": ""Room "" + str(room.Identificador) + "" not available to book in target dates""}))

    except Exception as e:
        #some error handling which is not getting hit now
        db.session.rollback()
        print(e, "": en linea"", lineno())
        excepcion = str((''.join(traceback.TracebackException.from_exception(e).format()).replace(""\n"",""&lt;/br&gt;""), ""&lt;/br&gt;Excepcion emitida ne la lnea: "", lineno()))
        sendExceptionEmail(excepcion, current_user)
        return (jsonify({""resultado"":""Error"",""mensaje"":""Error""}))

#function from point 4
def function4(RType, startDate, endDate): 
    delta = endDate - startDate
    print(startDate, endDate)
    print(delta)
    for ind_type in RType: 
        calendarUpdated=json.loads(ind_type.updated_availability_by_date)
        calendarUpdatedBackup=calendarUpdated
        casos={}
        actualizar=False
        error=False
        for i in range(delta.days):
            day = (startDate + timedelta(days=i))
            print(day, i)
            diaString=day.strftime(""%d-%m-%Y"")
            if day&gt;=datetime.now() or diaString==datetime.now().strftime(""%d-%m-%Y""): #only care about present and future dates
                disponibilidadLocal=calendarUpdated[diaString][""local""]
                yaReservadas=calendarUpdated[diaString][""local_booked""]
                disponiblesChannel=calendarUpdated[diaString][""avail""]
                #adjust availability data
                if somecondition==True:
                    actualizar=True
                    casos.update({diaString:""Happened X""})
                else:
                    actualizar=False
                    casos.update({diaString:""Happened Y""})
                    error=""Error""
        if actualizar==True: #this part of the code is hit normally and changes stored correctly
            ind_type.updated_availability_by_date=json.dumps(calendarUpdated)
            wubookproperty=WuBook_Properties.query.filter_by(id=ind_type.PropertyCode).first()
            wubookproperty.SyncPending=True
            ind_type.AvailUpdatePending=True
        elif actualizar==False: #some error occured, revert changes
            ind_type.updated_availability_by_date = json.dumps(calendarUpdatedBackup)

    db.session.commit()#this commit persists 
    return({""resultado"":error, ""casos"":casos}) #return to main function with all this chnages stored

",0,1658,"Realized nothing was wrong at session level, it was my fault in another function client side which sends a request to update same data which is just being updated but with the old data... so in fact, I was getting the data saved correctly in the database but overwrote few milliseconds later. It was just a return statement missing in a javascript file to avoid this outcome...
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/54888558,Syntax for late-binding many-to-many self-referential relationship,"I have found many explanations for how to create a self-referential many-to-many relationship (for user followers or friends) using a separate table or class:
Below are three examples, one from Mike Bayer himself:

Many-to-many self-referential relationship in sqlalchemy
How can I achieve a self-referencing many-to-many relationship on the SQLAlchemy ORM back referencing to the same attribute?
Miguel Grinberg's Flask Megatutorial on followers

But in every example I've found, the syntax for defining the primaryjoin and secondaryjoin in the relationship is an early-binding one:
# this relationship is used for persistence
friends = relationship(""User"", secondary=friendship, 
                       primaryjoin=id==friendship.c.friend_a_id,
                       secondaryjoin=id==friendship.c.friend_b_id,
)

This works great, except for one circumstance: when one uses a Base class to define the id column for all of your objects as shown in Mixins: Augmenting the base from the docs
My Base class and followers table are defined thusly:
from flask_sqlchalchemy import SQLAlchemy
db = SQLAlchemy()

class Base(db.Model):
    __abstract__ = True
    id = db.Column(db.Integer, primary_key=True)

user_flrs = db.Table(
    'user_flrs',
    db.Column('follower_id', db.Integer, db.ForeignKey('user.id')),
    db.Column('followed_id', db.Integer, db.ForeignKey('user.id')))

But now I have trouble with the followers relationship that has served me loyally for a while before I moved the id's to the mixin:
class User(Base):
    __table_name__ = 'user'
    followed_users = db.relationship(
        'User', secondary=user_flrs, primaryjoin=(user_flrs.c.follower_id==id),
        secondaryjoin=(user_flrs.c.followed_id==id),
        backref=db.backref('followers', lazy='dynamic'), lazy='dynamic')

db.class_mapper(User)  # trigger class mapper configuration

Presumably because the id is not present in the local scope, though it seems to throw a strange error for that:

ArgumentError: Could not locate any simple equality expressions involving locally mapped foreign key columns for primary join condition 'user_flrs.follower_id = :follower_id_1' on relationship User.followed_users.  Ensure that referencing columns are associated with a ForeignKey or ForeignKeyConstraint, or are annotated in the join condition with the foreign() annotation. To allow comparison operators other than '==', the relationship can be marked as viewonly=True.

And it throws the same error if I change the parentheses to quotes to take advantage of late-binding. I have no idea how to annotate this thing with foreign() and remote() because I simply don't know what sqlalchemy would like me to describe as foreign and remote on a self-referential relationship that crosses a secondary table! I've tried many combinations of this, but it hasn't worked thus far.
I had a very similar (though not identical) problem with a self-referential relationship that did not span a separate table and the key was simply to convert the remote_side argument to a late-binding one. This makes sense to me, as the id column isn't present during an early-binding process.
If it is not late-binding that I am having trouble with, please advise. In the current scope, though, my understanding is that id is mapped to the Python builtin id() and thus will not work as an early-binding relationship.
Converting id to Base.id in the joins results in the following error:

ArgumentError: Could not locate any simple equality expressions involving locally mapped foreign key columns for primary join condition 'user_flrs.follower_id = ""&lt;name unknown&gt;""' on relationship User.followed_users.  Ensure that referencing columns are associated with a ForeignKey or ForeignKeyConstraint, or are annotated in the join condition with the foreign() annotation. To allow comparison operators other than '==', the relationship can be marked as viewonly=True.

",0,387,"You can't use id in your join filters, no, because that's the built-in id() function, not the User.id column.

You have three options:


Define the relationship after creating your User model, assigning it to a new User attribute; you can then reference User.id as it has been pulled in from the base:

class User(Base):
    # ...

User.followed_users = db.relationship(
    User,
    secondary=user_flrs,
    primaryjoin=user_flrs.c.follower_id == User.id,
    secondaryjoin=user_flrs.c.followed_id == User.id,
    backref=db.backref('followers', lazy='dynamic'),
    lazy='dynamic'
)

Use strings for the join expressions. Any argument to relationship() that is a string is evaluated as a Python expression when configuring the mapper, not just the first argument:

class User(Base):
    # ...

    followed_users = db.relationship(
        'User',
        secondary=user_flrs,
        primaryjoin=""user_flrs.c.follower_id == User.id"",
        secondaryjoin=""user_flrs.c.followed_id == User.id"",
        backref=db.backref('followers', lazy='dynamic'),
        lazy='dynamic'
    )

Define the relationships as callables; these are called at mapper configuration time to produce the final object:

class User(Base):
    # ...

    followed_users = db.relationship(
        'User',
        secondary=user_flrs,
        primaryjoin=lambda: user_flrs.c.follower_id == User.id,
        secondaryjoin=lambda: user_flrs.c.followed_id == User.id,
        backref=db.backref('followers', lazy='dynamic'),
        lazy='dynamic'
    )



For the latter two options, see the sqlalchemy.orgm.relationship() documentation:


  Some arguments accepted by relationship() optionally accept a callable function, which when called produces the desired value. The callable is invoked by the parent Mapper at mapper initialization time, which happens only when mappers are first used, and is assumed to be after all mappings have been constructed. This can be used to resolve order-of-declaration and other dependency issues, such as if Child is declared below Parent in the same file*[.]*
  
  [...]
  
  When using the Declarative extension, the Declarative initializer allows string arguments to be passed to relationship(). These string arguments are converted into callables that evaluate the string as Python code, using the Declarative class-registry as a namespace. This allows the lookup of related classes to be automatic via their string name, and removes the need to import related classes at all into the local module space*[.]*
  
  [...]
  
  
  primaryjoin   
  
  [...]
  
  primaryjoin may also be passed as a callable function which is evaluated at mapper initialization time, and may be passed as a Python-evaluable string when using Declarative.
  
  
  [...]
  
  
  secondaryjoin 
  
  [...]
  
  secondaryjoin may also be passed as a callable function which is evaluated at mapper initialization time, and may be passed as a Python-evaluable string when using Declarative.
  


Both the string and the lambda define the same user_flrs.c.followed_id == User.id / user_flrs.c.follower_id == User.id expressions as used in the first option, but because they are given as a string and callable function, respectively, you postpone evaluation until SQLAlchemy needs to have those declarations finalised.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/49154668,SQLAlchemy plus FormAlchemy displays @hybrid_property name and not the returned value,"I have recently updated my SQLAlchemy and FormAlchemy to the newest ones and a strange thing has occurred. 

I do use Mako Templates to display data from my model. 

Python:

class Asset(object):
        id = Column(Integer, primary_key=True, nullable=False)
        ...

        @hybrid_property
        def underscore_name(self):
             return ""x_underscore_name_x""


Mako template:

  &lt;li&gt;Item: ${Asset.underscore_name}&lt;/li&gt;


Before the upgrade the web page rendered text was:

Item: x_underscore_name_x


After the upgrade it shows:

Item: Asset.underscore_name


Important! The method is being executed but the returned result is not being rendered on the webpage. Any ideas?

EDIT:

The library responsible for this behaviour is SQL Alchemy &gt;=1.1.0. Version 1.0.19 does not have this issue. Let's see what's the response from the developers.
",0,254,"By the courtesy of Michael Bayer who commented on reported issue #4214:
hybrid_property name being displayed rather than the returned value

The value of Asset.underscore_name is a SQL expression, which since the changes described in http://docs.sqlalchemy.org/en/latest/changelog/migration_11.html#hybrid-properties-and-methods-now-propagate-the-docstring-as-well-as-info is derived from the clause_element() method of the returned object that Core understands. The hybrid you illustrate above is not correctly written because it is not returning a Core SQL expression object:

    s = Session()
    print(s.query(Asset.underscore_name))

sqlalchemy.exc.InvalidRequestError: SQL expression, column, or mapped entity expected - got ''x_underscore_name_x''


if you are looking for a class-bound attribute to return a string, just assign it:

class Asset(...):
    underscore_name = ""x_underscore_name_x""


or if you want that to be a method that is callable at the classlevel, use a @classproperty, there's a recipe for that here: https://stackoverflow.com/a/5191224/34549 and SQLAlchemy has one you can copy: 
https://github.com/zzzeek/sqlalchemy/blob/master/lib/sqlalchemy/util/langhelpers.py#L1140

IMO there's no bug here because this is not the correct usage of @hybrid_property

Thank you very much Michael!
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/47710463,SQLite: the database is locked,"I'm using flask with sqlalchemy and sqlite db. I have 2 ajax that send some data from html to my .py file.

The problem is every time when i do any of these 2 operations, the second one become unavailable because of lock of db. Also, if first chosen action will be deleting, then exception firing no matter what operation will be chosen after. with first choice of adding, we can add without limitations that's strange too, because functions seem similar.

I've tried timeouts, closing sessions in a different ways, the result is always the same.

Here are two functions-handlers:

app = Flask(__name__)
csrf = CSRFProtect(app)

app.config.from_object('config')
db = SQLAlchemy(app)

import forms
import models


@app.route('/delete', methods = ['GET', 'POST'])
def delete():
    if request.method == ""POST"":
        if request.form['type'] == ""delete"":
            print(""delete"")
            engine = create_engine(SQLALCHEMY_DATABASE_URI)
            Session = sessionmaker(bind=engine)
            session = Session()

            try:
                print(""try"")
                requested = request.form['id']
                print(requested)
                models.Income.query.filter(models.Income.id == requested).delete()
                session.commit()
            except:
                print(""rollback"")
                session.rollback()
            finally:
                print(""fin"")
                session.close()

            ellist = models.Income.query.all()
            return render_template(""incomeSection.html"", list=ellist)


@app.route('/add', methods=['GET', 'POST'])
def add():
    if request.method == ""POST"":
        if request.form['type'] == ""add"":
            print('add')
            engine = create_engine(SQLALCHEMY_DATABASE_URI)
            Session = sessionmaker(bind=engine)
            session = Session()

            try:
                print(""try"")
                newItem = models.Income(name=request.form['name'], tag=request.form['tag'],
                                        account=request.form['account'],
                                        date=date(*(int(i) for i in request.form['date'].split(""-""))))
                session.add(newItem)
                session.commit()
            except:
                print('rollback')
                session.rollback()
            finally:
                print(""fin"")
                session.close()

            ellist = models.Income.query.all()
            print(ellist)
            return render_template(""incomeSection.html"", list=ellist)


I've read that this exception caused by non-closed connections, but I have .close() in every finally block. I think the problem might be because of the db = SQLAlchemy(app) but I don't know how to fix if that is the case. Because I use this variable to connect with db in forms.py where I have the form template and in models.py where I defined my tables within db.
",0,611,"So, aperrently, thete was an issue with number of connections.
The thing that solved my problem was the context manager for sqlalchemy, i used this one:

class SQLAlchemyDBConnection(object):

def __init__(self, connection_string):
    self.connection_string = connection_string
    self.session = None

def __enter__(self):
    engine = create_engine(self.connection_string)
    Session = sessionmaker()
    self.session = Session(bind=engine)
    return self

def __exit__(self, exc_type, exc_val, exc_tb):
    self.session.commit()
    self.session.close()


and in the handler just 

with SQLAlchemyDBConnection(SQLALCHEMY_DATABASE_URI) as db:
            newItem = models.Income(*your params*)
            db.session.add(newItem)


And now it works fine, but i still don't know what was the issue in version that was earlier. They are seem to be same just with or without context manager
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/35247731,more than one row returned by a subquery used as an expression,"All my SQLAlchemy models query just fine, but there is one that gives me:

more than one row returned by a subquery used as an expression


This is the (shortened) SQL generated by SQLAlchemy:

SELECT poi.key
,poi.pok
,poi.noc
,coalesce((
    SELECT CASE 
        WHEN (sum(item.noc) IS NULL)
            THEN NULL
        ELSE :param_1
        END AS anon_1
    FROM item,poi
    WHERE poi.key = item.poi_key
    GROUP BY item.key
    ), poi.noc) AS coalesce_1
,coalesce((
    SELECT sum(soi.noc) AS sum_1
    FROM soi
    WHERE soi.poi_key = poi.key
        AND soi.is_shipped = 0
    ), :param_2) AS coalesce_2
,coalesce((
    SELECT CASE 
        WHEN (sum(item.noc) IS NULL)
            THEN NULL
        ELSE :param_1
        END AS anon_1
    FROM item,poi
    WHERE poi.key = item.poi_key
    GROUP BY item.key
    ), poi.noc) - ee((
    SELECT sum(soi.noc) AS sum_1
    FROM soi
    WHERE soi.poi_key = poi.key
        AND soi.is_shipped = 0
    ), :param_2) AS anon_2
FROM poi


The model is:

class POI(Base):
    key = Column(Integer, primary_key=True)
    pok = Column(Integer, nullable=False)
    noc = Column(Integer, nullable=False)
    __table_args__ = (UniqueConstraint(pok,),
        ForeignKeyConstraint([pok],),{})


I'm using scoped_session since my application is multi-threaded and I thought that could be the problem. But it wasn't. I've tried every variation of using Session, but the problem persist. The strange thing is that upon application initialization, when multiple threads are started that fetch data from the db, only this query throws the error. Invoking only this query (manually) works fine somehow. So the problem is apparently in interaction with other queries.

The error is a bit vague to me, but I think the problem is that a subquery should return one result, not many. I'm at a loss where to start looking for the answer. Is it a threading issue? A Session issue? Something else?
",0,556,"I believe you don't need FROM item,poi in 2 subqueries and I also remove the group by in those too. Regading the case expression involving :param_1 I am not sure if what I suggest below is funcionally correct or not, but you need to create a SUM() value before you can test if that SUM() IS NULL.

SELECT
      poi.key
    , poi.pok
    , poi.noc
    , COALESCE((
            SELECT SUM(COALESCE(item.noc,:param_1))
            FROM item
            WHERE poi.key = item.poi_key
      )
      , poi.noc) AS coalesce_1
    , COALESCE((
            SELECT
                  SUM(soi.noc) AS sum_1
            FROM soi
            WHERE soi.poi_key = poi.key
                  AND soi.is_shipped = 0
      )
      , @param_2) AS coalesce_2
    , COALESCE((
            SELECT SUM(COALESCE(item.noc,:param_1))
            FROM item
            WHERE poi.key = item.poi_key
      )
      , poi.noc) - ee((
            SELECT
                  SUM(soi.noc) AS sum_1
            FROM soi
            WHERE soi.poi_key = poi.key
                  AND soi.is_shipped = 0
      )
      , :param_2) AS anon_2
FROM poi

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/24375722,sqlalchemy: dividing timedelta column by float giving strange results,"I'm trying to perform some manipulation on a column inside the query. I want to divide a timedelta column by 60. Unfortunately it is giving strange results.

In my query I return two datetimes, the timedelta between these two datetimes, and the timedelta between these two datetimes divided by 2. An example of the output:

(datetime.datetime(2013, 1, 1, 0, 0, 8), 
datetime.datetime(2013, 1, 1, 0, 7, 32), 
datetime.timedelta(0, 444), 
Decimal('362.0000'))


However, the timedelta 444 divided by 2 does not equal 362.0000. But this is what sqlalchemy is giving me. Is there anyway to get the correct result?

Its important to do this manipulation in the query because I am also trying to use the above as a subquery and average over the results but I'm getting strange averages back as well.

Here is my query:

    sub = session.query( 
                        merge.c.time_received,
                        merge.c.time_sent,
                        func.timediff(
                            merge.c.time_sent,
                            merge.c.time_received
                        ),
                        func.timediff(
                            merge.c.time_sent,
                            merge.c.time_received
                        ) / 2.0
                    ). \
                    filter(
                        merge.c.time_received != None,
                        merge.c.time_sent != None,
                    ). \
                    limit(1)

",0,589,"I am not sure what happens when you divide TIME by a number. But maybe converting it to seconds first using TIME_TO_SEC(time) will do the trick.

I assumed mysql` is used.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/17787042,SqlAlchemy connection string,"I faced very strange issue - my solution using sqlalchemy cannot connects to database. It depends on password I am using. for example, following records are working perfect:

PWD='123123123'
USR='test_user';
SQLALCHEMY_DATABASE_URI = 'mysql://{}:{}@localhost:3306/test_db'.format(USR, PWD)

#final result is 'mysql://test_user:123123123@localhost:3306/test_db'.format(USR, PWD)


But when I trying to put something serious to password (like '8yq+aB&amp;k$V') connection failed. How to 'escape' or encode password somehow that sqlalchemy passed it to mysql succesfully?
",12,36592,,,
SQLAlchemy strange issue,https://stackoverflow.com/questions/25768428,SQLAlchemy connection errors,,9,5967,,,
SQLAlchemy strange issue,https://stackoverflow.com/questions/14470688,SQLAlchemy Bidirectional Relationship association proxy,,8,2495,,,
SQLAlchemy strange issue,https://stackoverflow.com/questions/41514960,"Use DB data model to generate SQLAlchemy models, schemas, and JSON response","Using Flask and SQLAlchemy for a Python webapp, my goal is to create a system in which I can:


Import data models from an existing PostgreSQL DB, and map these to fields in corresponding SQLAlchemy models
Use these SQLAlchemy models to automatically generate a schema. This schema will then be used to perform data validation on user-submitted data.(I'm currently trying to use Marshmallow, but am open to other suggestions).
Perform JSON response formatting using the schema generated in step 2. (I'm currently trying to format my responses according to JsonAPI's schema - this could change if need be, but I would prefer it).


All of this will be packaged together in a single layer that should allow for simple data access, validation, and response formatting when writing an API, hopefully without ever having to manually define data models or schemas past the definitions that already exist in the DB. This leads me to my question:

Can I utilize existing frameworks to do everything that I'm trying to accomplish? I wouldn't expect that there's one single library that could do everything, but I'm hoping to be able to leverage several existing frameworks. However, I run into direct conflicts, particularly in steps 2 and 3. The stack I've been attempting to use so far is as follows:


Flask (My web framework)
SQLAlchemy (Used to access my data through reflection, see below)
Flask-Marshmallow (To generate a schema for data validation)
Marshmallow-JsonAPI (To format the JSON response according to the JsonAPI spec)


I have a solution for step 1: SQLAlchemy's reflection. This allows me to read the tables in an existing DB and map them to SQLAlchemy models. This works beautifully.

The combination of steps 2 and 3 is where it gets hazy - I'm trying to use Flask-Marshmallow and Marshmallow-JsonAPI. Flask-Marshmallow has the ModelSchema class, which allows you to generate a schema by passing it an existing SQLAlchemy Model. I can pass it my SQLAlchemy model that was generated in step 1, fulfilling my criteria for step 2. As for Marshmallow-JsonAPI, it has functionality such that you can define a schema for your model, and it will automatically create JSONAPI-compliant responses, fulfilling my criteria for step 3. I can individually get each of these frameworks to do what I need.

Unfortunately, since my schema has to inherit from Schema classes in both the Flask-Marshmallow and Marshmallow-JSONAPI, I run into issues. My Model for a ""Dummy"" class looks like this:

import flask
from marshmallow_jsonapi import Schema, fields
from sqlalchemy.ext.declarative import DeferredReflection
from flask_marshmallow import Marshmallow
from flask_sqlalchemy import SQLAlchemy

app = flask.Flask(__name__)
DB = SQLAlchemy()
DB.init_app(app)
MA = Marshmallow(app)
DeferredReflection.prepare(DB.get_engine(app))

class Dummy(DeferredReflection, DB.Model):
    __tablename__ = ""dummy""  # This model will be reflected from the Dummy table


class DummySchema(MA.ModelSchema, Schema):  # The problem line
    class Meta:
        model = Dummy  # Create schema for the Dummy model


Upon trying to start up my API, I get strange errors like this:

api_1        | [2017-01-05 23:16:13,379] ERROR in app: Exception on /api/dummy [POST]
api_1        | Traceback (most recent call last):
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1612, in full_dispatch_request
api_1        |     rv = self.dispatch_request()
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1598, in dispatch_request
api_1        |     return self.view_functions[rule.endpoint](**req.view_args)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask_restful/__init__.py"", line 477, in wrapper
api_1        |     resp = resource(*args, **kwargs)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask/views.py"", line 84, in view
api_1        |     return self.dispatch_request(*args, **kwargs)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask_restful/__init__.py"", line 587, in dispatch_request
api_1        |     resp = meth(*args, **kwargs)
api_1        |   File ""/dummyproj/api/src/dummyproj/api/v1/dummy_resource.py"", line 34, in post
api_1        |     dummy_schema = dummy.DummySchema()
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow_sqlalchemy/schema.py"", line 143, in __init__
api_1        |     super(ModelSchema, self).__init__(*args, **kwargs)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow_jsonapi/schema.py"", line 81, in __init__
api_1        |     super(Schema, self).__init__(*args, **kwargs)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow/schema.py"", line 358, in __init__
api_1        |     self._update_fields(many=many)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow/schema.py"", line 750, in _update_fields
api_1        |     self.__set_field_attrs(ret)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow/schema.py"", line 772, in __set_field_attrs
api_1        |     self.on_bind_field(field_name, field_obj)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow_jsonapi/schema.py"", line 164, in on_bind_field
api_1        |     field_obj.load_from = self.inflect(field_name)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow_jsonapi/schema.py"", line 190, in inflect
api_1        |     return self.opts.inflect(text) if self.opts.inflect else text
api_1        | AttributeError: 'SchemaOpts' object has no attribute 'inflect'


It seems that since my Schema class inherits from two different superclasses, it causes issues. I would expect the two marshmallow libraries to be pretty compatible, but there does seem to be an issue with this.

The upshot of all of this is that I'm not quite sure what to try next. I'm not really expecting any specific advice with the Marshmallow frameworks, more just trying to show my train of thought. Is there any sort of best practice surrounding this sort of design, or am I trying to solve too much at once?

(New to both Python and SO - apologies if any of this is unclear).
",7,2392,"I'm successfully combining marshmallow-jsonapi + marshmallow-sqlalchemy. Here's the magic:

import marshmallow
import marshmallow_jsonapi
import marshmallow_jsonapi.flask
import marshmallow_sqlalchemy

import myapp.database as db


def make_jsonapi_schema_class(model_class):
    class SchemaOpts(marshmallow_sqlalchemy.SQLAlchemyAutoSchemaOpts, marshmallow_jsonapi.SchemaOpts):
        pass

    class Schema(marshmallow_sqlalchemy.SQLAlchemyAutoSchema, marshmallow_jsonapi.flask.Schema):
        OPTIONS_CLASS = SchemaOpts

        @marshmallow.post_load
        def make_instance(self, data, **kwargs):
            # Return deserialized data as a dict, not a model instance
            return data

        # You can add default behavior here, for example
        # id = fields.Str(dump_only=True)

    # https://marshmallow-sqlalchemy.readthedocs.io/en/latest/recipes.html#automatically-generating-schemas-for-sqlalchemy-models
    class Meta:
        # Marshmallow-SQLAlchemy
        model = model_class
        sqla_session = db.session

        # Marshmallow-JSONAPI
        type_ = model_class.__name__.lower()
        self_view = type_ + '_detail'
        self_view_kwargs = {'id': '&lt;id&gt;'}
        self_view_many = type_ + '_list'

    schema_class = type(model_class.__name__ + 'Schema', (Schema,), {'Meta': Meta})
    return schema_class


You then call

FooSchema = make_jsonapi_schema_class(Foo)


to generate the Marshmallow schema class from your SQLAlchemy declarative model. You can then in turn subclass that class if you want to customize it.

(To implement the REST API, I'm using Flask-REST-JSONAPI, which is built on top of Flask + SQLAlchemy + marshmallow-jsonapi.)
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/63435264,&quot;Maximum number of parameters&quot; error with filter .in_(list) using pyodbc,"One of our queries that was working in Python 2 + mxODBC is not working in Python 3 + pyodbc; it raises an error like this: Maximum number of parameters in the sql query is 2100. while connecting to SQL Server. Since both the printed queries have 3000 params, I thought it should fail in both environments, but clearly that doesn't seem to be the case here. In the Python 2 environment, both MSODBC 11 or MSODBC 17 works, so I immediately ruled out a driver related issue.
So my question is:

Is it correct to send a list as multiple params in SQLAlchemy because the param list will be proportional to the length of list? I think it looks a bit strange; I would have preferred concatenating the list into a single string because the DB doesn't understand the list datatype.
Are there any hints on why it would be working in mxODBC but not pyodbc? Does mxODBC optimize something that pyodbc does not? Please let me know if there are any pointers - I can try and paste more info here. (I am still new to debugging SQLAlchemy.)

Footnote: I have seen lot of answers that suggest to chunk the data, but because of 1 and 2, I wonder if I am doing the correct thing in the first place.
(Since it seems to be related to pyodbc, I have raised an internal issue in the official repository.)
import sqlalchemy
import sqlalchemy.orm

from sqlalchemy import MetaData, Table
from sqlalchemy.ext.declarative import declarative_base

from  sqlalchemy.orm.session import Session

Base = declarative_base()

create_tables = """"""
CREATE TABLE products(
    idn NUMERIC(8) PRIMARY KEY
);
""""""

check_tables = """"""   
SELECT * FROM products;
""""""

insert_values = """"""
INSERT INTO products
(idn)
values
(1),
(2);
""""""

delete_tables = """"""
DROP TABLE products;
""""""

engine = sqlalchemy.create_engine('mssql+pyodbc://user:password@dsn')
connection = engine.connect()
cursor = engine.raw_connection().cursor()
Session = sqlalchemy.orm.sessionmaker(bind=connection)
session = Session()

session.execute(create_tables)

metadata = MetaData(connection)

class Products(Base):
   __table__ = Table('products', metadata, autoload=True)

try:
    session.execute(check_tables)
    session.execute(insert_values)
    session.commit()
    query = session.query(Products).filter(
        Products.idn.in_(list(range(0, 3000)))
    )
    query.all()
    f = open(""query.sql"", ""w"")
    f.write(str(query))
    f.close()
finally:
    session.execute(delete_tables)
    session.commit()

",5,1352,"When you do a straightforward .in_(list_of_values) SQLAlchemy renders the following SQL ...
SELECT team.prov AS team_prov, team.city AS team_city 
FROM team 
WHERE team.prov IN (?, ?)

... where each value in the IN clause is specified as a separate parameter value. pyodbc sends this to SQL Server as ...
exec sp_prepexec @p1 output,N'@P1 nvarchar(4),@P2 nvarchar(4)',N'SELECT team.prov AS team_prov, team.city AS team_city, team.team_name AS team_team_name 
FROM team 
WHERE team.prov IN (@P1, @P2)',N'AB',N'ON'

... so you hit the limit of 2100 parameters if your list is very long. Presumably, mxODBC inserted the parameter values inline before sending it to SQL Server, e.g.,
SELECT team.prov AS team_prov, team.city AS team_city 
FROM team 
WHERE team.prov IN ('AB', 'ON')

You can get SQLAlchemy to do that for you with
provinces = [""AB"", ""ON""]
stmt = (
    session.query(Team)
    .filter(
        Team.prov.in_(sa.bindparam(""p1"", expanding=True, literal_execute=True))
    )
    .statement
)
result = list(session.query(Team).params(p1=provinces).from_statement(stmt))

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/24231535,Flask-SQLAlchemy Can&#39;t Create Relationship - SAWarning,,3,650,,,
SQLAlchemy strange issue,https://stackoverflow.com/questions/39298822,SQLAlchemy secondary join model fails under strange conditions,,3,505,"The reason you get this error is that you've inadvertently added objects into the session.

Here is the MVCE:

engine = create_engine(""sqlite://"", echo=False)


def get_data():
    children = [
        Child(name=""Carol"", versions=[ChildVersion(age=0, fav_toy=""med"")]),
        Child(name=""Timmy"", versions=[ChildVersion(age=0, fav_toy=""med"")]),
    ]
    return Parent(
        name=""Zane"", children=children,
        versions=[
            ParentVersion(
                address=""123 Fake St"",
                children=[v for child in children for v in child.versions]
            )
        ]
    )


def main():
    Base.metadata.create_all(engine)

    session = Session(engine)
    parent_match = get_data()
    session.add(parent_match)
    session.commit()

    with session.no_autoflush:
        search_parent = get_data()

        parent_match.versions.append(search_parent.current_version)
        for search_child in search_parent.children[:]:
            child_match = next(c for c in parent_match.children if c.name == search_child.name)

            if child_match.current_version != search_child.current_version:
                child_match.versions.append(search_child.current_version)
            else:
                session.expunge(search_child.current_version)

            session.expunge(search_child)

        session.expunge(search_parent)
        session.commit()


Aside: this is what you needed to provide in the question itself. Providing a tarball with instructions is not the best way to get answers.

The line

parent_match.versions.append(search_parent.current_version)


not only adds search_parent.current_version, it also adds search_parent, which in turn adds all related objects, including the child versions of other children. Judging by the fact that you later expunge other related objects to prevent them from being added to the session, I conclude that you only want to add search_parent.current_version without adding other related objects. Due to the circular nature of your relationships you need to take care to lift only the objects you want out of search_parent before you add them. Here is the fixed MVCE:

with session.no_autoflush:
    search_parent = get_data()

    current_parent_version = search_parent.current_version
    search_parent.versions.remove(current_parent_version)
    current_parent_version.children = []  # &lt;--- this is key
    for search_child in search_parent.children[:]:
        child_match = next(c for c in parent_match.children if c.name == search_child.name)

        if child_match.current_version != search_child.current_version:
            current_child_version = search_child.current_version
            search_child.versions.remove(current_child_version)
            child_match.versions.append(current_child_version)
            current_parent_version.children.append(current_child_version)

    parent_match.versions.append(current_parent_version)

    session.commit()

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/66896139,Flask-Executor and Flask-SQLAlchemy: Can&#39;t get updated data from DB inside of executor function,,3,437,"use update
Project.query.filter_by(id=project_id).update({
 'last_activity': datetime.utcnow()
})

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/41240754,Why can I import certain modules in Python only with administrator rights?,"I'm struggling with some strange issues in Python 2.7. I wrote a very long tool where I import different modules, which I had to install first using pip. The tool is to be shared within the company, where different users have different rights on their specific machines.
The problem occurred when another user logged into my machine (I'm having administrator rights there) and tried to use the tool. He was unable to run it, because specific modules could not be imported because of his status as a ""non-admin"". 

The error message is simply ""No module named XY"".
When we looked into the file system, we found that we were not able to look into the folder where the module had been installed, simply because the access was denied by the system. 
We also got this error message when trying to run pip from the cmd; it prints ""Access denied"" and won't do anything.

How is it possible, that some modules can be accessed by anyone, while others can't? And how can I get around this problem?

Specifically, I'm talking about sqlalchemy and pyodbc.

Thanks a lot in advance.

EDIT 1: Oh, and we're talking about Windows here, not Linux...

EDIT 2: Due to company policy it is not possible to set administrator permissions to all users. I tried, as suggested, but it didn't work and I learned that it's not possible within the company.
",3,4816,"Got it...

Following the advice of Nabeel Ahmed, I first uninstalled the packages which caused the issues from my admin account. Then I changed the script to

pip install --user {module_name}


and voila... it works for all users now.

Thanks a lot for you help, guys!
","One simply solution is set permissions for site-package directory (where the packages gt installed) as per usable by all i.e. read and execute permission for all on the directory:

sudo chmod -Rv ugo+rX /usr/lib/python2.7/site-packages/


Also for the lib64 packages - the path to site-packages may vary for various Linux distros.



Edit 1: For windows look into this 'File and Folder Permissions' for setting Read &amp; Execute permissions for all, for a file or folder (i.e. site-packges) 

The path 'd be - C:\Python27\Lib\site-packages



Edit 2: in apropos of:


  EDIT 2: Due to company policy it is not possible to set administrator
  permissions to all users. I tried, as suggested, but it didn't work
  and I learned that it's not possible within the company.


if so, simply install sqlalchemy (or any other package) for specific user using pip:

pip install --user {module_name} 


Source: Per user site-packages directory.
","You should either use virtualenv as stated before or set the proper permissions to the site-packages folder. I should be in C:\Python27\Lib.
"
SQLAlchemy strange issue,https://stackoverflow.com/questions/19963083,SQLAlchemy + SQLite Locking in IPython Notebook,"I'm getting an OperationalError: (OperationalError) database is locked error when connection via SQLAlchemy in an IPython notebook instance and I'm not sure why.

I've written a Python interface to a SQLite database using SQLAlchemy and the Declarative Base syntax. I import the database models into an IPython notebook to explore the data. This worked just fine this morning. Here is the code:

from psf_database_interface import session, PSFTable
query = session.query(PSFTable).first()


But this afternoon after I closed my laptop with IPython running (it restarts the server just fine) I started getting this error. It's strange because I can still open the database from the SQLite3 command line tool and query data. I don't expect any other processes to be connecting to this database and running fuser on the database confirms this. My application is not using any concurrent processes (in the code I've written, IDK if something is buried in SQLAlchemy or IPython), and even if it were I'm just doing a read operation, which SQLite does support concurrently.

I've tried restarting the IPython kernel as well as killing and restarting the IPython notebook server. I've tried creating a backup of the database and replacing the database with the backup as suggested here: https://stackoverflow.com/a/2741015/1216837. Lastly, out of desperation, I tried adding the following to see if I could clean out something stuck in the session somehow:

print session.is_active
session.flush()
session.close()
session.close_all()
print session.is_active


Which returns True and True. Any ideas?

Update: I can run the code snippet that is causing errors from a python file without any issues, the issue only occurs in IPython.
",3,2057,"I faced the same problem. I can run python scripts but the IPython raise the below exception.

You need to check with fuser there is no process which is using this. But if you cannot find anything and your history of commands are not important to you, you can use the following workaround.

When I deleted the /home/my_user/.ipython/profile_default/history.sqlite file, I can start the IPython. The history is empty as I mentioned above.

    $ ipython                                                             
    [TerminalIPythonApp] ERROR | Failed to create history session in /home/my_user/.ipython/profile_default/history.sqlite. History will not be saved.
    Traceback (most recent call last):
    File ""/home/esadrfa/libs/anaconda3/lib/python3.6/site-packages/IPython/core/history.py"", line 543, in __init__
        self.new_session()
    File ""&lt;decorator-gen-22&gt;"", line 2, in new_session
    File ""/home/esadrfa/libs/anaconda3/lib/python3.6/site-packages/IPython/core/history.py"", line 58, in needs_sqlite
        return f(self, *a, **kw)
    File ""/home/esadrfa/libs/anaconda3/lib/python3.6/site-packages/IPython/core/history.py"", line 570, in new_session
        self.session_number = cur.lastrowid
    sqlite3.OperationalError: database is locked
    [TerminalIPythonApp] ERROR | Failed to open SQLite history :memory: (database is locked).

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/56317578,SQLAlchemy does not update/expire model instances with external changes,,2,1378,"I ran into a very similar situation with MySQL. I needed to ""see"" changes to the table that were coming from external sources in the middle of my code's database operations. I ended up having to set autocommit=True in my session call and use the begin() / commit() methods of the session to ""see"" data that was updated externally.

The SQLAlchemy docs say this is a legacy configuration:


  Warning
  
  autocommit mode is a legacy mode of use and should not be considered for new projects.


but also say in the next paragraph:


  Modern usage of autocommit mode tends to be for framework integrations that wish to control specifically when the begin state occurs


So it doesn't seem to be clear which statement is correct.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/39542204,Intermittent &quot;ORA-01458: invalid length inside variable character string&quot; error when inserting data into Oracle DB using SQLAlchemy,"I am working on a Python 3.5 server project and using SQLAlchemy 1.0.12 with cx_Oracle 5.2.1 to insert data into Oracle 11g. I noticed that many of my multi-row table insertions are failing intermittently with ""ORA-01458: invalid length inside variable character string"" error. 

I generally insert a few thousand to a few tens of thousands of rows at a time, and the data is mostly composed of strings, Pandas timestamps, and floating point numbers. I have made the following observations:


The error occurs on both Windows and Linux host OS for the Python server 
The error always occurs intermittently, even when the data doesn't change
If I don't insert floating point numbers, or if I round them, the error happens less often but still happens
If I insert the rows one at a time I don't encounter the error (but this is unacceptable for me performance-wise)


Additionally, I have tried to insert again if I encountered the error. The first thing I tried was to was put a try-except block around where I call execute on the sqlalchemy.engine.base.Connection object like the following:

try:
    connection.execute(my_table.insert(), records)
except DatabaseError as e:
    connection.execute(my_table.insert(), records)


I noticed that using this method the second insertion still often fails. The second thing I tried was to try the same in the implementation of do_executemany of OracleDialect_cx_oracle in the sqlalchemy package (sqlalchemy\dialects\oracle\cx_oracle.py):

def do_executemany(self, cursor, statement, parameters, context=None):
    if isinstance(parameters, tuple):
        parameters = list(parameters)

    # original code
    # cursor.executemany(statement, parameters)

    # new code
    try:
        cursor.executemany(statement, parameters)
    except Exception as e:
        print('trying again')
        cursor.executemany(statement, parameters)


Strangely, when I do it this way the second executemany call will always work if the first one fails. I'm not certain what this means but I believe this points to the cx_Oracle driver being the cause of the issue instead of sqlalchemy. 

I have searched everywhere online and have not seen any reports of the same problem. Any help would be greatly appreciated!
",2,3213,"I had the same problem, the code would some times fail and some times go through with no error. Apparently my chunk size was to big for buffer, the error did not occur anymore once I reduced the chunk size from 10K to 500 rows.
","We found out that if we replace all the float.nan objects with None before we do the insertion the error disappears completely. Very weird indeed!
",
SQLAlchemy strange issue,https://stackoverflow.com/questions/33161280,SQLAlchemy instantiate object from ORM fails with AttributeError: mapper,"I've been trying to get a decent-sized project going with SQLAlchemy on the backend. I have table models across multiple files, a declarative base in its own file, and a helper file to wrap common SQLAlchemy functions, and driver file.

I was uploading data, then decided to add a column. Since this is just test data I thought it'd be simplest to just drop all of the tables and start fresh... then when I tried to recreate the schema and tables, the common declarative base class suddenly had empty metadata. I worked around this by importing the class declaration files -- strange, since I didn't need those imports before -- and it was able to recreate the schema successfully.

But now when I try to create objects again, I get an error:

AttributeError: mapper


Now I'm totally confused! Can someone explain what's happening here? It was working fine before I dropped the schema and now I can't get it working.

Here's the skeleton of my setup:

base.py

from sqlalchemy.ext.declarative import declarative_base
Base = declarative_base()


models1.py

from base import Base
class Business(Base):
    __tablename__ = 'business'
    id = Column(Integer, primary_key=True)


models2.py:

from base import Base
class Category(Base):
    __tablename__ = 'category'
    id = Column(Integer, primary_key=True)


helper.py:

from base import Base

# I didn't need these two imports the first time I made the schema
# I added them after I was just getting an empty schema from base.Base
# but have no idea why they're needed now?
import models1
import models2

def setupDB():
    engine = getDBEngine(echo=True) # also a wrapped func (omitted for space)
    #instantiate the schema
    try:
        Base.metadata.create_all(engine, checkfirst=True)
        logger.info(""Successfully instantiated Database with model schema"")
    except:
        logger.error(""Failed to instantieate Database with model schema"")
        traceback.print_exc()

def dropAllTables():
    engine = getDBEngine(echo=True)
    # drop the schema
    try:
        Base.metadata.reflect(engine, extend_existing=True)
        Base.metadata.drop_all(engine)
        logger.info(""Successfully dropped all the database tables in the schema"")
    except:
        logger.error(""Failed to drop all tables"")
        traceback.print_exc()


driver.py:

import models1
import models2

# ^ some code to get to this point
categories []
categories.append(
                models2.Category(alias=category['alias'],
                                 title=category['title']) # error occurs here
                )


stack trace: (for completeness)

File ""./main.py"", line 16, in &lt;module&gt;
yelp.updateDBFromYelpFeed(fname)
  File ""/Users/thomaseffland/Development/projects/health/pyhealth/pyhealth/data/sources/yelp.py"", line 188, in updateDBFromYelpFeed
    title=category['title'])
  File ""&lt;string&gt;"", line 2, in __init__
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/instrumentation.py"", line 347, in _new_state_if_none
    state = self._state_constructor(instance, self)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 747, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/instrumentation.py"", line 177, in _state_constructor
    self.dispatch.first_init(self, self.class_)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/event/attr.py"", line 256, in __call__
    fn(*args, **kw)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/mapper.py"", line 2825, in _event_on_first_init
    configure_mappers()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/mapper.py"", line 2721, in configure_mappers
    mapper._post_configure_properties()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/mapper.py"", line 1710, in _post_configure_properties
    prop.init()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/interfaces.py"", line 183, in init
    self.do_init()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/relationships.py"", line 1616, in do_init
    self._process_dependent_arguments()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/relationships.py"", line 1673, in     _process_dependent_arguments
    self.target = self.mapper.mapped_table
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 833, in __getattr__
    return self._fallback_getattr(key)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 811, in _fallback_getattr
    raise AttributeError(key)
AttributeError: mapper


I know this post is long, but I wanted to give the complete picture. First I am confused why the base.Base schema was empty in the first place. Now I am confused why the Categories object is missing a mapper!

Any help/insight/advice is greatly appreciated, thanks!

Edit:

So the model files and helper.py are in a supackage and the driver.py is actually a file in a sibling subpackage and its code is wrapped in a function.  This driver function is called by a package-level main file.  So I don't think it can be because SQLAlchemy hasn't had time to initialize? (If I understand the answer correctly)  here is what the (relevant part of) main file looks like:

main.py:

import models.helper as helper
helper.setupDB(echo=true) # SQLAlchemy echos the correct statements

import driverpackage.driver as driver
driver.updateDBFromFile(fname) # error occurs in here


and driver.py actually looks like:

import ..models.models1
import ..models.models2

def updateDBFromFile(fname):
    # ^ some code to get to this point
    categories []
    categories.append(
                    models2.Category(alias=category['alias'],
                                     title=category['title']) # error occurs here
                    )
    # a bunch more code


Edit 2:
I'm beginning to suspect the underlying issue is the same reason I suddenly need to import all of the models to set up the schema in helper.py.  If I print the tables of the imported model objects, they have no bound MetaData or schema:

print YelpCategory.__dict__['__table__'].__dict__
####
{'schema': None, '_columns': &lt;sqlalchemy.sql.base.ColumnCollection object at 0x102312ef0&gt;, 
'name': 'yelp_category', 'description': 'yelp_category', 
'dispatch': &lt;sqlalchemy.event.base.DDLEventsDispatch object at 0x10230caf0&gt;, 
'indexes': set([]), 'foreign_keys': set([]), 
'columns': &lt;sqlalchemy.sql.base.ImmutableColumnCollection object at 0x10230fc58&gt;, 
'_prefixes': [], 
'_extra_dependencies': set([]), 
'fullname': 'yelp_category', 'metadata': MetaData(bind=None), 
'implicit_returning': True, 
'constraints': set([PrimaryKeyConstraint(Column('id', Integer(), table=&lt;yelp_category&gt;, primary_key=True, nullable=False))]), 'primary_key': PrimaryKeyConstraint(Column('id', Integer(), table=&lt;yelp_category&gt;, primary_key=True, nullable=False))}


I wonder why the metadata from the base that created the database is not gettng bound?
",2,2340,"The question seems to have died off, so I'll post my work around.  It's simple.  I refactored the code to explicitly supply the classes and the classic mappers, instead of using the declarative base and everything worked fine again...
","I guess the error happens because you are executing your code on Python module-level. This code is executed when Python imports the module.


Move your code to a function.
Call the function after SQLAlchemy has been properly initialized.
create_all() needs to call only once when the application is installed, because created tables persistent in the database
You need DBSession.configure(bind=engine) or related which will tell models to which database connection they are related. This is missing from the question.

",
SQLAlchemy strange issue,https://stackoverflow.com/questions/64855545,SQLalchemy rowcount always -1 for statements,,1,1388,"The single-row INSERT  VALUES (  ) is trivial: If the statement succeeds then one row was affected, and if it fails (throws an error) then zero rows were affected.
For a multi-row INSERT simply perform it inside a transaction and rollback if an error occurs. Then the number of rows affected will either be zero or len(values_list).
To get the number of rows that a SELECT will return, wrap the select query in a SELECT count(*) query and run that first, for example:
select_stmt = sa.select([Parent])
count_stmt = sa.select([sa.func.count(sa.text(""*""))]).select_from(
    select_stmt.alias(""s"")
)
with engine.connect() as conn:
    conn.execution_options(isolation_level=""SERIALIZABLE"")
    rows_found = conn.execute(count_stmt).scalar()
    print(f""{rows_found} row(s) found"")
    results = conn.execute(select_stmt).fetchall()
for item in results:
    print(item.id)

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/65100066,SQLAlchemy loads unrelated cached objects when using contains_eager,,1,551,"I asked this question on the SQLAlchemy GitHub page. The solution is to use populate_existing on any query that uses contains_eager and filter. In my specific example, this query does the right thing
session \
        .query(Parent) \
        .join(Child, Parent.id == Child.parent_id) \
        .options(
            contains_eager(Parent.children)
        ).filter(Child.value &lt; 15) \
        .order_by(Parent.id) \
        .populate_existing() \
        .all()

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/67975069,Phantom Queries in PyMSQL + SQLAlchemy on Lambda,,1,161,"snakecharmerb and Ilja Everil provided a few answers in the comments, all of which work. Thanks!

Upgrade SQLAlchemy to version 1.4
Don't use pandas to execute the SQL.
Use pandas read_sql_query function instead of read_sql.

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/59093282,Why is Oracle Pivot producing non-existent results?,,1,72,"It turns out that Nathan's comment was correct. Though it seems counter-intuitive (to me, at least), it appears that calling ResultSet.getString on a DATE column will in fact convert to Timestamp first. Timestamp has the unfortunate default behavior of using the system default timezone unless you specify otherwise explicitly.

This default behavior meant that daylight saving's time was taken into account when we didn't intend it to be, leading to the odd behavior described.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/57014580,Can&#39;t access sqlalchemy database from heroku. Strange error about some &#39;user&#39; model that I don&#39;t have not existing,"I have an SQLAlchemy database running on a flask app that works fine locally, but as soon as I run it on Heroku I get a strange issue (See stack trace below).

I don't have a model called 'user' so I don't know what's going on. I've made sure that I've run db.create_all().

Creating a row in my table works fine, but it's only once I try selecting some rows do I get an issue (__init__.py):

@app.route('/blog')
def blog():
    title = 'Blog'
    name = get_name()

    blog_text = []

    for post in sorted(Post.query.filter_by(category=Post.Category.blog),
                       key=lambda p: p.posted_at, reverse=True):
        content = post.content

        blog_text.append(content)

    content = '\n'.join(blog_text)

    return render_template('blog.html', **locals())


And my 'Post' logic in my models.py:


class Post(db.Model):
    __tablename__ = 'user'

    id = db.Column(db.Integer, primary_key=True)

    name = db.Column(db.String(200), nullable=False)
    posted_at = db.Column(db.DateTime, nullable=False, default=datetime.now())
    content = db.Column(db.String(200), nullable=False)
    content_type = db.Column(db.String(200), default=""md"", nullable=False)  # md / html
    category = db.Column(db.String(200), nullable=False, default=""blog"")  # poetry / blog

    class Type:
        md = ""md""
        html = ""html""

    class Category:
        poetry = ""poetry""
        blog = ""blog""

   ...



Here's the error:

2019-07-12T21:59:09.120100+00:00 app[web.1]: 10.63.22.102 - - [12/Jul/2019:21:59:09 +0000] ""GET /blog HTTP/1.1"" 500 290 ""https://secure-savannah-20745.herokuapp.com/"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""
2019-07-12T21:59:09.120443+00:00 heroku[router]: at=info method=GET path=""/blog"" host=secure-savannah-20745.herokuapp.com request_id=c32b2484-c8ff-4910-a4e2-5089eca7c6d3 fwd=""98.0.144.26"" dyno=web.1 connect=2ms service=66ms status=500 bytes=455 protocol=https
2019-07-12T21:59:13.632588+00:00 app[web.1]: 10.63.22.102 - - [12/Jul/2019:21:59:13 +0000] ""GET /projects HTTP/1.1"" 200 2342 ""https://secure-savannah-20745.herokuapp.com/"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""
2019-07-12T21:59:13.837771+00:00 app[web.1]: 10.63.22.102 - - [12/Jul/2019:21:59:13 +0000] ""GET /static/circles.js HTTP/1.1"" 200 0 ""https://secure-savannah-20745.herokuapp.com/projects"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""
2019-07-12T21:59:13.838434+00:00 heroku[router]: at=info method=GET path=""/static/circles.js"" host=secure-savannah-20745.herokuapp.com request_id=8476c020-fc32-47b3-a4e1-7b6faeb115ca fwd=""98.0.144.26"" dyno=web.1 connect=1ms service=6ms status=200 bytes=5449 protocol=https
2019-07-12T21:59:13.633394+00:00 heroku[router]: at=info method=GET path=""/projects"" host=secure-savannah-20745.herokuapp.com request_id=faa3d473-7bda-4e39-810b-e3994eac7390 fwd=""98.0.144.26"" dyno=web.1 connect=1ms service=1494ms status=200 bytes=2504 protocol=https
2019-07-12T21:59:18.074035+00:00 heroku[router]: at=info method=GET path=""/blog"" host=secure-savannah-20745.herokuapp.com request_id=e9748e89-dcce-43db-8db8-a16c7fe657ab fwd=""98.0.144.26"" dyno=web.1 connect=3ms service=12ms status=500 bytes=455 protocol=https
2019-07-12T21:59:18.070372+00:00 app[web.1]: [2019-07-12 21:59:18,069] ERROR in app: Exception on /blog [GET]
2019-07-12T21:59:18.070385+00:00 app[web.1]: Traceback (most recent call last):
2019-07-12T21:59:18.070396+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1244, in _execute_context
2019-07-12T21:59:18.070398+00:00 app[web.1]: cursor, statement, parameters, context
2019-07-12T21:59:18.070403+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 550, in do_execute
2019-07-12T21:59:18.070405+00:00 app[web.1]: cursor.execute(statement, parameters)
2019-07-12T21:59:18.070407+00:00 app[web.1]: sqlite3.OperationalError: no such table: user
2019-07-12T21:59:18.070410+00:00 app[web.1]: 
2019-07-12T21:59:18.070412+00:00 app[web.1]: The above exception was the direct cause of the following exception:
2019-07-12T21:59:18.070414+00:00 app[web.1]: 
2019-07-12T21:59:18.070416+00:00 app[web.1]: Traceback (most recent call last):
2019-07-12T21:59:18.070418+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app
2019-07-12T21:59:18.070421+00:00 app[web.1]: response = self.full_dispatch_request()
2019-07-12T21:59:18.070423+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request
2019-07-12T21:59:18.070425+00:00 app[web.1]: rv = self.handle_user_exception(e)
2019-07-12T21:59:18.070427+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception
2019-07-12T21:59:18.070430+00:00 app[web.1]: reraise(exc_type, exc_value, tb)
2019-07-12T21:59:18.070432+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/_compat.py"", line 35, in reraise
2019-07-12T21:59:18.070434+00:00 app[web.1]: raise value
2019-07-12T21:59:18.070436+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request
2019-07-12T21:59:18.070438+00:00 app[web.1]: rv = self.dispatch_request()
2019-07-12T21:59:18.070446+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 1799, in dispatch_request
2019-07-12T21:59:18.070450+00:00 app[web.1]: return self.view_functions[rule.endpoint](**req.view_args)
2019-07-12T21:59:18.070452+00:00 app[web.1]: File ""/app/main.py"", line 65, in blog
2019-07-12T21:59:18.070455+00:00 app[web.1]: key=lambda p: p.posted_at, reverse=True):
2019-07-12T21:59:18.070457+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3324, in __iter__
2019-07-12T21:59:18.070459+00:00 app[web.1]: return self._execute_and_instances(context)
2019-07-12T21:59:18.070461+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3349, in _execute_and_instances
2019-07-12T21:59:18.070463+00:00 app[web.1]: result = conn.execute(querycontext.statement, self._params)
2019-07-12T21:59:18.070466+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 988, in execute
2019-07-12T21:59:18.070468+00:00 app[web.1]: return meth(self, multiparams, params)
2019-07-12T21:59:18.070470+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/sql/elements.py"", line 287, in _execute_on_connection
2019-07-12T21:59:18.070472+00:00 app[web.1]: return connection._execute_clauseelement(self, multiparams, params)
2019-07-12T21:59:18.070474+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1107, in _execute_clauseelement
2019-07-12T21:59:18.070477+00:00 app[web.1]: distilled_params,
2019-07-12T21:59:18.070479+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1248, in _execute_context
2019-07-12T21:59:18.070481+00:00 app[web.1]: e, statement, parameters, cursor, context
2019-07-12T21:59:18.070484+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1466, in _handle_dbapi_exception
2019-07-12T21:59:18.070486+00:00 app[web.1]: util.raise_from_cause(sqlalchemy_exception, exc_info)
2019-07-12T21:59:18.070488+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 399, in raise_from_cause
2019-07-12T21:59:18.070491+00:00 app[web.1]: reraise(type(exception), exception, tb=exc_tb, cause=cause)
2019-07-12T21:59:18.070493+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 153, in reraise
2019-07-12T21:59:18.070495+00:00 app[web.1]: raise value.with_traceback(tb)
2019-07-12T21:59:18.070498+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1244, in _execute_context
2019-07-12T21:59:18.070500+00:00 app[web.1]: cursor, statement, parameters, context
2019-07-12T21:59:18.070502+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 550, in do_execute
2019-07-12T21:59:18.070504+00:00 app[web.1]: cursor.execute(statement, parameters)
2019-07-12T21:59:18.070507+00:00 app[web.1]: sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: user
2019-07-12T21:59:18.070513+00:00 app[web.1]: [SQL: SELECT user.id AS user_id, user.name AS user_name, user.posted_at AS user_posted_at, user.content AS user_content, user.content_type AS user_content_type, user.category AS user_category
2019-07-12T21:59:18.070515+00:00 app[web.1]: FROM user
2019-07-12T21:59:18.070517+00:00 app[web.1]: WHERE user.category = ?]
2019-07-12T21:59:18.070520+00:00 app[web.1]: [parameters: ('blog',)]
2019-07-12T21:59:18.070661+00:00 app[web.1]: (Background on this error at: http://sqlalche.me/e/e3q8)
2019-07-12T21:59:18.072919+00:00 app[web.1]: 10.63.22.102 - - [12/Jul/2019:21:59:18 +0000] ""GET /blog HTTP/1.1"" 500 290 ""https://secure-savannah-20745.herokuapp.com/projects"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""


",1,757,"You missed that your table is named user in your model:
class Post(db.Model):
    __tablename__ = 'user'
    #               ^^^^^^

The reason it doesn't exist is because Heroku doesn't keep your filesystem changes. You are using a SQLite database:
... app[web.1]: sqlite3.OperationalError: no such table: user
#               ^^^^^^^

but a sqlite database is stored on the filesystem of your Heroku dyno. Heroku cleans up (cycles) dynos all the time and spins up a new dyno for you for new requests. From their documentation on how Heroku works:

Changes to the filesystem on one dyno are not propagated to other dynos and are not persisted across deploys and dyno restarts. A better and more scalable approach is to use a shared resource such as a database or queue.

Each time a new dyno is spun up to handle your requests, you'll have a new, clean filesystem without your sqlite database, and so an empty database is created again without any tables in it.
Bottom line: You can't use sqlite on Heroku.
Use a Postgres database instead, see the Heroku Postgres documentation. Don't worry, there is a free tier you can use. When you provision the database as an add-on, the DATABASE_URL config var is set for you, which you can read from your app environment.
I usually configure Flask apps that are designed to run on Heroku to read the environment variable but to fall back to a SQLite database when the environment variable is not set:
import os
from pathlib import Path

_project_root = Path(__file__).resolve().parent.parent
_default_sqlite_db = _project_root / ""database.db""

SQLALCHEMY_DATABASE_URI = os.environ.get(
    ""DATABASE_URL"", f""sqlite:///{_default_sqlite_db}""
)

This makes it easy to develop the app locally on SQLite, or set up a local Postgres database for integration testing, and then deploy to Heroku for staging or production.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/53829617,SQLAlchemy Bakery -Function expects a certain array size,"I have a strange issue in my Python project. It uses SQLAlchemy and Bakery to have prepared queries. I have a function that takes the connection (db), bakery, and an array of objects.

This function is called several times by an other function in a for loop and here is my issue (at least what I understand):


Let's assume that the first time it receives an array with two elements.
The next time it is called the function will also expect an array with two elements




import sqlalchemy as sa
def cpe_filter(db, bakery, iterable):
    cpes = []

    try:
        query  = bakery(lambda s: s.query(Cpe))
        query += lambda y: y.filter(
            sa.or_(*[
                Cpe.cpe.like(sa.bindparam('cpe_{}'.format(i)))
                for i, _ in enumerate(iterable)
            ])
        )
        query += lambda y: y.filter_by(active=sa.bindparam('active'))

        cpes = query(db).params(active=True,
                                **{'cpe_{}'.format(i): e for i, e in enumerate(iterable)}) \
                        .all()
    except NoResultFound:
        log.info(""Found no CPE matching list {}."".format(iterable))


If the next array is smaller than the previous, I get this kind of error (Pastebin):

[2018-12-17 16:35:16 - INFO/sqlalchemy.engine.base.Engine:1151] SELECT cpe.id AS cpe_id, cpe.active AS cpe_active, cpe.date_created AS cpe_date_created, cpe.timestamp AS cpe_timestamp, cpe.cpe_part_id AS cpe_cpe_part_id, cpe.device_id AS cpe_device_id, cpe.cpe AS cpe_cpe, cpe.match_nvd AS cpe_match_nvd
FROM cpe
WHERE (cpe.cpe LIKE %(cpe_0)s OR cpe.cpe LIKE %(cpe_1)s OR cpe.cpe LIKE %(cpe_2)s) AND cpe.active = %(active)s
[2018-12-17 16:35:16 - INFO/sqlalchemy.engine.base.Engine:1154] {'cpe_0': 'cpe:/o:sun:solaris', 'cpe_1': 'cpe:/a:tritreal:ted_cde', 'cpe_2': 'cpe:/o:hp:hp-ux', 'active': 1}
[2018-12-17 16:35:16 - INFO/sqlalchemy.engine.base.Engine:1151] SELECT cpe.id AS cpe_id, cpe.active AS cpe_active, cpe.date_created AS cpe_date_created, cpe.timestamp AS cpe_timestamp, cpe.cpe_part_id AS cpe_cpe_part_id, cpe.device_id AS cpe_device_id, cpe.cpe AS cpe_cpe, cpe.match_nvd AS cpe_match_nvd
FROM cpe
WHERE (cpe.cpe LIKE %(cpe_0)s OR cpe.cpe LIKE %(cpe_1)s OR cpe.cpe LIKE %(cpe_2)s) AND cpe.active = %(active)s
[2018-12-17 16:35:16 - INFO/sqlalchemy.engine.base.Engine:1154] {'cpe_0': 'cpe:/a:hp:dtmail', 'cpe_1': 'cpe:/a:university_of_washington:pine', 'cpe_2': 'cpe:/o:sco:unixware', 'active': 1}
[2018-12-17 16:35:16 - ERROR/scap.abc:66] An error has occurred during task execution.
Traceback (most recent call last):
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1127, in _execute_context
    context = constructor(dialect, self, conn, *args)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 635, in _init_compiled
    grp, m in enumerate(parameters)]
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 635, in &lt;listcomp&gt;
    grp, m in enumerate(parameters)]
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/sql/compiler.py"", line 547, in construct_params
    % bindparam.key, code=""cd3x"")
sqlalchemy.exc.InvalidRequestError: A value is required for bind parameter 'cpe_2' (Background on this error at: http://sqlalche.me/e/cd3x)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/root/scap/project/scap/abc.py"", line 64, in run
    self(*args, **kwargs)
  File ""/root/scap/project/scap/tasks.py"", line 362, in __call__
    q.cve_insert_or_update(self.db, self.bakery, self.parse(name))
  File ""/root/scap/project/scap/queries.py"", line 148, in cve_insert_or_update
    cpes = list(cpe_filter(db, bakery, cpes))
  File ""/root/scap/project/scap/queries.py"", line 68, in cpe_filter
    **{'cpe_{}'.format(i): e for i, e in enumerate(products)}) \
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/ext/baked.py"", line 457, in all
    return list(self)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/ext/baked.py"", line 364, in __iter__
    return q._execute_and_instances(context)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3018, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 948, in execute
    return meth(self, multiparams, params)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/sql/elements.py"", line 269, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1060, in _execute_clauseelement
    compiled_sql, distilled_params
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1132, in _execute_context
    None, None)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1413, in _handle_dbapi_exception
    exc_info
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 265, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 248, in reraise
    raise value.with_traceback(tb)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1127, in _execute_context
    context = constructor(dialect, self, conn, *args)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 635, in _init_compiled
    grp, m in enumerate(parameters)]
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 635, in &lt;listcomp&gt;
    grp, m in enumerate(parameters)]
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/sql/compiler.py"", line 547, in construct_params
    % bindparam.key, code=""cd3x"")
sqlalchemy.exc.StatementError: (sqlalchemy.exc.InvalidRequestError) A value is required for bind parameter 'cpe_2' [SQL: 'SELECT cpe.id AS cpe_id, cpe.active AS cpe_active, cpe.date_created AS cpe_date_created, cpe.timestamp AS cpe_timestamp, cpe.cpe_part_id AS cpe_cpe_part_id, cpe.device_id AS cpe_device_id, cpe.cpe AS cpe_cpe, cpe.match_nvd AS cpe_match_nvd \nFROM cpe \nWHERE (cpe.cpe LIKE %(cpe_0)s OR cpe.cpe LIKE %(cpe_1)s OR cpe.cpe LIKE %(cpe_2)s) AND cpe.active = %(active)s'] [parameters: [{'active': True, 'cpe_0': 'cpe:/a:university_of_washington:imap', 'cpe_1': 'cpe:/a:netscape:messaging_server'}]] (Background on this error at: http://sqlalche.me/e/cd3x)


As you can see, the function is called three times, the first two times it works without any issue (3 elements each time), and the third time it has only two elements and it expect a third element according to the error.

NB: The iterable can reach about 50 elements most of the time.
",1,424,"The problem stems from the observations 4. and 5. under ""Synopsis"" in the baked queries documentation:


  
  In the above code, even though our application may call upon search_for_user() many times, and even though within each invocation we build up an entirely new BakedQuery object, all of the lambdas are only called once. Each lambda is never called a second time for as long as this query is cached in the bakery.
  The caching is achieved by storing references to the lambda objects themselves in order to formulate a cache key; that is, the fact that the Python interpreter assigns an in-Python identity to these functions is what determines how to identify the query on successive runs. For those invocations of search_for_user() where the email parameter is specified, the callable lambda q: q.filter(User.email == bindparam('email')) will be part of the cache key thats retrieved; when email is None, this callable is not part of the cache key.
  


If you inspect your cpe_filter() function using dis you'll note that the lambda-functions are constants and so keep their identity between calls. As explained in the referenced documentation, SQLAlchemy caches queries based on those identities and calls

query += lambda y: y.filter(
    sa.or_(*[
        Cpe.cpe.like(sa.bindparam('cpe_{}'.format(i)))
        for i, _ in enumerate(iterable)
    ])
)


only once. In other words the placeholders will be set the first time you call cpe_filter(), based on iterable. They will be ""reset"" only when this query has been evicted from the cache.

The solution depends on your DBMS in use. For example Postgresql has the ANY array comparison that could be used:

query += lambda y: y.filter(Cpe.cpe.like(sa.any_(sa.bindparam('cpe'))))


and the parameter would be passed as

# This relies on Psycopg2's lists adaptation:
# http://initd.org/psycopg/docs/usage.html#lists-adaptation
cpes = query(db).params(active=True, cpe=list(iterable)).all()


On MS SQL Server you could perhaps create a full-text index and use CONTAINS:

query += lambda y: y.filter(func.contains(Cpe.cpe, sa.bindparam('cpe')))


The bind param cpe should pass the search condition, which must be formed from iterable:

search_cond = "" OR "".join(iterable)
cpes = query(db).params(active=True, cpe=search_cond).all()


This of course requires that the items in iterable are valid full-text search terms.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/43475159,Strange issue in SQLAlchemy,"Consider the following code which creates a very simple table (without using SQLAlchemy), then adds an entry to it using SQLAlchemy ORM and retrieves it:

import sqlite3
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

DB_PATH = '/tmp/tst.db'

#create a DB
sqlite_conn = sqlite3.connect(DB_PATH)
sqlite_conn.execute('''CREATE TABLE tst (
    id INTEGER PRIMARY KEY ASC AUTOINCREMENT,
    c0 INTEGER,
    c1 INTEGER
);''')
sqlite_conn.commit()

#intialize an SA engine/session/mapped class
engine = create_engine('sqlite:///{}'.format(DB_PATH))
Base = declarative_base()
Base.metadata.reflect(bind=engine)
Session = sessionmaker(bind=engine)

class Tst(Base):
    __table_name__ = 'tst'
    __table__ = Base.metadata.tables[__table_name__]
    columns = list(__table__.columns)
    field_names = [c.name for c in columns]

#add an entry to the table
session = Session()
inst = Tst()
session.add(inst)
session.commit()

#retrieve an entry from the table
session = Session()
inst = session.query(Tst).first()
print inst.c1


One may expect that the code above will just print 'None', as 'c1' was not assigned a value. Instead of it, I'm getting the following error message:

Traceback (most recent call last):
  File ""..."", line 39, in &lt;module&gt;
    print inst.c1
AttributeError: 'Tst' object has no attribute 'c1'


But if the following line will be removed/commented:

    field_names = [c.name for c in columns]


the output will be as expected.

In general, it looks like the iteration over Table.columns inside the class definition will cause the last column to be omitted from the class instances.

Following this answer, I actually changed the code to use Inspector, and it worked fine. However, AFAIK, accessing Table.columns is completely legitimate, so I wanted to understand whether it's buggy behavior or something wrong on my side.

P.S. tested with SQLAlchemy 1.1.9

P.P.S. the issue doesn't appear to be related to a specific DB dialect - reproduced with MySQL, sqlite.
",1,468,"This is more of a Python version issue than an SQLAlchemy issue. The root cause is the leaking of the name c from the list-comprehension in Python 2. It becomes part of the namespace of the constructed class, and so SQLAlchemy sees it as if you were explicitly naming the last column in the list columns in your class definition. Your class definition is equivalent to:

class Tst(Base):
    __table_name__ = 'tst'
    __table__ = Base.metadata.tables[__table_name__]
    columns = list(__table__.columns)
    ...
    c = columns[-1]  # The last column of __table__


If you change your print statement to:

print inst.c


you'll get None as you expected. If you must have your field_names, you could for example remove the name from the namespace:

class Tst(Base):
    __table_name__ = 'tst'
    __table__ = Base.metadata.tables[__table_name__]
    columns = list(__table__.columns)
    field_names = [c.name for c in columns]
    del c


but this is unportable (and ugly) between Python 2 and 3, since the name would not actually exist in 3. You could also work around the issue with attrgetter():

from operator import attrgetter

class Tst(Base):
    __table_name__ = 'tst'
    __table__ = Base.metadata.tables[__table_name__]
    columns = list(__table__.columns)
    field_names = list(map(attrgetter('name'), columns))


or use a generator expression:

    field_names = list(c.name for c in columns)

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/30942439,DataError: (DataError) invalid input syntax for integer: &quot;None&quot;,"Background

We're using Flask to develop a fairly simple application to ""scratch an itch"" we realized we had while working on a previous project. Session management is handled through the Flask-Login extension, coupled with the Github-Flask extension which we use for user authentication. The app uses Flask-SQLAlchemy and Psycopg2 (2.6.3) to connect to a PostgreSQL 9.1 database where user data is stored.

Issue

When the app is run locally (or deployed to a remote server) for testing we're seeing a strange condition during the first login attempt. The first time I log in I successfully get through the Github authentication sets, but immediately see this error.

DataError: (DataError) invalid input syntax for integer: ""None""
LINE 3: WHERE ""user"".id = 'None'
                      ^
'SELECT ""user"".id AS user_id, ""user"".date_created AS user_date_created,
""user"".date_modified AS user_date_modified, ""user"".nickname AS user_nickname,
""user"".email AS user_email, ""user"".about_me AS user_about_me,
""user"".github_access_token AS user_github_access_token, ""user"".github_id AS
user_github_id \nFROM ""user"" \nWHERE ""user"".id = %(param_1)s' {'param_1': u'None'}


I can close the browser tab and then revisit the application URL and see the same message. However, if I close the browser window, completely clear my browser's cache, and then attempt to log back in, it works as expected and I'm able to use the application. From that point on I can log out, switch browsers, clear browser cache again, etc., and there don't appear to be any problems -- it's only that first login until browser cache is cleared that this happens.

My testing shows that this happens on any user and two different users can be in two different states at the same time (one has cleared cache and now isn't having problems, the other who hasn't taken those steps is still stuck in the error state.

Originally we used a local SQLite database instead of Postgres. The issue does not exist there. It only has occurred since the switch to Postgres.

I'm at a loss for what steps I can take to remedy this situation. So far the questions I know to ask haven't been able to lead to the right answers when I search online.

For reference, I'll include the full stack trace as well.

Full Stack Trace

Traceback (most recent call last):
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1836, in __call__
    return self.wsgi_app(environ, start_response)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1820, in wsgi_app
    response = self.make_response(self.handle_exception(e))
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1403, in handle_exception
    reraise(exc_type, exc_value, tb)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1817, in wsgi_app
    response = self.full_dispatch_request()
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1477, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1381, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1475, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask_debugtoolbar/__init__.py"", line 124, in dispatch_request
    return view_func(**req.view_args)
  File ""/Users/dev/PRODUCT/app/modules/mod_profile/controllers.py"", line 26, in profile
    title='PRODUCT')
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/templating.py"", line 126, in render_template
    ctx.app.update_template_context(context)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 716, in update_template_context
    context.update(func())
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask_login.py"", line 825, in _user_context_processor
    return dict(current_user=_get_user())
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask_login.py"", line 794, in _get_user
    current_app.login_manager._load_user()
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask_login.py"", line 363, in _load_user
    return self.reload_user()
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask_login.py"", line 325, in reload_user
    user = self.user_callback(user_id)
  File ""/Users/dev/PRODUCT/app/modules/mod_auth/controllers.py"", line 31, in load_user
    return User.query.get(id)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/orm/query.py"", line 840, in get
    return loading.load_on_ident(self, key)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/orm/loading.py"", line 231, in load_on_ident
    return q.one()
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2395, in one
    ret = list(self)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2438, in __iter__
    return self._execute_and_instances(context)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2453, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 729, in execute
    return meth(self, multiparams, params)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/sql/elements.py"", line 322, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 826, in _execute_clauseelement
    compiled_sql, distilled_params
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 958, in _execute_context
    context)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1159, in _handle_dbapi_exception
    exc_info
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/util/compat.py"", line 199, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 951, in _execute_context
    context)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/engine/default.py"", line 436, in do_execute
    cursor.execute(statement, parameters)
DataError: (DataError) invalid input syntax for integer: ""None""
LINE 3: WHERE ""user"".id = 'None'
                          ^
 'SELECT ""user"".id AS user_id, ""user"".date_created AS user_date_created, ""user"".date_modified AS user_date_modified, ""user"".nickname AS user_nickname, ""user"".email AS user_email, ""user"".about_me AS user_about_me, ""user"".github_access_token AS user_github_access_token, ""user"".github_id AS user_github_id \nFROM ""user"" \nWHERE ""user"".id = %(param_1)s' {'param_1': u'None'}

",1,5121,"Resolved!

This was a race condition. During the login process our application redirects an authenticated user to their profile page. This was getting triggered correctly, but for new users it was getting called before the user was actually committed to the database. 

Visually it happened at the same time, so you could confirm that the callback was supplied with the ""right"" information, but it wasn't until I deconstructed and then reconstructed the login steps that I realized that it was hitting the route just before the record was actually committed to the database. Since the function that added the record wasn't interrupted by the redirect it did what it was supposed to do, but just a hair late.

Perhaps someone else will find this extract of my code helpful when troubleshooting a similar issue:

## from mod_auth/controllers.py
@mod_auth.before_app_request
def before_request():
    g.user = current_user

@mod_auth.route('/login', methods=['GET'])
def login():
    if g.user.is_authenticated():
        redirect(url_for('mod_home.index'))
    return github.authorize()

@login_manager.user_loader
def load_user(id):
    return User.query.get(id)

@github.access_token_getter
def token_getter():
    user = g.user
    if user is not None:
        return user.github_access_token

@mod_auth.route('/github')
@github.authorized_handler
def authorized(oauth_token):

    next_url = request.args.get('next') or url_for('mod_home.index')
    if oauth_token is None:
        flash(""Authorization failed."")
        return redirect(next_url)

    user = User.query.filter_by(github_access_token=oauth_token).first()
    if user is None:
        user = User(github_access_token=oauth_token)
        db.session.add(user)
        db.session.commit() ## &lt;-- adding a commit here fixed the issue

    user.github_access_token = oauth_token

    login_user(user) 
    ## login_user called, but because the oauth_token exist and had been
    ## added to the session, the user was flagged as ""is_authenticated""
    ## and the redirect was triggered

    ghinfo = github.get('user')

    if 'login' in ghinfo:
        user.nickname = ghinfo['login'].lower()
    else:
        user.nickname = 'nameless'

    db.session.commit() ## this was the only commit, originally

    return redirect(url_for('mod_profile.profile', username=user.nickname))

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/28610563,SQLAlchemy: different output on queries &quot;field IS NULL&quot; and &quot;IS NOT NULL&quot; on aliased model,"I've noticed a very strange behaviour when querying against field from aliased model. Seems that SQLAlchemy breaks the aliasing while filtering relation against NOT NULL condition. Here is an example:

Base = declarative_base()

class Parent(Base):
    __tablename__ = 'parents'
    id = Column(Integer, primary_key=True)
    children = relation('Child', back_populates='parent')

class Child(Base):
    __tablename__ = 'children'
    id = Column(Integer, primary_key=True)
    parent_id = Column(Integer, ForeignKey('parents.id'))
    parent = relation('Parent', back_populates='children', uselist=False)

aChild = aliased(Child)

print Session.query(aChild.id).filter(aChild.parent == None)
"""""" SELECT children_1.id AS children_1_id
    FROM children AS children_1
    WHERE children_1.parent_id IS NULL
""""""

print Session.query(aChild.id).filter(aChild.parent != None)
"""""" SELECT children_1.id AS children_1_id
    FROM children AS children_1, children
    WHERE children.parent_id IS NOT NULL
""""""

print Session.query(aChild.id).filter(~(aChild.parent == None))
"""""" SELECT children_1.id AS children_1_id
    FROM children AS children_1
    WHERE children_1.parent_id IS NOT NULL
""""""


You can see there is a cross join in the second query, while 1st and 3rd are working as expected. Is there any explanation or solution for this issue except the 3rd example?
",1,345,,,
SQLAlchemy strange issue,https://stackoverflow.com/questions/49154668,SQLAlchemy plus FormAlchemy displays @hybrid_property name and not the returned value,,0,254,"By the courtesy of Michael Bayer who commented on reported issue #4214:
hybrid_property name being displayed rather than the returned value

The value of Asset.underscore_name is a SQL expression, which since the changes described in http://docs.sqlalchemy.org/en/latest/changelog/migration_11.html#hybrid-properties-and-methods-now-propagate-the-docstring-as-well-as-info is derived from the clause_element() method of the returned object that Core understands. The hybrid you illustrate above is not correctly written because it is not returning a Core SQL expression object:

    s = Session()
    print(s.query(Asset.underscore_name))

sqlalchemy.exc.InvalidRequestError: SQL expression, column, or mapped entity expected - got ''x_underscore_name_x''


if you are looking for a class-bound attribute to return a string, just assign it:

class Asset(...):
    underscore_name = ""x_underscore_name_x""


or if you want that to be a method that is callable at the classlevel, use a @classproperty, there's a recipe for that here: https://stackoverflow.com/a/5191224/34549 and SQLAlchemy has one you can copy: 
https://github.com/zzzeek/sqlalchemy/blob/master/lib/sqlalchemy/util/langhelpers.py#L1140

IMO there's no bug here because this is not the correct usage of @hybrid_property

Thank you very much Michael!
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/71451982,How to make alembic or flask migrate name foreign keys when autogenerating migrations?,,0,1258,"There is an answer to this question in the best practices suggested here, at the end of the section on The Importance of Naming Conventions. The solution is to add a naming_convention to your sqlalchemy metadata, like this:
convention = {
  ""ix"": ""ix_%(column_0_label)s"",
  ""uq"": ""uq_%(table_name)s_%(column_0_name)s"",
  ""ck"": ""ck_%(table_name)s_%(constraint_name)s"",
  ""fk"": ""fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s"",
  ""pk"": ""pk_%(table_name)s""
}

metadata = MetaData(naming_convention=convention)

More specifically, with Flask-SQLAlchemy, do this when initializing your db:
from sqlalchemy import MetaData

convention = {
  ""ix"": ""ix_%(column_0_label)s"",
  ""uq"": ""uq_%(table_name)s_%(column_0_name)s"",
  ""ck"": ""ck_%(table_name)s_%(constraint_name)s"",
  ""fk"": ""fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s"",
  ""pk"": ""pk_%(table_name)s""
}

db = SQLAlchemy(metadata=MetaData(naming_convention=convention))

And voila! If you run autogenerate, you'll get this:
def upgrade():
  op.create_foreign_key(op.f('fk_characters_background_id_backgrounds'), 'characters', 'backgrounds', ['background_id'], ['id'])

def downgrade():
  op.drop_constraint(op.f('fk_characters_background_id_backgrounds'), 'characters', type_='foreignkey')

Thanks (unsurprisingly) to  Miguel Grinberg, creator of Flask Migrate, for having linked to the correct page in the Alembic docs that finally allowed me to solve this problem! Someone had asked about this in an issue on Flask Migrate GitHub, and Miguel correctly pointed out that this was an Alembic issue, not a Flask Migrate issue.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/66004467,MySQL BIGINT inconsistent for inserts?,,0,326,"after racking my brains for days.. coming at it from all angles, i could not figure out why 1 table out of many, had issues with truncating the SHA'd value.
In the end, i have redesigned how i hold my Ids, and i no longer bother converting to BIGINT. it all works fine when i leave it as CHAR.
CAST(SHA(CONCAT(full_name,ga)) AS CHAR)

So changed all my Id columns to varchar(40) and use the above style.  All good now. Joins will use varchar instead of bigint - which i'm ok with.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/35247731,more than one row returned by a subquery used as an expression,,0,556,"I believe you don't need FROM item,poi in 2 subqueries and I also remove the group by in those too. Regading the case expression involving :param_1 I am not sure if what I suggest below is funcionally correct or not, but you need to create a SUM() value before you can test if that SUM() IS NULL.

SELECT
      poi.key
    , poi.pok
    , poi.noc
    , COALESCE((
            SELECT SUM(COALESCE(item.noc,:param_1))
            FROM item
            WHERE poi.key = item.poi_key
      )
      , poi.noc) AS coalesce_1
    , COALESCE((
            SELECT
                  SUM(soi.noc) AS sum_1
            FROM soi
            WHERE soi.poi_key = poi.key
                  AND soi.is_shipped = 0
      )
      , @param_2) AS coalesce_2
    , COALESCE((
            SELECT SUM(COALESCE(item.noc,:param_1))
            FROM item
            WHERE poi.key = item.poi_key
      )
      , poi.noc) - ee((
            SELECT
                  SUM(soi.noc) AS sum_1
            FROM soi
            WHERE soi.poi_key = poi.key
                  AND soi.is_shipped = 0
      )
      , :param_2) AS anon_2
FROM poi

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/69130661,"Flask SQL Alchemy try perform SET operation, when it is not needed","I am facing a very strange issue using SQL Alchemy. I have a table in SQL server, to store Sales, with this format :
CREATE TABLE Resale (
  [address] VARCHAR(100) NOT NULL,
  [id] BIGINT NOT NULL,
  [price] INT NOT NULL,
  [start_date] DATETIME2(7) NOT NULL  DEFAULT GETUTCDATE(),
  [sys_start_time] DATETIME2 (7) GENERATED ALWAYS AS ROW START HIDDEN NOT NULL,
  [sys_end_time] DATETIME2 (7) GENERATED ALWAYS AS ROW END HIDDEN NOT NULL,
  PERIOD FOR SYSTEM_TIME ([sys_start_time], [sys_end_time]),
  CONSTRAINT [PK_ReSale] PRIMARY KEY CLUSTERED ([id] ASC))
  WITH (SYSTEM_VERSIONING = ON (HISTORY_TABLE=[dbo].[ResaleHistory], DATA_CONSISTENCY_CHECK=ON));

in python I used flask-sqlalchemy and build this model :
class Resale(db.Model):
    __tablename__ = 'Resale'
    address = db.Column(db.String(100), nullable=False)
    id = db.Column(db.BigInteger, primary_key=True)
    price = db.Column(db.Integer, nullable=False)
    start_date = db.Column(db.DateTime, nullable=False, default=datetime.utcnow())

now I am just trying to insert a record in this table using :
resale = Resale(address=address, id=id, price=price)
db.session.add(resale)
db.session.commit()

and flask return :
pyodbc.ProgrammingError: ('42000', ""[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Table 'Resale' does not have the identity property. Cannot perform SET operation. (8106) (SQLExecDirectW)"")
[SQL: SET IDENTITY_INSERT [Resale] ON]

I really don't understand why I have that, I am not using an autoincrement in my table, so my flask wants me to do ""SET IDENTITY_INSERT Resale ON ?
honnestly I am completly lost with this message, I am just trying to add a record in database
",0,281,"As stated in this answer for a previous similiar question you should consider put the flag ""autoincrement "" to False on your ""id"".
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/66781354,Heroku Postgres throwing int out of range error for ints that should be in range,"I'm attempting to collect options data using a heroku dyno (hobby $7 version) and a heroku Postgres database (standard $50 version). I've got a small script set up that uses yfinance to collect options data for a set of ticker symbols and SQLAlchemy + psycopg2 to insert this data into the postgres database.
I seem to be encountering a very strange error when inserting into the database. For many of the records, I'm seeing a (psycopg2.errors.NumericValueOutOfRange) integer out of range Exception thrown for records that should not have any integers out of range... I've included an example of the error below as well as my model definition. At first I thought maybe it was the id / pk column, but according to SQLAlchemy's docs they automatically treat these fields as a serial data type. So I'm not sure why this error is being thrown with all integer columns clearly in the range of the INT data type.
Is this a heroku thing? Has anyone seen / dealt with this before? I'm not sure how to debug the underlying issue. Thanks in advance!
2021-03-24 12:25:16 ERROR    Unable to commit Option(id=None, ticker=EDIT, option_type=PUT, contractSymbol=EDIT230120P00017500, lastTradeDate=2020-11-05 14:38:12, strike=17.5, lastPrice=4.7, bid=0.0, ask=0.0, change=0.0, percentChange=0.0, volume=1.0, openInterest=0, impliedVolatility=0.12500875, inTheMoney=False, contractSize=REGULAR, currency=USD)
2021-03-24 12:25:16 ERROR    This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.NumericValueOutOfRange) integer out of range

(Background on this error at: http://sqlalche.me/e/14/9h9h) (Background on this error at: http://sqlalche.me/e/14/7s2a)
Continuing.

Model definition:
class Option(config.Base):

    __tablename__ = ""options""

    id = Column(Integer, primary_key=True)
    ticker = Column(String, nullable=False)
    dt_created_db = Column(
        DateTime, nullable=False, default=datetime.datetime.utcnow()
    )
    dt_updated_db = Column(
        DateTime, nullable=False, default=datetime.datetime.utcnow()
    )
    dt_created_market_time_db = Column(
        DateTime, nullable=False,
        default=datetime.datetime.now().astimezone(
            pytz.timezone(
                ""America/New_York""
            )
        )
    )
    option_type = Column(String, nullable=False) # CALL or PUT
    contractSymbol = Column(String, nullable=False)
    lastTradeDate = Column(DateTime, nullable=False)
    strike = Column(Float, nullable=False)
    lastPrice = Column(Float, nullable=False)
    bid = Column(Float)
    ask = Column(Float)
    change = Column(Float)
    percentChange = Column(Float)
    volume = Column(Float)
    openInterest = Column(Integer)
    impliedVolatility = Column(Numeric)
    inTheMoney = Column(Boolean)
    contractSize = Column(String)
    currency = Column(String)

    @classmethod
    def create_option_record(cls, ticker, record, opt_type):
        """"""Creates an Option model instance.

        Args:
            ticker: The ticker symbol.
            record: A pandas dataframe row with ""."" access to columns.
            opt_type: A string, either CALL or PUT.
        Returns:
            Option object.
        """"""
        return cls(
            ticker=ticker,
            option_type=opt_type,
            contractSymbol=record.contractSymbol,
            lastTradeDate=record.lastTradeDate,
            strike=record.strike,
            lastPrice=record.lastPrice,
            bid=record.bid,
            ask=record.ask,
            change=record.change,
            percentChange=record.percentChange,
            volume=record.volume,
            openInterest=record.openInterest,
            impliedVolatility=round(record.impliedVolatility, 20),
            inTheMoney=record.inTheMoney,
            contractSize=record.contractSize,
            currency=record.currency
        )

    def __str__(self):
        return (
            f""Option(""
            f""id={self.id}, ""
            f""ticker={self.ticker}, ""
            f""option_type={self.option_type}, ""
            f""contractSymbol={self.contractSymbol}, ""
            f""lastTradeDate={self.lastTradeDate}, ""
            f""strike={self.strike}, ""
            f""lastPrice={self.lastPrice}, ""
            f""bid={self.bid}, ""
            f""ask={self.ask}, ""
            f""change={self.change}, ""
            f""percentChange={self.percentChange}, ""
            f""volume={self.volume}, ""
            f""openInterest={self.openInterest}, ""
            f""impliedVolatility={self.impliedVolatility}, ""
            f""inTheMoney={self.inTheMoney}, ""
            f""contractSize={self.contractSize}, ""
            f""currency={self.currency}""
            f"")""
        )

",0,366,"I've solved this. It turns out the database logs were behind the compute logs and so the record I thought was causing the error was not actually causing the error.
The root cause was that an np.nan value was being inserted into an INTEGER column. I had to replace np.nan's with None and this solved the problem.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/60812701,Installed with pip3 modules no longer recognizable. Need to reinstall with pip,"I faced a strange issue: I'm working on a Python/Flask project on my local computer, don't use the virtual env. I installed different modules with pip3, like Flask-SQLAlchemy, Flask-Migrate, etc. Everything worked perfectly.
Suddenly, after a few weeks, when running the program, it started to appear the errors, like ModuleNotFoundError: No module named 'flask_sqlalchemy' - and the same for Flask-Migrate, Flask-WTF, etc.

The solution is to install all these modules again but with pip (not pip3). Uninstallation/Installation with pip3 does not help. Why did it happen? Is it possible to turn it back to pip3?

I didn't change the environment, I have the only one actually. Using Python 3.8
",0,37,"I would say that the reason why it is like this, it's because maybe there is some libraries for other your projects that clash with flask libraries and that's why you have issues. E.g. I had the same issue when I didn't use virtual environments when I was learning Python, and when I had multiple projects with different python libraries, some of them clashed and produced unexpected errors, so I started using virtual environments - different virtual environment for each project and it solved all the problems with clashes between different libraries.

Here is a good official tutorial on how to use virtual environments
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/37174982,how to dump data to the database at startup,"I have a flask application, and when it spins up, it needs to dump data to the database. The datafiles are each about 100MB, and currently there are three of them, but soon there will be ~15. 

@app.before_first_request
def populateDB():
        reader = csv.DictReader(file, delimiter='\t', fieldnames=[""not relevant""])
        OCRs = [dict(row.items()) for row in reader]
        db.engine.execute(OpenChromatinRegion.__table__.insert(), OCRs)


I have a function called populateDB which works, so long as the file is very small (300 rows). When I go to run it on any of the large files:

[2016-05-11 23:22:09 +0000] [7] [INFO] working on datafile MCF7-all.fdr0.01TF_anno.txt
[2016-05-11 23:22:24 +0000] [7] [INFO] Datafile MCF7-all.fdr0.01TF_anno.txt properly configured in memory
[2016-05-11 23:22:39 +0000] [1] [CRITICAL] WORKER TIMEOUT (pid:7)
[2016-05-11 23:22:41 +0000] [11] [INFO] Booting worker with pid: 11


And then it just hangs. 

Is there a better way of going about this, or some way of not failing? Perhaps doing it incrementally rather than trying to dump 100MB at once? 

-- UPDATE --

I tried incrementally, in 1000-row chunks, and although it got through a couple such chunks, the worker still timed out. 

So my current questions are: 
 - if I were to use celery what would that look like?
 - would alembic's op.bulk_insert do this better?

-- UPDATE 2 --

Tried using Flask-Script, which didn't work either. 

from flask.ext.sqlalchemy import SQLAlchemy
from flask.ext.script import Manager
from app import app, db
from models import *
manager = Manager(app)

@manager.command
def populateDB():
    (same function as above)


The worked still timed out. Might this have anything to do with Docker? I'm using this project as my base.

-- UPDATE 3 --

Strangely, I got this to work locally for one file by db.session.add(), db.session.commit() for each example. But that seems to be the slowest way of going about it, per the documentation. That solution also doesn't work remotely, on digitalocean. I think this might have to do with the memory-pressure. I get through about 1,750,000 rows before it crashes.

-- UPDATE 4 -- 

After the machine dies, I run dmesg and the output reveals:

[ 2323.138921] Out of memory: Kill process 6578 (python) score 843 or sacrifice child


So this is a memory issue after all. But I believe the database isn't in memory, it's on disk, so what is taking up all the memory? What part of this eats through all the gigabytes of memory, or does docker limit it to less than that? (note, instead of the function above, I'm putting things in the database incrementally, in 10k row chunks. 

-- UPDATE 5 --

I moved to a ludicrously big VM and the error went away. But now it still fails when I go to query the database with 1000 keys at once. All it says is ""worker killed"". The logs don't seem to have any relevant information, making this debugging difficult and frustrating. 

I looked at docker stats &lt;container&gt; and it seems the memory pressure is at 10%, which should be low enough. 

What don't I understand about docker? Why is it arbitrarily killing my containers?

-- UPDATE 6 --

I think the scope of this question has moved too much. It seems like there may have been two separate issues: not enough memory to insert everything into the database previously, and now the queries are timing out. WORKER TIMEOUT I think is actually a message from gunicorn. I'm going to open a new question about gunicorn timeouts and sqlalchemy queries. 

If someone finds similar issues in the future, the way I got through not being able to add everything is just increasing the size of the VM. Seems like it should be able to run on a smaller VM, but I guess you do what you can. 
",0,1149,"In order to do this, a better approach would be to use an asynchronous task queue alongside flask since you want to your application to be free to respond to client requests.

as mentioned in my above comment, celery could be an option. (http://www.celeryproject.org/)
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/34708435,Sqlalchemy Query Succeeds in Sublime Text IDE but fails when executed from the command line,"Hey everyone. I'm building an Albums DB application for a college project using sqlalchemy and pyqt (as both are libraries I'd like to gain a little experience with), all written in Python 3.
My IDE of choice is Sublime 3 and my database server is Mariadb running on an Ubuntu 14.04 64 bit variant (Elementary OS).

The problem I'm having is quiet strange and I don't know where to go from here (I actually thought I was nearly finished, duh!).

If I execute the application from inside Sublime (f7) then the program functions the way I would expect with all queries returning the desired values (hence why I thought I was nearly done).
However, If I make the script executable and execute it from the command line or from inside another IDE (Geany in this case) the thing falls apart but only when trying to make certain queries based on filtering arguments provided by user input. Queries elsewhere in the program succeed e.g. Songs.query.all() will return all songs in the DB.

The code below is a snippet of where the error occurs in the 'Read' section of the application. 'table' refers to a Table object (Songs, Albums or Artists) and the filter column &amp; clause refer to string arguments entered by the user.

    elif function == 'Read':

        # Debug Print - Display Details
        print('Table is ' + str(table))
        print('Clause is ' + clause)
        # Carry out the Query - Should only return 1 match - Fails Here
        r = table.query.filter(
            getattr(table, column.lower()) == clause).one()

        if self.db == 'Songs':

            # Query Corresponding Album &amp; Artist
            al = Albums.query.get(int(r.album_id))
            ar = Artists.query.get(int(r.artist_id))

            headers = 'Name;Artist;Album;Track Number;Genre'
            data = [r.name, ar.name, al.name,
                    r.track_num, r.genre]


My Models are as follows:

    class Artists(Base):
        __tablename__ = 'artists'

        artist_id = Column(Integer, autoincrement=True, primary_key=True)
        name = Column(String(30))

        albums = relationship(
            'Albums', backref='artists', cascade='all, delete-orphan')
        songs = relationship(
             'Songs', backref='artists', cascade='all, delete-orphan')

    class Albums(Base):
        __tablename__ = 'albums'

        album_id = Column(Integer, autoincrement=True, primary_key=True)
        name = Column(String(30), nullable=False)
        artist_id = Column(Integer, ForeignKey('artists.artist_id'))
        created_on = Column(
            DateTime(timezone=True), default=datetime.now, onupdate=datetime.now)

        songs = relationship(
            ""Songs"", backref=""albums"", cascade=""all, delete-orphan"")

    class Songs(Base):
        __tablename__ = 'songs'

        song_id = Column(Integer, autoincrement=True, primary_key=True)
        name = Column(String(50))
        track_num = Column(Integer)
        genre = Column(String(30))
        created_on = Column(
            DateTime(timezone=True), default=datetime.now, onupdate=datetime.now)

        album_id = Column(Integer, ForeignKey('albums.album_id'))
        artist_id = Column(Integer, ForeignKey('artists.artist_id'))


The Error when trying to match a song that is present in the DB is :


  sqlalchemy.exc.ProgrammingError: (_mysql_exceptions.ProgrammingError) (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '%s' at line 3"") [SQL: 'SELECT songs.song_id AS songs_song_id, songs.name AS songs_name, songs.track_num AS songs_track_num, songs.genre AS songs_genre, songs.created_on AS songs_created_on, songs.album_id AS songs_album_id, songs.artist_id AS songs_artist_id \nFROM songs \nWHERE songs.name = %s'] [parameters: ('WHAT WENT DOWN',)]




Any help on this issue would be wonderful, and if there is any additional information needed I'll be more than happy to provide. Thanks in advance
",0,225,"The error is complaining about the syntax of %s as a bind parameter. If I'm not mistaken, MySQL syntax uses ? for positional and :name for keyword bind params. %s is what the psycopg2 library uses because it does its own statement preparation. Your engine configuration is not shown here.

As for the difference between Sublime and running from console, look for (a) any potential difference in the Python interpreter being used and (b) any potential difference in imports due to PYTHONPATH issues.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/9814632,running a bottle app from mod_wsgi handle results in maximum recursion depth exceeded while calling a Python object,,0,1907,,,


NLTK unexpected issue,https://stackoverflow.com/questions/38966075,How to restrict anaconda from upgrading the module being installed if its a higher level dependency,,3,577,"Unless there's a specific reason you need to compile python yourself, I think what you're actually going after is conda bundle (http://conda.pydata.org/docs/commands/conda-bundle.html).  Unfortunately we've removed it in conda 4.2 which will be coming out soon, intending to move it to conda-build.  Since that hasn't happened yet, and if it ends up actually being useful to people, we can add it back.



You could also try this using conda-build...

Remove the whole source block in your meta.yaml file. Also remove all of the build requirements that are also not run requirements.  Then in your build.sh file

conda install --yes --quiet \
    python=2.7.10 \
    ipython=5.0.0 \
    numpy=1.11.1 \
    cython=0.24.1 \
    scipy=0.18.0 \
    pandas=0.18.1 \
    patsy=0.4.1 \
    statsmodels=0.6.1 \
    matplotlib=1.5.2 \
    ggplot=0.9.4 \
    scikit-learn=0.17.1 \
    distribute=0.6.45 \
    backports.ssl-match-hostname=3.5.0.1 \
    certifi=14.05.14 \
    nose_parameterized=0.5.0 \
    pyparsing=2.1.4 \
    python-dateutil=2.5.3 \
    pytz=2016.6.1 \
    pyzmq=15.3.0 \
    simplejson=3.3.3 \
    six=1.10.0 \
    sympy=1.0 \
    tornado=4.4.1 \
    virtualenv=13.0.1 \
    wsgiref=0.1.2 \
    python-swiftclient=2.7.0 \
    python-cinderclient=1.1.2 \
    python-glanceclient=0.17.2 \
    python-neutronclient=2.4.0 \
    networkx=1.11 \
    pysal=1.11.1 \
    pyyaml=3.11 \
    shapely=1.5.13 \
    beautifulsoup4=4.4.1 \
    nltk=3.2.1 \
    requests=2.10.0 \
    seaborn=0.5.0 \
    h5py=2.6.0 \
    xlrd=1.0.0 \
    markupsafe=0.23 \
    crypto=1.1.0 \
    jinja2=2.8 \
    openpyxl=2.3.2 \
    jaro_winkler=1.0.2 \
    bokeh=0.12.1 \
    numexpr=2.6.1 \
    pytables=3.2.3.1 \
    pycurl=7.43.0 \
    mgrs=1.1.0 \
    psutil=4.3.0 \
    biopython=1.67 \
    enaml=0.9.8 \
    mdp=3.5 \
    bitarray=0.8.1 \
    clusterpy=0.9.9 \
    pyside=1.2.1 \
    pyqt=4.11.4 \
    parsedatetime=1.4 \
    pymysql=0.6.7 \
    pyodbc=3.0.10 \
    tabulate=0.7.2


The big difference: by listing all of those packages as build requirements, you're actually ensuring that they won't be in your final conda package.  Think of build requirements more like a compiler, or something that's necessary when you're building the package, but not when you're actually running it.
",,
NLTK strange behavior,https://stackoverflow.com/questions/33157847,Lemmatizing words after POS tagging produces unexpected results,"I am using python3.5 with the nltk pos_tag function and the WordNetLemmatizer. My goal is to flatten words in our database to classify text. I am trying to test using the lemmatizer and I encounter strange behavior when using the POS tagger on identical tokens. In the example below, I have a list of three strings and when running them in the POS tagger every other element is returned as a noun(NN) and the rest are return as verbs (VBG). 

This affects the lemmatization. The out put looks like this:  

pos Of token: v
lemmatized token: skydive
pos Of token: n
lemmatized token: skydiving
pos Of token: v
lemmatized token: skydive


If I add more elements to the list of identical strings this same pattern continues. The code in full I am using is this:

tokens = ['skydiving', 'skydiving', 'skydiving']
lmtzr=WordNetLemmatizer()

def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return 'a'
    elif treebank_tag.startswith('V'):
        return 'v'
    elif treebank_tag.startswith('N'):
        return 'n'
    elif treebank_tag.startswith('R'):
        return 'r'
    elif treebank_tag.startswith('S'):
        return ''
    else:
        return ''

numTokens = (len(tokens))
for i in range(0,numTokens):
    tokens[i]=tokens[i].replace("" "","""")

noSpaceTokens = pos_tag(tokens)

for token in noSpaceTokens:
    tokenStr = str(token[1])
    noWhiteSpace = token[0].replace("" "", """")
    preLemmed = get_wordnet_pos(tokenStr)
    print(""pos Of token: "" + preLemmed)
    lemmed = lmtzr.lemmatize(noWhiteSpace,preLemmed)
    print(""lemmatized token: "" + lemmed)

",2,1531,"In short:

When POS tagging you need a context sentence not a list of ungrammatical tokens. 


E.g. to lemmatize a grammatical sentence, you can take a look at https://github.com/alvations/pywsd/blob/master/pywsd/utils.py#L100


When lemmatizing out of context sentence, the only way to get the right lemma is to manually specify the pos tags. 


E.g. from https://github.com/alvations/pywsd/blob/master/pywsd/utils.py#L66 , we have to specify the pos parameter for the lemmatize function. 
Otherwise it would be always assuming the n POS, see also WordNetLemmatizer not returning the right lemma unless POS is explicit - Python NLTK




In long:

POS tagger usually works on the full sentence and not individual words. When you try to tag a single word out of context, what you get is the most frequent tag.

To verify that when tagging a single word (i.e. a sentence with only 1 word), it always gives the same tag:

&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; ptb2wn_pos = {'J':'a', 'V':'v', 'N':'n', 'R':'r'}
&gt;&gt;&gt; sent = ['skydive']
&gt;&gt;&gt; most_frequent_tag = pos_tag(sent)[0][1]
&gt;&gt;&gt; most_frequent_tag
'JJ'
&gt;&gt;&gt; most_frequent_tag = ptb2wn_pos[most_frequent_tag[0]]
&gt;&gt;&gt; most_frequent_tag
'a'
&gt;&gt;&gt; for _ in range(1000): assert ptb2wn_pos[pos_tag(sent)[0][1][0]] == most_frequent_tag;
... 
&gt;&gt;&gt;


Now, since the tag is always 'a' by default if the sentence only have 1 word, then the WordNetLemmatizer will always return skydive:

&gt;&gt;&gt; wnl = WordNetLemmatizer()
&gt;&gt;&gt; wnl.lemmatize(sent[0], pos=most_frequent_tag)
'skydive'


Let's to to see the lemma of a word in context of a sentence:

&gt;&gt;&gt; sent2 = 'They skydrive from the tower yesterday'
&gt;&gt;&gt; pos_tag(sent2.split())
[('They', 'PRP'), ('skydrive', 'VBP'), ('from', 'IN'), ('the', 'DT'), ('tower', 'NN'), ('yesterday', 'NN')]
&gt;&gt;&gt; pos_tag(sent2.split())[1]
('skydrive', 'VBP')
&gt;&gt;&gt; pos_tag(sent2.split())[1][1]
'VBP'
&gt;&gt;&gt; ptb2wn_pos[pos_tag(sent2.split())[1][1][0]]
'v'


So the context of the input list of tokens matters when you do pos_tag.

In your example, you had a list ['skydiving', 'skydiving', 'skydiving'] meaning the sentence that you are pos-tagging is an ungrammatical sentence:


  skydiving skydiving skydiving


And the pos_tag function thinks is a normal sentence hence giving the tags:

&gt;&gt;&gt; sent3 = 'skydiving skydiving skydiving'.split()
&gt;&gt;&gt; pos_tag(sent3)
[('skydiving', 'VBG'), ('skydiving', 'NN'), ('skydiving', 'VBG')]


In which case the first is a verb, the second word a noun and the third word a verb, which will return the following lemma (which you do not desire):

&gt;&gt;&gt; wnl.lemmatize('skydiving', 'v')
'skydive'
&gt;&gt;&gt; wnl.lemmatize('skydiving', 'n')
'skydiving'
&gt;&gt;&gt; wnl.lemmatize('skydiving', 'v')
'skydive'


So if we have a valid grammatical sentence in your list of token, the output might look very different

&gt;&gt;&gt; sent3 = 'The skydiving sport is an exercise that promotes diving from the sky , ergo when you are skydiving , you feel like you are descending to earth .'
&gt;&gt;&gt; pos_tag(sent3.split())
[('The', 'DT'), ('skydiving', 'NN'), ('sport', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('exercise', 'NN'), ('that', 'IN'), ('promotes', 'NNS'), ('diving', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('sky', 'NN'), (',', ','), ('ergo', 'RB'), ('when', 'WRB'), ('you', 'PRP'), ('are', 'VBP'), ('skydiving', 'VBG'), (',', ','), ('you', 'PRP'), ('feel', 'VBP'), ('like', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('descending', 'VBG'), ('to', 'TO'), ('earth', 'JJ'), ('.', '.')]

",,
NLTK strange behavior,https://stackoverflow.com/questions/70286059,NLTK corpora download is hanging when run in AWS Lambda Python function,"I'm trying to download NLTK data onto the file storage of a Lambda function like so:
nltk.data.path.append(""/tmp"")
nltk.download(""popular"", download_dir=""/tmp"")

The Lambda function keeps timing out. When I check the Cloudwatch logs, I see no logs related to the download of different corpora files (e.g. Downloading package cmudict to /tmp...; instead the code seems to reach up to nltk.download(), then hang forever.
Has anyone seen this strange behavior?
",0,140,"Got it: My Lambda function was running in a VPC. I had to add an endpoint to enable the VPC to access S3.
",,
NLTK strange result,https://stackoverflow.com/questions/8842817,Selecting the most fluent text from a set of possibilities via grammar checking (Python),"Some background

I am a literature student at New College of Florida, currently working on an overly ambitious creative project. The project is geared towards the algorithmic generation of poetry. It's written in Python. My Python knowledge and Natural Language Processing knowledge come only from teaching myself things through the internet. I've been working with this stuff for about a year, so I'm not helpless, but at various points I've had trouble moving forward in this project. Currently, I am entering the final phases of development, and have hit a little roadblock.

I need to implement some form of grammatical normalization, so that the output doesn't come out as un- conjugated/inflected caveman-speak. About a month ago some friendly folks on SO gave me some advice on how I might solve this issue by using an ngram language modeller, basically -- but I'm looking for yet other solutions, as it seems that NLTK's NgramModeler is not fit for my needs. (The possibilities of POS tagging were also mentioned, but my text may be too fragmentary and strange for an implementation of such to come easy, given my amateur-ness.)

Perhaps I need something like AtD, but hopefully less complex

I think need something that works like After the Deadline or Queequeg, but neither of these seem exactly right. Queequeg is probably not a good fit -- it was written in 2003 for Unix and I can't get it working on Windows for the life of me (have tried everything). But I like that all it checks for is proper verb conjugation and number agreement.

On the other hand, AtD is much more rigorous, offering more capabilities than I need. But I can't seem to get the python bindings for it working. (I get 502 errors from the AtD server, which I'm sure are easy to fix, but my application is going to be online, and I'd rather avoid depending on another server. I can't afford to run an AtD server myself, because the number of ""services"" my application is going to require of my web host is already threatening to cause problems in getting this application hosted cheaply.)

Things I'd like to avoid

Building Ngram language models myself doesn't seem right for the task. my application throws a lot of unknown vocabulary, skewing all the results. (Unless I use a corpus that's so large that it runs way too slow for my application -- the application needs to be pretty snappy.)

Strictly checking grammar is neither right for the task. the grammar doesn't need to be perfect, and the sentences don't have to be any more sensible than the kind of English-like jibberish that you can generate using ngrams. Even if it's jibberish, I just need to enforce verb conjugation, number agreement, and do things like remove extra articles.

In fact, I don't even need any kind of suggestions for corrections. I think all I need is for something to tally up how many errors seem to occur in each sentence in a group of possible sentences, so I can sort by their score and pick the one with the least grammatical issues.

A simple solution? Scoring fluency by detecting obvious errors

If a script exists that takes care of all this, I'd be overjoyed (I haven't found one yet). I can write code for what I can't find, of course; I'm looking for advice on how to optimize my approach.

Let's say we have a tiny bit of text already laid out:

existing_text = ""The old river""

Now let's say my script needs to figure out which inflection of the verb ""to bear"" could come next. I'm open to suggestions about this routine. But I need help mostly with step #2, rating fluency by tallying grammatical errors:


Use the Verb Conjugation methods in NodeBox Linguistics to come up with all conjugations of this verb; ['bear', 'bears', 'bearing', 'bore', 'borne'].
Iterate over the possibilities, (shallowly) checking the grammar of the string resulting from existing_text + "" "" + possibility (""The old river bear"", ""The old river bears"", etc). Tally the error count for each construction. In this case the only construction to raise an error, seemingly, would be ""The old river bear"".
Wrapping up should be easy... Of the possibilities with the lowest error count, select randomly.

",30,3461,"Very cool project, first of all. 

I found a java grammar checker. I've never used it but the docs claim it can run as a server. Both java and listening to a port should be supported basically anywhere. 

I'm just getting into NLP with a CS background so I wouldn't mind going into more detail to help you integrate whatever you decide on using. Feel free to ask for more detail.
","The pylinkgrammar link provided above is a bit out of date.  It points to version 0.1.9, and the code samples for that version no longer work.  If you go down this path, be sure to use the latest version which can be found at:

https://pypi.python.org/pypi/pylinkgrammar
","Another approach would be to use what is called an overgenerate and rank approach.  In the first step you have your poetry generator generate multiple candidate generations.  Then using a service like Amazon's Mechanical Turk to collect human judgments of fluency.  I would actually suggest collecting simultaneous judgments for a number of sentences generated from the same seed conditions.  Lastly, you extract features from the generated sentences (presumably using some form of syntactic parser) to train a model to rate or classify question quality.  You could even thrown in the heuristics listed above.

Michael Heilman uses this approach for question generation.  For more details, read these papers:
Good Question! Statistical Ranking for Question Generation and
Rating Computer-Generated Questions with Mechanical Turk.
"
NLTK strange result,https://stackoverflow.com/questions/10407266,scikits learn and nltk: Naive Bayes classifier performance highly different,,14,5547,"Naive Bayes classifier usually means a Bayesian classfier over binary features that are assumed to be independent. This is what NLTK's Naive Bayes classifier implements. The corresponding scikit classifier is BernoulliNB classifier.

The restriction to boolean valued features is not actually necessary,
it is just the simplest to implement.
A naive Bayes classifier can be defined for (assumed) independent features from any parametric distribution.

MultinomialNB is for data with integer valued input features that are assumed to be multinomially distributed.

Sckit also has GaussianNB that for continuous valued features that are assumed to idependently Gaussian distributed.
","Is the default behavior for class weights the same in both libraries?  The difference in precision for the rare class (-1) looks like that might be the cause...
",
NLTK strange result,https://stackoverflow.com/questions/40480839,NLTK relation extraction returns nothing,"I am recently working on using nltk to extract relation from text. so i build a sample text:"" Tom is the cofounder of Microsoft."" and using following program to test and return nothing. I cannot figure out why.

I'm using NLTK version: 3.2.1, python version: 3.5.2.

Here is my code:

import re
import nltk
from nltk.sem.relextract import extract_rels, rtuple
from nltk.tokenize import sent_tokenize, word_tokenize


def test():
    with open('sample.txt', 'r') as f:
        sample = f.read()   # ""Tom is the cofounder of Microsoft""

    sentences = sent_tokenize(sample)
    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]
    tagged_sentences = [nltk.tag.pos_tag(sentence) for sentence in tokenized_sentences]

    OF = re.compile(r'.*\bof\b.*')

    for i, sent in enumerate(tagged_sentences):
        sent = nltk.chunk.ne_chunk(sent) # ne_chunk method expects one tagged sentence
        rels = extract_rels('PER', 'GPE', sent, corpus='ace', pattern=OF, window=10) 
        for rel in rels:
            print('{0:&lt;5}{1}'.format(i, rtuple(rel)))

if __name__ == '__main__':
    test()




1.After some debug, if found that when i changed the input as


  ""Gates was born in Seattle, Washington on October 28, 1955. ""


the nltk.chunk.ne_chunk() output is:


  (S
    (PERSON Gates/NNS)
    was/VBD
    born/VBN
    in/IN
    (GPE Seattle/NNP)
    ,/,
    (GPE Washington/NNP)
    on/IN
    October/NNP
    28/CD
    ,/,
    1955/CD
    ./.)


The test() returns:


  [PER: 'Gates/NNS'] 'was/VBD born/VBN in/IN' [GPE: 'Seattle/NNP']


2. After i changed the input as:


  ""Gates was born in Seattle on October 28, 1955. ""


The test() retuns nothing.

3. I digged into nltk/sem/relextract.py and find this strange

output is caused by function:
semi_rel2reldict(pairs, window=5, trace=False), which returns result only when len(pairs) &gt; 2, and that's why when one sentence with less than three NEs will return None.

Is this a bug or i used NLTK in wrong way?
",6,3435,"Firstly, to chunk NEs with ne_chunk, the idiom would look something like this 

&gt;&gt;&gt; from nltk import ne_chunk, pos_tag, word_tokenize
&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; chunked
Tree('S', [Tree('PERSON', [('Tom', 'NNP')]), ('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN'), Tree('ORGANIZATION', [('Microsoft', 'NNP')])])


(see also https://stackoverflow.com/a/31838373/610569)

Next let's look at the extract_rels function.

def extract_rels(subjclass, objclass, doc, corpus='ace', pattern=None, window=10):
    """"""
    Filter the output of ``semi_rel2reldict`` according to specified NE classes and a filler pattern.
    The parameters ``subjclass`` and ``objclass`` can be used to restrict the
    Named Entities to particular types (any of 'LOCATION', 'ORGANIZATION',
    'PERSON', 'DURATION', 'DATE', 'CARDINAL', 'PERCENT', 'MONEY', 'MEASURE').
    """"""


When you evoke this function:

extract_rels('PER', 'GPE', sent, corpus='ace', pattern=OF, window=10)


It performs 4 processes sequentially.

1. It checks whether your subjclass and objclassare valid

i.e. https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L202 :

if subjclass and subjclass not in NE_CLASSES[corpus]:
    if _expand(subjclass) in NE_CLASSES[corpus]:
        subjclass = _expand(subjclass)
    else:
        raise ValueError(""your value for the subject type has not been recognized: %s"" % subjclass)
if objclass and objclass not in NE_CLASSES[corpus]:
    if _expand(objclass) in NE_CLASSES[corpus]:
        objclass = _expand(objclass)
    else:
        raise ValueError(""your value for the object type has not been recognized: %s"" % objclass)


2. It extracts ""pairs"" from your NE tagged inputs:

if corpus == 'ace' or corpus == 'conll2002':
    pairs = tree2semi_rel(doc)
elif corpus == 'ieer':
    pairs = tree2semi_rel(doc.text) + tree2semi_rel(doc.headline)
else:
    raise ValueError(""corpus type not recognized"")


Now let's see given your input sentence Tom is the cofounder of Microsoft, what does tree2semi_rel() returns:

&gt;&gt;&gt; from nltk.sem.relextract import tree2semi_rel, semi_rel2reldict
&gt;&gt;&gt; from nltk import word_tokenize, pos_tag, ne_chunk
&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]


So it returns a list of 2 lists, the first inner list consist of a blank list and the Tree that contains the ""PERSON"" tag.

[[], Tree('PERSON', [('Tom', 'NNP')])] 


The second list consist of the phrase is the cofounder of and the Tree that contains ""ORGANIZATION"".

Let's move on.

3. extract_rel then tries to change the pairs to some sort of relation dictionary

reldicts = semi_rel2reldict(pairs)


If we look what the semi_rel2reldict function returns with your example sentence, we see that this is where the empty list gets returns:

&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[]


So let's look into the code of semi_rel2reldict https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L144:

def semi_rel2reldict(pairs, window=5, trace=False):
    """"""
    Converts the pairs generated by ``tree2semi_rel`` into a 'reldict': a dictionary which
    stores information about the subject and object NEs plus the filler between them.
    Additionally, a left and right context of length =&lt; window are captured (within
    a given input sentence).
    :param pairs: a pair of list(str) and ``Tree``, as generated by
    :param window: a threshold for the number of items to include in the left and right context
    :type window: int
    :return: 'relation' dictionaries whose keys are 'lcon', 'subjclass', 'subjtext', 'subjsym', 'filler', objclass', objtext', 'objsym' and 'rcon'
    :rtype: list(defaultdict)
    """"""
    result = []
    while len(pairs) &gt; 2:
        reldict = defaultdict(str)
        reldict['lcon'] = _join(pairs[0][0][-window:])
        reldict['subjclass'] = pairs[0][1].label()
        reldict['subjtext'] = _join(pairs[0][1].leaves())
        reldict['subjsym'] = list2sym(pairs[0][1].leaves())
        reldict['filler'] = _join(pairs[1][0])
        reldict['untagged_filler'] = _join(pairs[1][0], untag=True)
        reldict['objclass'] = pairs[1][1].label()
        reldict['objtext'] = _join(pairs[1][1].leaves())
        reldict['objsym'] = list2sym(pairs[1][1].leaves())
        reldict['rcon'] = _join(pairs[2][0][:window])
        if trace:
            print(""(%s(%s, %s)"" % (reldict['untagged_filler'], reldict['subjclass'], reldict['objclass']))
        result.append(reldict)
        pairs = pairs[1:]
    return result


The first thing that semi_rel2reldict() does is to check where there are more than 2 elements the output from tree2semi_rel(), which your example sentence doesn't:

&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; len(tree2semi_rel(chunked))
2
&gt;&gt;&gt; len(tree2semi_rel(chunked)) &gt; 2
False


Ah ha, that's why the extract_rel is returning nothing.

Now comes the question of how to make extract_rel() return something even with 2 elements from tree2semi_rel()? Is that even possible?

Let's try a different sentence:

&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft and now he is the founder of Marcohard""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; chunked
Tree('S', [Tree('PERSON', [('Tom', 'NNP')]), ('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN'), Tree('ORGANIZATION', [('Microsoft', 'NNP')]), ('and', 'CC'), ('now', 'RB'), ('he', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('founder', 'NN'), ('of', 'IN'), Tree('PERSON', [('Marcohard', 'NNP')])])
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])], [[('and', 'CC'), ('now', 'RB'), ('he', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('founder', 'NN'), ('of', 'IN')], Tree('PERSON', [('Marcohard', 'NNP')])]]
&gt;&gt;&gt; len(tree2semi_rel(chunked)) &gt; 2
True
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': 'and/CC now/RB he/PRP is/VBZ the/DT', 'subjtext': 'Tom/NNP'})]


But that only confirms that extract_rel can't extract when tree2semi_rel returns pairs of &lt; 2. What happens if we remove that condition of while len(pairs) &gt; 2?

Why can't we do while len(pairs) &gt; 1?

If we look closer into the code, we see the last line of populating the reldict, https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L169:

reldict['rcon'] = _join(pairs[2][0][:window])


It tries to access a 3rd element of the pairs and if the length of the pairs is 2, you'll get an IndexError.

So what happens if we remove that rcon key and simply change it to while len(pairs) &gt;= 2?

To do that we have to override the semi_rel2redict() function:

&gt;&gt;&gt; from nltk.sem.relextract import _join, list2sym
&gt;&gt;&gt; from collections import defaultdict
&gt;&gt;&gt; def semi_rel2reldict(pairs, window=5, trace=False):
...     """"""
...     Converts the pairs generated by ``tree2semi_rel`` into a 'reldict': a dictionary which
...     stores information about the subject and object NEs plus the filler between them.
...     Additionally, a left and right context of length =&lt; window are captured (within
...     a given input sentence).
...     :param pairs: a pair of list(str) and ``Tree``, as generated by
...     :param window: a threshold for the number of items to include in the left and right context
...     :type window: int
...     :return: 'relation' dictionaries whose keys are 'lcon', 'subjclass', 'subjtext', 'subjsym', 'filler', objclass', objtext', 'objsym' and 'rcon'
...     :rtype: list(defaultdict)
...     """"""
...     result = []
...     while len(pairs) &gt;= 2:
...         reldict = defaultdict(str)
...         reldict['lcon'] = _join(pairs[0][0][-window:])
...         reldict['subjclass'] = pairs[0][1].label()
...         reldict['subjtext'] = _join(pairs[0][1].leaves())
...         reldict['subjsym'] = list2sym(pairs[0][1].leaves())
...         reldict['filler'] = _join(pairs[1][0])
...         reldict['untagged_filler'] = _join(pairs[1][0], untag=True)
...         reldict['objclass'] = pairs[1][1].label()
...         reldict['objtext'] = _join(pairs[1][1].leaves())
...         reldict['objsym'] = list2sym(pairs[1][1].leaves())
...         reldict['rcon'] = []
...         if trace:
...             print(""(%s(%s, %s)"" % (reldict['untagged_filler'], reldict['subjclass'], reldict['objclass']))
...         result.append(reldict)
...         pairs = pairs[1:]
...     return result
... 
&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': [], 'subjtext': 'Tom/NNP'})]


Ah! It works but there's still a 4th step in extract_rels().

4. It performs a filter of the reldict given the regex you have provided to the pattern parameter, https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L222:

relfilter = lambda x: (x['subjclass'] == subjclass and
                       len(x['filler'].split()) &lt;= window and
                       pattern.match(x['filler']) and
                       x['objclass'] == objclass)


Now let's try it with the hacked version of semi_rel2reldict:

&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': [], 'subjtext': 'Tom/NNP'})]
&gt;&gt;&gt; 
&gt;&gt;&gt; pattern = re.compile(r'.*\bof\b.*')
&gt;&gt;&gt; reldicts = semi_rel2reldict(tree2semi_rel(chunked))
&gt;&gt;&gt; relfilter = lambda x: (x['subjclass'] == subjclass and
...                            len(x['filler'].split()) &lt;= window and
...                            pattern.match(x['filler']) and
...                            x['objclass'] == objclass)
&gt;&gt;&gt; relfilter
&lt;function &lt;lambda&gt; at 0x112e591b8&gt;
&gt;&gt;&gt; subjclass = 'PERSON'
&gt;&gt;&gt; objclass = 'ORGANIZATION'
&gt;&gt;&gt; window = 5
&gt;&gt;&gt; list(filter(relfilter, reldicts))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': [], 'subjtext': 'Tom/NNP'})]


It works! Now let's see it in tuple form:

&gt;&gt;&gt; from nltk.sem.relextract import rtuple
&gt;&gt;&gt; rels = list(filter(relfilter, reldicts))
&gt;&gt;&gt; for rel in rels:
...     print rtuple(rel)
... 
[PER: 'Tom/NNP'] 'is/VBZ the/DT cofounder/NN of/IN' [ORG: 'Microsoft/NNP']

",,
NLTK strange result,https://stackoverflow.com/questions/62735456,Understanding and using Coreference resolution Stanford NLP tool (in Python 3.7),"I am trying to understand the Coreference NLP Stanford tools.
This is my code and it is working:
import os
os.environ[""CORENLP_HOME""] = ""/home/daniel/StanfordCoreNLP/stanford-corenlp-4.0.0""

from stanza.server import CoreNLPClient

text = 'When he came from Brazil, Daniel was fortied with letters from Conan but otherwise did not know a soul except Herbert. Yet this giant man from the Northeast, who had never worn an overcoat or experienced a change of seasons, did not seem surprised by his past.'

with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
               properties={'annotators': 'coref', 'coref.algorithm' : 'neural'},timeout=30000, memory='16G') as client:

    ann = client.annotate(text)

chains = ann.corefChain
chain_dict=dict()
for index_chain,chain in enumerate(chains):
    chain_dict[index_chain]={}
    chain_dict[index_chain]['ref']=''
    chain_dict[index_chain]['mentions']=[{'mentionID':mention.mentionID,
                                          'mentionType':mention.mentionType,
                                          'number':mention.number,
                                          'gender':mention.gender,
                                          'animacy':mention.animacy,
                                          'beginIndex':mention.beginIndex,
                                          'endIndex':mention.endIndex,
                                          'headIndex':mention.headIndex,
                                          'sentenceIndex':mention.sentenceIndex,
                                          'position':mention.position,
                                          'ref':'',
                                          } for mention in chain.mention ]


for k,v in chain_dict.items():
    print('key',k)
    mentions=v['mentions']
    for mention in mentions:
        words_list = ann.sentence[mention['sentenceIndex']].token[mention['beginIndex']:mention['endIndex']]
        mention['ref']=' '.join(t.word for t in words_list)
        print(mention['ref'])
    

I tried three algorithms:

statistical (as in the code above). Results:


he
this giant man from the Northeast , who had never worn an overcoat or experienced a change of seasons
Daniel
his



neural


this giant man from the Northeast , who had never worn an overcoat or experienced a change of seasons ,
his



deterministic (I got the error below)
 &gt; Starting server with command: java -Xmx16G -cp
 &gt; /home/daniel/StanfordCoreNLP/stanford-corenlp-4.0.0/*
 &gt; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout
 &gt; 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties
 &gt; corenlp_server-9fedd1e9dfb14c9e.props -preload
 &gt; tokenize,ssplit,pos,lemma,ner,parse,depparse,coref Traceback (most
 &gt; recent call last):
 &gt; 
 &gt;   File ""&lt;ipython-input-58-0f665f07fd4d&gt;"", line 1, in &lt;module&gt;
 &gt;     runfile('/home/daniel/Documentos/Working Papers/Leader traits/Code/20200704 - Modeling
 &gt; Organizing/understanding_coreference.py',
 &gt; wdir='/home/daniel/Documentos/Working Papers/Leader
 &gt; traits/Code/20200704 - Modeling Organizing')
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"",
 &gt; line 827, in runfile
 &gt;     execfile(filename, namespace)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"",
 &gt; line 110, in execfile
 &gt;     exec(compile(f.read(), filename, 'exec'), namespace)
 &gt; 
 &gt;   File ""/home/daniel/Documentos/Working Papers/Leader
 &gt; traits/Code/20200704 - Modeling
 &gt; Organizing/understanding_coreference.py"", line 21, in &lt;module&gt;
 &gt;     ann = client.annotate(text)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/stanza/server/client.py"",
 &gt; line 470, in annotate
 &gt;     r = self._request(text.encode('utf-8'), request_properties, **kwargs)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/stanza/server/client.py"",
 &gt; line 404, in _request
 &gt;     raise AnnotationException(r.text)
 &gt; 
 &gt; AnnotationException: java.lang.RuntimeException:
 &gt; java.lang.IllegalArgumentException: No enum constant
 &gt; edu.stanford.nlp.coref.CorefProperties.CorefAlgorithmType.DETERMINISTIC



Questions:

Why am I getting this error with the deterministic?

Any piece of code using the NLP Stanford in Python seems to be much slower than the codes related with Spacy or NLTK. I know that there is no coreference in these other libraries. But for instance when I use import nltk.parse.stanford import StanfordDependencyParser for dependence parse it is much faster then this StanfordNLP library. Is there any way to acelerate this CoreNLPClient in Python?

I will use this library to work with long texts. Is it better to work with smaller pieces with the entire text? Long texts can cause wrong results for coreference resolution (I have found very strange results for this coreference library when I am using long texts)? Is there an optimal size?

Results:


The results from the statistical algorithm seems to be better. I expected that the best result would come from the neural algorithm. Do you agree with me? There are 4 valid mention in the statistical algorithm while only 2 when I am using the neural algorithm.
Am I missing something?
",2,1442,"
You may find the list of supported algorithms in Java documentation: link

You might want to start the server and then just use it, something like
# Here's the slowest partmodels are being loaded
client = CoreNLPClient(...)

ann = client.annotate(text)

...

client.stop()



But I cannot give you any clue regarding 3 and 4.
",,
NLTK strange result,https://stackoverflow.com/questions/48030920,"Tweet Feels: Always returns the same Sentiment Score, regardless tags","I am trying to use this library to generate sentiment score for cryptocurrencies:

https://github.com/uclatommy/tweetfeels/blob/master/README.md

When I use the code from the example trump, it returns a sentiment score of -0.00082536637608123106.

I have changed the tags to the following:

btc_feels = TweetFeels(login, tracking=['bitcoin'])
btc_feels.start(20)
btc_feels.sentiment.value


and it still gives me the same value.

I did notice something strange when I installed the library.

from the instructions:


  If for some reason pip did not install the vader lexicon:
  
  
    python3 -m nltk.downloader vader_lexicon
  


When I ran this, I got: 


  /anaconda/lib/python3.6/runpy.py:125: RuntimeWarning:
  'nltk.downloader' found in sys.modules after import of package 'nltk',
  but prior to execution of 'nltk.downloader'; this may result in
  unpredictable behaviour   warn(RuntimeWarning(msg))


Could this be why it appears not to be working?
",2,267,"By default, tweetfeels creates a database in your current directory. The next time you start the program, it will continue using the same database, and pick up where it left off. I don't know what tweetfeels does to handle you changing the keyword on it, but this behaviour of tweetfeels could be a problem. The solution would be to use a different database for different keywords, and then pass in the location of your database to the TweetFeels constructor.

I don't know that much about Tweetfeels, it just sounded interesting, so I've downloaded the project, and I have a working script that will perform the sentiment analysis on any keyword I give it. I can add a copy of the script here, if you're still having problems getting TweetFeels to work.



Edit: here the script I am using

I am currently having the following problems with the script.

1) I was getting some error that was different from the one you'd got, but I was able to fix the issue by replacing the tweetfeels library from pip with the latest code in their Github repository.

2) If a sentiment value does not get reported, sometimes tweetfeels fails to come to a complete stop, without forcefully sending a ctrl+c keyboard interrupt.

import os, sys, time
from threading import Thread
from pathlib import Path

from tweetfeels import TweetFeels

consumer_key = 'em...'
consumer_secret = 'aF...'
access_token = '25...'
access_token_secret = 'd3...'
login = [consumer_key, consumer_secret, access_token, access_token_secret]

try:
    kw = sys.argv[1]
except IndexError:
    kw = ""iota""

try:
    secs = int(sys.argv[2])
except IndexError:
    secs = 15

for arg in sys.argv:
    if (arg == ""-h"" or arg == ""--help""):
        print(""Gets sentiment from twitter.\n""
              ""Pass in a search term, and how frequently you would like the sentiment recalculated (defaults to 15 seconds).\n""
              ""The keyword can be a comma seperated list of keywords to look at."")
        sys.exit(0)

db = Path(f""~/tweetfeels/{kw}.sqlite"").expanduser()
if db.exists():
    print(""existing db detected. Continueing from where the last sentiment stream left off"")
else:
    #ensure the parent folder exists, the db will be created inside of this folder
    Path(f""~/tweetfeels"").expanduser().mkdir(exist_ok=True)

feels = TweetFeels(login, tracking=kw.split("",""), db=str(db))

go_on = True
def print_feels(feels, seconds):
    while go_on:
        if feels.sentiment:
            print(f""{feels.sentiment.volume} tweets analyzed from {feels.sentiment.start} to {feels.sentiment.end}"")
            print(f'[{time.ctime()}] Sentiment Score: {feels.sentiment.value}')
            print(flush=True)
        else:
            print(f""The datastream has not reported a sentiment value."")
            print(f""It takes a little bit for the first tweets to be analyzed (max of {feels._stream.retry_time_cap + seconds} seconds)."")
            print(""If this problem persists, there may not be anyone tweeting about the keyword(s) you used"")
            print(flush=True)
        time.sleep(seconds)


t = Thread(target=print_feels, kwargs={""feels"":feels,""seconds"":secs}, daemon=True)
print(f'Twitter posts containing the keyword(s) ""{kw}"" will be streamed, and a new sentiment value will be recalculated every {secs} seconds')
feels.start()
time.sleep(5)
t.start()

try:
    input(""Push enter at any time to stop the feed...\n\n"")
except (Exception, KeyboardInterrupt) as e:
    feels.stop()
    raise e

feels.stop()
go_on = False
print(f""Stopping feed. It may take up to {feels._stream.retry_time_cap} for the feed to shut down.\n"")
#we're waiting on the feels thread to stop

","No, the same sentiment value that you see printed is not related to the warning you've got when downloading the dataset.

The problem with the same sentiment score is coming from these lines:

for s in sentiments:
    pass
return s


I suspect that this unbound variable s remembers the previous value of the sentiment score.

But, the problem itself is that you are printing out the score right after you execute the start() function which starts a multi-threaded program to constantly update data from twitter - you should not expect the sentiment score to arrive right after you started the update. 

Note that the examples in the README are shown from the Python terminal where they wait after the execution of start() function until the Timer completed. Disconnecting now... message appears.
",
NLTK strange result,https://stackoverflow.com/questions/39865557,sentiment analysis joint list,,1,68,"You would do:

second_array = [' '.join(each) for each in first_array]


Alternatively you can tell sklearn.CountVectorizer to just use your tokens:

vect = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)
X = vect.fit_transform(first_array)

",,
NLTK strange result,https://stackoverflow.com/questions/63508107,"What causes the problem: csv, pandas or nltk?","I have a strange problem resulting in wrong output delivered by NLTK collocations. In short, when I pass pandas object created in python envi (PyCharm or Jupyter) to the function I get correct result. When I save this object to csv and upload it to the pandas object, functions returns single letters and/or numbers instead of full words. Must be sth wrong with csv upload through pandas but I have no idea what is wrong...
here is the code.
Function that is applied:
def counts(x):
    trigram_measures = nltk.collocations.BigramAssocMeasures()
    finder = BigramCollocationFinder.from_documents(x)
    finder.nbest(trigram_measures.pmi, 100)

    s = pd.Series(x)
    ngram_list = [pair for row in s for pair in ngrams(row, 3)]
    c = Counter(ngram_list).most_common(3)

    return pd.DataFrame([(x.name, ) + element for element in c], columns=['group', 'Ngram', 'Frequency'])

Here is the object:
d = {'words' : pd.Series((['coffee', 'maker', 'brewing', 'properly', '2', '420', '420', '420'],
    ['galley', 'work', 'table', 'stuck'],
    ['cloth', 'stuck'],
    ['stuck', 'coffee'])),
    'group' : pd.Series([1, 2, 1, 2])}
df_cleaned = pd.DataFrame(d)

Then I apply function from above + some extra functions:
output = df_cleaned.groupby('group', as_index=False).words.apply(counts).reset_index(drop=True)

Result is correct:

But when pandas object is saved and uploaded result is sth like this:
here is a code for saving and uploading:
df.to_csv('test_file.csv', index=False, sep=',')

df = pd.read_csv('path/test_file.csv',
sep=',', usecols=['group','words']) 

I found quotes in uploaded pandas object therefore I had removed them before applying the fucntion""
df = df.replace({'\'': ''}, regex=True)

output = df_cleaned.groupby('group', as_index=False).words.apply(counts).reset_index(drop=True)

Now it returns wrong results.

Do have any suggestions which way shall I go?
",0,65,"I reproduced what you described in the following steps. I don't see any errors
import pandas as pd
d = {'words' : pd.Series((['coffee', 'maker', 'brewing', 'properly', '2','420', '420', '420'],
    ['galley', 'work', 'table', 'stuck'],
    ['cloth', 'stuck'],
    ['stuck', 'coffee'])),
    'group' : pd.Series([1, 2, 1, 2])}
df_cleaned = pd.DataFrame(d)
df_cleaned

The function you're using is
import nltk
from nltk.util import ngrams
from nltk.collocations import *
from collections import Counter
def counts(x):
    trigram_measures = nltk.collocations.BigramAssocMeasures()
    finder = BigramCollocationFinder.from_documents(x)
    finder.nbest(trigram_measures.pmi, 100)
    s = pd.Series(x)
    ngram_list = [pair for row in s for pair in ngrams(row, 3)]
    c = Counter(ngram_list).most_common(3)
    return pd.DataFrame([(x.name, ) + element for element in c], columns=['group', 'Ngram', 'Frequency'])

You then apply counts to the data
output = df_cleaned.groupby('group', 
            as_index=False).words.apply(counts).reset_index(drop=True)

and save the results to file
output.to_csv('test_file.csv', index=False, sep=',')
df = pd.read_csv('test_file.csv',sep=',')

I don't see any problems
",,
NLTK strange result,https://stackoverflow.com/questions/39603633,nltk semantic word substitution,"I'm trying to find different ways of writing ""events in [city]"" which are semantically similar. I am trying to do this by finding words that are semantically similar to ""events"" so I can substitute them in. 

To find these words I'm using nltk's wordnet corpus, but I'm getting some pretty strange results. For example, using the hyponyms of 'event.n.01', I'm getting ""Miracles in Ottawa"". 

co-hyponyms and hypernyms seem just as bad or worse. I wonder if anyone understands the structure better and can offer a potential solution?

Here's some sample code:

!/usr/bin/python3

import nltk

lemma = 'event.n.01'
synset = nltk.corpus.wordnet.synset(lemma)

print(""%s: %s"" % (synset.name(), synset.definition()))

print(""\nFinding hyponyms..."")
print([s.split('.')[0] for w in synset.hyponyms() for s in w.lemma_names()])

print(""\nFinding hypernym paths..."")
print([s.split('.')[0] for hyprs in synset.hypernym_paths() for hypr in hyprs for s in hypr.lemma_names()])

print(""\nFinding co-hyponyms..."")
for hypers in synset.hypernym_paths():
        for hyper in hypers:
                print(hyper.name())
                for hypos in hyper.hyponyms():
                        print(""\t%s"" % (', '.join(hypos.lemma_names())))

print(synset.similar())

",0,602,"You can take a deep learning approach. Train a word2vec model and get the most similar vectors to the ""event"" vector.

You can test a model here Word2Vec Demo
","The hyponyms of ""event"" are types of ""event"". One of them is ""miracle"", some others are:

&gt;&gt;&gt; [s for w in synset.hyponyms() for s in w.lemma_names][:7]  # is 7 enough? :)
['zap', 'act', 'deed', 'human_action', 'human_activity', 'happening', 'occurrence']


""Event's"" hypernyms are the oposite. Terms that ""event"" is a type of:

&gt;&gt;&gt; synset.hypernyms()
[Synset('psychological_feature.n.01')]


You can see that ""event"" is one of it's hyponyms:

&gt;&gt;&gt; synset.hypernyms()[0].hyponyms()
[Synset('motivation.n.01'), Synset('cognition.n.01'), Synset('event.n.01')]


Those are not really ""similar"" terms (""Psychological features in Ottawa"" may seem like a correct result to a robot, but not to humans).

Perhaps it is better to go at it from a completely different angle, e.g.

&gt;&gt;&gt; text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())
&gt;&gt;&gt; text.similar('event')
time day man order state way case house one place action night point
situation work year act and area audience


Now, take those and sort them e.g. by path_similarity:

&gt;&gt;&gt; words = 'time day man order state way case house one place action night point'\
...         ' situation work year act and area audience'.split()
&gt;&gt;&gt; 
&gt;&gt;&gt; def get_symilarity(synset, word):
...     return max([synset.path_similarity(synset2)
...                for synset2 in nltk.corpus.wordnet.synsets(word)]+[0])
&gt;&gt;&gt; 
&gt;&gt;&gt; sorted(words, key=lambda w: get_symilarity(synset, w), reverse=True)[:5]
['act', 'case', 'action', 'time', 'way']


Is that a good result? I don't know. I guess it could work: ""Acts in Ottawa"", ""Cases in New York"", ""Action in Rome"", ""Time in Tokyo"", ""Ways in Amsterdam""...
",
NLTK strange result,https://stackoverflow.com/questions/34491819,Semantic Clustering,"I am looking for advice on how to find clusters of terms that are all related to a single concept. 

The goal is to improve a tag or keyword search for images that describe concepts or processes or situations. An image may describe a brainstorming session, or a particular theme. These images which are meant to be used in PowerPoint or other presentation material have user contributed tags. 

The issue is  our tag based search may bring back completely unrelated images. Our goal is to find the clusters within the tags in order to refine the tags related to a central concept and remove the outliers that are not related to the clusters. 

For example if you have a you had the tags meeting, planning, brainstorming, and round table. Ideally we would want to remove round table from the cluster as it doesn't fit the theme. 

I have worked with WordNet Similarity but the results are quite strange. I was wondering if there are any other tools in python's NLTK that could help me solve this.

Thanks!
",0,684,"Your question is based in the area called ""topic modeling"" you can use:
gensim
https://radimrehurek.com/gensim/
or lda
https://pypi.python.org/pypi/lda
",,
NLTK strange issue,https://stackoverflow.com/questions/8842817,Selecting the most fluent text from a set of possibilities via grammar checking (Python),,30,3461,"Very cool project, first of all. 

I found a java grammar checker. I've never used it but the docs claim it can run as a server. Both java and listening to a port should be supported basically anywhere. 

I'm just getting into NLP with a CS background so I wouldn't mind going into more detail to help you integrate whatever you decide on using. Feel free to ask for more detail.
","The pylinkgrammar link provided above is a bit out of date.  It points to version 0.1.9, and the code samples for that version no longer work.  If you go down this path, be sure to use the latest version which can be found at:

https://pypi.python.org/pypi/pylinkgrammar
","Another approach would be to use what is called an overgenerate and rank approach.  In the first step you have your poetry generator generate multiple candidate generations.  Then using a service like Amazon's Mechanical Turk to collect human judgments of fluency.  I would actually suggest collecting simultaneous judgments for a number of sentences generated from the same seed conditions.  Lastly, you extract features from the generated sentences (presumably using some form of syntactic parser) to train a model to rate or classify question quality.  You could even thrown in the heuristics listed above.

Michael Heilman uses this approach for question generation.  For more details, read these papers:
Good Question! Statistical Ranking for Question Generation and
Rating Computer-Generated Questions with Mechanical Turk.
"
NLTK strange issue,https://stackoverflow.com/questions/64945076,Using Natural Language Tool Kit with Django on Heroku - - Error: &#39;nltk.txt&#39; not found,"Ive got a basic Django project. One feature I am working on counts the number of most commonly occurring words in a .txt file, such as a large public domain book. Ive used the Python Natural Language Tool Kit to filter out stopwords (in SEO language, that means redundant words such as the, you, etc. ).
Anyways, Im getting this debug traceback when Django serves the template:

Resource [93mstopwords[0m not found. Please use the NLTK Downloader to
obtain the resource: [31m &lt;&lt;&lt; import nltk nltk.download('stopwords')
[0m For more information see: https://www.nltk.org/data.html

So I need to download the library of stopwords. To resolve the issue, I simply open a Python REPL on my remote server and invoke these two straightforward lines:
&lt;&lt;&lt; import nltk
&lt;&lt;&lt; nltk.download('stopwords')

That's covered at length elsewhere on SO. That resolves the issue, but only temporarily. As soon as the REPL session is terminated on my remote server, the error returns because the stopwords file just evaporates.
I noticed something strange when I use git to push my changes up to my remote server on Heroku. Check this:
remote: -----&gt; Python app detected
remote: -----&gt; No change in requirements detected, installing from cache
remote: -----&gt; Installing pip 20.1.1, setuptools 47.1.1 and wheel 0.34.2
remote: -----&gt; Installing SQLite3
remote: -----&gt; Installing requirements with pip
remote: -----&gt; Downloading NLTK corpora
remote:  !     'nltk.txt' not found, not downloading any corpora
remote:  !     Learn more: https://devcenter.heroku.com/articles/python-nltk 
remote: -----&gt; $ python manage.py collectstatic --noinput
remote:        122 static files copied to '/tmp/build_f2f9d10f/staticfiles', 388 post-processed.

That devcenter link is kind of like a stub, meaning that its not very detailed. Its sparse at best. The article says that to use Python nltk, you need to add an nltk.txt file to the project directory which specifies the list of objects for Heroku to download. So I went ahead and created an nltk text file which contained:

corpora

Here is this active nltk.txt currently located in my project directory. In addition to coprora, I also tried adding various combinations of the following three entries to nltk.txt:

corpus


stoplist


english

I tried adding all four, just two and just one. For example, here is an alternate nltk.txt that I tried verbatim. My feeling is that the main one I really need is just corpora, so that is the only entry in the nltk.txt that I am working with right now. With corpora there, when I push the change and Heroku builds the environment, I see this error and trace-back:
remote: -----&gt; Downloading NLTK corpora
remote: -----&gt; Downloading NLTK packages: corpora english stopwords corpus
remote: /app/.heroku/python/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour
remote:   warn(RuntimeWarning(msg))
remote: [nltk_data] Error loading corpora: Package 'corpora' not found in
remote: [nltk_data]     index
remote: Error installing package. Retry? [n/y/e]
remote: Traceback (most recent call last):
remote:   File ""/app/.heroku/python/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
remote:     ""__main__"", mod_spec)
remote:   File ""/app/.heroku/python/lib/python3.6/runpy.py"", line 85, in _run_code
remote:     exec(code, run_globals)
remote:   File ""/app/.heroku/python/lib/python3.6/site-packages/nltk/downloader.py"", line 2538, in &lt;module&gt;
remote:     halt_on_error=options.halt_on_error,
remote:   File ""/app/.heroku/python/lib/python3.6/site-packages/nltk/downloader.py"", line 790, in download

I am clearly not using nltk.txt properly because it isnt finding the corpora package. I can install nltk and have it run without issue in my local dev server but my remaining question is this: how do I make Heroku handle nltk properly remotely in this situation?
User Michael Godshall provides the same answer to more than one Stack Overflow question explaining that you can create a bin directory within the project root and add both a post_compile bash script and a install_nltk_data script. However this is no longer necessary because heroku-buildpack-python upstream maintainer Kenneth Reitz implemented an easy solution. All that is required now is to add an nltk.txt which contains the library you need. But I did that and I am still getting the error above.
The official nltk website documents how to use the library in general and how to install it which isnt helpful in the case of Heroku because Heroku seems to handle nltk differently.
",1,673,"Eureka! I got it working. My problem was with the name of the nltk library download. I tried stoplist when the actual name is stopwords. Ha! The contents of my nltk.txt is now simply: stopwords.  When I pushed to Heroku, the build succeeded and my website is now deployed and accessible on the web.
Special thanks goes out to @Darkknight for his patience and insight in the comment section of his answer.
","Yes, you need the nltk.txt file similar to the requirements.txt file properly. refer to the official doc here. if you still facing the same situation post the nltk.txt file here that will give us some way to find the solution
maybe this also will help you
",
NLTK strange issue,https://stackoverflow.com/questions/38523385,Not able to install numpy or nltk python-modules,"I am having a strange issue installing numpy or nltk python-modules in my windows-7 machine. I have successfully installed Python 2.7.12 but I get this error when I type pip install numpy as in this screenshot. I have also included the directory of pip.exe in the PATH. Any help would be appreciated.Thank you :)
",1,278,"Installing such these things in windows are sometime difficult, specially for someone new to python packages(also for some experts!)

Try to use Anaconda for windows: https://www.continuum.io/downloads#_windows
This install a python for you and many requirement packages(e.g Numpy, Scipy, Scikit and many more)

You can use older version of Anaconda, for python2.x if you want strictly python2.x



An alternative way is to download Numpy from github and then install it as a python package, that contain setup.py file

python setup.py install


Or you can download Numpy wheel package, then install it localy with pip
",,
NLTK strange issue,https://stackoverflow.com/questions/34491819,Semantic Clustering,,0,684,"Your question is based in the area called ""topic modeling"" you can use:
gensim
https://radimrehurek.com/gensim/
or lda
https://pypi.python.org/pypi/lda
",,
SciPy unexpected behavior,https://stackoverflow.com/questions/49280404,Shift interpolation does not give expected behaviour,"When using scipy.ndimage.interpolation.shift to shift a numpy data array along one axis with periodic boundary treatment (mode = 'wrap'), I get an unexpected behavior. The routine tries to force the first pixel (index 0) to be identical to the last one (index N-1) instead of the ""last plus one (index N)"".

Minimal example:

# module import
import numpy as np
from scipy.ndimage.interpolation import shift
import matplotlib.pyplot as plt

# print scipy.__version__
# 0.18.1

a = range(10)

plt.figure(figsize=(16,12))

for i, shift_pix in enumerate(range(10)):
    # shift the data via spline interpolation
    b = shift(a, shift=shift_pix, mode='wrap')

    # plotting the data
    plt.subplot(5,2,i+1)
    plt.plot(a, marker='o', label='data')
    plt.plot(np.roll(a, shift_pix), marker='o', label='data, roll')
    plt.plot(b, marker='o',label='shifted data')
    if i == 0:
        plt.legend(loc=4,fontsize=12)
    plt.ylim(-1,10)
    ax = plt.gca()
    ax.text(0.10,0.80,'shift %d pix' % i, transform=ax.transAxes)


Blue line: data before the shift
Green line: expected shift behavior
Red line: actual shift output of scipy.ndimage.interpolation.shift

Is there some error in how I call the function or how I understand its behavior with mode = 'wrap'? The current results are in contrast to the mode parameter description from the related scipy tutorial page and from another StackOverflow post. Is there an off-by-one-error in the code?

Scipy version used is 0.18.1, distributed in anaconda-2.2.0


",7,985,"It is worth noting that this behavior appears to be a bug, as noted in this SciPy issue:
https://github.com/scipy/scipy/issues/2640

The issue appears to effect every extrapolation mode in scipy.ndimage other than mode='mirror'.
","It seems that the behaviour you have observed is intentional.

The cause of the problem lies in the C function map_coordinate which translates the coordinates after shift to ones before shift:

map_coordinate(double in, npy_intp len, int mode)


The function is used as the subroutine in NI_ZoomShift that does the actual shift. Its interesting part looks like this:



Example. Lets see how the output for output = shift(np.arange(10), shift=4, mode='wrap') (from the question) is computed.

NI_ZoomShift computes edge values output[0] and output[9] in some special way, so lets take a look at computation of output[1] (a bit simplified):

# input  =         [0,1,2,3,4,5,6,7,8,9]
# output = [ ,?, , , , , , , , ]          '?' == computed position
# shift  = 4
output_index = 1

in  = output_index - shift    # -3
sz  = 10 - 1                  # 9
in += sz * ((-5 / 9) + 1)
#  +=  9 * ((     0) + 1) == 9
# in == 6

return input[in]  # 6 


It is clear that sz = len - 1 is responsible for the behaviour you have observed. It was changed from sz = len in a suggestively named commit dating back to 2007: Fix off-by-on errors in ndimage boundary routines. Update tests.

I don't know why such change was introduced. One of the possible explanations that come to my mind is as follows:

Function 'shift' uses splines for interpolation.
A knot vector of an uniform spline on interval [0, k] is simply [0,1,2,...,k]. When we say that the spline should wrap, it is natural to require equality on values for knots 0 and k, so that many copies of the spline could be glued together, forming a periodic function:

0--1--2--3-...-k              0--1--2--3-...-k              0--1-- ...
               0--1--2--3-...-k              0--1--2--3-...-k      ...


Maybe shift just treats its input as a list of values for spline's knots?
",
SciPy unexpected behavior,https://stackoverflow.com/questions/40451203,Cython parallel loop problems,"I am using cython to compute a pairwise distance matrix using a custom metric as a faster alternative to scipy.spatial.distance.pdist.

My Motivation

My metric has the form 

def mymetric(u,v,w):
     np.sum(w * (1 - np.abs(np.abs(u - v) / np.pi - 1))**2)


and the pairwise distance using scipy can be computed as

x = sp.spatial.distance.pdist(r, metric=lambda u, v: mymetric(u, v, w))


Here, r is a m-by-n matrix of m vectors with dimension of n and w is a ""weight"" factor with dimmension n.

Since in my problem m is rather high, the computation is really slow. For m = 2000 and n = 10 this takes approx 20 sec.

Initial solution with Cython

I implemented a simple function in cython that computes the pairwise distance and immediately got very promising results -- speedup of over 500x.

import numpy as np
cimport numpy as np
import cython

from libc.math cimport fabs, M_PI

@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size
    cdef np.ndarray[np.double_t, ndim=1] ans
    size = r.shape[0] * (r.shape[0] - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    c = -1
    for i in range(r.shape[0]):
        for j in range(i + 1, r.shape[0]):
            c += 1
            for k in range(r.shape[1]):
                ans[c] += w[k] * (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))**2.0

    return ans


Problems using OpenMP

I wanted to speed up the computation some more using OpenMP, however, the following solution is roughly 3 times slower than the serial version.

import numpy as np
cimport numpy as np

import cython
from cython.parallel import prange, parallel

cimport openmp

from libc.math cimport fabs, M_PI

@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_omp(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size, m, n
    cdef np.double_t a
    cdef np.ndarray[np.double_t, ndim=1] ans
    m = r.shape[0]
    n = r.shape[1]
    size = m * (m - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    with nogil, parallel(num_threads=8):
        for i in prange(m, schedule='dynamic'):
            for j in range(i + 1, m):
                c = i * (m - 1) - i * (i + 1) / 2 + j - 1
                for k in range(n):
                    ans[c] += w[k] * (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))**2.0

    return ans


I don't know why is it actually slower, but I tried to introduce the following changes. This resulted not only in even slightly worse performance but also, the resulting distance ans is computed correctly only in the beginning of the array, the rest is just zeros. The speedup achieved through this is negligible. 

import numpy as np
cimport numpy as np

import cython
from cython.parallel import prange, parallel

cimport openmp

from libc.math cimport fabs, M_PI
from libc.stdlib cimport malloc, free

@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_omp_2(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int k, l, c, m, n
    cdef Py_ssize_t i, j, d
    cdef size_t size
    cdef int *ci, *cj

    cdef np.ndarray[np.double_t, ndim=1, mode=""c""] ans

    cdef np.ndarray[np.double_t, ndim=2, mode=""c""] data
    cdef np.ndarray[np.double_t, ndim=1, mode=""c""] weight

    data = np.ascontiguousarray(r, dtype=np.float64)
    weight = np.ascontiguousarray(w, dtype=np.float64)

    m = r.shape[0]
    n = r.shape[1]
    size = m * (m - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)

    cj = &lt;int*&gt; malloc(size * sizeof(int))
    ci = &lt;int*&gt; malloc(size * sizeof(int))

    c = -1
    for i in range(m):
        for j in range(i + 1, m):
            c += 1
            ci[c] = i
            cj[c] = j

    with nogil, parallel(num_threads=8):
        for d in prange(size, schedule='guided'):
            for k in range(n):
                ans[d] += weight[k] * (1.0 - fabs(fabs(data[ci[d], k] - data[cj[d], k]) / M_PI - 1.0))**2.0

    return ans


For all functions, I am using the following .pyxbld file

def make_ext(modname, pyxfilename):
    from distutils.extension import Extension
    return Extension(name=modname,
                     sources=[pyxfilename],
                     extra_compile_args=['-O3', '-march=native', '-ffast-math', '-fopenmp'],
                     extra_link_args=['-fopenmp'],
                     )


Summary

I have zero experience with cython and know only basics of C. I would appreciate any suggestion of what may be the cause of this unexpected behavior, or even, how to rephrase my question better.



Best serial solution (10 % faster than original serial)

@cython.cdivision(True)
@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_2(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size
    cdef np.ndarray[np.double_t, ndim=1] ans
    cdef np.double_t accumulator, tmp
    size = r.shape[0] * (r.shape[0] - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    c = -1
    for i in range(r.shape[0]):
        for j in range(i + 1, r.shape[0]):
            c += 1
            accumulator = 0
            for k in range(r.shape[1]):
                tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
                accumulator += w[k] * (tmp*tmp)
            ans[c] = accumulator

    return ans


Best parallel solution (1 % faster then original parallel, 6 times faster then best serial using 8 threads)

@cython.cdivision(True)
@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_omp_2d(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size, m, n
    cdef np.ndarray[np.double_t, ndim=1] ans
    cdef np.double_t accumulator, tmp
    m = r.shape[0]
    n = r.shape[1]
    size = m * (m - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    with nogil, parallel(num_threads=8):
        for i in prange(m, schedule='dynamic'):
            for j in range(i + 1, m):
                c = i * (m - 1) - i * (i + 1) / 2 + j - 1
                accumulator = 0
                for k in range(n):
                    tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
                    ans[c] += w[k] * (tmp*tmp)

    return ans




Unsolved issues:

When I try to apply the accumulator solution proposed in the answer, I get the following error:

Error compiling Cython file:
------------------------------------------------------------
...
                c = i * (m - 1) - i * (i + 1) / 2 + j - 1
                accumulator = 0
                for k in range(n):
                    tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
                    accumulator += w[k] * (tmp*tmp)
                ans[c] = accumulator
                                   ^
------------------------------------------------------------
pdist.pyx:207:36: Cannot read reduction variable in loop body


Full code:

@cython.cdivision(True)
@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_omp(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size, m, n
    cdef np.ndarray[np.double_t, ndim=1] ans
    cdef np.double_t accumulator, tmp
    m = r.shape[0]
    n = r.shape[1]
    size = m * (m - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    with nogil, parallel(num_threads=8):
        for i in prange(m, schedule='dynamic'):
            for j in range(i + 1, m):
                c = i * (m - 1) - i * (i + 1) / 2 + j - 1
                accumulator = 0
                for k in range(n):
                    tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
                    accumulator += w[k] * (tmp*tmp)
                ans[c] = accumulator

    return ans

",5,3869,"I haven't timed this myself so it's possible this might not help too much, however:

If you run cython -a to get an annotated version of your initial attempt (pairwise_distance_omp) you'll find the ans[c] += ... line is yellow, suggesting it's got Python overhead. A look at that the C corresponding to that line suggests that it's checking for divide by zero. One key part of it starts:

if (unlikely(M_PI == 0)) {


You know this will never be true (and in any case you'd probably live with NaN values rather than an exception if it was). You can avoid this check by adding the following extra decorator to the function:

@cython.cdivision(True)
# other decorators
def pairwise_distance_omp # etc...


This cuts out quite a bit of C code, including bits that have to be run in a single thread. The flip-side is that most of that code should never be run, and the compiler should probably be able to work that out, so it isn't clear how much difference that will make.



Second suggestion:

# at the top
cdef np.double_t accumulator, tmp

    # further down later in the loop:
    c = i * (m - 1) - i * (i + 1) / 2 + j - 1
    accumulator = 0
    for k in range(r.shape[1]):
        tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
        accumulator = accumulator + w[k] * (tmp*tmp)
    ans[c] = accumulator


This has two advantages hopefully: 1) tmp*tmp should probably be quicker than floating point exponent to the power of 2. 2) You avoid reading from the ans array, which might be a bit slow because the compiler always has to be careful that some other thread hasn't changed it (even though you know it shouldn't have).
",,
SciPy unexpected behavior,https://stackoverflow.com/questions/45255265,Unexpected behavior of Gaussian filtering with Scipy,"Given that I have an image f(x,y) loaded, for example,



I want to compute the Gaussian derivative /x /y G*f of the image f, where G is a Gaussian filter and * denotes convolution. This is easily done using Scipy:

from scipy.ndimage.filters import gaussian_filter
imshow(gaussian_filter(g, sigma, order=1))


With sigma=50 this produces the following result:



Now, for applicationary reasons, I need to do the computation with mode='constant':

imshow(gaussian_filter(g, sigma, order=1, mode='constant', cval=0))


Still, the result looks reasonable:



However, note that my image's background's intensity is 1 and not 0. Hence, it should be reasonable to use cval=1:

imshow(gaussian_filter(g, sigma, order=1, mode='constant', cval=1))




Now this is unexpected! This result makes no sense, does it?

For the record, I also checked the partial differentials /x G*f and /y G*f. Whereas

imshow(gaussian_filter(g, sigma, order=[0, 1], mode='constant', cval=1)


looks reasonable



the other one

imshow(gaussian_filter(g, sigma, order=[1, 0], mode='constant', cval=1)


does not:



Why is that?
",5,1015,"There is a bug in gaussian_filter that manifests itself when both order and cval are nonzero. Specifically, it's here: 

for axis, sigma, order, mode in axes:
    gaussian_filter1d(input, sigma, axis, order, output, mode, cval, truncate)
    input = output


The filter performs repeated 1d convolution, and each time it passes in cval to 1d filter. Problem is, if there were any derivatives taken, then cval should be set to 0 because the derivative of any constant is zero. This is why the result is wrong with order=[1, 0] but not with order=[0, 1]. Without testing (don't have SciPy dev environment), I think the following would be correct: 

for axis, sigma, order, mode in axes:
    gaussian_filter1d(input, sigma, axis, order, output, mode, cval, truncate)
    if order &gt; 0: 
        cval = 0.0
    input = output


Workaround

A nonzero cval can be emulated by subtracting it from the image before filtering (and adding back after filtering only if the order is zero). Example:

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage.filters import gaussian_filter

g = np.ones((500, 500))
g[200:300, 200:300] = 2
sigma = 50
cval = 1
gf = gaussian_filter(g-cval, sigma, order=1, mode='constant')
plt.matshow(gf)
plt.show()


returns



which is the expected result. (My original image is a bit different from yours, and I use a different visualization tool.)
",,
SciPy unexpected behavior,https://stackoverflow.com/questions/18155972,unexpected result in numpy array slicing (view vs copy),"I'm trying to reduce the amount of copying in my code and I came across surprising behavior when dealing with numpy array slicing and views, as explained in:
Scipy wiki page on copying numpy arrays
I've stumbled across the following behavior, which is unexpected for me:
Case 1.:
import numpy as np
a = np.ones((3,3))
b = a[:,1:2]
b += 5
print a
print b.base is a

As expected, this outputs:
array([[ 1.,  6.,  1.],
       [ 1.,  6.,  1.],
       [ 1.,  6.,  1.]])
True

Case 2: When performing the slicing and addition in one line, things look different:
import numpy as np
a = np.ones((3,3))
b = a[:,1:2] + 5
print a
print b.base is a

The part that's surprising to me is that a[:,1:2] does not seem to create a view, which is then used as a left hand side argument, so, this outputs:
array([[ 1.,  1.,  1.],
       [ 1.,  1.,  1.],
       [ 1.,  1.,  1.]])
False

Maybe someone can shed some light on why these two cases are different, I think I'm missing something.
Solution: I missed the obvious fact that the ""+"" operator, other than the in-place operator ""+="" will always create a copy, so it's in fact not related but slicing other than how in-place operators are defined for numpy arrays.
To illustrate this, the following generates the same output as Case 2:
import numpy as np
a = np.ones((3,3))
b = a[:,1:2]
b = b + 5
print a
print b.base is a

",4,3293,"The above is no different than:

&gt;&gt;&gt; a=np.arange(5)
&gt;&gt;&gt; b=a
&gt;&gt;&gt; b
array([0, 1, 2, 3, 4])

&gt;&gt;&gt; b+=5
&gt;&gt;&gt; a
array([5, 6, 7, 8, 9])
&gt;&gt;&gt; b
array([5, 6, 7, 8, 9])

&gt;&gt;&gt; b=b+5
&gt;&gt;&gt; b
array([10, 11, 12, 13, 14])
&gt;&gt;&gt; a
array([5, 6, 7, 8, 9])


Which, at least to me, seem like completely expected behavior. The b+=x operator calls __iadd__ which importantly first tries to modify the array in place, so it will update b which is still a view of a. While the b=b+x operator calls __add__ which creates new temporary data and then assigns it to b.

For a[i] +=b the sequence is (in numpy): 

a.__setitem__(i, a.__getitem__(i).__iadd__(b))

","a[:, 1:2] creates a view, but you don't modify the view in the second example. Instead, + creates a new array from its arguments. Suppose you do

a = np.ones((3, 3))
b = a + 5


In that case, you wouldn't expect a change to a, because this isn't an in-place addition. The operator is +, rather than +=. It's the same with the second example.

b = a[:, 1:2] + 5


doesn't modify a[:, 1:2], because this isn't in-place addition.
","The default thing to do when slicing a numpy array is to create b as a view of a, thus when you change b, a changes as well, which is confirmed by your first case. 

The second case is more tricky. You are not telling that b is a slice of a and then adding a number. What you are doing is creating b as something that does not coincide with a, so numpy is forced to copy the data instead of just creating a view. 
"
SciPy unexpected behavior,https://stackoverflow.com/questions/52450658,Python - unexpected shape parameter behavior in scipy genextreme fit,"I've been trying to fit the GEV distribution to some annual maximum river discharge using Scipy's stats.genextreme function, but I've found some weird behavior of the fit. Depending on how small your data is (i.e., 1e-5 vs. 1e-1), the shape parameter that is returned can be dramatically different. For example: 

import scipy as scipy
import numpy as np
from scipy.stats import genextreme as gev
from scipy.stats import gumbel_r as gumbel

#Set up arrays of values to fit curve to 
sample=np.random.rand(1,30) #Random set of decimal values 
smallVals = sample*1e-5     #Scale to smaller values 

#If the above is not creating different values, this instance of random numbers has:
bugArr = np.array([[0.25322987, 0.81952358, 0.94497455, 0.36295543, 0.72272746, 0.49482558,0.65674877, 0.40876558, 0.64952248, 0.23171052, 0.24645658, 0.35359126,0.27578928, 0.24820775, 0.69789187, 0.98876361, 0.22104156,0.40019593,0.0756707,  0.12342556, 0.3601186,  0.54137089,0.43477705, 0.44622486,0.75483338, 0.69766687, 0.1508741,  0.75428996, 0.93706003, 0.1191987]])
bugArr_small = bugArr*1e-5

#This array of random numbers gives the same shape parameter regardless 
fineArr = np.array([[0.7449611,  0.82376693, 0.32601009, 0.18544293, 0.56779629, 0.30495415,
        0.04670362, 0.88106521, 0.34013959, 0.84598841, 0.24454428, 0.57981437,
        0.57129427, 0.8857514,  0.96254429, 0.64174078, 0.33048637, 0.17124045,
        0.11512589, 0.31884749, 0.48975204, 0.87988863, 0.86898236, 0.83513966,
        0.05858769, 0.25889509, 0.13591874, 0.89106616, 0.66471263, 0.69786708]])
fineArr_small = fineArr*1e-5

#GEV fit for both arrays - shouldn't dramatically change distribution 
gev_fit      = gev.fit(sample)
gevSmall_fit = gev.fit(smallVals)

gevBug      = gev.fit(bugArr)
gevSmallBug = gev.fit(bugArr_small)

gevFine      = gev.fit(fineArr)
gevSmallFine = gev.fit(fineArr_small)


I get the following output for the GEV parameters estimated for the bugArr/bugArr_small and fineArr/fineArr_small:

Known bug array
Random values:         (0.12118250540401079, 0.36692231766996053, 0.23142400358716353)
Random values scaled:  (-0.8446554391074808, 3.0751769299431084e-06, 2.620390405092363e-06)

Known fine array
Random values:         (0.6745399522587823, 0.47616297212022757, 0.34117425062278584)
Random values scaled:  (0.6745399522587823, 4.761629721202293e-06, 3.411742506227867e-06)


Why would the shape parameter change so dramatically when the only difference in the data is a change in scaling? I would've expected the behavior to be consistent with the FineArr results (no change in shape parameter, and appropriate scaling of location and scale parameters). I've repeated the test in Matlab, but the results there are in line with what I expected (i.e., no change in shape parameter). 
",2,506,"I think I know why this might be happening. It is possible to pass initial shape parameter estimates when fitting, see the documentation for scipy.stats.rv_continuous.fit where it states ""Starting value(s) for any shape-characterizing arguments (those not provided will be determined by a call to _fitstart(data)). No default value."" Here is some extremely ugly, functional, code using my pyeq3 statistical distribution fitter which internally attempts to use different estimates, fit them, and return the parameters for best nnlf of the different fits. This example code does not show the behavior you observe, and gives the same shape parameters regardless of scaling. You would need to install pyeq3 with ""pip3 install pyeq3"" to run this code. The pyeq3 code is designed for text input from a web interface on zunzun.com, so hold you nose - here is the example code:

import numpy as np

#Set up arrays of values to fit curve to 
sample=np.random.rand(1,30) #Random set of decimal values 
smallVals = sample*1e-5     #Scale to smaller values 

#If the above is not creating different values, this instance of random numbers has:
bugArr = np.array([0.25322987, 0.81952358, 0.94497455, 0.36295543, 0.72272746, 0.49482558,0.65674877, 0.40876558, 0.64952248, 0.23171052, 0.24645658, 0.35359126,0.27578928, 0.24820775, 0.69789187, 0.98876361, 0.22104156,0.40019593,0.0756707,  0.12342556, 0.3601186,  0.54137089,0.43477705, 0.44622486,0.75483338, 0.69766687, 0.1508741,  0.75428996, 0.93706003, 0.1191987])
bugArr_small = bugArr*1e-5

#This array of random numbers gives the same shape parameter regardless 
fineArr = np.array([0.7449611,  0.82376693, 0.32601009, 0.18544293, 0.56779629, 0.30495415,
        0.04670362, 0.88106521, 0.34013959, 0.84598841, 0.24454428, 0.57981437,
        0.57129427, 0.8857514,  0.96254429, 0.64174078, 0.33048637, 0.17124045,
        0.11512589, 0.31884749, 0.48975204, 0.87988863, 0.86898236, 0.83513966,
        0.05858769, 0.25889509, 0.13591874, 0.89106616, 0.66471263, 0.69786708])
fineArr_small = fineArr*1e-5

bugArr_str = ''
for i in range(len(bugArr)):
    bugArr_str += str(bugArr[i]) + '\n'
bugArr_small_str = ''
for i in range(len(bugArr_small)):
    bugArr_small_str += str(bugArr_small[i]) + '\n'
fineArr_str = ''
for i in range(len(fineArr)):
    fineArr_str += str(fineArr[i]) + '\n'
fineArr_small_str = ''
for i in range(len(fineArr_small)):
    fineArr_small_str += str(fineArr_small[i]) + '\n'
import pyeq3

simpleObject_bugArr = pyeq3.IModel.IModel()
simpleObject_bugArr._dimensionality = 1
pyeq3.dataConvertorService().ConvertAndSortColumnarASCII(bugArr_str, simpleObject_bugArr, False)
solver = pyeq3.solverService()
result_bugArr = solver.SolveStatisticalDistribution('genextreme', simpleObject_bugArr.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')
simpleObject_bugArr_small = pyeq3.IModel.IModel()
simpleObject_bugArr_small._dimensionality = 1
pyeq3.dataConvertorService().ConvertAndSortColumnarASCII(bugArr_small_str, simpleObject_bugArr_small, False)
solver = pyeq3.solverService()
result_bugArr_small = solver.SolveStatisticalDistribution('genextreme', simpleObject_bugArr_small.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')

simpleObject_fineArr = pyeq3.IModel.IModel()
simpleObject_fineArr._dimensionality = 1
pyeq3.dataConvertorService().ConvertAndSortColumnarASCII(fineArr_str, simpleObject_fineArr, False)
solver = pyeq3.solverService()
result_fineArr = solver.SolveStatisticalDistribution('genextreme', simpleObject_fineArr.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')

simpleObject_fineArr_small = pyeq3.IModel.IModel()
simpleObject_fineArr_small._dimensionality = 1
pyeq3.dataConvertorService().ConvertAndSortColumnarASCII(fineArr_small_str, simpleObject_fineArr_small, False)
solver = pyeq3.solverService()
result_fineArr_small = solver.SolveStatisticalDistribution('genextreme', simpleObject_fineArr_small.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')

print('ba',result_bugArr[1]['fittedParameters'])
print('ba_s',result_bugArr_small[1]['fittedParameters'])
print()
print('fa',result_fineArr[1]['fittedParameters'])
print('fa_s',result_fineArr_small[1]['fittedParameters'])

",,
SciPy unexpected behavior,https://stackoverflow.com/questions/59281884,Unexpected behavior of `scipy.ndimage.zoom()` for `order=0`,"I have difficulties understanding the behavior of scipy.ndimage.zoom() when order=0.

Consider the following code:

import numpy as np
import scipy as sp
import scipy.ndimage

arr = np.arange(3) + 1
print(arr)
for order in range(5):
    zoomed = sp.ndimage.zoom(arr.astype(float), 4, order=order)
    print(order, np.round(zoomed, 3))


whose output is:

0 [1. 1. 1. 2. 2. 2. 2. 2. 2. 3. 3. 3.]
1 [1.    1.182 1.364 1.545 1.727 1.909 2.091 2.273 2.455 2.636 2.818 3.   ]
2 [1.    1.044 1.176 1.394 1.636 1.879 2.121 2.364 2.606 2.824 2.956 3.   ]
3 [1.    1.047 1.174 1.365 1.601 1.864 2.136 2.399 2.635 2.826 2.953 3.   ]
4 [1.    1.041 1.162 1.351 1.59  1.86  2.14  2.41  2.649 2.838 2.959 3.   ]


So, when order=0 the values are (expectedly) not interpolated.
However, I was expecting to have:

[1. 1. 1. 1. 2. 2. 2. 2. 3. 3. 3. 3.]


i.e. exactly the same number of elements for each value, since the zoom is a whole number.
Hence, I was expecting to get the same result as np.repeat():

print(np.repeat(arr.astype(float), 4))
[1. 1. 1. 1. 2. 2. 2. 2. 3. 3. 3. 3.]


Why is there a variation in the number of times each element gets repeated?



Note that np.repeat() does not directly work with multi-dimensional arrays and that is the reason why I would like to get the ""correct"" behavior from scipy.ndimage.zoom().



My NumPy and SciPy versions are:

print(np.__version__)
# 1.17.4
print(sp.__version__)
# 1.3.3




I found this:
`scipy.ndimage.zoom` vs `skimage.transform.rescale` with `order=0`
which points toward some unexpected behavior for scipy.ndimage.zoom() but I am not quite sure it is the same effect being observed.
",2,3324,"This is a bin/edge array interpretation issue.
The behavior of scipy.ndimage.zoom() is based on the edge interpretation of the array values, while the behavior that would produce equally-sized blocks for integer zoom factors (mimicking np.repeat()) is based on the bin interpretation.

Let's illustrate with some ""pictures"".

Bin Interpretation

Consider the array [1 2 3], and let's assign each value to a bin.
The edges of each bin would be: 0 and 1 for 1, 1 and 2 for 2, etc.

0 1 2 3
|1|2|3|


Now, let's zoom this array by a factor of 4:

                    1 1 1
0 1 2 3 4 5 6 7 8 9 0 1 2
|   1   |   2   |   3   |


Hence, the values to assign to the bins using the Next-door Neighbor method are:

                    1 1 1
0 1 2 3 4 5 6 7 8 9 0 1 2
|1 1 1 1|2 2 2 2|3 3 3 3|


Edge Interpretation

Consider the same array as before [1 2 3], but now let's assign each value to an edge:

0 1 2
| | |
1 2 3


Now, let's zoom this array by a factor of 4:

                    1 1
0 1 2 3 4 5 6 7 8 9 0 1
| | | | | | | | | | | |
1          2          3



Hence, the values to assign to the edges using the Next-door Neighbor method are:

                    1 1
0 1 2 3 4 5 6 7 8 9 0 1
| | | | | | | | | | | |
1 1 1 2 2 2 2 2 2 3 3 3


and edge 3 is assigned to 2 because 2 has position 5.5 while 1 has position 0 and (5.5 - 3 = 2.5) &lt; (3 - 0 = 3).
Similarly, edge 8 is assigned to 2 because (8 - 5.5 = 2.5) &lt; (11 - 8 = 3).



Comments

In Physics, the ""bin array interpretation"" is generally more useful, because measurements are typically ""the result of some integration over a certain bin in an appropriate domain"" (notably signal of any form -- including images -- collected at a given time interval), hence I was expecting a ""bin interpretation"" for scipy.ndimage.zoom() but I acknowledge that the ""edge interpretation"" is equally valid (although I am not sure which applications benefit the most from it).



(Thanks to @Patol75 for pointing me into the right direction)
","I think that this is the expected behaviour.

Consider your initial list, [1, 2, 3]. You ask scipy to zoom on it 4 times, which thereby creates a 4x3=12 elements list. The first element of the list has to be 1, the last one has to be 3. Then, for 2, well we have an even number of elements, so it would make sense to have 2 as both the 6th and 7th elements. This gives [1, , , , , 2, 2, , , , , 3]. From here, you provided zoom with order=0, which means zoom is going to fill in for the missing values with splines of order 0. First case, zoom needs to fill in for 4 missing values between 1 and 2. This has to be [1, 1, 2, 2]. Second case, 4 missing values between 2 and 3. Same logic, [2, 2, 3, 3]. Final result [1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3].

Now consider a 5x zoom, which generates a 15 elements array. Same story, except that there is a ""middle"" element, so that only one 2 is initially placed in the new list, at the 8th spot. With six elements to fill in between each pair, we get with the same logic [1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3].

Therefore, you get more 2s than 1s or 3s because 2 is involved in two interpolation operations, opposed to one for both 1 &amp; 3. 
",
SciPy unexpected behavior,https://stackoverflow.com/questions/29306538,Unexpected behavior in scipy isf,"I am using scipy's stats module to try and determine values of a distribution at which the upper tail probability reaches some small value, but I am getting some very unrealistic results. For example:

I fit a beta distribution to an array of the square of normalized correlation coefficients for a signal matching operation (correlation coefficient is always between -1 and 1 so its square is between 0 and 1). Using 

import scipy, numpy as np
bd=scipy.beta.fit(np.square(data),floc=0,fscale=1) #fitted beta dist


which gives me the beta distribution parameters of 
    (0.42119596435034012, 16939.046996018118, 0, 1) the data array is about 3 million elements long. 

Now when I plot the distribution it is clear that most the area of the distribution is very near 0 on the x axis

import matplotlib.pyplot as plt
x=x=np.linspace(0,1,num=1000000)
plt.plot(x,scipy.stats.beta.pdf(x,betaparams[0],betaparams[1]))
plt.xlim([0,.0001])




Now when I try to find the x value for which some upper tail probability remains I get some unexpected behavior. For example

for expon in [-1,-2,-3,-4,-5,-6,-7,-8,-9,-10]:
    print (expon,scipy.stats.beta.isf(10**expon,betaparams[0],betaparams[1]))


yeilds:

(-1, 6.9580465891063448e-05)
(-2, 0.00018124328968143608)
(-3, 0.00030250611696189104)
(-4, 0.00042796070123291116)
(-5, 0.0005557482540313166)
(-6, 0.00068501413697673774)
(-7, 0.99999966996999767)
(-8, 0.99999996699699967)
(-9, 0.99999999669970008)
(-10, 0.99999999966997)


Clearly scipy is returning poor estimates around 10**-7. My question is why, why it would express this behavior silently, and how to fix it. 

Thanks
",1,234,"This appears to be a bug in scipy.special.btdtri which is supposed to compute quantiles for the beta distribution.  Maybe you can file a bug report.

&gt;&gt;&gt; from scipy import special
&gt;&gt;&gt; special.btdtri (betaparams[0],betaparams[1], 1-1e-6)
0.00068501413697504238
&gt;&gt;&gt; special.btdtri (betaparams[0],betaparams[1], 1-1e-7)
0.99999966996999767


I can't figure out where btdtri is defined.

EDIT: For the record, here is the SciPy bug report: https://github.com/scipy/scipy/issues/4677
",,
SciPy unexpected behavior,https://stackoverflow.com/questions/54968954,scipy.optimize.curve_fit: Default value of max_nfev broken?,"I am experiencing unexpected behavior when calling scipy.optimize.curve_fit with the max_nfev argument. The documentation states that extra kwargs are passed on to leastsq for method='lm' and to least_squares otherwise. Also, method should default to 'trf' (i.e. not 'lm') if bounds are provided. Finally, least_squares takes the max_nfev argument, which defaults to 100*len(x) if not provided (or explicitly passed as max_nfev=None), x being one of the arrays on which to do the curve fitting.

I have a dataset (and matching function) on which scipy.optimize.curve_fit fails. The time it takes before the routine gives up depends linearly on max_nfev, as expected. However, this time is vastly different between specifying max_nfev=100*len(x) in the call to scipy.optimize.curve_fit and not passing in max_nfev at all, which seems to go against the documented behavior.

Here is a script which demonstrates this:

import time
import numpy as np
import scipy.optimize

x, y = np.loadtxt('data.txt', unpack=True)

# Attempt curve fit
initial_guess = (1, 0)
bounds = ([-np.inf, 0], [np.inf, 15])
for max_nfev in (None, 1*len(x), 10*len(x), 100*len(x)):
    print('\nRunning with max_nfev = {}:'.format(max_nfev))
    t0 = time.time()
    try:
        scipy.optimize.curve_fit(
            lambda x, factor, exponent: factor*x**exponent,
            x,
            y,
            initial_guess,
            bounds=bounds,
            ftol=1e-10,
            maxfev=max_nfev,
        )
        deltat = time.time() - t0
        print('Succeeded after', deltat, 's')
    except RuntimeError:
        deltat = time.time() - t0
        print('Failed after', deltat, 's')


The script needs the dataset in data.txt (24 KB), which you can download here.

On my system, the output of this script is


  Running with max_nfev = None:
  Failed after 0.10752344131469727 s
  
  Running with max_nfev = 441:
  Failed after 0.17525863647460938 s
  
  Running with max_nfev = 4410:
  Failed after 1.732572078704834 s
  
  Running with max_nfev = 44100:
  Failed after 17.796284437179565 s


I would expect the first (max_nfev=None) and last (max_nfev=100*len(x)) call to take roughly the same amount of time to fail. To add to the mystery, it seems that instead of passing max_nfev I might just as well pass maxfev, which is not a valid argument of least_squares, but instead the maxfev-equivalent argument taken by leastsq. 

Have I misunderstood something, or is the documentation or the implementation wrong?

I am experiencing this under both {SciPy 1.1.0, Python 3.6.5} and {SciPy 1.2.0, Python 3.7.1}.
",1,1487,"
  I would expect the first (max_nfev=None) and last (max_nfev=100*len(x)) call to take roughly the same amount of time


Set a breakpoint at line 250 of scipy.optimize._lsq.trf.py:

    if max_nfev is None:
        max_nfev = x0.size * 100


At this point x0 has just two elements, so your call that passed in None could equivalently have passed in 200. x0 came from p0, which defaulted to [1 0].

Based on this, your observed timings do make sense.
",,
SciPy unexpected behavior,https://stackoverflow.com/questions/57477723,Unexpected behaviour of scipy.integrate,"I want to use scipy.integrate for some numerical calculations. I just ran a little example to try it and ran across some unexpected behavior. 

I made some clean code to demonstrate the problem. I use a very simple exponential distribution to test. 

Here's my code:

import numpy as np
import sys
import scipy as sc
from scipy import integrate


print(sys.version)
print(np.version.version)
print(sc.version.version)
print()

r1 = integrate.quad(lambda x: sc.exp(-x), 0, 10)
r2 = integrate.quad(lambda x: sc.exp(-x), 0, 100000)
r3 = integrate.quad(lambda x: sc.exp(-x), 0, np.inf)

print(r1)
print(r2)
print(r3)

print()
r4 = integrate.quad(lambda x: sc.exp(-x), 0, 10000)
print(r4)


The output is

3.7.2 (default, Jan  2 2019, 17:07:39) [MSC v.1915 64 bit (AMD64)]
1.15.4
1.1.0

r1 (0.9999546000702375, 2.8326146575791917e-14)
r2 (2.0614532085314573e-45, 4.098798466247153e-45)
r3 (1.0000000000000002, 5.842606996763696e-11)

r4 (1.0, 1.6059202674761255e-14)


I expect all the output to be always approximately one. But in r2 i get an incredibly small value. Strangely, when integrating to infinity (r3), or a very small border (r1), the problem doesn't show up. Also, by decreasing the limit by one order of magnitude (r4) I also get a perfect result.

Does anyone know why this problem appears in scipy? 
I would call this a bug, but maybe I violated some restrictions?
How do I know in advance to prevent wrong results in my applied problems?

Thank you in advance

Output for full_output:

r2 (2.0614532085314573e-45, 4.098798466247153e-45, {'neval': 63, 'last': 2, 'iord': array([      1,       2,       3,       4,       5, 6357060, 6357108,
       4259932, 6357102, 7274595, 6553710, 3342433, 7077980, 6422633,
       7536732, 7602281, 2949221, 6357104, 7012451, 6750305, 7536741,
       7536732, 6881379, 7929968, 7274588, 7602288, 7143529, 7995497,
       6029413, 7209055, 7077998, 3014771, 7340131, 3604531, 7798829,
       7209065, 6357087, 6553709, 3407926, 7340078, 6553721, 3276846,
       5046318, 7209057, 6684777, 7536741,     116, 6619136, 7602291,
             0], dtype=int32), 'alist': array([0.00000000e+000, 5.00000000e+004, 0.00000000e+000, 0.00000000e+000,
       6.88436472e-272, 3.80218509e-136, 2.65902947e-068, 2.20016853e-034,
       1.04474528e-019, 3.09734336e-016, 9.03970673e-019, 8.23342652e-316,
       8.23342968e-316, 8.23343284e-316, 8.23343601e-316, 8.23343917e-316,
       8.23344233e-316, 8.23344549e-316, 8.23344865e-316, 8.23345182e-316,
       8.23345498e-316, 8.23345814e-316, 8.23346130e-316, 8.23346446e-316,
       8.23346763e-316, 8.23347079e-316, 8.23347395e-316, 8.23347711e-316,
       8.23348027e-316, 8.23348344e-316, 8.23348660e-316, 8.23348976e-316,
       8.23349292e-316, 8.23349608e-316, 8.23349925e-316, 8.23350241e-316,
       8.23350557e-316, 8.23350873e-316, 8.23351189e-316, 8.23351506e-316,
       8.23351822e-316, 8.23352138e-316, 8.23352454e-316, 8.23352770e-316,
       8.23353087e-316, 8.23353403e-316, 8.23353719e-316, 8.23354035e-316,
       8.23354351e-316, 8.23354668e-316]), 'blist': array([5.00000000e+004, 1.00000000e+005, 0.00000000e+000, 0.00000000e+000,
       6.88436472e-272, 3.80218509e-136, 2.65902947e-068, 2.20016853e-034,
       1.04474528e-019, 3.09734336e-016, 9.03970673e-019, 1.20736675e+285,
       1.05117823e-153, 1.05132391e-153, 1.05146958e-153, 3.79823888e-258,
       1.61465766e+184, 3.11517960e+161, 4.26137323e+257, 6.01346953e-154,
       6.01366349e-154, 1.19632546e-153, 3.64465882e-086, 1.31100174e-259,
       1.20679441e-153, 1.20679327e-153, 3.24245662e-086, 3.64465882e-086,
       6.01357764e-154, 1.20679441e-153, 5.75105581e+072, 2.20791354e+214,
       1.27734658e-152, 5.29444423e+160, 6.19633416e+223, 2.25563599e-153,
       8.21947530e+223, 6.09892510e-013, 1.06097757e-153, 2.86747940e-110,
       6.06154135e-154, 6.06445477e-154, 6.96312298e-077, 3.00226946e-067,
       6.03810921e-154, 1.30421760e-076, 1.21438942e-067, 4.61448322e-072,
       8.51221910e-053, 3.73237334e+069]), 'rlist': array([2.06145321e-045, 0.00000000e+000, 6.73898103e+149, 3.51023756e+151,
       4.50937881e-292, 9.43293441e-314, 4.65203811e+151, 6.99386802e-283,
       3.53886392e-308, 1.33360313e+241, 1.15420781e+171, 9.30281767e+242,
       1.17364463e+214, 3.12671297e+185, 2.85341794e-313, 8.18432962e-085,
       6.45840689e+170, 4.42638830e-239, 9.78681729e+199, 3.38460675e+125,
       3.11732880e+150, 9.78747303e+199, 2.27948172e-191, 1.04972250e+214,
       4.77402433e+180, 1.12985581e+277, 3.16464606e-307, 1.33360315e+241,
       1.76252970e-310, 1.02318154e-012, 1.15549302e-313, 1.03539814e-308,
       1.33360293e+241, 5.67421675e-311, 5.00120719e-162, 6.46048250e-313,
       1.68400738e-019, 1.10811151e-302, 1.66468912e-312, 1.09403545e-303,
       1.27613271e-303, 7.10020498e-270, 4.99875566e-111, 9.11927054e-304,
       9.11571045e-304, 9.11749048e-304, 9.11571042e-304, 9.60205653e+303,
       5.43239349e-312, 1.79972786e-304]), 'elist': array([4.09879847e-045, 0.00000000e+000, 6.47287707e+170, 5.98178835e-154,
       1.69375668e+190, 4.44389806e+252, 1.12297399e+219, 1.87673453e-152,
       7.20706153e+159, 1.27826731e-152, 2.43812981e-152, 5.52716101e+228,
       6.01346953e-154, 1.57761457e+214, 7.19938459e+252, 3.94357072e+180,
       3.44210870e+175, 3.62478142e+228, 1.23732543e-259, 3.53810655e+155,
       4.81222029e+233, 1.06843264e-258, 9.15000112e+199, 4.26614628e+180,
       3.53387914e+246, 2.35509149e+251, 1.69375944e+190, 1.57762309e+214,
       6.19634286e+223, 8.95533289e-106, 5.98148090e-154, 1.17914189e+195,
       5.42869734e+213, 6.72794695e+199, 5.30383390e+180, 1.02188594e-152,
       2.16452413e+233, 7.50052033e+247, 6.98907523e+096, 7.69843824e+218,
       3.23097122e+174, 9.84214185e-154, 1.36723829e+161, 1.19346501e+243,
       1.94670285e+227, 2.21366476e+214, 8.95533289e-106, 8.75378213e+247,
       1.87673453e-152, 2.50722129e-310])})



",1,1213,"It is not a bug, it has to do with the numerical precision of the integration, and the fact that you are integrating a function that is (almost) 0 in most of the interval.
From the docs:


  Be aware that pulse shapes and other sharp features as compared to the
  size of the integration interval may not be integrated correctly using
  this method.


Based on your output, the function is using only two (last=2) intervals, evaluating values for rlist=(2.06145321e-045, 0.00000000e+000, ..) on each (see the docs for more detail on the output)

You can add points to the interval to force the routine to use points closer to the left limit.

a = quad(lambda x: np.exp(-x), 0, 1e9, points=np.logspace(-10,3,10))
print(a)
(0.9999999999999997, 2.247900608926337e-09)


Adding to the explanation (thanks to @norok2): Note that points is a sequence of break points in the bounded integration interval where local difficulties of the integrand may occur (e.g., singularities, discontinuities). In this case, I'm not using it to point out discontinuities, but rather to force quad to perform more integration steps near the left boundary, using a log-spaced interval since a I have an exponential function (this is of course arbitrary and for this function, since I know its shape).
","There is no need to convert your integral into one over a (very large) interval. There is a specific integration scheme for integrals of the form

,

namely Gauss-Laguerre quadrature. It's also included in quadpy (a project of mine). Simply try out

import numpy
import quadpy

scheme = quadpy.e1r.gauss_laguerre(1)

val = scheme.integrate(lambda x: numpy.ones(x.shape[1:]))

print(val)


1.0

",
SciPy unexpected behavior,https://stackoverflow.com/questions/61594854,Packaged dags - Airflow can&#39;t find module installed,"I'm trying to use apache airlfow with packaged dags (https://airflow.apache.org/docs/stable/concepts.html#packaged-dags).

I've written my code as a python package and obviously my code depends on other popular libraries such as numpy, scipy etc.

EDIT:
This is setup.py of my custom python package:

from setuptools import setup, find_packages
from pathlib import Path
from typing import List

import distutils.text_file

def parse_requirements(filename: str) -&gt; List[str]:
    """"""Return requirements from requirements file.""""""
    # Ref: https://stackoverflow.com/a/42033122/
    return distutils.text_file.TextFile(filename=str(Path(__file__).with_name(filename))).readlines()


setup(name='classify_business',
      version='0.1',
      python_requires=""&gt;=3.6"",
      description='desc',
      url='https://urlgitlab/datascience/classifybusiness',
      author='Marco fumagalli',
      author_email='marco.fumagalli@mycompany.com',
      packages = find_packages(),
      license='MIT',
      install_requires=
      parse_requirements('requirements.txt'),
      zip_safe=False,
      include_package_data=True)


requirements.txt contains packages ( vertica_python, pandas, numpy etc) along with their version needed for my code.

I wrote a litte shell script based on the one provied in the doc:

set -eu -o pipefail

if [ $# == 0 ]; then
    echo ""First param should be /srv/user_name/virtualenvs/name_virtual_env""
    echo ""Second param should be name of temp_directory""
    echo ""Third param directory should be git url""
    echo ""Fourth param should be dag zip name, i.e dag_zip.zip to be copied into AIRFLOW__CORE__DAGS__FOLDER""
    echo ""Fifth param should be package name, i.e classify_business""
fi


venv_path=${1}
dir_tmp=${2}
git_url=${3}
dag_zip=${4}
pkg_name=${5}



python3 -m venv $venv_path
source $venv_path/bin/activate
mkdir $dir_tmp
cd $dir_tmp

python3 -m pip install --prefix=$PWD git+$git_url

zip -r $dag_zip *
cp $dag_zip $AIRFLOW__CORE__DAGS_FOLDER

rm -r $dir_tmp


The shell will install my package along with dependencies directly from gitlab, zip and then move to the dags folder.

This is the content of the folder tmp_dir before being zipped.

bin  
lib  
lib64  
predict_dag.py  
train_dag.py


Airflow doesn't seem to be able to import package installed in lib or lib64.
I'm getting this error


  ModuleNotFoundError: No module named 'vertica_python'


I even tried to move my custom package outside of lib:

bin
my_custom_package
lib  
lib64  
predict_dag.py  
train_dag.py


But still getting same error.

PS: One of the problem I think relies on how to use pip to install package in a specific location.
Airflow example use --install-option=""--install-lib=/path/"" but it's unsupported:


  Location-changing options found in --install-option: ['--install-lib']
  from command line. This configuration may cause unexpected behavior
  and is unsupported. pip 20.2 will remove support for this
  functionality. A possible replacement is using pip-level options like
  --user, --prefix, --root, and --target. You can find discussion regarding this at https://github.com/pypa/pip/issues/7309.


Using --prefix leads to a structure like above, with module not found error.

Using --target leads to every package installed in the directory specified.
In this case I have a pandas related error

C extension: No module named 'pandas._libs.tslibs.conversion' not built


I guess that it's related to dynamic libraries that should be available at a system level?

Any hint?

Thanks
",1,5544,"Ciao Marco,
I know this is an old question, but I had to go through the very same process and what worked for me was to use:
pip install -r ../requirements_dag.txt --target=""$PWD""

The same works for packages hosted on git. The key difference is the use of --target rather than --prefix.
","The Airflow documentation page you're referring to says this about packaged DAGs:


  To allow this you can create a zip file that contains the DAG(s) in the root of the zip file and have the extra modules unpacked in directories.


The way I interpret this is different from yours. I don't think Airflow handles these packaged DAGs as a real python package. It just seems like a custom zip folder that will be added to your DAGs folder. So the lib or lib64 folders you have are probably not real python modules (they don't have a __init__.py file). That's why they say that ""the extra modules should be unpacked in directories"".

Look at the example zip file they give:

my_dag1.py
my_dag2.py
package1/__init__.py
package1/functions.py


package1 has a __init__.py file. So in your case, your vertica_python library should be directly importable like this:

my_custom_package
vertica_python/
predict_dag.py  
train_dag.py


However, I don't think you should do this. I have the impression that the modules that you should add here are your own developed modules, not third party libraries.

So I suggest that you install the libraries you need to run your packaged DAGs beforehand.
",
SciPy unexpected behavior,https://stackoverflow.com/questions/29103540,Scipy Sparse: Unexpected Identity Behavior,"Following is my attempt to create a sparse matrix that has c as its diagonal. I know there is also alternative methods for this, but I'm rather curious why the following code is not working as expected:

import numpy as np
import scipy.sparse as sparse

c = np.arange(0,5)
&gt;&gt;&gt; np.identity(5)*c
array([[ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  2.,  0.,  0.],
       [ 0.,  0.,  0.,  3.,  0.],
       [ 0.,  0.,  0.,  0.,  4.]])
&gt;&gt;&gt; sparse.identity(5)*c
array([ 0.,  1.,  2.,  3.,  4.])
#expected output:
&lt;5x5 sparse matrix of type '&lt;type 'numpy.float64'&gt;'
with 5 stored elements (1 diagonals) in DIAgonal format&gt;
# and (sparse.identity(5)*c).todense() == np.identity(5)*c

",0,98,"There are 2 common types of array multiplication, element-by-element, and matrix.

In MATLAB, * is the matrix version, .* is the element by element one.

In numpy, * is element by element (with broadcasting), 'np.dot' is the basic form of matrix multiplication.  Python developers have approved @ as an operator that could be used for matrix multiplication (eventually).

For numpy matrix subclass, * is the matrix multiplication, np.multiply is used for element-by-element.  (np.multiply also works for ndarray.)  

scipy follows the np.matrix convention.  * is the matrix multiplication.  sparse.identity(5).multiply(c) does the element by element multiplication (though it returns a np.matrix, not a sparse one).

As to why, it comes down to conventions that the developers were used to.  For linear algebra problems, matrix multiplication is common, hence it's use in sparse.  np.matrix copies the MATLAB conventions.  MATLAB was created to give access to FORTRAN matrix libraries.

In physics there's another convention, Einstein notation.  This is a generalized matrix multiplication, extended to more dimensions. np.einsum does this. It can implement element by element multiplication, though at its core it uses a 'sum of products' method.  But it's not been implemented for np.matrix or sparse (and really isn't needed since those are always 2d).

Notice how similar the specifications are for your example

np.einsum('ij,j-&gt;ij',np.identity(5),c) # element by element
np.einsum('ij,j-&gt;i',np.identity(5),c)  # matrix (sum on j)

","In the expression sparse.identity(5)*c, the multiplication operator of the sparse matrix is used, which is the algebraic matrix multiplication (i.e. a matrix times a vector gives a vector).

You can create a sparse diagonal matrix with a given diagonal using scipy.sparse.diags:

In [18]: from scipy import sparse

In [19]: c = np.arange(5)

In [20]: d = sparse.diags(c, 0)

In [21]: d
Out[21]: 
&lt;5x5 sparse matrix of type '&lt;type 'numpy.float64'&gt;'
    with 5 stored elements (1 diagonals) in DIAgonal format&gt;

In [22]: d.A
Out[22]: 
array([[ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  2.,  0.,  0.],
       [ 0.,  0.,  0.,  3.,  0.],
       [ 0.,  0.,  0.,  0.,  4.]])

",
SciPy unexpected behavior,https://stackoverflow.com/questions/67142952,scipy weird unexpected behavior curve_fit large data set for sin wave,"For some reason when I am trying to large amount of data to a sin wave it fails and fits it to a horizontal line. Can somebody explain?
Minimal working code:
import numpy as np
import matplotlib.pyplot as plt
from scipy import optimize
# Seed the random number generator for reproducibility
import pandas

np.random.seed(0)

# Here it work as expected
# x_data = np.linspace(-5, 5, num=50)
# y_data = 2.9 * np.sin(1.05 * x_data + 2) + 250 + np.random.normal(size=50)

# With this data it breaks
x_data = np.linspace(0, 2500, num=2500)
y_data = -100 * np.sin(0.01 * x_data + 1) + 250 + np.random.normal(size=2500)

# And plot it

plt.figure(figsize=(6, 4))
plt.scatter(x_data, y_data)


def test_func(x, a, b, c, d):
    return a * np.sin(b * x + c) + d

# Used to fit the correct function
# params, params_covariance = optimize.curve_fit(test_func, x_data, y_data)

# making some guesses
params, params_covariance = optimize.curve_fit(test_func, x_data, y_data,
                                               p0=[-80, 3, 0, 260])

print(params)
plt.figure(figsize=(6, 4))
plt.scatter(x_data, y_data, label='Data')
plt.plot(x_data, test_func(x_data, *params),
         label='Fitted function')

plt.legend(loc='best')

plt.show()




Does anybody know, how to fix this issue. Should I use a different fitting method not least square? Or should I reduce the number of data points?
",0,184,"Given your data, you can use the more robust lmfit instead of scipy.
In particular, you can use SineModel (see here for details).
SineModel in lmfit is not for ""shifted"" sine waves, but you can easily deal with the shift doing
y_data_offset = y_data.mean()
y_transformed = y_data - y_data_offset
plt.scatter(x_data, y_transformed)
plt.axhline(0, color='r')


Now you can fit to sine wave
from lmfit.models import SineModel

mod = SineModel()

pars = mod.guess(y_transformed, x=x_data)
out = mod.fit(y_transformed, pars, x=x_data)

you can inspect results with print(out.fit_report()) and plot results with
plt.plot(x_data, y_data, lw=7, color='C1')
plt.plot(x_data, out.best_fit+y_data_offset, color='k')
#           we add the offset ^^^^^^^^^^^^^


or with the builtin plot method out.plot_fit(), see here for details.
Note that in SineModel all parameters ""are constrained to be non-negative"", so your defined negative amplitude (-100) will be positive (+100) in the parameters fit results. So the phase too won't be 1 but +1 (PS: they call shift the phase)
print(out.best_values)

{'amplitude': 99.99631403054289,
 'frequency': 0.010001193681616227,
 'shift': 4.1400215410836605}

",,
SciPy unexpected result,https://stackoverflow.com/questions/24242660,pymc3 : Multiple observed values,,11,5730,,,
SciPy unexpected result,https://stackoverflow.com/questions/49280404,Shift interpolation does not give expected behaviour,,7,985,"It is worth noting that this behavior appears to be a bug, as noted in this SciPy issue:
https://github.com/scipy/scipy/issues/2640

The issue appears to effect every extrapolation mode in scipy.ndimage other than mode='mirror'.
",,
SciPy unexpected result,https://stackoverflow.com/questions/40451203,Cython parallel loop problems,,5,3869,,,
SciPy unexpected result,https://stackoverflow.com/questions/45255265,Unexpected behavior of Gaussian filtering with Scipy,,5,1015,,,
SciPy unexpected result,https://stackoverflow.com/questions/50831551,parallel/multithread differential evolution in python,"I'm trying to model a biochemical process, and I structured my question as an optimization problem, that I solve using  differential_evolution from scipy.
So far, so good, I'm pretty happy with the  implementation of a simplified model with 15-19 parameters.
I expanded the model and now, with 32 parameters, is taking way too long. Not  totally unexpected, but still an issue, hence the question.

I've seen:
- an almost identical question for R  Parallel differential evolution
- and a github issue https://github.com/scipy/scipy/issues/4864 on the topic  

but it would like to stay in python (the model is within a python pipeline), and the pull request did not lead to and officially accepted solution yet, although some options have been suggested.

Also, I can't parallelize the code within the function to be optimised because is a series of sequential calculations each requiring the result of the previous step. The ideal option would be to have something that evaluates some individuals in parallel and return them to the population.

Summing up:
- Is there any option within scipy that allows parallelization of differential_evolution that I dumbly overlooked? (Ideal solution)
- Is there a suggestion for an alternative algorithm in scipy that is either (way) faster in serial or possible to parallelize?
- Is there any other good package that offers parallelized differential evolution funtions? Or other applicable optimization methods?
- Sanity check: am I overloading DE with 32 parameter and I need to radically change approach?

PS
I'm a biologist, formal math/statistics isn't really my strenght, any formula-to-english translation would be hugely appreciated :)

PPS
As an extreme option I could try to migrate to R, but I can't code C/C++ or other languages.
",5,4707,"Scipy differential_evolution can now be used in parallel extremely easily, by specifying the workers:


  workers int or map-like callable, optional
  
  If workers is an int the population is subdivided into workers
  sections and evaluated in parallel (uses multiprocessing.Pool). Supply
  -1 to use all available CPU cores. Alternatively supply a map-like callable, such as multiprocessing.Pool.map for evaluating the
  population in parallel. This evaluation is carried out as
  workers(func, iterable). This option will override the updating
  keyword to updating='deferred' if workers != 1. Requires that func be
  pickleable.
  
  New in version 1.2.0.


scipy.optimize.differential_evolution documentation
","Thanks to @jp2011 for pointing to pygmo

First, worth noting the difference from pygmo 1, since the fist link on google still directs to the older version.

Second, Multiprocessing island are available only for python 3.4+

Third, it works. The processes I started when I first asked the question are still running while I write, the pygmo archipelago running an extensive test of all the 18 possible DE variations present in saDE made in less than 3h. The compiled version using Numba as suggested here https://esa.github.io/pagmo2/docs/python/tutorials/coding_udp_simple.html will probably finish even earlier. Chapeau.

I personally find it a bit less intuitive than the scipy version, given the need to build a new class (vs a signle function in scipy) to define the problem but is probably just a personal preference. Also, the mutation/crossing over parameters are defined less clearly, for someone approaching DE for the first time might be a bit obscure.
But, since serial DE in scipy just isn't cutting it, welcome pygmo(2).

Additionally I found a couple other options claiming to parallelize DE. I didn't test them myself, but might be useful to someone stumbling on this question.  

Platypus, focused on multiobjective evolutionary algorithms 
https://github.com/Project-Platypus/Platypus

Yabox
https://github.com/pablormier/yabox

from Yabox creator a detailed, yet IMHO crystal clear, explaination of DE
https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/
","I suggest the batch mode of PyFDE.
https://pythonhosted.org/PyFDE/tutorial.html#batch-mode
In batch mode, the fitness function will be called only once per iteration to evaluate the fitness of all the population.
The example w/o the batch mode:
import pyfde
from math import cos, pi
import time
import numpy

t1=time.time()
def fitness(p):
    x, y = p[0], p[1]
    val = 20 + (x**2 - 10*cos(2*pi*x)) + (y**2 - 10*cos(2*pi*y))
    return -val
    
solver = pyfde.ClassicDE(fitness, n_dim=2, n_pop=40, limits=(-5.12, 5.12))
solver.cr, solver.f = 0.9, 0.45
best, fit = solver.run(n_it=150)
t2=time.time()
print(""Estimates: "",best)
print(""Normal mode elapsed time (s): "",t2-t1)

The batch mode example:
t1=time.time()
def vec_fitness(p,fit):
    x, y = numpy.array(p[:,0]), numpy.array(p[:,1])
    val = 20 + (x**2 - 10*numpy.cos(2*pi*x)) + (y**2 - 10*numpy.cos(2*pi*y))
    fit[:] = -val
    
solver = pyfde.ClassicDE(vec_fitness, n_dim=2, n_pop=40, limits=(-5.12, 5.12), batch=True)
solver.cr, solver.f = 0.9, 0.45
best, fit = solver.run(n_it=150)
t2=time.time()
print(""Estimates: "",best)
print(""Batch mode elapsed time (s): "",t2-t1)

The output is:
Estimates:  [1.31380987e-09 1.12832169e-09]
Normal mode elapsed time (s):  0.015959978103637695
Estimates:  [2.01733383e-10 1.23826873e-10]
Batch mode elapsed time (s):  0.006017446517944336
############################################################
It's 1.5x faster, but only for a simple question. You can see &gt;10x faster for a complex question.
The code runs on a single CPU core (no multi-processing), and the performance improvement comes from the use of vectorization and MIMD (multiple instruction, multiple data). Combining vectorization and parallel/multi-processing will result in a double-improvement.
"
SciPy unexpected result,https://stackoverflow.com/questions/28321286,pandas rolling_quantile bug?,"i recently bumped an unexpected issue with pandas rolling funcs. rolling_quantile for example:

&gt;&gt; row = 10
&gt;&gt; col = 5
&gt;&gt; idx = pd.date_range(20100101,periods=row,freq='B')
&gt;&gt; a = pd.DataFrame(np.random.rand(row*col).reshape((row,-1)),index=idx)
&gt;&gt; a
                   0           1           2           3           4
2010-01-01  0.341434    0.497274    0.596341    0.259909    0.872207
2010-01-04  0.222653    0.056723    0.064019    0.936307    0.785647
2010-01-05  0.179067    0.647165    0.931266    0.557698    0.713282
2010-01-06  0.049766    0.259756    0.945736    0.380948    0.282667
2010-01-07  0.385036    0.517609    0.575958    0.050758    0.850735
2010-01-08  0.628169    0.510453    0.325973    0.263361    0.444959
2010-01-11  0.099133    0.976571    0.602235    0.181185    0.506316
2010-01-12  0.987344    0.902289    0.080000    0.254695    0.753325
2010-01-13  0.759198    0.014548    0.139858    0.822900    0.251972
2010-01-14  0.404149    0.349788    0.038714    0.280568    0.197865

&gt;&gt; a.quantile([0.25,0.5,0.75],axis=0)
               0           1           2           3           4
0.25    0.189963    0.282264    0.094964    0.255999    0.323240
0.50    0.363235    0.503864    0.450966    0.271964    0.609799
0.75    0.572164    0.614776    0.600761    0.513510    0.777567

&gt;&gt; np.percentile(a,[25,50,75],axis=0)
[array([ 0.18996316,  0.28226404,  0.09496441,  0.25599853,  0.32323997]),
 array([ 0.36323529,  0.50386356,  0.45096554,  0.27196429,  0.60979881]),
 array([ 0.57216415,  0.61477607,  0.6007611 ,  0.51351021,  0.7775667 ])]

&gt;&gt; pd.rolling_quantile(a,row,0.25).tail(1)
                   0           1       2           3           4
2010-01-14  0.179067    0.259756    0.08    0.254695    0.282667


looks like pandas.DataFrame.quantile member func is consistent with the numpy.percentile func.  however the pandas.rolling_quantile func returns diff results.  reduce the row number to 5, the problem will be gone (all three methods return the same results).  any thoughts?

ps: i also tested rolling_std func which will ""random"" generate error with 10^-7 ~ 10^-8 scales for long (row-wise) pandas.DataFrames

python environment:


python 3.4.2
cython 0.21.1
numpy 1.8.2
scipy 0.14.0
pandas 0.15.1
statsmodels 0.6.0

",4,1150,,,
SciPy unexpected result,https://stackoverflow.com/questions/55428503,Scipy sparse.kron gives non-sparse matrix,"I am getting unexpected non-sparse results when using the kron method of Scipy's sparse module. Specifically, matrix elements that are equal to zero after performing the kronecker product are being kept in the result, and I'd like to understand what I should do to ensure the output is still fully sparse.

Here's an example of what I mean, taking the kronecker product of two copies of the identity:

import scipy.sparse as sp

s = sp.eye(2)

S = sp.kron(s,s)

S 
&lt;4x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
with 8 stored elements (blocksize = 2x2) in Block Sparse Row format&gt;

print(S)

(0, 0)  1.0
(0, 1)  0.0
(1, 0)  0.0
(1, 1)  1.0
(2, 2)  1.0
(2, 3)  0.0
(3, 2)  0.0
(3, 3)  1.0


The sparse matrix S should only contain the 4 (diagonal) non-zero entries, but here it also has other entries that are equal to zero. Any pointers on what I am doing wrong would be much appreciated.
",3,578,"In 

Converting from sparse to dense to sparse again decreases density after constructing sparse matrix

I point out that sparse.kron produces, by default a BSR format matrix.  That's what your display shows.  Those extra zeros are part of the dense blocks.

If you specify another format, kron will not produce  those zeros:

In [672]: sparse.kron(s,s,format='csr')                                         
Out[672]: 
&lt;4x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
    with 4 stored elements in Compressed Sparse Row format&gt;
In [673]: _.A                                                                   
Out[673]: 
array([[1., 0., 0., 0.],
       [0., 1., 0., 0.],
       [0., 0., 1., 0.],
       [0., 0., 0., 1.]])

",,
SciPy unexpected result,https://stackoverflow.com/questions/50932433,Scipy and Sklearn chi2 implementations give different results,"I an using sklearn.feature_selection.chi2 for feature selection and found out some unexpected results (check the code). Do anyone knows what is the reason or can point me to some documentation or pull request?

I include a comparison of the results I got and the expected ones obtained by hand and using scipy.stats.chi2_contingency.

The code:

import numpy as np
import pandas as pd
from scipy.stats import chi2_contingency
from sklearn.feature_selection import chi2, SelectKBest

x = np.array([[1, 1, 1, 0, 1], [1, 0, 1, 0, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 0], [0, 0, 0, 1, 1], [0, 0, 0, 1, 0]])
y = np.array([1, 1, 2, 2, 3, 3])

scores = []
for i in range(x.shape[1]):
    result = chi2_contingency(pd.crosstab(x[:, i], y))
    scores.append(result[0])

sel = SelectKBest(score_func=chi2, k=3)
sel.fit(x, y)

print(scores)
print(sel.scores_)
print(sel.get_support())


The results are:

[6., 2.4, 6.0, 6.0, 0.0] (Expected)
[4. 2. 2. 2. 0.] (Unexpected)
[ True  True False  True False]


Using scipy, it keeps features 0, 2, 3, while, with sklearn it keeps features 0,1,3.
",3,1485,"Yes, they do give different results. And I think you should trust the results from scipy, and reject the results from sklearn.
But let me provide details of my reasoning, because I could be wrong.
I lately observed a similar effect to what you describe, with a data set of 300 data points: the results of the two chi2 implementations differ indeed. In my case the difference was striking. I described the issue in details in this article , followed by this Cross Validated discussion thread and I also submitted a bug request to sklearn, available for review here.
The added value from my research, if any, seems to be that the results delivered by the scipy implementation seem correct, while the results from sklearn are incorrect. Please see the article for the details. But I only focused on my sample, so the conclusion may not be universally true. Sadly the source code analysis is beyond my capability, but I hope this input can help someone to possibly either improve the code, or disprove my reasoning if wrong.
",,
SciPy unexpected result,https://stackoverflow.com/questions/50142269,Why does scipy bessel root finding not return roots at zero?,"I am trying to use code which uses Bessel function zeros for other calculations. I noticed the following piece of code produces results that I consider unexpected.

    import scipy
    from scipy import special

    scipy.special.jn_zeros(1,2)


I would expect the result from this call to be

    array([0., 3.83170597])


instead of

    array([3.83170597, 7.01558667])


Is there a reason a reason why the root at x=0.0 is not being returned?

From what I can see the roots are symmetric along the x-axis except for any found at the origin, but I do not think this would be enough of a reason to leave off the root completely.

The computer I am using has python version 2.7.10 installed and is using scipy version 0.19.0

P.S. the following function is what I am trying to find the zeros of

    scipy.special.j1

",3,486,,,
SciPy unexpected result,https://stackoverflow.com/questions/46556376,Scipy&#39;s cut_tree() doesn&#39;t return requested number of clusters and the linkage matrices obtained with scipy and fastcluster do not match,"I'm doing an agglomerative hierarchical clustering (AHC) experiment using the fastcluster package in connection with scipy.cluster.hierarchy module functions, in Python 3, and I found a puzzling behaviour of the cut_tree() function.

I cluster data with no problem and get a linkage matrix, Z, using linkage_vector() with method=ward. Then, I want to cut the dendogram tree to get a fixed number of clusters (e.g. 33) and I do this properly using cut_tree(Z, n_clusters=33). (Remember that AHC is a deterministic method yielding a binary tree connecting all your datapoints, which sit at the leafs of the tree; you can look at this tree at any level to ""see"" the number of clusters you want in the end; all cut_tree() does is to return a set of 'n_cluster' integer labels from 0 to n_clusters - 1, attributed to every point of the dataset.)

I've done this many times in other experiments and I always get the number of clusters I request. The problem is that with this one dataset, when I ask cut_tree() for 33 clusters, it gives me only 32. I don't see why this is the case. Could it be a bug? Are you aware of any bug with cut_tree()? I tried to debug this behaviour and performed the same clustering experiment using scipy's linkage() function. With the resulting linkage matrix as input to cut_tree() I didn't get an unexpected number of clusters as output. I also verified that the linkage matrices output by the two methods are not equal.

The [dataset] I'm using consists of 10680 vectors, each with 20 dimensions. Check the following experiment:

import numpy as np
import fastcluster as fc
import scipy.cluster.hierarchy as hac
from scipy.spatial.distance import pdist

### *Load dataset (10680 vectors, each with 20 dimensions)*
X = np.load('dataset.npy')

### *Hierarchical clustering using traditional scipy method*
dists = pdist(X)
Z_1 = hac.linkage(dists, method='ward')

### *Hierarchical clustering using optimized fastcluster method*
Z_2 = fc.linkage_vector(X, method='ward')

### *Comparissons*

## Are the linkage matrices equal?
print(""Z_1 == Z_2 ? "", np.allclose(Z_1, Z_2))

## Is scipy's cut_tree() returning the requested number of clusters when using Z_2?
print(""Req.\tGot\tequal?"")
for i in range(1,50):
    cut = hac.cut_tree(Z_2, i)
    uniq = len(np.unique(cut))
    print(i,""\t"",uniq,""\t"",i==uniq)

## The same as before, but in condensed form. When requesting cut_tree() for clusters
#  in the range [1,50] does it return wrong results at some point?
print(""Any problem cutting Z_1 for n_clusters in [1,50]? "", not np.all([len(np.unique(
                                      hac.cut_tree(Z_1, i)))==i for i in range(1,50)]))
print(""Any problem cutting Z_2 for n_clusters in [1,50]? "", not np.all([len(np.unique(
                                      hac.cut_tree(Z_2, i)))==i for i in range(1,50)]))

#Output:
#
#Z_1 == Z_2 ?  False
#
#Req.    Got     equal?
#1        1       True
#2        2       True
#3        3       True
#4        4       True
#5        5       True
#6        6       True
#7        7       True
#8        8       True
#9        9       True
#10       10      True
#11       11      True
#12       12      True
#13       13      True
#14       14      True
#15       15      True
#16       16      True
#17       17      True
#18       18      True
#19       19      True
#20       20      True
#21       21      True
#22       22      True
#23       23      True
#24       24      True
#25       25      True
#26       26      True
#27       27      True
#28       28      True
#29       29      True
#30       30      True
#31       31      True
#32       32      True
#33       32      False
#34       33      False
#35       34      False
#36       35      False
#37       36      False
#38       37      False
#39       38      False
#40       39      False
#41       40      False
#42       41      False
#43       42      False
#44       43      False
#45       44      False
#46       45      False
#47       46      False
#48       47      False
#49       48      False
#
#Any problem cutting Z_1 for n_clusters in [1,50]?  False
#Any problem cutting Z_2 for n_clusters in [1,50]?  True


You might have noticed the dataset contains 37 vectors with at least an exact copy, and counting all the copies there is a total 55 vectors with at least a copy in the dataset.

For inspection, I decided to plot the dendrogram tree up to a shallow depth level for the two linkage matrices, which you can see on the image bellow (Z_1 at the top and Z_2 at the bottom). Numbers inside parenthesis indicate the population contained bellow in that branch; numbers without parenthesis are leafs of the tree (the number is the index of the vector in the X matrix). One can see the only difference (at the plotted level) is at the branches marked with the red square, which coalesce at 0 distance as they contain overlapping vectors.



So, I ran the clustering procedures as shown in the previous code again, but this time with only the subset of the data containing the 55 vectors which have at least a copy. I obtained X_subset with:

uniqs, uniqs_indices, uniqs_count = np.unique(X, axis=0, return_index=True, return_counts=True)
duplicate_rows_indices = list( set(range(len(X))) - set(uniqs_indices) )
number_of_duplicate_rows = len(X)-len(uniqs) # 37

all_duplicate_rows = set()
for i in duplicate_rows_indices:
    _rows = set(np.where(X == X[i])[0])
    for j in _rows:
        all_duplicate_rows.add(j)

rows_with_at_least_a_copy = list(all_duplicate_rows)
number_of_rows_with_at_least_a_copy = len(rows_with_at_least_a_copy)  # 55

X_subset = X[rows_with_at_least_a_copy]


and my output this time was:

#Z_1 == Z_2 ?  False
#Req.    Got     equal?
#1        1       True
#2        2       True
#3        2       False
#4        3       False
#5        4       False
#6        5       False
#7        6       False
#8        7       False
#9        8       False
#10       9       False
#11       10      False
#12       11      False
#13       12      False
#14       13      False
#15       14      False
#16       15      False
#17       16      False
#18       17      False
#19       18      False
#20       20      True
#21       21      True
#22       22      True
#23       23      True
#24       24      True
#25       25      True
#26       26      True
#27       27      True
#28       28      True
#29       29      True
#30       30      True
#31       31      True
#32       32      True
#33       33      True
#34       34      True
#35       35      True
#36       36      True
#37       37      True
#38       38      True
#39       39      True
#40       40      True
#41       41      True
#42       42      True
#43       43      True
#44       44      True
#45       45      True
#46       46      True
#47       47      True
#48       48      True
#49       49      True
#Any problem cutting Z_1 for n_clusters in [1,50]?  False
#Any problem cutting Z_2 for n_clusters in [1,50]?  True


Thus, fastcluster and scipy are not returning the same results, and if it is only due to the overlapping points this could be acceptable because of the ambiguity of that clustering situation. But the problem is cut_tree() which sometimes doesn't return the requested number of clusters in these cases when given the linkage matrix obtained by linkage_vector(). How can this be fixed?

Library versions used: scipy '0.19.1', numpy '1.13.3', fastcluster '1.1.24'

Edit: It's also posted here: https://github.com/scipy/scipy/issues/7977.
",3,1203,,,
SciPy unexpected result,https://stackoverflow.com/questions/59281884,Unexpected behavior of `scipy.ndimage.zoom()` for `order=0`,,2,3324,"This is a bin/edge array interpretation issue.
The behavior of scipy.ndimage.zoom() is based on the edge interpretation of the array values, while the behavior that would produce equally-sized blocks for integer zoom factors (mimicking np.repeat()) is based on the bin interpretation.

Let's illustrate with some ""pictures"".

Bin Interpretation

Consider the array [1 2 3], and let's assign each value to a bin.
The edges of each bin would be: 0 and 1 for 1, 1 and 2 for 2, etc.

0 1 2 3
|1|2|3|


Now, let's zoom this array by a factor of 4:

                    1 1 1
0 1 2 3 4 5 6 7 8 9 0 1 2
|   1   |   2   |   3   |


Hence, the values to assign to the bins using the Next-door Neighbor method are:

                    1 1 1
0 1 2 3 4 5 6 7 8 9 0 1 2
|1 1 1 1|2 2 2 2|3 3 3 3|


Edge Interpretation

Consider the same array as before [1 2 3], but now let's assign each value to an edge:

0 1 2
| | |
1 2 3


Now, let's zoom this array by a factor of 4:

                    1 1
0 1 2 3 4 5 6 7 8 9 0 1
| | | | | | | | | | | |
1          2          3



Hence, the values to assign to the edges using the Next-door Neighbor method are:

                    1 1
0 1 2 3 4 5 6 7 8 9 0 1
| | | | | | | | | | | |
1 1 1 2 2 2 2 2 2 3 3 3


and edge 3 is assigned to 2 because 2 has position 5.5 while 1 has position 0 and (5.5 - 3 = 2.5) &lt; (3 - 0 = 3).
Similarly, edge 8 is assigned to 2 because (8 - 5.5 = 2.5) &lt; (11 - 8 = 3).



Comments

In Physics, the ""bin array interpretation"" is generally more useful, because measurements are typically ""the result of some integration over a certain bin in an appropriate domain"" (notably signal of any form -- including images -- collected at a given time interval), hence I was expecting a ""bin interpretation"" for scipy.ndimage.zoom() but I acknowledge that the ""edge interpretation"" is equally valid (although I am not sure which applications benefit the most from it).



(Thanks to @Patol75 for pointing me into the right direction)
","I think that this is the expected behaviour.

Consider your initial list, [1, 2, 3]. You ask scipy to zoom on it 4 times, which thereby creates a 4x3=12 elements list. The first element of the list has to be 1, the last one has to be 3. Then, for 2, well we have an even number of elements, so it would make sense to have 2 as both the 6th and 7th elements. This gives [1, , , , , 2, 2, , , , , 3]. From here, you provided zoom with order=0, which means zoom is going to fill in for the missing values with splines of order 0. First case, zoom needs to fill in for 4 missing values between 1 and 2. This has to be [1, 1, 2, 2]. Second case, 4 missing values between 2 and 3. Same logic, [2, 2, 3, 3]. Final result [1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3].

Now consider a 5x zoom, which generates a 15 elements array. Same story, except that there is a ""middle"" element, so that only one 2 is initially placed in the new list, at the 8th spot. With six elements to fill in between each pair, we get with the same logic [1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3].

Therefore, you get more 2s than 1s or 3s because 2 is involved in two interpolation operations, opposed to one for both 1 &amp; 3. 
",
SciPy unexpected result,https://stackoverflow.com/questions/76011928,Python: np.linalg.eigvalsh returning negatve eigen values,"I have a 7x7 covariance matrix (represented as an numpy array).
t = np.array(
    [
        [1.4, 0.3, 0.4, 0.8, 0.4, 0.9, 0.3],
        [0.3, 1.3, 0.4, 2.3, 0.4, 2.4, 0.4],
        [0.4, 0.4, 1.3, 2.8, 0.4, 1.0, 0.3],
        [0.8, 2.3, 2.8, 9.5, 1.0, 7.0, 1.0],
        [0.4, 0.4, 0.4, 1.0, 1.1, 1.2, 0.3],
        [0.9, 2.4, 1.0, 7.0, 1.2, 7.7, 1.0],
        [0.3, 0.4, 0.3, 1.0, 0.3, 1.0, 0.5],
    ],
    dtype=np.float64,
)

I have checked this matrix to be symmetric.
np.allclose(t, t.T)
True

And np.linalg.svd returns valid non-negative singular values. However, np.linalg.eigvalsh, is returning a negative eigenvalue.
min(np.linalg.eigvalsh(t))
-0.06473876145336957

This doesnt make too much sense to me as I have checked that the column of the matrix are linearly independent (getting the reduced row echelon form of the matrix).
import sympy
reduced_form, inds = sympy.Matrix(t.values).rref()

I see similar issues where people have reported eigvalsh to return negative eigenvalue for a well behaved matrix but none of the suggestions have helped.
Examples:

Why is scipy's eigh returning unexpected negative eigenvalues?
numpy.cov or numpy.linalg.eigvals gives wrong results

I was wondering if anyone has faced a similar issue with np.linalg.eigvalsh and have any recommendations on how to solve it?
Thank you so much.
",2,295,"It's totally normal for symmetric matrices to have negative eigenvalues. A matrix being Hermitian only guarantees that its eigenvalues are real. It doesn't guarantee that the eigenvalues are positive.
It's not normal for a covariance matrix to have negative eigenvalues, meaning this isn't actually a covariance matrix, despite what you thought. Perhaps you rounded the entries of an actual covariance matrix?
",,
SciPy unexpected result,https://stackoverflow.com/questions/60172680,Scipy LDL decomposition returning unexpected result,"I have generated a random 5*5 matrix x like so:

&gt;&gt;&gt; x = np.random.randn(5,5)


And decomposed it using the scipy.linalg.ldl decomposition like so:

&gt;&gt;&gt; l, d, p = la.ldl(x)


Using l, d and p I want to return x. I thought I would be able to do the following:

&gt;&gt;&gt; l[p,:] @ d @ l[p,:].transpose() - x


But this does not give me zero as I would have expected. Can anyone explain where I am going wrong?

My aim is to obtain the lower diagonal matrix L such that x = LDL^T without the need for the row permutation matrix p, but I am very confused as to what scipy is giving as output.
",2,1211,"The LDL decomposition algorithm only applies for Hermitian/symmetric matrices. You are passing it a matrix with random values which is very unlikely to be symmetric. In addition, the matrix multiplication should be performed without applying the permutation matrix to the lower triangular matrices.

When a non-symmetric matrix is passed to scipy.linalg.ldl, only the lower or upper triangular part of the matrix is referenced, depending on the value of the lower keyword argument, which defaults to True. We can see the effects of this with np.isclose():

&gt;&gt;&gt; x = np.random.randn(5,5)
&gt;&gt;&gt; l, d, p = la.ldl(x)
&gt;&gt;&gt; np.isclose(l.dot(d).dot(l.T) - x, 0)
[[ True False False False False]
 [ True  True False False False]
 [ True  True  True False False]
 [ True  True  True  True False]
 [ True  True  True  True  True]]


Here we see that the upper triangular part of the matrix has been assumed to be symmetric, and so the algorithm has returned values that would be correct if that were the case.

Below, we pass la.ldl an actual symmetric matrix, and obtain the expected result.

&gt;&gt;&gt; x = np.array([[1, 2, 3],
                  [2, 4, 5],
                  [3, 5, 6]])
&gt;&gt;&gt; l, d, p = la.ldl(x)
&gt;&gt;&gt; print(np.isclose(l.dot(d).dot(l.T) - x, 0))
[[ True  True  True]
 [ True  True  True]
 [ True  True  True]]


If you're looking for a decomposition into LDL^T in general, without having permutations, this reduces the field of matrices even further. Your matrix also needs to be positive definite.

Here is an example with one such matrix:

&gt;&gt;&gt; x = np.array([[2, -1, 0],
                  [-1, 3, -1],
                  [0, -1, 4]])
&gt;&gt;&gt; l, d, p = la.ldl(x)
&gt;&gt;&gt; l
array([[ 1. ,  0. ,  0. ],
       [-0.5,  1. ,  0. ],
       [ 0. , -0.4,  1. ]])
&gt;&gt;&gt; d
array([[2. , 0. , 0. ],
       [0. , 2.5, 0. ],
       [0. , 0. , 3.6]])
&gt;&gt;&gt; p
array([0, 1, 2], dtype=int64)


As you can see, the permutation p is just [0, 1, 2], and l is already lower triangular.
",,
SciPy unexpected result,https://stackoverflow.com/questions/58212061,calculating euclidean distance using scipy giving unexpected results,"I want to calculate euclidean distance between X and Y, where X is not static. So, X is define by combination of range.:

a= np.arange(0.2,0.41,0.1)
b= np.arange(2,8,1)
c= np.arange(40,61,5)
d= np.arange(40,61,5)
e= np.arange(0,11,5)
f= np.arange(25,71,5)

from itertools import product
X = list(product(a,b,c,d,e,f))
X = np.around(X,2)

from scipy.spatial.distance import euclidean
dis = []
for x in np.nditer(X):
    d = euclidean(x,[0.2,2,40,40,0,25]) #Inset Y here
    dis.append(d)

min(dis)


Since in this case, to test if everything was working as I wanted, I input Y which is one of possible value of X. I expected my minimum distance to be 0, however in this case, it was not so (~47).
",2,65,"The problem is that you're iterating over every individual number in X, rather than just iterating over the rows. Just remove the call to np.nditer(X) and iterate over X directly

for x in np.nditer(X):


should just be 

for x in X:

",,
SciPy unexpected result,https://stackoverflow.com/questions/40050369,Different eigenvalues between scipy.sparse.linalg.eigs and numpy/scipy.eig,,2,3125,,,
SciPy unexpected result,https://stackoverflow.com/questions/32920920,Stepsize control of dopri5 integrator,,2,1979,,,
SciPy unexpected result,https://stackoverflow.com/questions/57477723,Unexpected behaviour of scipy.integrate,,1,1213,"It is not a bug, it has to do with the numerical precision of the integration, and the fact that you are integrating a function that is (almost) 0 in most of the interval.
From the docs:


  Be aware that pulse shapes and other sharp features as compared to the
  size of the integration interval may not be integrated correctly using
  this method.


Based on your output, the function is using only two (last=2) intervals, evaluating values for rlist=(2.06145321e-045, 0.00000000e+000, ..) on each (see the docs for more detail on the output)

You can add points to the interval to force the routine to use points closer to the left limit.

a = quad(lambda x: np.exp(-x), 0, 1e9, points=np.logspace(-10,3,10))
print(a)
(0.9999999999999997, 2.247900608926337e-09)


Adding to the explanation (thanks to @norok2): Note that points is a sequence of break points in the bounded integration interval where local difficulties of the integrand may occur (e.g., singularities, discontinuities). In this case, I'm not using it to point out discontinuities, but rather to force quad to perform more integration steps near the left boundary, using a log-spaced interval since a I have an exponential function (this is of course arbitrary and for this function, since I know its shape).
","There is no need to convert your integral into one over a (very large) interval. There is a specific integration scheme for integrals of the form

,

namely Gauss-Laguerre quadrature. It's also included in quadpy (a project of mine). Simply try out

import numpy
import quadpy

scheme = quadpy.e1r.gauss_laguerre(1)

val = scheme.integrate(lambda x: numpy.ones(x.shape[1:]))

print(val)


1.0

",
SciPy unexpected result,https://stackoverflow.com/questions/69916926,Unexpected error when trying to install conda environment from .yaml file,,1,915,"There is no problem in your environment, I managed to install it, getting
Successfully built MDAnalysis
Installing collected packages: msgpack, tqdm, mrcfile, mmtf-python, gsd, fasteners, biopython, GridDataFormats, torchani, MDAnalysis
Successfully installed GridDataFormats-1.0.1 MDAnalysis-2.6.0.dev0 biopython-1.81 fasteners-0.18 gsd-3.1.1 mmtf-python-1.1.3 mrcfile-1.4.3 msgpack-1.0.5 torchani-2.2.3 tqdm-4.66.1

done
#
# To activate this environment, use
#
#     $ conda activate ael

in the end.
The problem you encountering is probably related to https://github.com/conda/conda/issues/9590
which has a workaround
sudo rm -r ~/.condarc

or editing the ~/.condarc like https://github.com/conda/conda/issues/9590#issuecomment-1003211237
",,
SciPy unexpected result,https://stackoverflow.com/questions/29306538,Unexpected behavior in scipy isf,,1,234,,,
SciPy unexpected result,https://stackoverflow.com/questions/77945259,"slurm didn&#39;t execute my Python code after running a few lines but also didn&#39;t stop, whereas it worked well on my local Linux","my code:
from datasets import load_dataset
MAX_LEN = 512
dataset = load_dataset(""glue"",""mrpc"")
from transformers import AutoTokenizer
from transformers import RobertaTokenizerFast

#tokenizer =AutoTokenizer.from_pretrained(""bert-base-uncased"")
tokenizer = RobertaTokenizerFast.from_pretrained(""/data/home//raw_roberta/Roberta_Tokenizer"", max_length=MAX_LEN, padding='max_length', return_tensors='pt')

print(""mapped_dataset"")

mapped_dataset = dataset.map(lambda x: tokenizer(x[""sentence1""], x[""sentence2""], max_length = MAX_LEN, truncation=True, padding='max_length', return_tensors='pt'), batched=True)

print(""completeed mapped_dataset"")

from transformers import DataCollatorWithPadding
data_collator= DataCollatorWithPadding(tokenizer=tokenizer)

from transformers import AutoModelForSequenceClassification
from transformers import RobertaForMaskedLM

#model = AutoModelForSequenceClassification.from_pretrained(""bert-base-uncased"",num_labels = 2)
#model = AutoModelForSequenceClassification.from_pretrained(""data/home//raw_roberta/Roberta_Model/checkpoint-90000"", num_labels = 2)
#model = AutoModelForSequenceClassification.from_pretrained(""data/home//raw_roberta/Roberta_Model/checkpoint-90000"")
base_model = RobertaForMaskedLM.from_pretrained('/data/home//raw_roberta/Roberta_Model/checkpoint-90000').roberta

from transformers import TrainingArguments
print(base_model.config)

I run above code on my local linux,it only takes about 2 minutes to execute.the log:
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization.
The tokenizer class you load from this checkpoint is 'BertTokenizer'.
The class this function is called from is 'RobertaTokenizer'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization.
The tokenizer class you load from this checkpoint is 'BertTokenizer'.
The class this function is called from is 'RobertaTokenizerFast'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
mapped_dataset
completeed mapped_dataset
RobertaConfig {
  ""_name_or_path"": ""/data/home//raw_roberta/Roberta_Model/checkpoint-90000"",
  ""architectures"": [
    ""RobertaForMaskedLM""
  ],
  ""attention_probs_dropout_prob"": 0.1,
  ""bos_token_id"": 0,
  ""classifier_dropout"": null,
  ""eos_token_id"": 2,
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 768,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 3072,
  ""layer_norm_eps"": 1e-12,
  ""max_position_embeddings"": 514,
  ""model_type"": ""roberta"",
  ""num_attention_heads"": 12,
  ""num_hidden_layers"": 12,
  ""pad_token_id"": 1,
  ""position_embedding_type"": ""absolute"",
  ""torch_dtype"": ""float32"",
  ""transformers_version"": ""4.33.2"",
  ""type_vocab_size"": 1,
  ""use_cache"": true,
  ""vocab_size"": 52000
}


but when I upload my code to slurm, it run for 4 hours,only get these log:
/data/home//anaconda3/envs/py38v1/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.24.4)
  warnings.warn(f""A NumPy version &gt;={np_minversion} and &lt;{np_maxversion} is required for this version of ""
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'BertTokenizer'. 
The class this function is called from is 'RobertaTokenizer'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'BertTokenizer'. 
The class this function is called from is 'RobertaTokenizerFast'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
slurmstepd: error: *** JOB xxxxx ON compute-9-0 CANCELLED AT 2024-02-05T08:35:30 DUE TO TIME LIMIT ***

this problem truly confuses me,anyone know how to fix it? thanks!
",1,42,"check the python, scipy and numpy versions on the local workstation and the computing node(s) on the cluster. If you can create conda envs on the cluster, create a conda environment with just the python + relevant libs. First on the local workstation to check that it works, then replicate it on HPC.
",,
SciPy unexpected result,https://stackoverflow.com/questions/76667318,conda environment.yaml package conflict,"When I'm trying to build a git project, the environment,yml won't work properly. It seems like some package conflict, but other user doesn't seem to run into this issue, and I couldn't figure out a solution. My conda version is conda 4.8.2,and down below is the environment.yaml, requirment.txt and error
environment.yml:
name: oneposeplus
channels:
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - python=3.7
  - pytorch=1.8.0
  - torchvision=0.9.1
  - cudatoolkit=10.1
  - ipython
  - tqdm
  - matplotlib
  - pylint
  - conda-forge::jupyterlab
  - conda-forge::h5py=3.1.0
  - conda-forge::loguru=0.5.3
  - conda-forge::scipy
  - conda-forge::numba
  - conda-forge::ipdb
  - conda-forge::albumentations=0.5.1
  - pip
  - pip:
    - -r requirements.txt

requirment.txt:
pytorch-lightning==1.5.10
ray==1.13.0
aiohttp==3.7
aioredis==1.3.1
pydegensac==0.1.2
opencv_python==4.4.0.46
yacs&gt;=0.1.8
pytorch_memlab
joblib
pytorch3d
open3d
einops==0.3.0
kornia==0.4.1
autopep8
pickle5==0.0.11
timm&gt;=0.3.2
hydra-core
omegaconf
pycocotools
wandb
rich
transforms3d
natsort
plyfile
pycolmap==0.3.0

error:
rvl224@eervl224:~/OnePose_Plus_Plus-main$ conda env create -f environment.yaml
Collecting package metadata (repodata.json): done
Solving environment: \ 
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
Examining conflict for pylint numba python matplotlib jupyterlab ipython tqdm scExamining conflict for pylint numba python matplotlib torchvision jupyterlab ipyExamining conflict for pylint numba python matplotlib torchvision jupyterlab ipyExamining conflict for pylint jupyterlab ipython: : 20it [06:56, 20.25s/it]     Examining conflict for albumentations numba matplotlib torchvision h5py scipy pyExamining conflict for albumentations numba matplotlib torchvision h5py scipy pyExamining conflict for albumentations torchvision: : 24it [08:51,  8.89s/it]    Examining conflict for albumentations pytorch torchvision: : 25it [09:21, 37.46sExamining conflict for albumentations pytorch torchvision: : 26it [09:21, 35.40sExamining conflict for albumentations h5py: : 26it [09:35, 35.40s/it]           Examining conflict for albumentations matplotlib torchvision: : 27it [09:41, 28.Examining conflict for albumentations matplotlib torchvision: : 28it [09:41, 21.Examining conflict for albumentations matplotlib: : 28it [10:08, 21.91s/it]     Examining conflict for albumentations numba matplotlib torchvision scipy: : 29itExamining conflict for albumentations numba matplotlib torchvision scipy: : 30itExamining conflict for albumentations numba python matplotlib torchvision h5py cExamining conflict for albumentations numba python matplotlib torchvision h5py cExamining conflict for albumentations pytorch: : 31it [11:39, 25.74s/it]        failed                                                                  

# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

    Traceback (most recent call last):
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/exceptions.py"", line 1079, in __call__
        return func(*args, **kwargs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/cli/main.py"", line 80, in do_call
        exit_code = getattr(module, func_name)(args, parser)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/cli/main_create.py"", line 111, in execute
        result[installer_type] = installer.install(prefix, pkg_specs, args, env)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/installers/conda.py"", line 32, in install
        prune=getattr(args, 'prune', False), update_modifier=UpdateModifier.FREEZE_INSTALLED)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 117, in solve_for_transaction
        should_retry_solve)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 158, in solve_for_diff
        force_remove, should_retry_solve)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 281, in solve_final_state
        ssc = self._run_sat(ssc)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 808, in _run_sat
        should_retry_solve=ssc.should_retry_solve
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 1318, in solve
        self.find_conflicts(specs, specs_to_add, history_specs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 347, in find_conflicts
        bad_deps = self.build_conflict_map(specs, specs_to_add, history_specs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 507, in build_conflict_map
        root, search_node, dep_graph, num_occurances)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 369, in breadth_first_search_for_dep_graph
        last_spec = MatchSpec.union((path[-1], target_paths[-1][-1]))[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 481, in union
        return cls.merge(match_specs, union=True)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 475, in merge
        reduce(lambda x, y: x._merge(y, union), group) if len(group) &gt; 1 else group[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 475, in &lt;lambda&gt;
        reduce(lambda x, y: x._merge(y, union), group) if len(group) &gt; 1 else group[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 502, in _merge
        final = this_component.union(that_component)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 764, in union
        return '|'.join(options)
    TypeError: sequence item 0: expected str instance, Channel found

`$ /home/rvl224/anaconda3/bin/conda-env create -f environment.yaml`

  environment variables:
                 CIO_TEST=&lt;not set&gt;
        CMAKE_PREFIX_PATH=/opt/ros/noetic
  CONDA_AUTO_UPDATE_CONDA=false
                CONDA_EXE=/home/rvl224/anaconda3/bin/conda
         CONDA_PYTHON_EXE=/home/rvl224/anaconda3/bin/python
               CONDA_ROOT=/home/rvl224/anaconda3
              CONDA_SHLVL=0
            DEFAULTS_PATH=/usr/share/gconf/ubuntu.default.path
          LD_LIBRARY_PATH=/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-
                          gnu:/usr/local/cuda/lib64:
           MANDATORY_PATH=/usr/share/gconf/ubuntu.mandatory.path
                     PATH=/home/rvl224/anaconda3/bin:/home/rvl224/anaconda3/condabin:/opt/ros/no
                          etic/bin:/usr/local/cuda/bin:/home/rvl224/.local/bin:/home/rvl224/.loc
                          al/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/u
                          sr/games:/usr/local/games:/snap/bin
          PKG_CONFIG_PATH=/opt/ros/noetic/lib/pkgconfig:/opt/ros/noetic/lib/x86_64-linux-
                          gnu/pkgconfig
               PYTHONPATH=/opt/ros/noetic/lib/python3/dist-packages
       REQUESTS_CA_BUNDLE=&lt;not set&gt;
         ROS_PACKAGE_PATH=/opt/ros/noetic/share
            SSL_CERT_FILE=&lt;not set&gt;
               WINDOWPATH=2

     active environment : None
            shell level : 0
       user config file : /home/rvl224/.condarc
 populated config files : /home/rvl224/.condarc
          conda version : 4.8.2
    conda-build version : 3.18.11
         python version : 3.7.6.final.0
       virtual packages : __cuda=12.0
                          __glibc=2.31
       base environment : /home/rvl224/anaconda3  (writable)
           channel URLs : https://conda.anaconda.org/conda-forge/linux-64
                          https://conda.anaconda.org/conda-forge/noarch
                          https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /home/rvl224/anaconda3/pkgs
                          /home/rvl224/.conda/pkgs
       envs directories : /home/rvl224/anaconda3/envs
                          /home/rvl224/.conda/envs
               platform : linux-64
             user-agent : conda/4.8.2 requests/2.22.0 CPython/3.7.6 Linux/5.15.0-76-generic ubuntu/20.04.4 glibc/2.31
                UID:GID : 1000:1000
             netrc file : None
           offline mode : False


An unexpected error has occurred. Conda has prepared the above report.

If submitted, this report will be used by core maintainers to improve
future releases of conda.
Would you like conda to send this report to the core maintainers?

[y/N]: 
Timeout reached. No report sent.

down below are attempt updates to fix the error:
installing micromamba:
## install micromamba
curl micro.mamba.pm/install.sh | bash
export MAMBA_ROOT_PREFIX=/home/rvl224/anaconda3
micromamba update -n base conda

micromamba create -n oneposeplus -f environment.yaml

stuck at:
pkgs/main/linux-64                                            No change
pkgs/r/linux-64                                               No change
pkgs/r/noarch                                                 No change
pkgs/main/noarch                                              No change
pytorch/linux-64                                              No change
pytorch/noarch                                                No change
conda-forge/noarch                                  11.7MB @   2.3MB/s  5.2s
conda-forge/linux-64                                29.2MB @   4.4MB/s  6.9s

install mamba
#install mamba
micromamba install -c conda-forge mamba --root-prefix=/home/rvl224/anaconda3 -n base

mamba create -n oneposeplus -f environment.yaml

error:
Looking for: ['environment.yaml']

warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
pkgs/main/noarch                                   851.4kB @   1.3MB/s  0.7s
pkgs/r/linux-64                                      1.4MB @   1.6MB/s  0.9s
pkgs/r/noarch                                        1.3MB @   1.0MB/s  0.6s
pkgs/main/linux-64                                 @   2.5MB/s  2.4s
conda-forge/noarch                                 @   3.0MB/s  4.6s
conda-forge/linux-64                               @   4.8MB/s  7.4s
Encountered problems while solving:
  - nothing provides requested environment.yaml

tried installing packages one by one in a test environmemt,down below are some error while installing:
mamba install -n test_environment cudatoolkit=10.1 --yes

Encountered problems while solving:
  - nothing provides pytorch 1.10.2 cpu_py37h76afcab_0 needed by pytorch-cpu-1.10.2-cpu_py37h718b53a_0


mamba install -n test_environment ""conda-forge::h5py=3.1.0"" --yes

Encountered problems while solving:
  - package pulseaudio-daemon-16.1-ha8d29e2_3 requires openssl &gt;=3.1.0,&lt;4.0a0, but none of the providers can be installed

mamba install -n test_environment -c conda-forge -c pytorch -c defaults --file requirements.txt --yes


Encountered problems while solving:
  - nothing provides requested ray 1.13.0
  - nothing provides requested pydegensac 0.1.2
  - nothing provides requested opencv_python 4.4.0.46
  - nothing provides requested pytorch_memlab
  - nothing provides requested pytorch3d
  - nothing provides requested open3d
  - nothing provides requested kornia 0.4.1
  - package pycolmap-0.3.0-cpu_py39h5202583_1 requires python &gt;=3.9,&lt;3.10.0a0, but none of the providers can be installed

change into:
cudatoolkit=10.2
h5py (without version)

still figuring what to do with requirement.txt
",1,986,"Had to manually install every package,idk why environment.yml doesn't work
mamba create -n oneposeplus python=3.7 --yes
conda activate oneposeplus

pip install torch==1.8.0 --no-input
pip install torchvision==0.9.1 --no-input
pip install matplotlib --no-input
pip install ipython --no-input
pip install tqdm --no-input
pip install pylint --no-input
pip install jupyterlab --no-input
pip install ""h5py==3.1.0"" --no-input
pip install loguru==0.5.3 --no-input
pip install scipy --no-input
pip install numba --no-input
pip install ipdb --no-input
pip install ""albumentations==0.5.1"" --no-input

pip install pytorch-lightning==1.5.10 --no-input
pip install ray==1.13.0 --no-input
pip install aiohttp==3.7 --no-input
pip install aioredis==1.3.1 --no-input
pip install pydegensac==0.1.2 --no-input
pip install opencv-python==4.4.0.46 --no-input
pip install ""yacs&gt;=0.1.8"" --no-input
pip install pytorch_memlab --no-input
pip install joblib --no-input
pip install pytorch3d --no-input
pip install open3d --no-input
pip install ""einops==0.3.0"" --no-input
pip install ""kornia==0.4.1"" --no-input
pip install autopep8 --no-input
pip install ""pickle5==0.0.11"" --no-input
pip install ""timm&gt;=0.3.2"" --no-input
pip install hydra-core --no-input
pip install omegaconf --no-input
pip install pycocotools --no-input
pip install wandb --no-input
pip install rich --no-input
pip install transforms3d --no-input
pip install natsort --no-input
pip install plyfile --no-input
pip install ""pycolmap==0.3.0"" --no-input

Thanks for @merv for helping.
",,
SciPy unexpected result,https://stackoverflow.com/questions/68080031,numpy.any(axis=i) for scipy.sparse,"import numpy
a = numpy.array([
    [0, 1, 0, 0],
    [1, 0, 0, 0],
    [0, 0, 1, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0],
])
numpy.any(a, axis=0)
numpy.any(a, axis=1)

produces
array([ True,  True,  True, False])
array([ True,  True,  True, False, False])

However, after
from scipy import sparse
a = sparse.csr_matrix(a)

the same numpy.any(a, axis) calls produces
&lt;5x4 sparse matrix of type '&lt;class 'numpy.intc'&gt;'
        with 3 stored elements in Compressed Sparse Row format&gt;

and
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""&lt;__array_function__ internals&gt;"", line 5, in any
  File ""C:\Users\user\.conda\envs\py385\lib\site-packages\numpy\core\fromnumeric.py"", line 2330, in any
    return _wrapreduction(a, np.logical_or, 'any', axis, None, out, keepdims=keepdims)
  File ""C:\Users\user\.conda\envs\py385\lib\site-packages\numpy\core\fromnumeric.py"", line 87, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
numpy.AxisError: axis 1 is out of bounds for array of dimension 0

Of course, a is actually so large a sparse matrix that converting to normal numpy array is not an option.  How can I acquire the same (or equivalent) result for a csr_matrix and other scipy.sparse matrices?
ADDED:
According to Usage information in official scipy documentation,

Despite their similarity to NumPy arrays, it is strongly discouraged to use NumPy functions directly on these matrices because NumPy may not properly convert them for computations, leading to unexpected (and incorrect) results. If you do want to apply a NumPy function to these matrices, first check if SciPy has its own implementation for the given sparse matrix class, or convert the sparse matrix to a NumPy array (e.g., using the toarray() method of the class) first before applying the method.

I'm looking for ""its own implementation"" or equivalent.
",1,543,"you can use sum instead of any on bool arrays
import numpy
a = numpy.array([
    [0, 1, 0, 0],
    [1, 0, 0, 0],
    [0, 0, 1, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0],
])

from scipy import sparse
a = sparse.csr_matrix(a.astype(bool))
# Use sum instead of any on a bool array
print(a.sum(axis=0).astype(bool))
print(a.sum(axis=1).flatten().astype(bool))

output:
[[ True  True  True False]]
[[ True  True  True False False]]

If you want to do 'all' that would be a little tricky since scipy doesn't appear to have an implementation for 'prod'.
But this post has an answer for that case.
",,
SciPy unexpected result,https://stackoverflow.com/questions/66640632,Python Kolmogorov-Smirnov (KS) Test Inconsistent Results,"I am trying to compute the KS test specifying the CDF as a array, however, I encountered unexpected results. Upon further evaluation, I found different results based on whether I specified the CDF as a callable, string or array. My code is as follows:
import scipy.stats as st

random_variables = st.norm.rvs(loc=1, scale=1,size=1000000)
cdf_data = st.norm.cdf(random_variables, loc=1,scale=1)
params = st.norm.fit(data=random_variables)
display(params)
print('\n')

#test 1
out = kstest(rvs=random_variables,cdf='norm',args=params)
display(out, out[0], out[1])
print('\n')

#test 2
out = kstest(rvs=random_variables,cdf=st.norm.cdf,args=params)
display(out, out[0], out[1])
print('\n')

#test 3
out = kstest(rvs=random_variables,cdf=cdf_data)
display(out, out[0], out[1])

The results from this code are:
(1.0004825310590526, 0.9996641807017618)


KstestResult(statistic=0.0007348981302804924, pvalue=0.6523439724424506)
0.0007348981302804924
0.6523439724424506


KstestResult(statistic=0.0007348981302804924, pvalue=0.6523439724424506)
0.0007348981302804924
0.6523439724424506


KstestResult(statistic=0.500165, pvalue=0.0)
0.500165
0.0

Given the large sample data is compared against its the exact distribution from which the sample was generated, I expect a failure to reject the null hypothesis. This is the case in test 1 and 2, but it is not the case in test 3. I want to be able to replicate this test using an array argument for the ""cdf"" argument. Any help as to what I am doing wrong for test 3 would be very helpful. My numpy is version 1.19.2 and scipy is 1.5.2. Thank you!
",1,667,"I think there are two things that may be contributing to your confusion.

I don't think you want to be comparing to cdf_data = st.norm.cdf(random_variables, loc=1,scale=1). This is returning the value of the cumulative distribution function at all the x values of random-variables. In a KS test you are comparing two distributions, and your cdf_data and random_variable are two very different distributions, so you would expect to get a p-value of 0. I suggest you replace cdf_data with something like random_variable_2 = st.norm.rvs(loc=1,scale=1, size=size)
Additionally you are performing two different KS tests between your first two (one sample) and third (two sample) test. In the first two you compare your data to a fixed functional form to check if the data is consistent with that functional distribution. Since you have the same data and distribution between cases one and two, you would expect the output to be the same. However in case three you are testing two independent distributions to see if they are consistent with each other. Since the cdf_data should just be other normally distributed data points, you should find that the two distributions are consistent, but it should not necessarily give you the exact same answer as the previous two cases, just KS test statistic and p-value that suggests the two data sets come from the same underlying distribution.

",,
SciPy unexpected result,https://stackoverflow.com/questions/65010384,Differing results in Scipy vs Matlab transfer functions,"I can't understand why the scipy.signal.ss2tf() and scipy.signal.StateSpace().to_tf() give (the same) unexpected result.
Example:
A=[[0, 1, 0], [0, 0, 1], [-3, -4, -2]]
B=[[0], [0], [1]]
C=[[5, 1, 0]]
D=[[0]]

The result for scipy is
num = array([[0, 0, 0, 4]]),
den = array([1., 2., 4., 3.])

in Matlab the result is
num = [0,0,1,5],
den = [1,2,4,3]

It seems that the denominator is always right, I tried other examples, but the numerator of the transfer function doesn't correspond.
Am I using scipy in an incorrect way?
(another example)
A=[[0, 1, 0], [0, 0, 1], [-8, -14, -7]]
B=[[0], [0], [1]]
C=[[15, 5, 0]]
D=[[0]]

",1,311,"This is a bug in SciPy that was fixed in SciPy 1.6.0.  When ss2tf creates an array to hold the numerator, it uses the data types of the inputs to determine the data type of the numerator array.  In your case, the values are all integers, so the data type of the numerator is integer.  However, the coefficients for the numerator are the result of a floating point calculation, and therefore are subject to loss of precision.  When the computed values are copied into the numerator array, those values are truncated to integers.  In your example, that results in a large error.  When I reproduce the floating point calculation of the numerator, I get [0.0, 0.0, 0.9999999999999947, 4.999999999999995], and when those values are copied into the integer array, the result is [0, 0, 0, 4].
A work-around is to ensure that your inputs (or at least A) contain floating point values.  For example,
In [33]: A = [[0., 1., 0.], [0., 0., 1.], [-8., -14., -7.]] 
    ...: B = [[0], [0], [1]] 
    ...: C = [[15, 5, 0]] 
    ...: D = [[0]]                                                              

In [34]: num, den = ss2tf(A, B, C, D)                                           

In [35]: num                                                                    
Out[35]: array([[0.00000000e+00, 1.77635684e-15, 5.00000000e+00, 1.50000000e+01]])

",,
SciPy unexpected result,https://stackoverflow.com/questions/54766766,OpenCV filter2d gives incorrect result,"I am currently trying to filter an image with a Laplacian kernel that I have constructed myself. However, when filtering an input image with this kernel it gives an unexpected result compared to the implementation in SciPy.

The Laplacian kernel I have constructed should be verified by the following images

 


The code for filtering the image:

im = cv2.imread(""test.png"",0)
im = im.astype(np.float32)

def lkern(t=1.):
    ax = np.arange(np.round(-5*np.sqrt(t),0),np.round(5*np.sqrt(t),0)+1)
    xx, yy = np.meshgrid(ax, ax)

    kernel = -1/(np.sqrt(2*np.pi*t)*t)*np.exp(-(xx**2+yy**2)/(2*t))+
        (xx**2+yy**2)/(np.sqrt(2*np.pi*t)*t**2)*np.exp(-(xx**2+yy**2)/(2*t))


    return kernel.astype(np.float)

t = 25**2/2
l = lkern(t)

L = cv2.filter2D(im/255,-1,l)

plt.figure()
plt.imshow(L,cmap=""gray"")
plt.show()


which results in



Comparing to SciPy's ndimage.gaussian_laplace, the result should have been



which is very different, and I cannot figure out how to do this properly.
",1,1299,"The code in the OP seems to take the equation for a 1D Laplace of Gaussian and use that to construct a 2D radially symmetric function. That is, along any of the diameters of the kernel the function looks like a 1D Laplace of Gaussian. This is not the correct way to create a 2D Laplace of Gaussian.

The Laplace of Gaussian is defined as the sum of the second order derivative of the Gaussian kernel along each of the axes. That is,

LoG = d/dx G + d/dy G


With G the Gaussian kernel.

With Numpy, you can construct this kernel as follows. I'm using the separability of the Gaussian to reduce the computational complexity.

s = 5;
x = np.arange(np.floor(-4*s),np.ceil(4*s)+1)
g = 1/(np.sqrt(2*np.pi)*s)*np.exp(-x**2/(2*s**2))
d2g = (x**2 - s**2)/(s**4) * g
log = g * d2g[:,None] + g[:,None] * d2g


Tricks here: g and d2g are 1D functions. g[:,None] turns the 1D function on its side, so that the multiplication causes broadcasting, leading to a 2D output.

I have written the kernel this way, rather than expressing the full 2D equation in one go, because this leads to large efficiencies in your code: The convolution of an image f with the kernel log can be written as:

conv(f, log) = conv(f, g * d2g[:,None] + g[:,None] * d2g)
             = conv(conv(f, g), d2g[:,None]) + conv(conv(f, g[:,None]), d2g)


That is, instead of one convolution with a large 2D kernel, we compute 4 convolutions with relatively small 1D kernels. Note that the actual order here does not matter:


One applies a 1D kernel g and on the result a 1D kernel d2g along the other axis. These two operations can be reversed.
Then one repeats this process changing the axes along which each of the operations is applied.
Finally one adds the two results.


(It is OK to use cv2.filter2D where I wrote conv. conv just indicates any convolution function, but a correlation function like filter2D is fine because the kernels are all symmetric.)
",,
SciPy unexpected result,https://stackoverflow.com/questions/31040188,Dot product between 1D numpy array and scipy sparse matrix,,1,4408,,,
SciPy unexpected result,https://stackoverflow.com/questions/77619352,Voronoi diagram gives unexpected results in scipy,"I have the following pandas dataframe:
import pandas as pd
pd.DataFrame({'cl': {0: 'A', 1: 'C', 2: 'H', 3: 'M', 4: 'S'},
 'd': {0: 245.059986986012,
  1: 320.49044143557785,
  2: 239.79023081978914,
  3: 263.38325791238833,
  4: 219.53334398353175},
 'p': {0: 10.971011721360075,
  1: 10.970258360366753,
  2: 13.108487516946218,
  3: 12.93241352743668,
  4: 13.346107628161008}})

    cl  d           p
0   A   245.059987  10.971012
1   C   320.490441  10.970258
2   H   239.790231  13.108488
3   M   263.383258  12.932414
4   S   219.533344  13.346108

I want to create a Voronoi diagram. To do so I am using the package from scipy.
I am using the following code:
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial import Voronoi, voronoi_plot_2d

centers2 = np.array(
    centers_dt[['d', 'p']]
)

scatter_x = np.array(centers_dt['d'])
scatter_y = np.array(centers_dt['p'])
group = np.array(centers_dt['cl'])
cdict = {'C': 'red', 'A': 'blue', 'H': 'green', 'M': 'yellow', 'S': 'black'}

fig, ax = plt.subplots()
for g in np.unique(group):
    ix = np.where(group == g)
    ax.scatter(scatter_x[ix], scatter_y[ix], c = cdict[g], label = g, s = 100)
ax.legend()

vor = Voronoi(centers2)
fig = voronoi_plot_2d(vor,plt.gca())

plt.show()
plt.close()

But the result I am getting is unexpected:

Since there is a boarder missing plus the boarders seem a bit off.
Any ideas ?
",0,69,"The code in voronoi_plot_2d draws segments with finite length, even for borders that should extend to infinity.  It is possible that with the default axis limits chosen by voronoi_plot_2d, a border segment doesn't appear in the plot, because the finite length chosen by vorono_plot_2d is too small for it to extend into the displayed window.
Here's the plot that I get from your code if I add plt.ylim(-400, 400).

Note that the dashed line extending upwards from the lowest intersection point should continue upwards to infinity, but the code draws only a finite segment.
Also note that the axis scales are not equal, so the dividing lines between points appear to be not perpendicular to the lines that would connect the points.  Here's the plot with equal axis scales:

",,
SciPy unexpected result,https://stackoverflow.com/questions/76564629,Cannot import protobuf builder when deploying Azure function,"The Problem
I have an Azure function built using the Python v1 programming model which builds a basic Keras model and trains it using some data from an Azure file share.
Tested locally and everything works.
I'm deploying to Azure using Azure Pipelines. It's invoking correctly, but then failing. Checking the detailed invocation history reveals the following error:
Result: Failure Exception: ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/internal/__init__.py). 
Please check the requirements.txt file for the missing module. For more info, please refer the troubleshooting guide: https://aka.ms/functions-modulenotfound
Stack: File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/dispatcher.py"", line 380, in _handle__function_load_request func = loader.load_function( 
File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/utils/wrappers.py"", line 48, in call raise extend_exception_message(e, message) File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/utils/wrappers.py"", line 44, in call return func(*args, **kwargs)
File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/loader.py"", line 132, in load_function mod = importlib.import_module(fullmodname)
File ""/usr/local/lib/python3.9/importlib/__init__.py"", line 127, in import_module return _bootstrap._gcd_import(name[level:], package, level) File ""&lt;frozen importlib._bootstrap&gt;"", line 1030, in _gcd_import
File ""&lt;frozen importlib._bootstrap&gt;"", line 1007, in _find_and_load
File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _find_and_load_unlocked 
File ""&lt;frozen importlib._bootstrap&gt;"", line 680, in _load_unlocked
File ""&lt;frozen importlib._bootstrap_external&gt;"", line 850, in exec_module
File ""&lt;frozen importlib._bootstrap&gt;"", line 228, in _call_with_frames_removed 
File ""/home/site/wwwroot/func-gc-imgorientation-train/__init__.py"", line 20, in &lt;module&gt; from core.data.training_image_filestore import TrainingImageFilestore
File ""/home/site/wwwroot/core/data/training_image_filestore.py"", line 4, in &lt;module&gt; import tensorflow as tf
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/__init__.py"", line 37, in &lt;module&gt; from tensorflow.python.tools import module_util as _module_util
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/python/__init__.py"", line 37, in &lt;module&gt; from tensorflow.python.eager import context
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/python/eager/context.py"", line 28, in &lt;module&gt; from tensorflow.core.framework import function_pb2
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/core/framework/function_pb2.py"", line 5, in &lt;module&gt; from google.protobuf.internal import builder as _builder

Well Documented Error with a Solution that Works Outside of Azure
This error is well documented and understood as in the following version:
ImportError: cannot import name 'builder' from 'google.protobuf.internal'
In general, arising from API changes in v3.20:
https://stackoverflow.com/a/71984564/1928761
TF adopted these changes, but there were bugs when using TF with protobuf &gt;= v3.20 which were resolved in the most recent Tensorflow release so protobuf 4.23.3 should work with TF 2.12.0 as here:
https://github.com/tensorflow/tensorflow/issues/59221
Problem is resolved locally
I resolved this issue locally and have tested my function. All works fine. The problem only occurs in Azure.
Suspected Root Cause in Azure
Looking through the error above I noticed that all my custom modules, and Tensorflow, are installed to the same root: /home/site/wwwroot/
For instance, my custom core.data module is installed at /home/site/wwwroot/core/data/
Tensorflow is installed at /home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/
I've confirmed that all modules in my requirements.txt are being installed as expected in /home/site/wwwroot/.python_packages/lib/site-packages/ - including the latest protobuf with the builder module as expected.
However, protobuf is being imported from /azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/internal/init.py
Presumably the implication is that TF is using the version of protobuf that's bundled with Azure's python distribution on the host, rather than the version in my site packages.
To test this theory I added the following code to the top of my init.py for the function:
import sys
print(sys.path)
import google.protobuf
print(google.protobuf.__version__)
print(google.protobuf.__path__)

This confirmed that protobuf version 3.19 was being loaded from the following path and not from site packages: /azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/
The Question
The question therefore is can I upgrade the version of protobuf that's included in this python bundle?
Alternatively, can I force TF to use the version of protobuf in my site packages rather than the ones in the Azure python bundle?
Attempted Solutions
ADO is building and installing the correct versions of both protobuf and TF, and the builder file is definitely in the installed site-packages. I've confirmed this by downloading the package from my storage account and unzipping it.
To resolve the issue, I've tried the following:

Confirmed that I can import other modules and that this is specific to importing the protobuf builder
Confirmed that I can import google.protobuf.internal.
Tried clearing the protobuf pycache by adding an rm -rvf command to my bash script immediately after the pip install --target....
Using subprocess to run pip install --upgrade protobuf==4.23.3 from within my function and before importing TF (which runs without error, but does not do the job).

At this point, I'm all out of ideas.
requirements.txt
# DO NOT include azure-functions-worker in this file
# The Python Worker is managed by Azure Functions platform
# Manually managing azure-functions-worker may cause unexpected issues

# Protobuf comes first to force the very latest version
protobuf==4.23.3

# Tensorflow, Dotenv, Pillow
absl-py==1.4.0
astunparse==1.6.3
cachetools==5.3.1
certifi==2023.5.7
charset-normalizer==3.1.0
contourpy==1.1.0
cycler==0.11.0
flatbuffers==23.5.26
fonttools==4.40.0
gast==0.4.0
google-auth==2.20.0
google-auth-oauthlib==1.0.0
google-pasta==0.2.0
graphviz==0.20.1
grpcio==1.54.2
h5py==3.8.0
idna==3.4
jax==0.4.12
keras==2.12.0
kiwisolver==1.4.4
libclang==16.0.0
Markdown==3.4.3
MarkupSafe==2.1.3
matplotlib==3.7.1
ml-dtypes==0.2.0
numpy==1.23.5
oauthlib==3.2.2
opt-einsum==3.3.0
packaging==23.1
Pillow==9.5.0
pyasn1==0.5.0
pyasn1-modules==0.3.0
pydot==1.4.2
pyparsing==3.0.9
python-dateutil==2.8.2
python-dotenv
requests==2.31.0
requests-oauthlib==1.3.1
rsa==4.9
scipy==1.10.1
six==1.16.0
tensorboard==2.12.3
tensorboard-data-server==0.7.1
tensorflow==2.12.0
tensorflow-estimator==2.12.0
termcolor==2.3.0
typing_extensions==4.6.3
urllib3==1.26.16
Werkzeug==2.3.6
wrapt==1.14.1

# Azure libraries
azure-functions
azure-identity
azure-keyvault

Deployment pipeline YAML
# Python Function App to Linux on Azure
# Build a Python function app and deploy it to Azure as a Linux function app.
# Add steps that analyze code, save build artifacts, deploy, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/python

trigger:
- main

variables:
  # Azure Resource Manager connection created during pipeline creation
  azureSubscription: '4ae24131-0b22-421c-8e3e-6d766e891ece'

  # Function app name
  functionAppName: 'func-xxxx-dev'

  # Agent VM image name
  vmImageName: 'ubuntu-latest'

  # Working Directory
  workingDirectory: '$(System.DefaultWorkingDirectory)'

stages:
- stage: Build
  displayName: Build stage

  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName)

    steps:
    - bash: |
        if [ -f extensions.csproj ]
        then
            dotnet build extensions.csproj --runtime ubuntu.16.04-x64 --output ./bin
        fi
      workingDirectory: $(workingDirectory)
      displayName: 'Build extensions'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.9'
      inputs:
        versionSpec: 3.9 # Functions V2 supports Python 3.6 as of today

    - bash: |
        python -m pip install --upgrade pip
        pip install --target=""./.python_packages/lib/site-packages"" -r ./requirements.txt
      workingDirectory: $(workingDirectory)
      displayName: 'Install application dependencies'

    - task: ArchiveFiles@2
      displayName: 'Archive files'
      inputs:
        rootFolderOrFile: '$(workingDirectory)'
        includeRootFolder: false
        archiveType: zip
        archiveFile: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
        replaceExistingArchive: true

    - publish: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
      artifact: drop

- stage: Deploy
  displayName: Deploy stage
  dependsOn: Build
  condition: succeeded()

  jobs:
  - deployment: Deploy
    displayName: Deploy
    environment: 'development'
    pool:
      vmImage: $(vmImageName)

    strategy:
      runOnce:
        deploy:

          steps:
          - task: AzureFunctionApp@1
            displayName: 'Azure functions app deploy'
            inputs:
              azureSubscription: '$(azureSubscription)'
              appType: functionAppLinux
              appName: $(functionAppName)
              package: '$(Pipeline.Workspace)/drop/$(Build.BuildId).zip'

Log stream
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778401,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778404,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778801,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:51:00Z   [Information]   Executing 'Functions.func-xxxx-train' (Reason='Timer fired at 2023-06-27T11:51:00.0016150+00:00', Id=3795600b-379e-423c-b29f-65fec390289a)
2023-06-27T11:51:00Z   [Verbose]   Sending invocation id: '3795600b-379e-423c-b29f-65fec390289a
2023-06-27T11:51:00Z   [Verbose]   Posting invocation id:3795600b-379e-423c-b29f-65fec390289a on workerId:250ae2e1-4416-41ef-b355-0684a59d0a91
2023-06-27T11:51:00Z   [Error]   Executed 'Functions.func-xxxx-train' (Failed, Id=3795600b-379e-423c-b29f-65fec390289a, Duration=2ms)
2023-06-27T11:51:00Z   [Verbose]   Function 'func-xxx-train' updated status: Last='2023-06-27T11:51:00.0015724+00:00', Next='2023-06-27T11:52:00.0000000+00:00', LastUpdated='2023-06-27T11:51:00.0015724+00:00'
2023-06-27T11:51:00Z   [Verbose]   Timer for 'func-gc-imgorientation-train' started with interval '00:00:59.9674482'.
2023-06-27T11:51:05Z   [Information]   Host Status: {
  ""id"": ""func-gc-imgorientation-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2800545,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}

",0,1441,"I had a similar issue and found a different solution. There is an app setting that isolates the application's dependencies from the azure function runtime dependencies:
https://learn.microsoft.com/en-us/azure/azure-functions/functions-app-settings#python_isolate_worker_dependencies
Adding the PYTHON_ISOLATE_WORKER_DEPENDENCIES setting with a value of 1 to my function in azure fixed the issue for me.
","The root cause of the problem, as highlighted by @SiddheshDesai, turned out to be that the Azure Functions host was loading a version of protobuf (3.19.6) into the cache which was earlier than the version that the latest Tensorflow needed (4.23.3).
Even when placing my protobuf/TF imports at the top of my function app module, the same error arose.
I was, however, able to reload protobuf from my site-packages using importlib.
This cause was proven by adding the following code to my function:
import logging
import importlib
import google.protobuf

logging.info(google.protobuf.__version__)
logging.info(google.protobuf.__file__)

importlib.reload(google.protobuf)

logging.info(google.protobuf.__version__)
logging.info(google.protobuf.__file__)

However, the reload then interfered with the Azure Functions library and host causing a heap of other exceptions.
My conclusion therefore is that this is an issue with Azure Functions rather than an issue with my code.
Rather than trying to work around these issues, I've now containerised my function and re-deployed. This has worked as expected.
The answer, therefore, turns out to be - as SiddheshDesai suggested above - that any application needing to use versions of protobuf &gt;= 3.20.0 need to be containerised rather than deployed as code/run from packages.
","If you want to use Protobuf module in Azure Functions, You need to downgrade it to 3.20.* and add it in your requirements.txt.
I added protobuf==3.20.* in my requirements.txt and the Http Trigger got deployed successfully in Azure Functions via DevOps YAML pipeline, Refer below:-
My requirements.txt:-
azure-functions
protobuf==3.20.*

My init.py:-
import logging

import azure.functions as func


def main(req: func.HttpRequest) -&gt; func.HttpResponse:
    logging.info('Python HTTP trigger function processed a request.')

    name = req.params.get('name')
    if not name:
        try:
            req_body = req.get_json()
        except ValueError:
            pass
        else:
            name = req_body.get('name')

    if name:
        return func.HttpResponse(f""Hello, {name}. This HTTP triggered function executed successfully."")
    else:
        return func.HttpResponse(
             ""This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response."",
             status_code=200
        )


My YAML pipeline:-
trigger:
- master

variables:
  
  azureSubscription: 'xxxxxxxx-xxxxx-xxx9bbd4354dd'

 
  functionAppName: 'valleyfunc541'

 
  vmImageName: 'ubuntu-latest'

 
  workingDirectory: '$(System.DefaultWorkingDirectory)'

stages:
- stage: Build
  displayName: Build stage

  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName)

    steps:
    - bash: |
        if [ -f extensions.csproj ]
        then
            dotnet build extensions.csproj --runtime ubuntu.16.04-x64 --output ./bin
        fi
      workingDirectory: $(workingDirectory)
      displayName: 'Build extensions'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.10'
      inputs:
        versionSpec: 3.10 # Functions V2 supports Python 3.6 as of today

    - bash: |
        pip install --target=""./.python_packages/lib/site-packages"" -r ./requirements.txt
      workingDirectory: $(workingDirectory)
      displayName: 'Install application dependencies'

    - task: ArchiveFiles@2
      displayName: 'Archive files'
      inputs:
        rootFolderOrFile: '$(workingDirectory)'
        includeRootFolder: false
        archiveType: zip
        archiveFile: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
        replaceExistingArchive: true

    - publish: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
      artifact: drop

- stage: Deploy
  displayName: Deploy stage
  dependsOn: Build
  condition: succeeded()

  jobs:
  - deployment: Deploy
    displayName: Deploy
    environment: 'development'
    pool:
      vmImage: $(vmImageName)

    strategy:
      runOnce:
        deploy:

          steps:
          - task: AzureFunctionApp@1
            displayName: 'Azure functions app deploy'
            inputs:
              azureSubscription: '$(azureSubscription)'
              appType: functionAppLinux
              appName: $(functionAppName)
              package: '$(Pipeline.Workspace)/drop/$(Build.BuildId).zip'

Output:-

The HTTP Trigger got deployed successfully:-


Reference:- My SO thread answer
"
SciPy unexpected result,https://stackoverflow.com/questions/75879324,How to use kwargs in scipy.optimize.curve_fit() to pass a parameter that isn&#39;t being fitted,"The problem originates from fitting a diffraction pattern to data, where the number of slits is known beforehand. I have given a simplified version below that highlights the same issue. The function should fit the values of a and b in the data while passing n to the function. I could use a global n which would solve my issues, however, I would like to do this using **kwargs as shown in the scipy.optimize.curve_fit() reference.
Here is an example of the issue. The code generates the curve of 4sin(2x)+3cos(2x) with some noise as the data:

import numpy as np
import scipy
import matplotlib.pyplot as plt

def curve(x,a,b,**kwargs):
    n = kwargs[""n""]
    return a*np.sin(n*x)+b*np.cos(n*x)

x = np.linspace(-5,5,1000)
y = np.random.normal(loc=curve(x, 4, 3, n=2), scale=0.2, size=None)
result = scipy.optimize.curve_fit(curve, x, y, n = 2)
y2 = curve(x, *result[0], n=2)

plt.plot(x, y2)
plt.plot(x,y)
plt.show()


This returns the error
  File ""C:\Users\HP\OneDrive\Documents\Uni\lab year 2\diffraction\kwargs.py"", line 13, in &lt;module&gt;
    result = scipy.optimize.curve_fit(curve, x, y, n = 2)

  File ""C:\Users\HP\anaconda3\lib\site-packages\scipy\optimize\_minpack_py.py"", line 834, in curve_fit
    res = leastsq(func, p0, Dfun=jac, full_output=1, **kwargs)

TypeError: leastsq() got an unexpected keyword argument 'n'

",0,333,"You might consider using lmfit for this (main author here). Your example would look like:
import numpy as np
import matplotlib.pyplot as plt
from lmfit import Model

def curve(x, a, b, n=2):
    return a*np.sin(n*x)+b*np.cos(n*x)

x = np.linspace(-5,5,1000)
y = np.random.normal(loc=curve(x, 4, 3, n=2), scale=0.2, size=None)

# turn your curve function into an lmfit Model:
model = Model(curve)

# create parameters for the fit, giving decent initial values
# note that n will use the default function value by default
params = model.make_params(a=3.2, b=2.5)

# fix n so that it is not varied in the fit:
params['n'].vary = False

# do the fit
result = model.fit(y, params, x=x)

# print out fit report
print(result.fit_report())

# plot data and best fit
plt.plot(x, y, label='data')
plt.plot(x, result.best_fit, label='fit')
plt.legend()
plt.show()


which will print a report of
[[Model]]
    Model(curve)
[[Fit Statistics]]
    # fitting method   = leastsq
    # function evals   = 7
    # data points      = 1000
    # variables        = 2
    chi-square         = 39.7903631
    reduced chi-square = 0.03987010
    Akaike info crit   = -3220.13053
    Bayesian info crit = -3210.31502
    R-squared          = 0.99678449
[[Variables]]
    a:  4.00080256 +/- 0.00914251 (0.23%) (init = 3.2)
    b:  3.00787199 +/- 0.00873116 (0.29%) (init = 2.5)
    n:  2 (fixed)

and a plot of

","I had a similar issue with curve_fit not accepting the args argument and after following the suggestion
here:, managed to get something working with a curried function
import numpy as np
import scipy
import matplotlib.pyplot as plt

def curve_curry(n):
    def curve(x, a, b):
        return a * np.sin(n * x) + b * np.cos(n * x)
    
    return curve

n = 2
x = np.linspace(-5, 5, 1000)
y = np.random.normal(loc=curve_curry(n)(x, 4, 3), scale=0.2, size=None)
result = scipy.optimize.curve_fit(curve_curry(n=2), x, y)
y2 = curve_curry(n)(x, *result[0])

plt.plot(x, y, '.', color='lightgrey')
plt.plot(x, y2)
plt.show()

In my case, I had fit parameters [p0, p1, p2], constants c1, c2, c3  and pre-defined functions f1, f2.  My code looked something like this:
def func_curry(c1, c2, c3, f1, f2):
    def func(x, *p):
        
        yinv = f1(c1) + x * c2 * (c3 + p[1] + p[2] * x ) / p[0]
        y = f2(yinv)
        
        return y
    
    return func 


# Curve fit using scipy

args = (c1, c2, c3, f1, f2)
p0 = [p0, p1, p2]
popt, pcov = scipy.optimize.curve_fit(func_curry(*args), xdata, ydata, p0=p0)

",
SciPy unexpected result,https://stackoverflow.com/questions/71634466,Curve fitting using Scipy leads to unexpected result,"I have a data set which I can fit to a given function. I did it using Desmos graphing calculator and got the expected results. But when I implement it using Matplotlib, I get a totally different curve and I can't figure out the error in my code. Here is my code:
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

data = np.loadtxt('data.txt')


def func(x, a, b):
    return a/(np.sin((x-b)/2))**4

x = data[:, 0]
y = data[:, 1]

popt, pcov = curve_fit(func, x, y)

plt.plot(x, y, 'ko', label=""Observed data points"")


plt.plot(x, func(x, *popt), 'r-', label=""Fitted Curve"")


plt.legend()
plt.show()

Here is the expected plot (plotted using Desmos): 
And here is what I got from Matplotlib: 
Any help would be appreciated. This is the data.txt for reference:




X
Y




-0.523598776
0.530580093


-0.436332313
1.016423844


-0.34906585
6.38245854


-0.261799388
18.70139225


-0.174532925
30.7389007


0.174532925
13.59465343


0.261799388
4.011313119


0.34906585
0.805865977


0.436332313
0.50894953


0.523598776
0.253654518



",0,127,"You have a singularity at x==b (your function tends to infinity). That makes it very difficult for curve_fit to find a value of b that ""crosses"" any of your data points. Since your data is about zero centered, you'll have better luck providing a an initial value such as
popt, pcov = curve_fit(func, x, y, [0.0, 0.0])

",,
SciPy unexpected result,https://stackoverflow.com/questions/67284779,Scipy rv_continuous fit does not check input data bounds,"I am fitting synthetic data to various distributions in scipy, however, I am observing some unexpected results. My data contains negative numbers and I do not get an error when I fit this data to distributions with non negative supports when fixing location and scale. My code is as follows:
import scipy.stats as st
import numpy as np
import pandas as pd

np.random.seed(7)
test_data = pd.Series(0.5 + 0.1*np.sin(np.linspace(0, 3*np.pi, 100)) + 0.5*np.random.normal(0,1,size=100))
print(np.min(test_data))

Which returns:
-0.5900934692403015

Confirming I have generated negative observations. When I fit scipy lognorm, which has a non inclusive non negative support, I get the expected result of an error where the data bounds are violated:
st.lognorm.fit(test_data, floc=0, fscale=1)

---------------------------------------------------------------------------
FitDataError                              Traceback (most recent call last)
&lt;ipython-input-13-fbeaae8f3c2e&gt; in &lt;module&gt;
----&gt; 1 st.lognorm.fit(test_data, floc=0, fscale=1)

~\Miniconda3\lib\site-packages\scipy\stats\_continuous_distns.py in fit(self, data, *args, **kwds)
   5087             data = data - floc
   5088         if np.any(data &lt;= 0):
-&gt; 5089             raise FitDataError(""lognorm"", lower=floc, upper=np.inf)
   5090         lndata = np.log(data)
   5091 

FitDataError: Invalid values in `data`.  Maximum likelihood estimation with 'lognorm' requires that 0.0 &lt; x &lt; inf for each x in `data`.

However, with the following distributions, I am able to fit the data, despite the fact that all of these distributions have non negative data bounds (as defined by their scipy documentation) and fixed location and scale.
st.burr.fit(test_data, floc=0, fscale=1)

st.expon.fit(test_data)

st.chi2.fit(test_data, floc=0, fscale=1)

st.invgauss.fit(test_data, floc=0, fscale=1)

st.invgamma.fit(test_data, floc=0, fscale=1)

Which yield:
(4.435119987970436, 0.32475585134451646, 0, 1)
(-0.5900934692403015, 1.1171187649605647)
(1.349414062500001, 0, 1)
(0.6815429687499996, 0, 1)
(2.301074218750003, 0, 1)

Additionally, the distribution expon without any shape parameters is able parameters was able to execute which was surprising. If someone could explain how these distributions are able to fit to the data despite the fact that their support bounds have been violated I would really appreciate it.
I am running numpy 1.19.2 and scipy 1.5.2
Thank you!
",0,1013,"The fact that those fit didn't throw any error doesn't mean that they have been a good fit or that they can describe your data.
I'm using scipy==1.6.1.
You can check plotting results
x = np.linspace(test_data.min(), test_data.max(), 100)

Burr: no error, bu cannot describe data &lt;0
burr_pars = sps.burr.fit(test_data, floc=0, fscale=1)
y = sps.burr(*burr_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);


Expon: no error, but very bad fit
expon_pars = sps.expon.fit(test_data)
y = sps.expon(*expon_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);


Chi2: no error but very bad fit and cannot describe data &lt;0
chi2_pars = sps.chi2.fit(test_data, floc=0, fscale=1)
y = sps.chi2(*chi2_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);


Invgauss: error
invgauss_pars = sps.invgauss.fit(test_data, floc=0, fscale=1)
FitDataError: Invalid values in `data`.  Maximum likelihood estimation with 'invgauss' requires that 0 &lt; (x - loc)/scale  &lt; inf for each x in `data`.

If you don't set loc and scale, works best for x&gt;=0, but given the formula of its PDF there is no reason why it should throw an error for x&lt;0
invgauss_pars = sps.invgauss.fit(test_data)
y = sps.invgauss(*invgauss_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);


Invgamma: a warning, bad fit and cannot describe for x&lt;0
invagamm_pars = sps.invgamma.fit(test_data, floc=0, fscale=1)
y = sps.invgauss(*invagamm_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);
RuntimeWarning: invalid value encountered in double_scalars
  Lhat = muhat - Shat*mu


EDIT
From https://github.com/scipy/scipy/blob/v1.6.3/scipy/stats/_continuous_distns.py you see that FitDataError is called only by beta, expon (but if floc is None then floc = data_min), gamma,  invgauss (but only np.any(data - floc &lt; 0)), lognorm, pareto, rayleigh, uniform.
For other distributions FitDataError is not implemented.
",,
SciPy unexpected result,https://stackoverflow.com/questions/65152041,Using sp.ndimage.label on Xarray DataArray with apply_ufunc,"Lets say I have the simple array:
data = [1,1,0,0,1,1,1]

I can apply labeling to this data with the scipy ndimage module with:
groups, _ = sp.ndimage.label(data)

Resulting in
In [68]: print(groups)
[1 1 0 0 2 2 2]

Now, I would like to do the same labeling function on a  xarray DataArray.
xr_data = xr.DataArray([1,1,0,0,1,1,1], coords = [(""x"", [0,1,2,3,4,5,6])])

I know I could call the same function as before on the xr_data, but the output of doing this call is a numpy array, which in my actual dataset, is too large to fit in memory.
It seems like the xr.apply_ufunc function is what I need. However, I am having trouble getting it to work.
def xr_label(arr):
    return xr.apply_ufunc(sp.ndimage.label, arr)

xr_groups, _ = xr_label(xr_data)

This results in:
""ValueError: applied function returned data with unexpected number of dimensions. Received 0 dimension(s) but expected 1 dimensions with names: ('x',)""
I'm finding the documentation on the apply_ufunc method difficult to interpret. Can someone help me out with this?
",0,203,"You have to define input_core_dims and output_core_dims as parameters to apply_ufunc. See the documentation at: http://xarray.pydata.org/en/stable/generated/xarray.apply_ufunc.html
In your case I think this will be:
xr.apply_ufunc(sp.ndimage.label, arr, input_core_dims=[['x']], output_core_dims=[['x']])

I also recently struggled with understanding apply_ufunc (to be fair, I still don't have a full understanding), however the example at http://xarray.pydata.org/en/stable/examples/apply_ufunc_vectorize_1d.html helped me a lot.
",,
SciPy unexpected result,https://stackoverflow.com/questions/50960016,What is the cosine distance of something and itself?,"I'm playing with scipy's cosine distance. From what I've gathered, the closer a cosine distance is to 1, the more similar the vectors are. I got some unexpected results in a text mining project, so I decided to investigate the simplest case.

import numpy as np
import scipy.spatial
arr1 = np.array([1,1])
arr2 = np.array([1,1])
print scipy.spatial.distance.cosine(arr1, arr2)


My program prints 0.0.

Shouldn't the result be 1.0? Why or why not?
",0,608,"It is the cosine distance, not the cosine similarity.  A basic requirement for a function d(u, v) to be a distance is that d(u, u) = 0.

See the definition of the formula in the docstring of scipy.spatial.distance.cosine, and notice that the formula begins 1 - (...).  Your expectation of the function is probably based on the quantity in (...), but that expression is the cosine similarity.
",,
SciPy unexpected result,https://stackoverflow.com/questions/48675322,How to make all elements in many columns of a sparse csc matrix 0,"my_csr_matrix


&lt;338232x1783504 sparse matrix of type ''
    with 15740456 stored elements in Compressed Sparse Column format&gt;

my_csr_matrix[:,736225:1783504] = 0

Traceback (most recent call last):
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-135-b0e125d5d27e&gt;"", line 1, in &lt;module&gt;
    my_csr_matrix[:,736225:1783504] = 0
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\scipy\sparse\compressed.py"", line 695, in __setitem__
    i, j = self._swap((i.ravel(), j.ravel()))
MemoryError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2879, in run_code
    self.showtraceback(running_compiled_code=True)
TypeError: showtraceback() got an unexpected keyword argument 'running_compiled_code'

",0,97,,,
SciPy unexpected result,https://stackoverflow.com/questions/45036419,Solver (fsolve in python.scipy) not working,"I have been trying to solve the following system of equations using a scipy solver:

    from scipy.optimize import fsolve
    import math
    import numpy as np

    S0 = 1000
    u  = 1
    d  = 1

    delta_t = 1
    r = 0
    psi_r = (np.exp(-r*delta_t))**(-1)
    fi_r = np.exp(-r*delta_t)
    fi_2r = np.exp(-2*r*delta_t)

    def equations(p):
        p_u,p_m,p_d = p
        return (p_u+p_m+p_d - 1, fi_r*(p_u*(S0+u) + p_m*S0 + p_u*(S0-d)) -S0,fi_2r*
(p_u*p_u*(S0+2*u) + 2*p_m*p_u*(S0+u) + (2*p_u*p_d+p_m*p_m)*S0 + 2*p_m*p_d*(S0-d) + p_d*p_d*(S0-2*d)) - S0)

    p_u,p_m,p_d = fsolve(equations,(0.3,0.5,0.4))

    print(equations((p_u,p_m,p_d)))


The problem is that, despite the first equation stating that the sum of my unknowns should be 1, it never gives a result that would satisfy this. What I get is unexpected numbers on the order of 10 to the -12, or sometimes even negative numbers, which I know cannot be the correct solution.

I know I have to try several initial guesses, but what concerns me is that none of the guesses so far has given me probabilities which sum up to 1. 
",0,2026,,,
SciPy unexpected result,https://stackoverflow.com/questions/40377810,"interpolate.griddata shifts data northwards, is it a bug?","I observe unexpected results from scipy.interpolate.griddata. I am trying to visualize a set of irregularly spaced points using matplotlib.basemap and scipy.interpolate.griddata.

The data is given as three lists: latitudes, longitudes and values. To get them on the map I interpolate the data onto a regular grid and visualize it using Basemap's imshow function.

I observe that the interpolated data is shifted northwards from true positions.

Here is an example. Here I want to highlight a cell formed by two meridians and two parallels. I expect to get something like this:



However what I get is something like this:



You can see that the red rectangle is visibly shifted northwards.

I have tried to vary the grid resolution and the number of points, however this does not seem to have any effect on this observed shift.

Here is an IPython notebook that illustrates the issue.

Also below is the complete code:

import numpy as np
from numpy import random
from scipy import interpolate
import matplotlib.pyplot as plt
from mpl_toolkits.basemap import Basemap

# defining the region of interest
r = {'lon':[83.0, 95.5], 'lat':[48.5,55.5]}
# initializing Basemap
m = Basemap(projection='merc', 
            llcrnrlon=r['lon'][0],
            llcrnrlat=r['lat'][0],
            urcrnrlon=r['lon'][1],
            urcrnrlat=r['lat'][1],
            lon_0=r['lon'][0], 
            ellps='WGS84',
            fix_aspect=True,
            resolution='h')
# defining the highlighted block
block = {'lon':[89,91],'lat':[50.5,52.5]}
# generating the data
npixels = 100000
lat_range = r['lat'][1] - r['lat'][0]
lats = lat_range * random.random(npixels) + r['lat'][0]
lon_range = r['lon'][1] - r['lon'][0]
lons = lon_range * random.random(npixels) + r['lon'][0]
values = np.zeros(npixels)
for p in range(npixels):
    if block['lat'][0] &lt; lats[p] &lt; block['lat'][1] \
    and block['lon'][0] &lt; lons[p] &lt; block['lon'][1]:
        values[p] = 1.0 
# plotting the original data without interpolation
plt.figure(figsize=(5, 5))
m.drawparallels(np.arange(r['lat'][0], r['lat'][1] + 0.25, 2.0),
                    labels=[True,False,True,False])
m.drawmeridians(np.arange(r['lon'][0], r['lon'][1] + 0.25, 2.0), 
                    labels=[True,True,False,True])
m.scatter(lons,lats,c=values,latlon=True,edgecolors='none')
# interpolating on the regular grid
nx = ny = 500
mapx = np.linspace(r['lon'][0],r['lon'][1],nx)
mapy = np.linspace(r['lat'][0],r['lat'][1],ny)
mapgridx,mapgridy = np.meshgrid(mapx,mapy)
mapdata = interpolate.griddata(list(zip(lons,lats)),values,
                   (mapgridx,mapgridy),method='nearest')
# plotting the interpolated data
plt.figure(figsize=(5, 5))
m.drawparallels(np.arange(r['lat'][0], r['lat'][1] + 0.25, 2.0),
                    labels=[True,False,True,False])
m.drawmeridians(np.arange(r['lon'][0], r['lon'][1] + 0.25, 2.0), 
                    labels=[True,True,False,True])
m.imshow(mapdata)


I am seeing this with SciPy 0.17.0
",0,288,,,
SciPy unexpected result,https://stackoverflow.com/questions/25033995,How do I calculate expected values of a Poisson distributed random variable in Python using Scipy?,,0,3194,,,
SciPy unexpected issue,https://stackoverflow.com/questions/10518729,add2virtualenv (virtualenv wrapper) does not work with scipy,"I want to create a virtualenv without global python packages, but with the scipy distribution that is shared; installing scipy takes quite a while and I don't want to go through the motions too often.

So I run add2virtualenv /Library/Python/2.7/site-packages/scipy and after running add2virtualenv it shows the directory is added. (I doublechecked, it is the right directory).
Then I issue workon myfile to be sure the working directories are reloaded. 
However, when I try to load scipy, it is an ImportError: No module named scipy. This is unexpected.

Has anyone used a global scipy in a non-global-sitepackages virtualenv?
",6,4997,"So, to summarize, the actual problem here is that the directory including the packages to be imported must be used, instead of the specific package. That is, instead of

add2virtualenv /Library/Python/2.7/site-packages/scipy


It should be

add2virtualenv /Library/Python/2.7/site-packages


Beware: this solution has the drawback that you do not only include scipy, but any other packages in /Library/Python/2.7/site-packages.



An alternate, space-efficent solution could be symlinking scipy directory inside the virtual env's site-package. This can be done, in your virtual env, through:

cdsitepackages
ln -s /Library/Python/2.7/site-packages/scipy scipy




All credits go to @rubik (see the comments)

Check out this answer to find your site-packages path in case it is different than the one used here.
",,
SciPy unexpected issue,https://stackoverflow.com/questions/40451203,Cython parallel loop problems,,5,3869,"I haven't timed this myself so it's possible this might not help too much, however:

If you run cython -a to get an annotated version of your initial attempt (pairwise_distance_omp) you'll find the ans[c] += ... line is yellow, suggesting it's got Python overhead. A look at that the C corresponding to that line suggests that it's checking for divide by zero. One key part of it starts:

if (unlikely(M_PI == 0)) {


You know this will never be true (and in any case you'd probably live with NaN values rather than an exception if it was). You can avoid this check by adding the following extra decorator to the function:

@cython.cdivision(True)
# other decorators
def pairwise_distance_omp # etc...


This cuts out quite a bit of C code, including bits that have to be run in a single thread. The flip-side is that most of that code should never be run, and the compiler should probably be able to work that out, so it isn't clear how much difference that will make.



Second suggestion:

# at the top
cdef np.double_t accumulator, tmp

    # further down later in the loop:
    c = i * (m - 1) - i * (i + 1) / 2 + j - 1
    accumulator = 0
    for k in range(r.shape[1]):
        tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
        accumulator = accumulator + w[k] * (tmp*tmp)
    ans[c] = accumulator


This has two advantages hopefully: 1) tmp*tmp should probably be quicker than floating point exponent to the power of 2. 2) You avoid reading from the ans array, which might be a bit slow because the compiler always has to be careful that some other thread hasn't changed it (even though you know it shouldn't have).
",,
SciPy unexpected issue,https://stackoverflow.com/questions/50831551,parallel/multithread differential evolution in python,,5,4707,"Scipy differential_evolution can now be used in parallel extremely easily, by specifying the workers:


  workers int or map-like callable, optional
  
  If workers is an int the population is subdivided into workers
  sections and evaluated in parallel (uses multiprocessing.Pool). Supply
  -1 to use all available CPU cores. Alternatively supply a map-like callable, such as multiprocessing.Pool.map for evaluating the
  population in parallel. This evaluation is carried out as
  workers(func, iterable). This option will override the updating
  keyword to updating='deferred' if workers != 1. Requires that func be
  pickleable.
  
  New in version 1.2.0.


scipy.optimize.differential_evolution documentation
","Thanks to @jp2011 for pointing to pygmo

First, worth noting the difference from pygmo 1, since the fist link on google still directs to the older version.

Second, Multiprocessing island are available only for python 3.4+

Third, it works. The processes I started when I first asked the question are still running while I write, the pygmo archipelago running an extensive test of all the 18 possible DE variations present in saDE made in less than 3h. The compiled version using Numba as suggested here https://esa.github.io/pagmo2/docs/python/tutorials/coding_udp_simple.html will probably finish even earlier. Chapeau.

I personally find it a bit less intuitive than the scipy version, given the need to build a new class (vs a signle function in scipy) to define the problem but is probably just a personal preference. Also, the mutation/crossing over parameters are defined less clearly, for someone approaching DE for the first time might be a bit obscure.
But, since serial DE in scipy just isn't cutting it, welcome pygmo(2).

Additionally I found a couple other options claiming to parallelize DE. I didn't test them myself, but might be useful to someone stumbling on this question.  

Platypus, focused on multiobjective evolutionary algorithms 
https://github.com/Project-Platypus/Platypus

Yabox
https://github.com/pablormier/yabox

from Yabox creator a detailed, yet IMHO crystal clear, explaination of DE
https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/
","I've been having exactly the same problem. Perhaps, you could try pygmo, which does support different optimisation algorithms (including DE) and has a model for parallel computation. However, I'm finding that the community is not big as it is for scipy. Their tutorials, documentation, and examples are good quality and one can get things to work from that. 
"
SciPy unexpected issue,https://stackoverflow.com/questions/28321286,pandas rolling_quantile bug?,,4,1150,"As described here, the problem seems to be that the rolling_quantile() function (now in pandas 0.18 is rolling().quantile()) does not interpolate, it simply uses the nearest point.

A workaround is to apply the numpy percentile function after rolling:

a.rolling(row).apply(func=np.percentile, args=(25,)).tail(1)


which gives the correct interpolated results.
","This has be fixed in pandas 0.21.0. I just tried it. BTW, 0.20.3 hasn't fixed it.  The fix is here: https://github.com/pandas-dev/pandas/pull/16247
",
SciPy unexpected issue,https://stackoverflow.com/questions/46556376,Scipy&#39;s cut_tree() doesn&#39;t return requested number of clusters and the linkage matrices obtained with scipy and fastcluster do not match,,3,1203,"First, I am not concerned about the differences in the output of the three methods scipy.cluster.hierarchy.linkage(), fastcluster.linkage(), and fastcluster.linkage_vector(). Differences may arise for two reasons:


Small differences in cluster distances due to the limitations of floating-point arithmetic (algebraically equivalent formulae yield different results).
Aside from arithmetic differences, the algorithms are allowed to resolve ties differently. A tie in this context are two pairs of clusters (A,B), (C,D) with exactly the same distances between A and B, and C and D. As the OP wrote, there are e.g. numerous pairs of points with distance 0 at the beginning of the process, which the algorithms may cluster in any order.


Second, the question why scipy.cluster.hierarchy.cut_tree() does not yield the desired number of clusters is the really interesting issue here. Algebraically, the Ward clustering method cannot produce so-called inversions in the dendrogram. (An inversion occurs when a clustering distance in one step is bigger than the clustering distance in the next step.) That is, the sequence of distances in the third column of the stepwise dendrogram (the output of linkage()) is ideally a monotonically increasing sequence for the Ward method. However, due to floating-point inaccuracies, in the dataset supplied by the OP, the clustering distance in step 35 is reported as 2.2e16 by fastcluster.linkage_vector(), but 0.0 in the next step 36.

SciPy's cut_tree() does unfortunately not handle these inversions well. Even when they happen deep down in the dendrogram (like in the present case), this confuses the algorithm for the entire rest of the merging process, with the effect of wrongly formed clusters. (At first glance, I believe that the dendrogram is not only cut at the wrong level but that the clusters are actually wrong. I have not analyzed the situation completely, though.)

This is even more troubling as inversions occur not only by numerical inaccuracies as in the present case, but the centroid and median clustering methods produce them quite often, even with perfect arithmetic.

Lastly, an imperfect solution: In order to resolve the problem, replace two marked lines in the loop in SciPy's cut_tree() function:

for i, node in enumerate(nodes):
    idx = node.pre_order()
    this_group = last_group.copy()
    # ------------- Replace this:
    this_group[idx] = last_group[idx].min()
    this_group[this_group &gt; last_group[idx].max()] -= 1
    # -------------
    if i + 1 in cols_idx:
        groups[np.where(i + 1 == cols_idx)[0]] = this_group
    last_group = this_group


by the following lines:

    unique_idx = np.unique(last_group[idx])
    this_group[idx] = unique_idx[0]
    for ui in unique_idx[:0:-1]:
        this_group[this_group &gt; ui] -= 1


(Look at the SciPy source code for context.)

However, I would rather recommend to reimplement the cut_tree() method from scratch, as the current implementation is overly complicated, and the task can be performed more efficiently in terms of runtime complexity.
",,
SciPy unexpected issue,https://stackoverflow.com/questions/38966075,How to restrict anaconda from upgrading the module being installed if its a higher level dependency,,3,577,"Unless there's a specific reason you need to compile python yourself, I think what you're actually going after is conda bundle (http://conda.pydata.org/docs/commands/conda-bundle.html).  Unfortunately we've removed it in conda 4.2 which will be coming out soon, intending to move it to conda-build.  Since that hasn't happened yet, and if it ends up actually being useful to people, we can add it back.



You could also try this using conda-build...

Remove the whole source block in your meta.yaml file. Also remove all of the build requirements that are also not run requirements.  Then in your build.sh file

conda install --yes --quiet \
    python=2.7.10 \
    ipython=5.0.0 \
    numpy=1.11.1 \
    cython=0.24.1 \
    scipy=0.18.0 \
    pandas=0.18.1 \
    patsy=0.4.1 \
    statsmodels=0.6.1 \
    matplotlib=1.5.2 \
    ggplot=0.9.4 \
    scikit-learn=0.17.1 \
    distribute=0.6.45 \
    backports.ssl-match-hostname=3.5.0.1 \
    certifi=14.05.14 \
    nose_parameterized=0.5.0 \
    pyparsing=2.1.4 \
    python-dateutil=2.5.3 \
    pytz=2016.6.1 \
    pyzmq=15.3.0 \
    simplejson=3.3.3 \
    six=1.10.0 \
    sympy=1.0 \
    tornado=4.4.1 \
    virtualenv=13.0.1 \
    wsgiref=0.1.2 \
    python-swiftclient=2.7.0 \
    python-cinderclient=1.1.2 \
    python-glanceclient=0.17.2 \
    python-neutronclient=2.4.0 \
    networkx=1.11 \
    pysal=1.11.1 \
    pyyaml=3.11 \
    shapely=1.5.13 \
    beautifulsoup4=4.4.1 \
    nltk=3.2.1 \
    requests=2.10.0 \
    seaborn=0.5.0 \
    h5py=2.6.0 \
    xlrd=1.0.0 \
    markupsafe=0.23 \
    crypto=1.1.0 \
    jinja2=2.8 \
    openpyxl=2.3.2 \
    jaro_winkler=1.0.2 \
    bokeh=0.12.1 \
    numexpr=2.6.1 \
    pytables=3.2.3.1 \
    pycurl=7.43.0 \
    mgrs=1.1.0 \
    psutil=4.3.0 \
    biopython=1.67 \
    enaml=0.9.8 \
    mdp=3.5 \
    bitarray=0.8.1 \
    clusterpy=0.9.9 \
    pyside=1.2.1 \
    pyqt=4.11.4 \
    parsedatetime=1.4 \
    pymysql=0.6.7 \
    pyodbc=3.0.10 \
    tabulate=0.7.2


The big difference: by listing all of those packages as build requirements, you're actually ensuring that they won't be in your final conda package.  Think of build requirements more like a compiler, or something that's necessary when you're building the package, but not when you're actually running it.
",,
SciPy unexpected issue,https://stackoverflow.com/questions/76011928,Python: np.linalg.eigvalsh returning negatve eigen values,,2,295,"It's totally normal for symmetric matrices to have negative eigenvalues. A matrix being Hermitian only guarantees that its eigenvalues are real. It doesn't guarantee that the eigenvalues are positive.
It's not normal for a covariance matrix to have negative eigenvalues, meaning this isn't actually a covariance matrix, despite what you thought. Perhaps you rounded the entries of an actual covariance matrix?
",,
SciPy unexpected issue,https://stackoverflow.com/questions/32920920,Stepsize control of dopri5 integrator,,2,1979,"No, there is nothing wrong. You are telling the integrator to perform an integration step to t_final and it performs that step. Internal steps of the integrator are not reported.



The sensible thing to do is to give the desired sampling points as input of the algorithm, set for example dt=0.1 and use

state_ode_f.integrate( min(state_ode_f.t+dt, t_final) )


There is no single-step method in dopri5, only vode has it defined, see the source code https://github.com/scipy/scipy/blob/v0.14.0/scipy/integrate/_ode.py#L376, this could account for the observed differences.

As you found in Using adaptive step sizes with scipy.integrate.ode, one can force single-step behavior by setting the iteration bound nsteps=1. This will produce a warning every time, so one has to suppress these specific warnings to see a sensible result.



You should not use a parameter (which is a constant for the integration interval) for a time-dependent force. Use inside MassSpring_with_force the evaluation f=force(t). Possibly you could pass the function handle of force as parameter.
",,
SciPy unexpected issue,https://stackoverflow.com/questions/40690045,conda update failed in windows 10,"In Anaconda prompt as an administrator (Window 10 - 64 bit) after this command : conda update --all I see these errors as below:
I want to update and install packages but after enter every command these errors appear.

conda update --all
Fetching package metadata ..........An unexpected error has occurred.
Please consider posting the following information to the
conda GitHub issue tracker at:

    https://github.com/conda/conda/issues



Current conda install:

               platform : win-64
          conda version : 4.2.9
       conda is private : False
      conda-env version : 4.2.9
    conda-build version : 2.0.2
         python version : 3.5.2.final.0
       requests version : 2.11.1
       root environment : D:\Anaconda3  (writable)
    default environment : D:\Anaconda3
       envs directories : D:\Anaconda3\envs
          package cache : D:\Anaconda3\pkgs
           channel URLs : http://www.lfd.uci.edu/~gohlke/pythonlibs/win-64/
                          http://www.lfd.uci.edu/~gohlke/pythonlibs/noarch/
                          http://github.com/scipy/scipy.git/win-64/
                          http://github.com/scipy/scipy.git/noarch/
                          https://repo.continuum.io/pkgs/free/win-64/
                          https://repo.continuum.io/pkgs/free/noarch/
                          https://repo.continuum.io/pkgs/pro/win-64/
                          https://repo.continuum.io/pkgs/pro/noarch/
                          https://repo.continuum.io/pkgs/msys2/win-64/
                          https://repo.continuum.io/pkgs/msys2/noarch/
            config file : C:\Users\Behnaz Eslami\.condarc
           offline mode : False



`$ D:\Anaconda3\Scripts\conda-script.py update --all`




    Traceback (most recent call last):
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 131, in fetch_repodata
        cache = json.loads(json_str)
      File ""D:\Anaconda3\lib\json\__init__.py"", line 319, in loads
        return _default_decoder.decode(s)
      File ""D:\Anaconda3\lib\json\decoder.py"", line 339, in decode
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
      File ""D:\Anaconda3\lib\json\decoder.py"", line 357, in raw_decode
        raise JSONDecodeError(""Expecting value"", s, err.value) from None
    json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

    During handling of the above exception, another exception occurred:

    Traceback (most recent call last):
      File ""D:\Anaconda3\lib\site-packages\conda\exceptions.py"", line 473, in conda_exception_handler
        return_value = func(*args, **kwargs)
      File ""D:\Anaconda3\lib\site-packages\conda\cli\main.py"", line 144, in _main
        exit_code = args.func(args, p)
      File ""D:\Anaconda3\lib\site-packages\conda\cli\main_update.py"", line 65, in execute
        install(args, parser, 'update')
      File ""D:\Anaconda3\lib\site-packages\conda\cli\install.py"", line 238, in install
        prefix=prefix)
      File ""D:\Anaconda3\lib\site-packages\conda\api.py"", line 24, in get_index
        index = fetch_index(channel_urls, use_cache=use_cache, unknown=unknown)
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 293, in fetch_index
        for url in urls]
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 293, in &lt;listcomp&gt;
        for url in urls]
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 70, in func
        res = f(*args, **kwargs)
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 137, in fetch_repodata
        .format(url, filename, e))
    conda.exceptions.CondaRuntimeError: Runtime error: Invalid index file: http://www.lfd.uci.edu/~gohlke/pythonlibs/win-64/repodata.json: Expecting value: line 1 column 1 (char 0)


How can I solve this issue?
",2,10297,"Try to run conda update conda on command prompt (admin)

or Go to your anaconda installation folder, right-click on ""Anaconda3"", go to Security, in ""Group or user names"" click on Users, then check all under ""Allow"".

See this Image
","You need to add the following reference to your (environment variables) path:
&lt;AnacondaInstallationDirectory&gt;\condabin

It's important to add it before any other existing Anaconda references there, so that conda.exe can be executed with the right privileges.
Also you make sure that you have these as well:
&lt;AnacondaInstallationDirectory&gt;\Library\bin
&lt;AnacondaInstallationDirectory&gt;\Scripts
&lt;AnacondaInstallationDirectory&gt;

","instead of running running in a normal terminal window or as administrator, open an anaconda terminal.

In the windows 10 search bar, type cmd (don't press enter)
In the results, you should see one titled ""Anaconda prompt (anaconda 3)""
type your command in this anaconda terminal e.g. conda update conda

"
SciPy unexpected issue,https://stackoverflow.com/questions/61594854,Packaged dags - Airflow can&#39;t find module installed,,1,5544,"Ciao Marco,
I know this is an old question, but I had to go through the very same process and what worked for me was to use:
pip install -r ../requirements_dag.txt --target=""$PWD""

The same works for packages hosted on git. The key difference is the use of --target rather than --prefix.
","The Airflow documentation page you're referring to says this about packaged DAGs:


  To allow this you can create a zip file that contains the DAG(s) in the root of the zip file and have the extra modules unpacked in directories.


The way I interpret this is different from yours. I don't think Airflow handles these packaged DAGs as a real python package. It just seems like a custom zip folder that will be added to your DAGs folder. So the lib or lib64 folders you have are probably not real python modules (they don't have a __init__.py file). That's why they say that ""the extra modules should be unpacked in directories"".

Look at the example zip file they give:

my_dag1.py
my_dag2.py
package1/__init__.py
package1/functions.py


package1 has a __init__.py file. So in your case, your vertica_python library should be directly importable like this:

my_custom_package
vertica_python/
predict_dag.py  
train_dag.py


However, I don't think you should do this. I have the impression that the modules that you should add here are your own developed modules, not third party libraries.

So I suggest that you install the libraries you need to run your packaged DAGs beforehand.
",
SciPy unexpected issue,https://stackoverflow.com/questions/40377810,"interpolate.griddata shifts data northwards, is it a bug?",,0,288,"Pauli Virtanen on SciPy bugtracker answered the question.

The issue goes away if one replaces basemap.imshow() with matplotlib.pyplot.pcolormesh()

Replacing above

m.imshow(mapdata)


with

meshx,meshy = m(mapx,mapy)
plt.pcolormesh(meshx,meshy,mapdata)


produces correctly aligned image.



It is not clear what I am doing wrong with basemap.imshow, but that is probably another question.
",,
SciPy unexpected issue,https://stackoverflow.com/questions/76309535,Quantile Forest error &quot;predict() got an unexpected keyword argument &#39;quantiles&#39;&quot;,,0,180,"I notice that the code you've provided is an example from the sklearn-quantile package. Perhaps confusingly, both packages -- sklearn-quantile and quantile-forest -- provide a RandomForestQuantileRegressor class, but the packages have different ways of passing the quantiles to the class methods. As a result, the RandomForestQuantileRegressor classes from the two packages are not currently interchangeable.
In the code snippet you've provided, it's not clear what imports are being used, but it appears that you may be using the RandomForestQuantileRegressor class from the sklearn-quantile package. This class expects the quantiles to be passed to the initialization function instead of the predict function and would lead to the error you've presented. If this is correct, then you can fix this by importing the RandomForestQuantileRegressor class from the quantile-forest package and passing the quantiles to the predict function instead of the initialization function.
If the above is not helpful or you still are running into errors, you're welcome to create an issue in the quantile-forest repository here for additional troubleshooting.
",,
SciPy unexpected issue,https://stackoverflow.com/questions/61973962,pypy3 dyld can not be found in homebrew,"I got the following error when I use homebrew. Does anybody know how to fix the problem?

dyld: Library not loaded: /usr/local/opt/libffi/lib/libffi.6.dylib
  Referenced from: /usr/local/Cellar/pypy/7.1.1_1/libexec/lib/libpypy-c.dylib
  Reason: image not found


Here is the output of brew doctor.

$ brew doctor
Please note that these warnings are just used to help the Homebrew maintainers
with debugging if you file an issue. If everything you use Homebrew for is
working fine: please don't worry or file an issue; just ignore this. Thanks!

Warning: Some installed formulae were deleted!
You should find replacements for the following formulae:
  inchi
  python@2
  libmpc@0.8
  pygtk
  isl@0.11
  f2c
  indigo
  mpfr@2
  gmp@4
  phantomjs
  phantomjs
  rdkit
  casperjs
  mumps
  ecj
  pygobject
  isl@0.12
  gcc@4.8
  gcc@4.8
Warning: Calling a Formula#patches definition is deprecated! Use 'patch do' block calls instead.
Please report this issue to the docmunch/pdftk tap (not Homebrew/brew or Homebrew/core), or even better, submit a PR to fix it:
  /usr/local/Homebrew/Library/Taps/docmunch/homebrew-pdftk/pdftk.rb:15


Warning: You have the following deprecated, official taps tapped:
  Homebrew/homebrew-dupes
  Homebrew/homebrew-fuse
  Homebrew/homebrew-head-only
  Homebrew/homebrew-versions
  Homebrew/homebrew-x11
Untap them with `brew untap`.

Warning: ""config"" scripts exist outside your system or Homebrew directories.
`./configure` scripts often look for *-config scripts to determine if
software packages are installed, and which additional flags to use when
compiling and linking.

Having additional scripts in your path can confuse software installed via
Homebrew if the config script overrides a system or Homebrew-provided
script of the same name. We found the following ""config"" scripts:
  /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7-config
  /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7m-config
  /Library/Frameworks/Python.framework/Versions/3.7/bin/python3-config
  /Library/Frameworks/Python.framework/Versions/2.7/bin/python2-config
  /Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7-config
  /Library/Frameworks/Python.framework/Versions/2.7/bin/python-config

Warning: Putting non-prefixed coreutils in your path can cause gmp builds to fail.

Warning: Putting non-prefixed findutils in your path can cause python builds to fail.

Warning: Unbrewed dylibs were found in /usr/local/lib.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected dylibs:
  /usr/local/lib/libgcc_ext.10.4.dylib
  /usr/local/lib/libgcc_ext.10.5.dylib
  /usr/local/lib/libgcc_s.10.4.dylib
  /usr/local/lib/libgcc_s.10.5.dylib
  /usr/local/lib/libgcc_s_x86_64.1.dylib
  /usr/local/lib/libgfortran.2.0.0.dylib
  /usr/local/lib/libgfortran.3.dylib
  /usr/local/lib/libgomp.1.dylib
  /usr/local/lib/libpocketsphinx.3.dylib
  /usr/local/lib/libpostal.1.dylib
  /usr/local/lib/libquadmath.0.dylib
  /usr/local/lib/libsphinxad.3.dylib
  /usr/local/lib/libsphinxbase.3.dylib
  /usr/local/lib/libtcl8.6.dylib
  /usr/local/lib/libtk8.6.dylib
  /usr/local/lib/libwkhtmltox.0.12.1.dylib
  /usr/local/lib/libwkhtmltox.0.12.2.dylib
  /usr/local/lib/libwkhtmltox.0.12.4.dylib

Warning: Unbrewed header files were found in /usr/local/include.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected header files:
  /usr/local/include/fakemysql.h
  /usr/local/include/fakepq.h
  /usr/local/include/fakesql.h
  /usr/local/include/itcl.h
  /usr/local/include/itcl2TclOO.h
  /usr/local/include/itclDecls.h
  /usr/local/include/itclInt.h
  /usr/local/include/itclIntDecls.h
  /usr/local/include/itclMigrate2TclCore.h
  /usr/local/include/itclTclIntStubsFcn.h
  /usr/local/include/libpostal/libpostal.h
  /usr/local/include/mysqlStubs.h
  /usr/local/include/node/js_native_api.h
  /usr/local/include/node/js_native_api_types.h
  /usr/local/include/node/libplatform/libplatform-export.h
  /usr/local/include/node/libplatform/libplatform.h
...  /usr/local/include/tclPlatDecls.h
  /usr/local/include/tclThread.h
  /usr/local/include/tclTomMath.h
  /usr/local/include/tclTomMathDecls.h
  /usr/local/include/tdbc.h
  /usr/local/include/tdbcDecls.h
  /usr/local/include/tdbcInt.h
  /usr/local/include/tk.h
  /usr/local/include/tkDecls.h
  /usr/local/include/tkPlatDecls.h
  /usr/local/include/wkhtmltox/image.h
  /usr/local/include/wkhtmltox/pdf.h

Warning: Unbrewed .la files were found in /usr/local/lib.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected .la files:
  /usr/local/lib/libgfortran.la
  /usr/local/lib/libgomp.la
  /usr/local/lib/libpocketsphinx.la
  /usr/local/lib/libpostal.la
  /usr/local/lib/libquadmath.la
  /usr/local/lib/libsphinxad.la
  /usr/local/lib/libsphinxbase.la

Warning: Unbrewed .pc files were found in /usr/local/lib/pkgconfig.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected .pc files:
  /usr/local/lib/pkgconfig/libpostal.pc
  /usr/local/lib/pkgconfig/pocketsphinx.pc
  /usr/local/lib/pkgconfig/sphinxbase.pc
  /usr/local/lib/pkgconfig/tcl.pc
  /usr/local/lib/pkgconfig/tk.pc

Warning: Unbrewed static libraries were found in /usr/local/lib.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected static libraries:
  /usr/local/lib/libgfortran.a
  /usr/local/lib/libgomp.a
  /usr/local/lib/libpocketsphinx.a
  /usr/local/lib/libpostal.a
  /usr/local/lib/libquadmath.a
  /usr/local/lib/libsphinxad.a
  /usr/local/lib/libsphinxbase.a
  /usr/local/lib/libtclstub8.6.a
  /usr/local/lib/libtkstub8.6.a

Warning: You have unlinked kegs in your Cellar.
Leaving kegs unlinked can lead to build-trouble and cause brews that depend on
those kegs to fail to run properly once built. Run `brew link` on these:
  inchi
  mpich
  ssed
  python
  libmpc@0.8
  nss
  isl@0.11
  numpy
  mpfr@2
  gmp@4
  poppler
  nspr
  scipy
  gcc@4.8

Warning: Some installed formulae are not readable:
  indigo: Unsupported special dependency :python

  rdkit: Unsupported special dependency :python3

Warning: Your Xcode (10.3) is outdated.
Please update to Xcode 11.3.1 (or delete it).
Xcode can be updated from the App Store.


Warning: Broken symlinks were found. Remove them with `brew cleanup`:
  /usr/local/share/man/man5/package-lock.json.5
  /usr/local/share/man/man7/removing-npm.7

Warning: Some installed formulae are missing dependencies.
You should `brew install` the missing dependencies:
  brew install imlib2 libcaca opusfile

Run `brew missing` for more details.

",0,630,"This is a duplicate of issue 3229 on the PyPy issue tracker. It seems our buildbot is using a brew installed version of libffi. Until we solve this, you should be able to do brew install pypy3. Help from someone who can make the PyPy build look more like the CPython one (which apparently does not have this problem) are welcome.
",,
SciPy unexpected issue,https://stackoverflow.com/questions/25161815,scipy curve_fit returns error for keyword absolute_sigma,"I am using ubuntu 14.04 on python 2.7 with numpy version 1.8.1 and scipy version 0.13.3. When I do a curve_fit with the keyword absolute_sigma=True, I get the message:


  TypeError: leastsq() got an unexpected keyword argument 'absolute_sigma'


I updated numpy and scipy to the versions mentioned above as the keyword absolute_sigma seems to be a new feature. However, no success!

any ideas how to solve this issue? Do I need scipy 0.14... ?

Thanks a lot
",0,2500,"The absolute_sigma parameter was added in version 0.14. Compare the call signatures:


version 0.14
version 0.13

",,
SciPy strange behavior,https://stackoverflow.com/questions/3812896,Can some explain this strange behavior of the hypergeometric distribution in scipy?,"I am running Python 2.6.5 on Mac OS X 10.6.4 (this is not the native version, I installed it myself) with Scipy 0.8.0. If I do the following:

&gt;&gt;&gt; from scipy.stats import hypergeom
&gt;&gt;&gt; hypergeom.sf(5,10,2,5)


I get an IndexError. Then I do:

&gt;&gt;&gt; hypergeom.sf(2,10,2,2)
-4.44....


I suspect the negative value is due to bad floating point precision. Then I do the first one again:

&gt;&gt;&gt; hypergeom.sf(5,10,2,5)
0.0


Now it works! Can someone explain this? Are you seeing this behavior too?
",11,1399,"The problem seems to arise based if the first call to the survival function is in the range that should obviously be zero (see my comment to the previous answer).  E.g., for calls to hypergeom.sf(x,M,n,N) it fails if the first call to a hypergeometric function to the function is a situation where x &gt; n, where the survival function will always be zero.  

You could trivially fix this temporarily by:

def new_hypergeom_sf(k, *args, **kwds):
    from scipy.stats import hypergeom
    (M, n, N) = args[0:3]
    try:
        return hypergeom.sf(k, *args, **kwds)
    except Exception as inst:
        if k &gt;= n and type(inst) == IndexError:
            return 0 ## or conversely 1 - hypergeom.cdf(k, *args, **kwds)
        else:
            raise inst


Now if you have no problem editing the /usr/share/pyshared/scipy/stats/distributions.py (or equivalent file), the fix is likely on line 3966 where right now it reads:

    place(output,cond,self._sf(*goodargs))
    if output.ndim == 0:
        return output[()]
    return output


But if you change it to:

    if output.ndim == 0:
        return output[()]
    place(output,cond,self._sf(*goodargs))
    if output.ndim == 0:
        return output[()]
    return output


It now works without the IndexError.  Basically if the output is zero dimensional because it fails the checks, it tries to call place, fails, and doesn't generate the distribution.  (This doesn't happen if a previous distribution has already been created which is likely why this wasn't caught on earlier tests.)  Note that place (defined in numpy's function_base.py) will change elements of the array (though I'm not sure if it changes the dimensionality) so it may be best to still have it leave the 0 dim check after place too.  I haven't fully tested this to see if this change breaks anything else (and it applies to all discrete random variable distributions), so it maybe its best to do the first fix.

It does break it; e.g., stats.hypergeom.sf(1,10,2,5) returns as zero (instead of 2/9).  

This fix seems to work much better, in the same section:

class rv_discrete(rv_generic):
...
    def sf(self, k, *args, **kwds):
    ...
        if any(cond):
            place(output,cond,self._sf(*goodargs))
        if output.ndim == 0:
            return output[()]
        return output

","I don't know python, but the function is defined like this:
hypergeom.sf(x,M,n,N,loc=0) 

M is the number of interesting objects, N the total number of objects, and n is how often you ""pick one"" (Sorry, German statistician).

If you had a bowl with 20 balls, 7 of those yellow (an interesting yellow), then N is 20 and M is 7.

Perhaps the function behaves undefined for the (nonsense) case when M&gt;N ?
",
SciPy strange behavior,https://stackoverflow.com/questions/65585639,Eigenvalues in Python: A Bug?,"Here are two assumptions about eigenvectors and eigenvalues of square matrices. I believe that both are true:

If a matrix is symmetric and contains only real values, then it is a Hermitian matrix, and then all eigenvalues should be real numbers and all components of all eigenvectors should also be real numbers. No complex numbers should appear in the results when you calculate eigenvectors and eigenvalues from Hermitian matrices.

The eigenvector of a given eigenvalue, calculated from a given matrix should always point into a direction that is determined only by the matrix and the eigenvalue. The algorithm used to calculate it has no influence on the result, as long as the algorithm is implemented correctly.


But both assumptions do not hold when you use standard libraries in Python to calculate eigenvectors and eigenvalues. Do those methods contain bugs?
There are four different methods to calculate eigenvalues and eigenvectors from Hermitian matrices:

numpy.linalg.eig
scipy.linalg.eig
numpy.linalg.eigh
scipy.linalg.eigh

#1 and #2 can be used for any square matrix (including Hermitian matrices).
#3 and #4 are made for Hermitian matrices only. As far as I did understand their purpose is just that they run faster, but the results should be the same (as long as the input is really Hermitian).
But the four methods deliver three different results for the very same input. Here is the program that I used to test all four methods:
#!/usr/bin/env python3

import numpy as np
import scipy.linalg as la

A = [
    [19, -1, -1, -1, -1, -1, -1, -1],
    [-1, 19, -1, -1, -1, -1, -1, -1],
    [-1, -1, 19, -1, -1, -1, -1, -1],
    [-1, -1, -1, 19, -1, -1, -1, -1],
    [-1, -1, -1, -1, 19, -1, -1, -1],
    [-1, -1, -1, -1, -1, 19, -1, -1],
    [-1, -1, -1, -1, -1, -1, 19, -1],
    [-1, -1, -1, -1, -1, -1, -1, 19]
]

A = np.array(A, dtype=np.float64)

delta = 1e-12
A[5,7] += delta
A[7,5] += delta

if np.array_equal(A, A.T):
    print('input is symmetric')
else:
    print('input is NOT symmetric')

methods = {
    'np.linalg.eig'  : np.linalg.eig,
    'la.eig'         : la.eig,
    'np.linalg.eigh' : np.linalg.eigh,
    'la.eigh'        : la.eigh
}

for name, method in methods.items():

    print('============================================================')
    print(name)
    print()

    eigenValues, eigenVectors = method(A)
    
    for i in range(len(eigenValues)):
        print('{0:6.3f}{1:+6.3f}i '.format(eigenValues[i].real, eigenValues[i].imag), end=' |  ')
        line = eigenVectors[i]
        for item in line:
            print('{0:6.3f}{1:+6.3f}i '.format(item.real, item.imag), end='')
        print()

    print('---------------------')

    for i in range(len(eigenValues)):
        if eigenValues[i].imag == 0:
            print('real    ', end=' |  ')
        else:
            print('COMPLEX ', end=' |  ')
        line = eigenVectors[i]
        for item in line:
            if item.imag == 0:
                print('real    ', end='')
            else:
                print('COMPLEX ', end='')
        print()

    print()

And here is the output it produces:
input is symmetric
============================================================
np.linalg.eig

12.000+0.000i  |  -0.354+0.000i  0.913+0.000i  0.204+0.000i -0.013+0.016i -0.013-0.016i  0.160+0.000i -0.000+0.000i  0.130+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.183+0.000i  0.208+0.000i  0.379-0.171i  0.379+0.171i -0.607+0.000i  0.000+0.000i -0.138+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.468-0.048i -0.468+0.048i  0.153+0.000i  0.001+0.000i -0.271+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i  0.657+0.000i  0.657-0.000i  0.672+0.000i -0.001+0.000i  0.617+0.000i 
20.000-0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.276+0.101i -0.276-0.101i -0.361+0.000i  0.001+0.000i -0.644+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.001+0.000i -0.612+0.000i -0.001+0.000i -0.001-0.000i  0.001+0.000i  0.706+0.000i -0.000+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.276+0.101i -0.276-0.101i -0.018+0.000i -0.000+0.000i  0.306+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.001+0.000i -0.612+0.000i -0.001+0.000i -0.001-0.000i  0.001+0.000i -0.708+0.000i  0.000+0.000i 
---------------------
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
COMPLEX  |  real    real    real    real    real    real    real    real    
COMPLEX  |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    

============================================================
la.eig

12.000+0.000i  |  -0.354+0.000i  0.913+0.000i  0.204+0.000i -0.013+0.016i -0.013-0.016i  0.160+0.000i -0.000+0.000i  0.130+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.183+0.000i  0.208+0.000i  0.379-0.171i  0.379+0.171i -0.607+0.000i  0.000+0.000i -0.138+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.468-0.048i -0.468+0.048i  0.153+0.000i  0.001+0.000i -0.271+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i  0.657+0.000i  0.657-0.000i  0.672+0.000i -0.001+0.000i  0.617+0.000i 
20.000-0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.276+0.101i -0.276-0.101i -0.361+0.000i  0.001+0.000i -0.644+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.001+0.000i -0.612+0.000i -0.001+0.000i -0.001-0.000i  0.001+0.000i  0.706+0.000i -0.000+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.276+0.101i -0.276-0.101i -0.018+0.000i -0.000+0.000i  0.306+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.001+0.000i -0.612+0.000i -0.001+0.000i -0.001-0.000i  0.001+0.000i -0.708+0.000i  0.000+0.000i 
---------------------
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
COMPLEX  |  real    real    real    real    real    real    real    real    
COMPLEX  |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    

============================================================
np.linalg.eigh

12.000+0.000i  |  -0.354+0.000i  0.000+0.000i  0.000+0.000i -0.086+0.000i  0.905+0.000i -0.025+0.000i  0.073+0.000i  0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.000+0.000i -0.374+0.000i  0.149+0.000i -0.236+0.000i -0.388+0.000i  0.682+0.000i  0.206+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.001+0.000i  0.551+0.000i  0.136+0.000i -0.180+0.000i  0.616+0.000i  0.317+0.000i  0.201+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.001+0.000i -0.149+0.000i  0.719+0.000i -0.074+0.000i -0.042+0.000i -0.534+0.000i  0.207+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.005+0.000i  0.505+0.000i -0.386+0.000i -0.214+0.000i -0.556+0.000i -0.274+0.000i  0.203+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.707+0.000i -0.004+0.000i  0.002+0.000i  0.001+0.000i  0.002+0.000i -0.000+0.000i -0.612+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.003+0.000i -0.529+0.000i -0.535+0.000i -0.203+0.000i  0.398+0.000i -0.262+0.000i  0.203+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.707+0.000i  0.001+0.000i  0.001+0.000i  0.000+0.000i -0.005+0.000i -0.001+0.000i -0.612+0.000i 
---------------------
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    

============================================================
la.eigh

12.000+0.000i  |  -0.354+0.000i  0.000+0.000i  0.000+0.000i -0.225+0.000i  0.882+0.000i  0.000+0.000i  0.065+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.000+0.000i -0.395+0.000i  0.332+0.000i -0.156+0.000i  0.227+0.000i  0.701+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.001+0.000i  0.612+0.000i  0.011+0.000i -0.204+0.000i -0.597+0.000i  0.250+0.000i -0.200+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.001+0.000i -0.086+0.000i  0.689+0.000i  0.030+0.000i -0.054+0.000i -0.589+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.005+0.000i  0.413+0.000i -0.264+0.000i -0.245+0.000i  0.711+0.000i -0.165+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.707+0.000i -0.004+0.000i -0.000+0.000i  0.001+0.000i -0.002+0.000i -0.001+0.000i  0.612+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.003+0.000i -0.540+0.000i -0.542+0.000i -0.309+0.000i -0.290+0.000i -0.261+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.707+0.000i  0.001+0.000i -0.000+0.000i  0.001+0.000i  0.005+0.000i -0.001+0.000i  0.612+0.000i 
---------------------
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real 

As you can see, numpy.linalg.eig and scipy.linalg.eig produce complex numbers in their output, but they shouldn't. This could be accepted as some kind of rounding error, if the magnitude of the imaginary part would by tiny compared to the magnitude of the real part. But this is not the case. One of the numbers that are produced is -0.013+0.016i. Here the imaginary part has an even higher magnitude than the real part.
Even worse: The four methods produce three different results.
All four methods calculate only once an eigenvalue of 12 and 7 times an eigenvalue of 20. And all eigenvectors always have the length 1. This means, all four methods should produce the very same eigenvector for eigenvalue 12. But only numpy.linalg.eig and scipy.linalg.eig produce the same output.
Here are the components of the eigenvector for eigenvalue 12. Have a closer look to the lines marked with an arrow (&lt;==). Here you find three different values, but the values should be exactly equal. And if you have a second look, you will see, that only in the 1st line all three values are equal. In all other lines you will find 2 or 3 different values.
numpy.linalg.eig  |                     |
scipy.linalg.eig  |  numpy.linalg.eigh  |  scipy.linalg.eigh
------------------+---------------------+-------------------
   -0.354+0.000i  |      -0.354+0.000i  |      -0.354+0.000i
    0.913+0.000i  |       0.000+0.000i  |       0.000+0.000i
    0.204+0.000i  |       0.000+0.000i  |       0.000+0.000i
   -0.013+0.016i  |      -0.086+0.000i  |      -0.225+0.000i   &lt;===
   -0.013-0.016i  |       0.905+0.000i  |       0.882+0.000i   &lt;===
    0.160+0.000i  |      -0.025+0.000i  |       0.000+0.000i   &lt;===
   -0.000+0.000i  |       0.073+0.000i  |       0.065+0.000i   &lt;===
    0.130+0.000i  |       0.205+0.000i  |      -0.205+0.000i

Here are my questions:

How is this possible?
Are these bugs?
Is one of the results correct?
If there is a method that delivers correct results: Which is it?


p.s: Here are relevant version informations:

I did run this code on an iMac (macOS Catalina Version 10.15.7)
The python version is 3.8.5
The version of numpy is 1.19.5
The version of scipy is 1.6.0


This is the output of numpy.show_config()
(as requested in a comment):
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
None


ADDENDUM (added 1 day after the question was asked)
Reaction to comments:

complex eigenvectors of real symmetric matrices
@Rethipher: Thank you! I did read and understand the question you linked to (Can a real symmetric matrix have complex eigenvectors?), and I also did read the answers, but I didnt understand them. Did they say yes or no? (rhetoric question, no need to answer, see next line)
@Mark Dickinson &amp; @bnaecker: Thank you for making clear, that my assumption was wrong.

real symmetric matrices vs. Hermitian matrices
@bnaecker: The set of real numbers is a subset of the set of complex numbers. Those complex numbers which are equal to their own complex conjugate are called real. So, the set of real symmetric matrices is a subset of Hermitian matrices. This is important, because numpy.linalg.eigh and scipy.linalg.eigh are designed to handle Hermitian matrices. And because every real symmetric matrix is a Hermitian matrix, those modules also can be used for my purposes.

mixing up rows and columns
@Mark Dickinson &amp; @bnaecker: Thank you, I think you are right. Also the documentations says so, I should have read it more carefully. But even if you compare columns instead of rows you will still find that the 4 methods produce 3 different results. But if the result contains a 7-dimensional subspace that can be described with 7 real basis vectors only, I still find it strange, that an algorithm produces a complex basis.

a bug would be surprising
@bnaecker: This is true, but surprising bugs do exist. (Like Heartbleed and some others.) So, this is not really an argument.

I get reals - your sample matrix doesn't contain floats
@Stef &amp; @JohanC: Sorry, you didnt read my program carefully enough. I added a value of 1e-12 to A[5,7] and A[7,5] to simulate tiny rounding errors that appear inevitably in my real app before it comes to the calculation of eigenvalues and eigenvectors. (What Ive posted here is just a tiny test program, just big enough to demonstrate the issue.)
And you are right, Stef: Without adding this tiny noise, I also get real results. But only a tiny change of one millionth of one millionth makes such a big difference, and I can't understand why.


Reaction to DavidB2013s answer :
I tried the tool you suggested, and I got different results. I think you also forgot to add that little noise of 1e-12 to A[5,7] and A[7,5]. However, all results are still real. I did get these eigenvalues:
12.000000000000249
20
20.00000000000075
19.999999999999
20
20
20
20

and these eigenvectors:
0.3535533905932847   0.9128505045937204      0.20252576206455747  0.002673672081814904   -0.09302397289286794   -0.09302397289286794   -0.09302397289286794   -0.09302397289286794     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406 -0.20415317121194954   -0.20415317121194954   -0.20415317121194954   -0.20415317121194954     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406 -0.20415317121194954   -0.20415317121194954   -0.20415317121194954    0.9080920678356449     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406 -0.20415317121194954    0.9080920678356449    -0.20415317121194954   -0.20415317121194954     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406  0.9080920678356449    -0.20415317121194954   -0.20415317121194954   -0.20415317121194954     
0.35355339059324065 -0.00011103276380548543 -0.6116010247648269   0.7060012169461334      0.0005790869815273477  0.0005790869815273477  0.0005790869815273477  0.0005790869815273477     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406 -0.20415317121194954   -0.20415317121194954    0.9080920678356449    -0.20415317121194954     
0.35355339059324054  0.0002333904819895115  -0.6131412438770024  -0.7082055415560993      0.0009655029234935232  0.0009655029234935232  0.0009655029234935232  0.0009655029234935232     

Only the vector for eigenvalue 12 has the same values as your calculation: (There is a difference of approx. 1.1e-14 in 6 dimensions and 3.3e-14 in the two other dimensions, but I count this as rounding error.) All other vectors are significantly different (the smallest differences are of the size of 0.02). It puzzles me, that a tiny rounding error of 1e-12 in just 2 elements of the input matrix can produce so big differences.

I calculated the eigenvalues with another method (with the help of https://www.wolframalpha.com), and when I didnt add the tiny delta values, which should simulate rounding errors, I only get two different eigenvalues which are 12 and 20.
The characteristic polynomial of the given matrix is:
(20 - )^7 * (12 - )

So, it has one root at =12 and 7 roots at =20 and these 8 roots are the 8 eigenvalues. All of them real numbers.
When I add the tiny delta values, I get this characteristic polynomial:
(20 - )^5 * (19999999999999/1000000000000 - ) * (1000000000000 ^2 - 32000000000001  + 240000000000014)/1000000000000

It has these roots:
=12.00000000000024999999999998 (rounded)
=19.999999999999 (exact value)
=20 (exact value)  
=20 (exact value)  
=20 (exact value)  
=20 (exact value)  
=20 (exact value)  
=20.00000000000075000000000002 (rounded)

And again all 8 eigenvalues are real numbers.
Then I calculated the eigenvectors. Without adding 1e-12 I get this results:
Vector for eigenvalue 12:
v = (1,1,1,1,1,1,1,1)

The length of this vector is sqrt(8), and if you multiply the vector with 1/sqrt(8), you get exactly the result from the other calculations (0.35355339 in each dimension).
But the seven eigenvectors for eigenvalue 20 are very different. They are:
(-1,1,0,0,0,0,0,0)
(-1,0,1,0,0,0,0,0)
(-1,0,0,1,0,0,0,0)
(-1,0,0,0,1,0,0,0)
(-1,0,0,0,0,1,0,0)
(-1,0,0,0,0,0,1,0)
(-1,0,0,0,0,0,0,1)

Even if you bring them to the length 1, they are different from all other results and it is very easy to see that they are correct. The other results are also correct, but I would prefer these simple results.
I also calculated the eigenvalues for the version with the tiny noise. All 8 vectors are so close to the noise-less results, that even Wolfram Alpha rounded them to exactly the same values as before. And this is exactly the behavior that I would expect from an algorithm that calculates eigenvalues and eigenvectors:

Tiny variations in the input should - when ever it is possible - return tiny variations in the results.

",10,1316,"As far as I know, assumption 1 is correct, but assumption 2 is not.
A Real Symmetric matrix produces eigenvalues and eigenvectors that are real only.
However, for a given eigenvalue, the associated eigenvector isn't necessarily unique.
Furthermore, round-off error shouldn't be so significant for a matrix that actually isn't that big, or contain numbers that aren't very small.
For comparison, I ran your test matrix through a JavaScript version of RG.F (Real General, from the EISPACK Library):  Eigenvalues and Eigenvectors Calculator
Here is the output:
Eigenvalues:
   20
   12
   20
   20
   20
   20
   20
   20

Eigenvectors:
 0.9354143466934854     0.35355339059327395     -0.021596710639534     -0.021596710639534     -0.021596710639534     -0.021596710639534     -0.021596710639533997     -0.021596710639533997
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999622     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999622

No imaginary components.
To confirm, or deny, the validity of results, you could always write a small program that plugs the results back into the original equation. Simple matrix and vector multiplication. Then you'd know for sure whether or not the outputs are correct. Or, if they are wrong, how far away from correct answers they are.
",,
SciPy strange behavior,https://stackoverflow.com/questions/25752444,"Scipy error: numpy.dtype size changed, may indicate binary incompatibility (and associated strange behavior)","I am installing numpy/scipy/scikit-learn on OS X 10.9.4, and am getting errors about ""numpy.dtype size changed, may indicate binary incompatibility"".

Here's what I did to construct the repo:

mkvirtualenv thm
workon thm
pip install numpy scipy pandas ipython # and some other stuff
cd /path/to/our/repo
# run tests


Here's a traceback excerpt of a relevant warning (turned into an error because we use warnings.simplefilter('error') at the beginning of our tests):

======================================================================
ERROR: Failure: RuntimeWarning (numpy.dtype size changed, may indicate binary in
compatibility)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/nose/loader.py"",
 line 414, in loadTestsFromName
    addr.filename, addr.module)
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/nose/importer.py
"", line 47, in importFromPath
    return self.importFromDir(dir_path, fqname)
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/nose/importer.py
"", line 94, in importFromDir
    mod = load_module(part_fqname, fh, filename, desc)
  File ""/Users/ben/code/thm/alpha/prosper/base/stats/test_auc.py"", line 3, in &lt;m
odule&gt;
    import sklearn.metrics
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/sklearn/metrics/
__init__.py"", line 6, in &lt;module&gt;
    from .metrics import (accuracy_score,
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/sklearn/metrics/metrics.py"", line 27, in &lt;module&gt;
    from scipy.spatial.distance import hamming as sp_hamming
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/scipy/spatial/__init__.py"", line 90, in &lt;module&gt;
    from .ckdtree import *
  File ""__init__.pxd"", line 155, in init scipy.spatial.ckdtree (scipy/spatial/ckdtree.c:20570)
RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility


I'm told that this warning is caused by scipy being compiled against a different version of numpy than the one installed. However, I installed them all with pip in what I thought was a pretty standard way, so this shouldn't be a problem, I would think.

Weirdly, although running our entire test suite as a whole (via python -m unittest discover) gives these errors, running the individual tests (via python -m unittest &lt;module&gt;) works fine.

According to the tests, here's some relevant version info:

numpy version 1.9.0 (rev 07601a64cdfeb1c0247bde1294ad6380413cab66)
scipy version 0.14.0 (built against numpy 1.9.0)
sklearn version 0.15.2
pandas version 0.14.1


Happy to provide more info on request!
",10,16174,"How did you build sklearn 0.14.1? Did you build it against the same version of numpy as you did for scipy?

Recent versions of scikit-learn, scipy and numpy have prebuilt-packages. In particular scikit-learn 0.15.2 should be binary compatible with numpy 1.7+. I think the same is true with scipy 0.14.0 but you said you built it yourself from source, which is not what pip should do by default (it should just install the prebuilt whl package).

Edit: have you tried to do:

pip install -U scipy scikit-learn pandas


to make sure that you are using the latest stable versions of the whl for those packages?

Edit: The comment below has the actual answer that works and is presumably why this answer was accepted.  Namely:

pip uninstall -y scipy scikit-learn
pip install --no-binary scipy scikit-learn

",,
SciPy strange behavior,https://stackoverflow.com/questions/17130795,Periodogram in Octave/Matlab vs Scipy,"I am porting some matlab code to python using scipy and got stuck with the following line:

Matlab/Octave code

[Pxx, f] = periodogram(x, [], 512, 5)


Python code

f, Pxx = signal.periodogram(x, 5, nfft=512)


The problem is that I get different output on the same data. More specifically, Pxx vectors are different. I tried different windows for signal.periodogram, yet no luck (and it seems that default scypy's boxcar window is the same as default matlab's rectangular window) Another strange behavior is that in python, first element of Pxx is always 0, no matter what data input is.

Am i missing something? Any advice would be greatly appreciated!  



Simple Matlab/Octave code with actual data: http://pastebin.com/czNeyUjs
Simple Python+scipy code with actual data: http://pastebin.com/zPLGBTpn
",10,4456,"After researching octave's and scipy's periodogram source code I found that they use different algorithm to calculate power spectral density estimate. Octave (and MATLAB) use FFT, whereas scipy's periodogram use the Welch method. 

As @georgesl has mentioned, the output looks quite alike, but still, it differs. And for porting reason it was critical. In the end, I simply wrote a small function to calculate PSD estimate using FFT, and now output is the same. According to timeit testing, it works ~50% faster (1.9006s vs 2.9176s on a loop with 10.000 iterations). I think it's due to the FFT being faster than Welch in scipy's implementation, of just being faster. 

Thanks to everyone who showed interest.
","I faced the same problem but then I came across the documentation of scipy's periodogram  

As you would see there that detrend='constant' is the default argument. This means that python automatically subtracts the mean of the input data from each point. (Read here). While Matlab/Octave do no such thing. I believe that is the reason why the outputs are different. Try specifying detrend=False, while calling scipy's periodogram you should get the same output as Matlab.
","After reading the Matlab and Scipy documentation, another contribution to the different values could be that they use different default window function. Matlab uses a Hamming window, and Scipy uses a Hanning. The two window functions and similar but not identical.
"
SciPy strange behavior,https://stackoverflow.com/questions/20037444,scipy.signal.resample behaves strangely,"I am currently working on some signal processing (using scipy), but I encountered a strange problem and can't figure out what's wrong. Namely, I am reading some audio data from a .wav file, but have to resample before further processing. The signal has more than 500,000 samples.

Now, scipy.signal.resample takes more than 10 minutes on just one of the channels. OK, I thought, this might be normal because there are a lot of samples. However, then I decided to experiment with two other ""signals"" (i.e. a randomly generated array of numbers and an array of zeros) with 1,000,000 samples and resample these ones. Strangely, resampling in this case takes only a few milliseconds, so the size is obviously not a problem.

My final experiment was extracting the zeros from my original signal (there are about 50,000 samples that are zero-valued) and resampling them. I was totally surprised to see that resampling only 50,000 zeros takes about a minute. Previously, I resampled an array of zeros that had 1,000,000 samples in a few milliseconds and now I have to wait about a minute for an array of 50,000 samples. Something has to be wrong, but I can't figure out what.

I really don't see any reason for this behavior; especially the zeros (1,000,000 and just a few milliseconds vs 50,000 and a minute) surprise me a lot.

Here's a sample code, so that you know what I'm talking about:

import scipy.io.wavfile as wavfile
import numpy
import scipy.signal as signal

sample_rate, signal_data = wavfile.read('file.wav')

test_channel = numpy.array(signal_data[:,0], dtype=float)
channel_zeros = numpy.array(signal_data[numpy.where(signal_data[:,0]==0)[0],0], dtype=float)
test_signal = numpy.random.rand((1000000))
test_signal_2 = numpy.zeros((1000000))

number_of_samples = 500

#both of these are executed in less than a second
resampled_random = signal.resample(test_signal, number_of_samples)
resampled_zeros = signal.resample(test_signal_2, number_of_samples)

#this takes minutes
resamples_original_signal = signal.resample(test_channel, number_of_samples)

#this takes about a minute
resampled_original_zeros = signal.resample(channel_zeros, number_of_samples)


Do you have any idea what might be wrong with this? Thanks in advance.
",9,3527,"The numpy implementation of FFT (based on FFTPACK) is fastest when the length of the data is a power of 2 (e.g. 2, 4, 8, 16, 32) and slowest when it is a prime. To speed up processing of the signal, you can zero-pad the data to a power of 2 length.

In Python you can use the following code to find the next largest power of 2 for a given number:

y = np.floor(np.log2(n))
nextpow2 = np.power(2, y+1)


You can use this with numpy.pad to pad your data array to this size:

sample_rate, signal_data = wavfile.read('file.wav')
n = signal_data.shape[0]

y = np.floor(np.log2(n))
nextpow2  = np.power(2, y+1)

signal_data  = np.pad(signal_data , ((0, nextpow2-n), (0,0)), mode='constant')


For more background on scipy/numpy and FFT in general in see this question.
",,
SciPy strange behavior,https://stackoverflow.com/questions/37592643,Scipy interpolate returns a &#39;dimensionless&#39; array,"I understand that interp1d expects an array of values to interpolate, but the behavior when passing it a float is strange enough to ask what is going on and what exactly is being returned

import numpy as np
from scipy.interpolate import interp1d

x = np.array([1,2,3,4])
y = np.array([5,7,9,15])
f = interp1d(x,y, kind='cubic')
a = f(2.5)

print(repr(a))
print(""type is {}"".format(type(a)))
print(""shape is {}"".format(a.shape))
print(""ndim is {}"".format(a.ndim))
print(a)


Output:

array(7.749999999999992)
type is &lt;class 'numpy.ndarray'&gt;
shape is ()
ndim is 0
7.749999999999992


EDIT: To clarify, I would not expect numpy to even have a dimensionless, shapeless array much less a scipy function return one. 

print(""Numpy version is {}"".format(np.__version__))
print(""Scipy version is {}"".format(scipy.__version__))

Numpy version is 1.10.4
Scipy version is 0.17.0

",4,3305,"The interp1d returns a value that matches the input in shape - after wrapping in np.array() if needed:

In [324]: f([1,2,3])
Out[324]: array([ 5.,  7.,  9.])

In [325]: f([2.5])
Out[325]: array([ 7.75])

In [326]: f(2.5)
Out[326]: array(7.75)

In [327]: f(np.array(2.5))
Out[327]: array(7.75)


Many numpy operations do return scalars instead of 0d arrays.

In [330]: np.arange(3).sum()
Out[330]: 3


though actually it returns a numpy object

In [341]: type(np.arange(3).sum())
Out[341]: numpy.int32


which does have a shape () and ndim 0.

Whereas interp1d returns an array. 

In [344]: type(f(2.5))
Out[344]: numpy.ndarray


You can extract the value with [()] indexing

In [345]: f(2.5)[()]
Out[345]: 7.75

In [346]: type(f(2.5)[()])
Out[346]: numpy.float64


This may just be an oversight in the scipy code.  How often do people want to interpolate at just one point?  Isn't interpolating over a regular grid of points more common?

==================

The documentation for f.__call__ is quite explicit about returning an array.

Evaluate the interpolant

Parameters
----------
x : array_like
    Points to evaluate the interpolant at.

Returns
-------
y : array_like
    Interpolated values. Shape is determined by replacing
    the interpolation axis in the original array with the shape of x.


===============

The other side to the question is why does numpy even have a 0d array.  The linked answer probably is sufficient.  But often the question is asked by people who are used to MATLAB.  In MATLAB nearly everything is 2d.  There aren't any (true) scalars.  Now MATLAB has structures and cells, and matrices with more than 2 dimensions.  But I recall a time (in the 1990s) when it didn't have those.  Everything, literal, was a 2d matrix.

The np.matrix approximates that MATLAB case, fixing its arrays at 2d. But it does have a _collapse method that can return a 'scalar'.
",,
SciPy strange behavior,https://stackoverflow.com/questions/27004245,Numpy array bug or feature (catsing to int behind the scenes)?,"Today I have noticed strange behavior of Numpy/Scipy arrays. It looks like adding array cell with integer inside to float can have two different results, depending on the variable to which the result is assigned. Instead of long explanations, below I present the code:

import scipy as sp
array_int = sp.array([[0], [0]])
float_operand = 0.1
print array_int[0, 0] + float_operand #gives 0.1


But

import scipy as sp
array_int = sp.array([[0], [0]])
float_operand = 0.1
array_int[0, 0] = array_int[0, 0] + float_operand 
print array_int[0, 0] #gives 0


I could understand if this kind of behavior was inherited from Python, but:

In contrary to behavior of ""bare"" Python (2.7):

integer_for_sure = int(0) 
integer_for_sure = integer_for_sure + 0.1
print integer_for_sure #gives 0.1 as expected


Is this kind of feature somwhere documented? Has anybody encounterd it before?
",3,135,"Henry Keiter has explained it well enough. I would only add one technical detail.

In contrast to the regular assignment which simply rewires integer_for_sure to refer to the float object that results from integer_for_sure + 0.1, thus changing the type of the variable, assignment to array elements such as

array_int[0, 0] = array_int[0, 0] + float_operand


is actually syntactic sugar for the more verbose

array_int.__setitem__((0,0), array_int.__getitem__((0,0)) + float_operand)


(this applies to old-style classes; it looks a bit different for new-style classes but the idea stays the same)

The __setitem__ method for each array type performs a typecast of its value argument to the type of the array. The actual C code that implements the assignment is kind of ugly and involves a custom preprocessor.

On the other side

print array_int[0, 0] + float_operand


is

print array_int.__getitem__((0,0)) + float_operand


i.e. it fetches the integer value from array_int, sums it with float_operand and the resulting float object is passed to print. There is no intermediate assignment to array_int[0, 0] and therefore no typecast.
","This is not a behavior ""inherited from Python"" -- as you can see, adding a float to an int in pure Python produces a float result. Rather, you can think of this behavior as ""inherited from C."" Unlike Python lists, numpy arrays have strong element types. The array constructor includes a telltale optional keyword argument that alludes to this:

dtype : data-type, optional
The desired data-type for the array. If not given, then the type will be determined as the minimum type required to hold the objects in the sequence. This argument can only be used to upcast the array.

The emphasis is mine. When you create an array with np.array([[0], [0]]), you get a two-dimentional integer array, because an integer is the smallest datatype that can contain 0. Once the integer array is created, it may only contain integers. If you attempt to insert a float, it will be cast to an integer as you noticed in order to be placed in the array.

If you want to store floats in the array eventually, you need to initialize your array as a float array ('upcast' it). This can be accomplished by using the dtype argument mentioned above, or simply by putting a float value into the initial array (e.g. 0.0 instead of the integer 0).
import scipy as sp
array_int = sp.array([[0], [0]])
array_float = sp.array([[0.0], [0]])  # Note the extra .0 to make one element a float

array_int[0, 0] = array_int[0, 0] + 0.1
print array_int[0, 0] # 0

array_float[0, 0] = array_float[0, 0] + 0.1
print array_float[0, 0] # 0.1

",
SciPy strange behavior,https://stackoverflow.com/questions/57477723,Unexpected behaviour of scipy.integrate,,1,1213,"It is not a bug, it has to do with the numerical precision of the integration, and the fact that you are integrating a function that is (almost) 0 in most of the interval.
From the docs:


  Be aware that pulse shapes and other sharp features as compared to the
  size of the integration interval may not be integrated correctly using
  this method.


Based on your output, the function is using only two (last=2) intervals, evaluating values for rlist=(2.06145321e-045, 0.00000000e+000, ..) on each (see the docs for more detail on the output)

You can add points to the interval to force the routine to use points closer to the left limit.

a = quad(lambda x: np.exp(-x), 0, 1e9, points=np.logspace(-10,3,10))
print(a)
(0.9999999999999997, 2.247900608926337e-09)


Adding to the explanation (thanks to @norok2): Note that points is a sequence of break points in the bounded integration interval where local difficulties of the integrand may occur (e.g., singularities, discontinuities). In this case, I'm not using it to point out discontinuities, but rather to force quad to perform more integration steps near the left boundary, using a log-spaced interval since a I have an exponential function (this is of course arbitrary and for this function, since I know its shape).
","There is no need to convert your integral into one over a (very large) interval. There is a specific integration scheme for integrals of the form

,

namely Gauss-Laguerre quadrature. It's also included in quadpy (a project of mine). Simply try out

import numpy
import quadpy

scheme = quadpy.e1r.gauss_laguerre(1)

val = scheme.integrate(lambda x: numpy.ones(x.shape[1:]))

print(val)


1.0

",
SciPy strange behavior,https://stackoverflow.com/questions/65513251,Scipy coo_matrix.max() alters data attribute,"I am building a recommendation system using an open source library, LightFM. This library requires certain pieces of data to be in a sparse matrix format, specifically the scipy coo_matrix. It is here that I am encountering strange behavior. It seems like a bug, but it's more likely that I am doing something wrong.
Basically, I let LightFM.Dataset build me a sparse matrix, like so:
interactions, weights = dataset.build_interactions(data=_get_interactions_data())

The method, build_interactions, returns ""Two COO matrices: the interactions matrix and the corresponding weights matrix"" -- LightFM Official Doc.
When I inspect the contents of this sparse matrix (in practice, I use a debugger), like so:
for i in interactions.data:
    print(i, end=', ')

1, 1, 1, 1, 1, ....
It prints a long list of 1s, which indicates that the sparse matrix's nonzero elements are only 1s.
However, when I first check the max of the sparse matrix, it indicates that the maximum values in the sparse matrix is not a 1, its a 3. Furthermore, printing the matrix after that check will print a long list of 1s, 2s, and 3s. This is the code for that:
print(interactions.max())
for i in interactions.data:
    print(i, end=', ')

3 
1, 1, 3, 2, 1, 2, ...
Any idea what is going on here? Python is 3.6.8. Scipy is 1.5.4. CentOS7.
Thank you.
",1,199,"A 'raw' coo_matrix can have duplicate elements (repeats of the same row and col values), but when converted to csr format for calculations those duplicates are summed. It must be doing the same, but in-place, in order to find that max.
In [9]: from scipy import sparse
In [10]: M = sparse.coo_matrix(([1,1,1,1,1,1],([0,0,0,0,0,0],[0,0,1,0,1,2])))
In [11]: M.data
Out[11]: array([1, 1, 1, 1, 1, 1])
In [12]: M.max()
Out[12]: 3
In [13]: M.data
Out[13]: array([3, 2, 1])
In [14]: M
Out[14]: 
&lt;1x3 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    with 3 stored elements in COOrdinate format&gt;

Tracing through the max code I find it uses sum_duplicates
In [33]: M = sparse.coo_matrix(([1,1,1,1,1,1],([0,0,0,0,0,0],[0,0,1,0,1,2])))
In [34]: M.data
Out[34]: array([1, 1, 1, 1, 1, 1])
In [35]: M.sum_duplicates?
Signature: M.sum_duplicates()
Docstring:
Eliminate duplicate matrix entries by adding them together

This is an *in place* operation
File:      /usr/local/lib/python3.8/dist-packages/scipy/sparse/coo.py
Type:      method
In [36]: M.sum_duplicates()
In [37]: M.data
Out[37]: array([3, 2, 1])

",,
SciPy strange behavior,https://stackoverflow.com/questions/58164513,Problems with bisplrep and bisplev from scipy.interpolate,"I receive some quite strange behavior with bisplrep and bisplev from the scipy.interpolate module. I try to reproduce the example from the scipy homepage (https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html). I have some function f(x,y) which calculates me some z-values which I then interpolate using bisplrep. If I recalculate the data using bisplev and plot the data the resulting values appear rotated about 90. Even if I exchange the x and y values the plot is still rotated. Can somebody tell me if I am doing something completely wrong here? The following code should be sufficient to reproduce the error. I am using the most recent version of scipy and the error occurs in Jupyter, Spyder and in IDLE. 

import matplotlib.pyplot as plt
import numpy as np
from scipy import interpolate

def f(x, y):
    return x**2 + y**2

x, y = np.linspace(0, 5, 15), np.linspace(-2*np.pi, 2*np.pi, 15)
xx, yy = np.meshgrid(x, y)
zz = f(xx, yy)

tck = interpolate.bisplrep(xx, yy, zz)

plt.pcolor(xx, yy, zz)

x_new, y_new = np.linspace(0, 5, 100), np.linspace(-2*np.pi, 2*np.pi, 100)

z_new = interpolate.bisplev(x_new, y_new, tck)

plt.figure()    
plt.pcolor(x_new, y_new, z_new)

plt.figure()    
plt.pcolor(y_new, x_new, z_new)

plt.show()

",1,1443,"I used another definition for the grid. Now, it should work:

import matplotlib.pyplot as plt
import numpy as np
from scipy import interpolate

def f(x, y):
    return x**2 + y**2

#x, y = np.linspace(0, 5, 15), np.linspace(-2*np.pi, 2*np.pi, 15)
xx, yy = np.mgrid[0:5:15j, -2*np.pi:2*np.pi:15j]
zz = f(xx, yy)

tck = interpolate.bisplrep(xx, yy, zz)

plt.pcolor(xx, yy, zz)

#x_new, y_new = np.linspace(0, 5, 100), np.linspace(-2*np.pi, 2*np.pi, 100)
xx_new, yy_new = np.mgrid[0:5:100j, -2*np.pi:2*np.pi:100j]

zz_new = interpolate.bisplev(xx_new[:,0], yy_new[0,:], tck)

plt.figure()    
plt.pcolor(xx_new, yy_new, zz_new)

plt.figure()    
plt.pcolor(yy_new, xx_new, zz_new)

plt.show()

",,
SciPy strange behavior,https://stackoverflow.com/questions/31685156,Normalizing vector produces nan in Numpy,"I'm getting some strange behavior from scipy/numpy that I suspect is a bug but someone may know better?  I've got a pair of long arrays which I'm breaking into frames which are of length 2-4 for debugging purposes.  I want to normalize each pair of frames and take the dot product.  The code that does it (with some debugging output) is:

   tf = numpy.copy(t_frame) / norm(t_frame)
   pf = numpy.copy(p_frame) / norm(p_frame)
   print ""OPF:""
   print p_frame
   print ""PF: ""
   print pf
   print ""TF norm is: "" + str(norm(tf))
   print ""PF norm is: "" + str(norm(pf))
   print numpy.dot(tf, pf)
   return numpy.dot(tf, pf)


This does what I'd expect for a while (specifically giving a norm of 1 for tf and pf) but then I start to see lines like this:


  OPF:
  
  [ -91 -119 -137 -132]
  
  PF: 
  
  [ nan  nan  nan  nan]


What??  This can be normalized fine in a new Python window:

&gt;&gt;&gt; p = [ -91, -119, -137, -132] 
&gt;&gt;&gt; p / norm(p)
array([-0.37580532, -0.49143773, -0.56577285, -0.54512421])


For what it's worth, I've tried numpy.linalg.norm, scipy.linalg.norm, and defining a function to return the square root of the dot product.

Any ideas?

UPDATE:
Thanks for the suggestions!  I tried switching the dtype to float128 and am sadly getting similar behavior.  I'm actually inclined to believe that it's a bug in Python rather than numpy at this point:


If it were a straightforward overflow issue, it seems like I'd get it consistently with a given list.  But the norm computes fine if I do it in a new python session.
I tried rolling my own:

def norm(v):
   return (  sum(numpy.array(v)*numpy.array(v)))**(0.5)


This only uses numpy  to represent the arrays.  I still get the same issue, but later in the data set (and no runtime warnings).  It's doing about 37000 of these computations.
I'm actually computing the norm on two frames, a t_frame and a p_frame.  The computation of one chokes if and only if the computation for the other one does.


Put together, I think there's some weird buffer overflow somewhere in the bowels of Python (2.7.9)???  I ultimately need these computations to be fast as well; so I'm thinking of just switching over to Cython for that computation.

Update 2:
I tried really rolling my own:

def norm(v):
  sum = float(0)
  for i in range(len(v)):
    sum += v[i]**2
  return sum**(0.5)


and the problem disappears.  So I would guess that it is a bug in numpy (1.9.0 on Gentoo Linux).
",1,5875,"It looks like this is a bug in numpy.  I can reproduce the problem if the data type of the array is np.int16:

In [1]: np.__version__
Out[1]: '1.9.2'

In [2]: x = np.array([ -91, -119, -137, -132], dtype=np.int16)

In [3]: x
Out[3]: array([ -91, -119, -137, -132], dtype=int16)

In [4]: np.linalg.norm(x)
/Users/warren/anaconda/lib/python2.7/site-packages/numpy/linalg/linalg.py:2061: RuntimeWarning: invalid value encountered in sqrt
  return sqrt(sqnorm)
Out[4]: nan


The problem also occurs in the master branch of the development version of numpy.  I created an issue here: https://github.com/numpy/numpy/issues/6128

If p_frame is, in fact, a 16 bit integer array, a simple work-around is something like:

x = np.asarray(p_frame, dtype=np.float64)
pf = x / norm(x)

","Following one of Warren's links, I get this warning:

In [1016]: np.linalg.norm(100000*np.ones(2).astype('int16'))
/usr/local/lib/python2.7/site-packages/numpy/linalg/linalg.py:2051: RuntimeWarning: invalid value encountered in sqrt
  return sqrt(add.reduce((x.conj() * x).real, axis=None))


For this x2, the inner expression is negative - the result of overflow in a small dtype.

In [1040]: x2=100000*np.ones(2).astype('int16')
In [1041]: np.add.reduce((x2.conj()*x2).real,axis=None)
Out[1041]: -1474836480


similarly with an x1:

In [1042]: x1
Out[1042]: array([ -9100, -11900, -13700, -13200], dtype=int16)
In [1043]: np.add.reduce((x1.conj()*x1).real,axis=None)
Out[1043]: -66128


If the sum of the 'dot' becomes too large for the dtype, it can be negative, producing a nan when passed through sqrt.

(I'm using 1.8.2 and 1.9.0 under linux).
",
SciPy strange behavior,https://stackoverflow.com/questions/77227994,Parallel scipy milp optimizing with multiprocessing,"I want to solve a linear programming problem using the scipy. Everything works in one process. If you try to do this in several processes, then everything works too.
However, if you first do optimization using multiprocessing, then without it, and then with it, then everything hangs. Any idea why?
if you run the code below, it will run for an infinite time:
import numpy as np
from multiprocessing import Pool

from scipy.optimize import LinearConstraint, milp

def task(identifier):
    c = -np.array([1., 2])
    A = np.array([[-1., 1], [-3, 2], [2, 3]])
    b_u = np.array([1., 12, 12])
    b_l = np.full_like(b_u, -np.inf)

    constraints = LinearConstraint(A, b_l, b_u)
    integrality = np.ones_like(c)

    res = milp(c=c, constraints=constraints, integrality=integrality)
    return res.x


with Pool() as pool:
    for result in pool.map(task, range(3)):
        print(f'Before: Got result: {result}')

print(task(4)) # if you comment out - everything works, otherwise it hangs

with Pool() as pool:
    for result in pool.map(task, range(3)):
        print(f'After: Got result: {result}')

Python version and Libraries:
Python 3.10.11, Numpy 1.24.2, SciPy 1.11.3
Who knows why such strange behavior and how it can be fixed?
",0,68,"I was able to run your code from the command line with a script containing:
import numpy as np
from multiprocessing import Pool

from scipy.optimize import LinearConstraint, milp


def task(identifier):
    c = -np.array([1.0, 2])
    A = np.array([[-1.0, 1], [-3, 2], [2, 3]])
    b_u = np.array([1.0, 12, 12])
    b_l = np.full_like(b_u, -np.inf)

    constraints = LinearConstraint(A, b_l, b_u)
    integrality = np.ones_like(c)

    res = milp(c=c, constraints=constraints, integrality=integrality)
    return res.x


if __name__ == ""__main__"":
    with Pool() as pool:
        for result in pool.map(task, range(3)):
            print(f""Before: Got result: {result}"")

    print(task(4))  # if you comment out - everything works, otherwise it hangs

    with Pool() as pool:
        for result in pool.map(task, range(3)):
            print(f""After: Got result: {result}"")

Result:
Before: Got result: [3. 2.]
Before: Got result: [3. 2.]
Before: Got result: [3. 2.]
[3. 2.]
After: Got result: [3. 2.]
After: Got result: [3. 2.]
After: Got result: [3. 2.]

If you are trying to run your code within Jupyter/IPython, there are known complications with using multiprocessing in these environments. Take a look at this question for some possible solutions.
",,
SciPy strange behavior,https://stackoverflow.com/questions/51089817,scipy rankdata with masked array,"I notice the following strange behavior with rankdata with maksed_array. Here is the code:

import numpy as np
import scipy.stats as stats

m = [True, False]
print(stats.mstats.rankdata(np.ma.masked_array([1.0, 100], mask=m)))
# result [0. 1.]

print(stats.mstats.rankdata(np.ma.masked_array([1.0, np.nan], mask=m)))
# result [1. 0.]

print(stats.mstats.rankdata([1.0, np.nan]))
# result [1. 2.]


According the scipy doc, masked values will be assigned 0 (use_missing=False). So why it outputs [1 0] in the 2nd one? Bug?
",0,336,"After tracing I find it is related to the argsort method of masked_array. When mstats.rankdata calls argsort, it does not specify fill_value, and endwith input parameters, which defaults to np.nan and True respectively. Based on the following code from numpy, the fill_value is np.nan.

if fill_value is None:
    if endwith:
        # nan &gt; inf
        if np.issubdtype(self.dtype, np.floating):
            fill_value = np.nan


So in the case of masked_array of [1, 100], it is argsorting [nan, 100], which is [1, 0]. In the case of masked_array of [1, np.nan], it is argsoring [nan, nan], which can be [0,1]. Then in the rankdata function, it assume first n (n=1) from argsort is valid, which is not correct here.

n = data.count()
rk = np.empty(data.size, dtype=float)
idx = data.argsort()
rk[idx[:n]] = np.arange(1,n+1)

",,
SciPy strange behavior,https://stackoverflow.com/questions/23897118,pylab.plot &quot;cannot convert float NaN to integer&quot; after calling scipy.stats.multivariate_normal,"While testing a regression algorithm I found this strange behavior: for some covariance matrices, the multivariate_normal function gives correct samples but then an exception is raised (only) the first time pylab.plot() is called:

ValueError: cannot convert float NaN to integer

The following code reproduces the error:
import numpy as np
from scipy.stats import multivariate_normal as mnorm
from matplotlib import pyplot as plt

B = np.array([ 0, 0, 0])

# works fine
v1 = np.array([[1, 0, 0],
              [0, 1, 0],
              [0, 0, 1]])


# OK. non positive semidefinite, well raised exception
v2 = np.array([[ 0.2 , -0.2, -0.3],
              [-0.2,  0.4, -0.9],
              [-0.3, -0.9,  0.7]])

# KO. exception (?)
v3 = np.array([[ 0.2 , -0.02, -0.026],
              [-0.02,  0.014, -0.009],
              [-0.026, -0.009,  0.017]])



w = mnorm(mean=B, cov=v3).rvs()
print w

plt.plot(w)
plt.show()

And if plt.plot(w) is called a second time, then it works. Any ideas?
Versions:

python 2.7.5 Anaconda 1.9.1 (64-bit)
scipy 0.14.0
matplotlib 1.3.1
numpy 1.8.1

",0,1457,"Well, it works fine here, and says :

[-0.72849048  0.15439657  0.00146853]


and shows :



I use python 2.7.6

other packages are same as yours.

Hope it helped. Good luck !
",,
SciPy strange result,https://stackoverflow.com/questions/17097236,Replace invalid values with None in Pandas DataFrame,"Is there any method to replace values with None in Pandas in Python?

You can use df.replace('pre', 'post') and can replace a value with another, but this can't be done if you want to replace with None value, which if you try, you get a strange result.

So here's an example:

df = DataFrame(['-',3,2,5,1,-5,-1,'-',9])
df.replace('-', 0)


which returns a successful result.

But,

df.replace('-', None)


which returns a following result:

0
0   - // this isn't replaced
1   3
2   2
3   5
4   1
5  -5
6  -1
7  -1 // this is changed to `-1`...
8   9


Why does such a strange result be returned?

Since I want to pour this data frame into MySQL database, I can't put NaN values into any element in my data frame and instead want to put None. Surely, you can first change '-' to NaN and then convert NaN to None, but I want to know why the dataframe acts in such a terrible way.


  Tested on pandas 0.12.0 dev on Python 2.7 and OS X 10.8. Python is a
  pre-installed version on OS X and I installed pandas by using SciPy
  Superpack script, for your information.

",119,291916,"With Pandas version 1.0.0, I would use DataFrame.replace or Series.replace:
df.replace(old_val, pd.NA, inplace=True)

This is better for two reasons:

It uses pd.NA instead of None or np.nan.
It optionally works in-place which could be more memory efficient depending upon the internal implementation.

","Alternatively you can also use mask:
df.mask(df=='-', None)

","df.replace('-', np.nan).astype(""object"")


This will ensure that you can use isnull() later on your dataframe
"
SciPy strange result,https://stackoverflow.com/questions/25328818,python 2.7: cannot pip on windows &quot;bash: pip: command not found&quot;,"I am trying to install the SciPy stack located at https://scipy.org/stackspec.html [I am only allowed 2 links; trying to use them wisely].  I realize that there are much easier ways to do this, but I think there is a lot to be learned by doing it manually.  I am relatively new to a lot of this stuff, so I apologize if I sound ignorant at any point.
I am running  Windows 7 Enterprise - 64 bit.  Here is what I have done so far:

Installed python-2.7.8.msi (32-bit) from https://www.python.org/download/releases/2.7.8/

Installed numpy-1.8.1-win32-superpack-python2.7 from
http://sourceforge.net/projects/numpy/files/
Test: import numpy as np ---&gt; no errors

Installed scipy library,
scipy-0.14.0-win32-superpack-python2.7.exe from
(SCIPY DOT ORG LINK REMOVED)
Test: import scipy as sp ---&gt; no errors

Installed matplotlib: matplotlib-1.3.1.win32-py2.7.exe from
(MATPLOTLIB DOT ORG LINK REMOVED)

Installed PIP by running script here:
https://raw.githubusercontent.com/pypa/pip/master/contrib/get-pip.py
I just copied-pasted script to a new file in IDLE,
saved as C:\Python27\Scripts\pip_install.py and clicked Run&gt;module. No errors reported.


Does the path on which I saved
pip_install.py matter?




HERE IS WHERE I FAIL
Attempted to install matlibplot dependency dateutil: Opened a
Cygwin Shell, and typed
        cd C:\Python27          ! is it necessary to cd to python directtory?
        pip install python-dateutil

This results in the error:
    bash: pip: command not found

I get the same error attempting from cmd.
Any help  is appreciated; the closest I found was bash: pip: command not found.  But the OSX nature of it is just enough to confise me further.

UPDATE:
I added the pip-path per Paul H's suggestion below.  It made the error go away, but strangely, nothing I pip actually installs. For example, in Cygwin, I type:
cbennett2&gt; pip install python-dateutil
cbennett2&gt;                            

You can see that there is no output or feedback from the shell (which I think there should be).  Then when I go to a new python shell:
&gt;&gt;&gt; from dateutil.parser import parse
Traceback (most recent call last):
  File ""&lt;pyshell#12&gt;"", line 1, in &lt;module&gt;
    from dateutil.parser import parse
ImportError: No module named dateutil.parser
&gt;&gt;&gt;&gt;

This happens with all of the modules that I thought I had pip'd ... pandas, tornado, etc.
",45,166470,"The problem is that your Python version and the library you want to use are not same versionally (Python). Even if you install Python's latest version, your PATH might not change properly and automatically. Thus, you should change it manually.After matching their version, it will work.

Ex: When I tried to install Django3, I got same error. I noticed that my PATH still seems C:\python27\Scripts though I already install Python3.8, so that I manually edited my PATH C:\python38\Scripts and reinstalled pip install Django and everything worked well. 
",,
SciPy strange result,https://stackoverflow.com/questions/65585639,Eigenvalues in Python: A Bug?,,10,1316,"As far as I know, assumption 1 is correct, but assumption 2 is not.
A Real Symmetric matrix produces eigenvalues and eigenvectors that are real only.
However, for a given eigenvalue, the associated eigenvector isn't necessarily unique.
Furthermore, round-off error shouldn't be so significant for a matrix that actually isn't that big, or contain numbers that aren't very small.
For comparison, I ran your test matrix through a JavaScript version of RG.F (Real General, from the EISPACK Library):  Eigenvalues and Eigenvectors Calculator
Here is the output:
Eigenvalues:
   20
   12
   20
   20
   20
   20
   20
   20

Eigenvectors:
 0.9354143466934854     0.35355339059327395     -0.021596710639534     -0.021596710639534     -0.021596710639534     -0.021596710639534     -0.021596710639533997     -0.021596710639533997
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999622     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999622

No imaginary components.
To confirm, or deny, the validity of results, you could always write a small program that plugs the results back into the original equation. Simple matrix and vector multiplication. Then you'd know for sure whether or not the outputs are correct. Or, if they are wrong, how far away from correct answers they are.
",,
SciPy strange result,https://stackoverflow.com/questions/34890585,In scipy why doesn&#39;t idct(dct(a)) equal to a?,"I am trying to implement JPEG compression using python. When I tried to apply the DCT, quantization, IDCT process for a tiff image, I found something strange for scipy.fftpack.dct/idct.

Since there is only 1D dct/idct within scipy package, I was doing this for a 2D dct

import numpy as np
from scipy.fftpack import dct, idct

def dct2(block):
    return dct(dct(block.T).T)

def idct2(block):
    return idct(idct(block.T).T)


I tested the 2D dct/idct using a simple 3x3 matrix. I was expecting to get a True matrix with this test case.

a = np.random.randint(0,255,9).reshape(3,3)
print a == idct2(dct2(a))


However it turned out that after idct2(dct2(a)) the result was scaled by a constant factor compared with the original a matrix.

I would like to ask if there is a way to implement a set of 2D dct/idct such that after a idct(dct(a)) operation I can get the same output as the input.
",10,7085,"You need to normalize idct2(dct2(a)) and convert the data type to uint8. One option is to use cv2 for this normalization:
import numpy as np
from scipy.fftpack import dct, idct

def dct2(block):
    return dct(dct(block.T).T)

def idct2(block):
    return idct(idct(block.T).T)
a = np.random.randint(0,255,9).reshape(3,3)

# convert to uint8
norm_image = cv2.normalize(idct2(dct2(a)), None, 
                           alpha = a.min(), beta = a.max(), 
                           norm_type = cv2.NORM_MINMAX, 
                           dtype = cv2.CV_32F).astype(np.uint8)
print(a == norm_image)
# [[ True  True  True]
#  [ True  True  True]
#  [ True  True  True]]

",,
SciPy strange result,https://stackoverflow.com/questions/16746974,Scipy Sparse Matrix - Dense Vector Multiplication Performance - Blocks vs Large Matrix,,7,3319,,,
SciPy strange result,https://stackoverflow.com/questions/64324685,Why my PCA is not invariant to rotation and axis swap?,"I have a voxel (np.array) with size 3x3x3, filled with some values, this setup is essential for me. I want to have rotation-invariant representation of it. For this case, I decided to try PCA representation which is believed to be invariant to orthogonal transformations. another
For simplicity, I took some axes swap, but in case I'm mistaken there can be np.rot90.
I have interpereted my 3d voxels as a set of weighted 3d cube point vectors which I incorrectly called ""basis"", total 27 (so that is some set of 3d point in space, represented by the vectors, obtained from cube points, scaled by voxel values).
import numpy as np

voxel1 = np.random.normal(size=(3,3,3))
voxel2 =  np.transpose(voxel1, (1,0,2)) #np.rot90(voxel1) #


basis = []
for i in range(3):
    for j in range(3):
        for k in range(3):
            basis.append([i+1, j+1, k+1]) # avoid 0
basis = np.array(basis)


voxel1 = voxel1.reshape((27,1))
voxel2 = voxel2.reshape((27,1))

voxel1 = voxel1*basis # weighted basis vectors
voxel2 = voxel2*basis

print(voxel1.shape)
(27, 3)

Then I did PCA to those 27 3-dimensional vectors:
def pca(x):
    center = np.mean(x, 0)
    x = x - center

    cov = np.cov(x.T) / x.shape[0]

    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)

    v = e_vectors[:, order].transpose()

    return x.dot(v)

vp1 = pca(voxel1)
vp2 = pca(voxel2)

But the results in vp1 and vp2 are different. Perhaps, I have a mistake (though I beleive this is the right formula), and the proper code must be
x.dot(v.T)
But in this case the results are very strange. The upper and bottom blocks of the transofrmed data are the same up to the sign:
&gt;&gt;&gt; np.abs(np.abs(vp1)-np.abs(vp2)) &gt; 0.01
array([[False, False, False],
       [False, False, False],
       [False, False, False],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True, False,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [False, False, False],
       [False, False, False],
       [False, False, False],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True, False,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [False, False, False],
       [False, False, False],
       [False, False, False]])

What I'm doing wrong?
What I want to do is to find some invariant representation of my weighted voxel, something like positioning according to the axes of inertia or principal axes. I would really appreciate if someone helps me.
UPD: Found the question similar to mine, but code is unavailable
EDIT2: Found the code InertiaRotate and managed to monkey-do the following:
import numpy as np

# https://github.com/smparker/orient-molecule/blob/master/orient.py

voxel1 = np.random.normal(size=(3,3,3))
voxel2 =  np.transpose(voxel1, (1,0,2))

voxel1 = voxel1.reshape((27,))
voxel2 = voxel2.reshape((27,))


basis = []
for i in range(3):
    for j in range(3):
        for k in range(3):
            basis.append([i+1, j+1, k+1]) # avoid 0
basis = np.array(basis)
basis = basis - np.mean(basis, axis=0)



def rotate_func(data, mass):

    #mass = [ masses[n.lower()] for n in geom.names ]

    inertial_tensor = -np.einsum(""ax,a,ay-&gt;xy"", data, mass, data)
    # negate sign to reverse the sorting of the tensor
    eig, axes = np.linalg.eigh(-inertial_tensor)
    axes = axes.T

    # adjust sign of axes so third moment moment is positive new in X, and Y axes
    testcoords = np.dot(data, axes.T) # a little wasteful, but fine for now
    thirdmoment = np.einsum(""ax,a-&gt;x"", testcoords**3, mass)

    for i in range(2):
        if thirdmoment[i] &lt; 1.0e-6:
            axes[i,:] *= -1.0

    # rotation matrix must have determinant of 1
    if np.linalg.det(axes) &lt; 0.0:
        axes[2,:] *= -1.0

    return axes

axes1 = rotate_func(basis, voxel1)
v1 = np.dot(basis, axes1.T)
axes2 = rotate_func(basis, voxel2)
v2 = np.dot(basis, axes2.T)


print(v1)
print(v2)

It seems to use basis (coordinates) and mass separately. The results are quite similar to my problem above: some parts of the transformed data match up to the sign, I believe those are some cube sides
print(np.abs(np.abs(v1)-np.abs(v2)) &gt; 0.01)
[[False False False]
 [False False False]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [False False False]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [False False False]
 [False False False]]


Looking for some explanation. This code is designed for molecules, and must work...
UPD: Tried to choose 3 vectors as a new basis from those 24 - the one with biggest norm, the one with the smallest and their cross product. Combined them into the matrix V, then used the formula V^(-1)*X to transform coordinates, and got the same problem - the resulting sets of vectors are not equal for rotated voxels.

UPD2: I agree with meTchaikovsky that my idea of multiplying voxel vectors by weights and thus creating some non-cubic point cloud was incorrect. Probably, we indeed need to take the solution for rotated ""basis""(yes, this is not a basis, but rather a way to determine point cloud) which will work later when ""basis"" is the same, but the weights are rotated according to the 3D rotation.
Based on the answer and the reference provided by meTchaikovsky, and finding other answers we together with my friend came to conclusion that rotate_func from molecular package mentioned above tries to invent some convention for computing the signs of the components. Their solution tries to use 3rd moment for the first 2 axes and determinant for the last axis (?). We tried a bit another approach and succeeded to have half of the representations matching:
# -*- coding: utf-8 -*-
""""""
Created on Fri Oct 16 11:40:30 2020

@author: Dima
""""""


import numpy as np
from numpy.random import randn
from numpy import linalg as la
from scipy.spatial.transform import Rotation as R
np.random.seed(10)

rotate_mat = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])

def pca(feat, x):
    # pca with attemt to create convention on sign changes
    
    x_c =x- np.mean(x,axis=0)
    x_f= feat*x
    x_f-= np.mean(x_f, axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)[::-1]
    #print(order)
    v = e_vectors[:,order]
    v= v/np.sign(v[0,:])
    if(la.det(v)&lt;0):
        v= -v
    return x_c @ v

def standardize(x):
    # take vector with biggest norm, with smallest and thir cross product as basis
    x -= np.mean(x,axis=0)
    nrms= la.norm(x, axis=1)
    imin= argmin(nrms)
    imax= argmax(nrms)
    vec1= x[imin, :]
    vec2= x[imax, :]
    vec3= np.cross(vec1, vec2)
    Smat= np.stack([vec1, vec2, vec3], axis=0)
    if(la.det(Smat)&lt;0):
        Smat= -Smat
    return(la.inv(Smat)@x.T)

    

angles = np.linspace(0.0,90.0,91)
voxel1 = np.random.normal(size=(3,3,3))    
res = []
for angle in angles:

    
    voxel2 = voxel1.copy()
    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)
    
    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    basis1 = basis1+1e-4*randn(27,3) # perturbation
    basis2 = basis1 @rotate_mat(np.deg2rad(angle))
    #voxel1 = voxel1*basis1
    #voxel2 = voxel2*basis2

    #print(angle,(np.abs(pca(voxel1) - pca(voxel2) )))
    #gg= np.abs(standardize(basis1) - standardize(basis2) )
    gg= np.abs(pca(voxel1, basis1) - pca(voxel1, basis2) )
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4) 
           
    print(angle,ss,  bl)
    #res.append(np.all(np.abs(pca(voxel1) - pca(voxel2) &lt; 1e-6)))
    del basis1, basis2

The results are good up to 58 degree angle (yet we're still experimenting with rotation of x, y axes). After that we have constant difference which indicates some uncounted sign reverse. This is better than the less consistent result of rotate_func:
0.0 0.0 True
1.0 1.1103280567106161e-13 True
2.0 5.150139890290964e-14 True
3.0 8.977126225544196e-14 True
4.0 5.57341699240722e-14 True
5.0 4.205149954378956e-14 True
6.0 3.7435437643664957e-14 True
7.0 1.2943967187158123e-13 True
8.0 5.400185371573149e-14 True
9.0 8.006410204958181e-14 True
10.0 7.777189536904011e-14 True
11.0 5.992073021576436e-14 True
12.0 6.3716122222085e-14 True
13.0 1.0120048110065158e-13 True
14.0 1.4193029076233626e-13 True
15.0 5.32774440341853e-14 True
16.0 4.056702432878251e-14 True
17.0 6.52062429116855e-14 True
18.0 1.3237663595853556e-13 True
19.0 8.950259695710006e-14 True
20.0 1.3795067925438317e-13 True
21.0 7.498727794307339e-14 True
22.0 8.570866862371226e-14 True
23.0 8.961510590826412e-14 True
24.0 1.1839169916779899e-13 True
25.0 1.422193407555868e-13 True
26.0 6.578778015788652e-14 True
27.0 1.0042963537887101e-13 True
28.0 8.438153062569065e-14 True
29.0 1.1299103064863272e-13 True
30.0 8.192453876745831e-14 True
31.0 1.2618492405483406e-13 True
32.0 4.9237819394886296e-14 True
33.0 1.0971028569666842e-13 True
34.0 1.332138304559801e-13 True
35.0 5.280024600049296e-14 True

From the code above, you can see that we tried to use another basis: vector with the biggest norm, vector with the smallest and their cross product. Here we should have only two variants (direction of the cross product) which could be later fixed, but I couldn't manage this alternative solution to work.
I hope that someone can help me finish this and obtain rotation-invariant representation for voxels.

EDIT 3. Thank you very much meTchaikovsky, but the situation is still unclear. My problem initially lies in processing 3d voxels which are (3,3,3) numpy arrays. We reached the conclusion that for finding invariant representation, we just need to fix 3d voxel as weights used for calculating cov matrix, and apply rotations on the centered ""basis"" (some vectors used for describing point cloud).
Therefore, when we achieved invariance to ""basis"" rotations, the problem should have been solved: now, when we fix ""basis"" and use rotated voxel, the result must be invariant. Surprisingly, this is not so. Here I check 24 rotations of the cube with basis2=basis1 (except small perturbation):
import scipy.ndimage

def pca(feat, x):

    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]

    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_c @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign

    return proj

def rotate_3d(image1, alpha, beta, gamma):
    # z
    # The rotation angle in degrees.
    image2 = scipy.ndimage.rotate(image1, alpha, mode='nearest', axes=(0, 1), reshape=False)

    # rotate along y-axis
    image3 = scipy.ndimage.rotate(image2, beta, mode='nearest', axes=(0, 2), reshape=False)

    # rotate along x-axis
    image4 = scipy.ndimage.rotate(image3, gamma, mode='nearest', axes=(1, 2), reshape=False)
    return image4



voxel10 = np.random.normal(size=(3,3,3))

angles = [[x,y,z] for x in [-90,0,90] for y in [-90,0,90] for z in [-90,0,90]]
res = []
for angle in angles:

    voxel2 = rotate_3d(voxel10, angle[0], angle[1], angle[2])
    voxel1 = voxel10.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)

    basis1 += 1e-4*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1
    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel2, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))


difference before pca 0.000, difference after pca 45.738 False
difference before pca 0.000, difference after pca 12.157 False
difference before pca 0.000, difference after pca 26.257 False
difference before pca 0.000, difference after pca 37.128 False
difference before pca 0.000, difference after pca 52.131 False
difference before pca 0.000, difference after pca 45.436 False
difference before pca 0.000, difference after pca 42.226 False
difference before pca 0.000, difference after pca 18.959 False
difference before pca 0.000, difference after pca 38.888 False
difference before pca 0.000, difference after pca 12.157 False
difference before pca 0.000, difference after pca 26.257 False
difference before pca 0.000, difference after pca 50.613 False
difference before pca 0.000, difference after pca 52.132 False
difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 52.299 False

Here basis1=basis2 (hence basis difference before pca=0), and you can see 0 for (0,0,0) rotation. But rotated voxels give different result. In case scipy does something wrong, I've checked the approach with numpy.rot90 with the same result:
rot90 = np.rot90

def rotations24(polycube):
    # imagine shape is pointing in axis 0 (up)

    # 4 rotations about axis 0
    yield from rotations4(polycube, 0)

    # rotate 180 about axis 1, now shape is pointing down in axis 0
    # 4 rotations about axis 0
    yield from rotations4(rot90(polycube, 2, axis=1), 0)

    # rotate 90 or 270 about axis 1, now shape is pointing in axis 2
    # 8 rotations about axis 2
    yield from rotations4(rot90(polycube, axis=1), 2)
    yield from rotations4(rot90(polycube, -1, axis=1), 2)

    # rotate about axis 2, now shape is pointing in axis 1
    # 8 rotations about axis 1
    yield from rotations4(rot90(polycube, axis=2), 1)
    yield from rotations4(rot90(polycube, -1, axis=2), 1)

def rotations4(polycube, axis):
    """"""List the four rotations of the given cube about the given axis.""""""
    for i in range(4):
        yield rot90(polycube, i, axis)



def rot90(m, k=1, axis=2):
    """"""Rotate an array k*90 degrees in the counter-clockwise direction around the given axis""""""
    m = np.swapaxes(m, 2, axis)
    m = np.rot90(m, k)
    m = np.swapaxes(m, 2, axis)
    return m


voxel10 = np.random.normal(size=(3,3,3))

gen = rotations24(voxel10)

res = []
for voxel2 in gen:

    #voxel2 = rotate_3d(voxel10, angle[0], angle[1], angle[2])
    voxel1 = voxel10.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)

    basis1 += 1e-4*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1

    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel2, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

I tried to investigate this case, and the only perhaps irrelevant thing I found the following:
voxel1 = np.ones((3,3,3))
voxel1[0,0,0] = 0 # if I change 0 to 0.5 it stops working at all

# mirrored around diagonal
voxel2 = np.ones((3,3,3))
voxel2[2,2,2] = 0

for angle in range(1):

    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1) 

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)

    basis1 = basis1 + 1e-4 * randn(27,3) # perturbation
    basis2 = basis1

# If perturbation is used we have 

# difference before pca 0.000, difference after pca 0.000 True
# correct for 100.0 percent of time

# eigenvalues for both voxels
# [1.03417495 0.69231107 0.69235402]
# [0.99995368 0.69231107 0.69235402]


# If no perturbation applied for basis, difference is present

# difference before pca 0.000, difference after pca 55.218 False
# correct for 0.0 percent of time

# eignevalues for both voxels (always have 1.):
# [0.69230769 1.03418803 0.69230769]
# [1.         0.69230769 0.69230769]




Currently don't know how to proceed from there.

EDIT4:
I'm currently thinking that there is some problem with voxel rotations transformed into basis coefficients via voxel.reshape()
Simple experiment with creating array of indices
indices = np.arange(27)
indices3d = indices.reshape((3,3,3))
voxel10 = np.random.normal(size=(3,3,3))
assert voxel10[0,1,2] == voxel10.ravel()[indices3d[0,1,2]]

And then using it for rotations
gen = rotations24(indices3d)

res = []
for ind2 in gen:

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    voxel1 = voxel10.copy().reshape(27,1) #np.array([voxel10[i,j,k] for k in range(3) for j in range(3) for i in range(3)])[...,np.newaxis]

    voxel2 = voxel1[ind2.reshape(27,)]

    basis1 += 1e-4*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1[ind2.reshape(27,)]

    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel2, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

Shows that those rotations are not correct, because on my opinion rotated voxel and basis should match:
difference before pca 0.000, difference after pca 0.000 True
difference before pca 48.006, difference after pca 87.459 False
difference before pca 72.004, difference after pca 70.644 False
difference before pca 48.003, difference after pca 71.930 False
difference before pca 72.004, difference after pca 79.409 False
difference before pca 84.005, difference after pca 36.177 False


EDIT 5: Okaaay, so here we go at least for 24 rotations. At first, we had a slight change of logic lurked into our pca function. Here we center x_c (basis) and forget about it, further centering x_f (features*basis) and transforming it with pca. This does not work perhaps because our basis is not centered and multiplication by features further increased the bias. If we center x_c first, and multiply it by features, everything will be Ok. Also, previously we had proj = x_c @ v with v computed from x_f which was totally wrong in this case, as x_f and x_c were centered around different centers.
def pca(feat, x):
    
    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    
    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    
    
    return proj

Secondly, as we already found, we need to sort vectors obtained by pca, for example by the first column:
    basis2 = basis1

    original_diff = np.sum(np.abs(basis1-basis2))

    a = pca(voxel1, basis1)
    t1 = a[a[:,0].argsort()]

    a = pca(voxel2, basis2)
    t2 = a[a[:,0].argsort()]

    gg= np.abs(t1-t2)

And the last thing we also discovered already, is that simple reshape is wrong for voxel, it must correspond to rotation:
voxel2 = voxel1[ind2.reshape(27,)] #np.take(voxel10, ind2).reshape(27,1).
One more important comment to understand the solution. When we perform PCA on the 3d vectors (point cloud, defined by our basis) with weights assigned (analogously to the inertia of the rigid body), the actual assignment of the weights to the points is sort of external information, which becomes hard-defined for the algorithm. When we rotated basis by applying rotation matrices, we did not change the order of the vectors in the array, hence the order of the mass assignments wasn't changed too. When we start to rotate voxel, we change the order of the masses, so in general PCA algorithm will not work without the same transformation applied to the basis. So, only if we have some array of 3d vectors, transformed by some rotation AND the list of masses re-arranged accordingly, we can detect the rotation of the rigid body using PCA. Otherwise, if we detach masses from points, that would be another body in general.
So how does it work for us then? It works because our points are fully symmetric around the center after centering basis. In this case reassignment of the masses does not change ""the body"" because vector norms are the same. In this case we can use the same (numerically) basis2=basis1 for testing 24 rotations and rotated voxel2 (rotated point cloud cubes match, just masses migrate). This correspond to the rotation of the point cloud with mass points around the center of the cube. PCA will transform vectors with the same lengths and different masses in the same way according to the body's ""inertia"" then (after we reached convention on the signs of the components). The only thing left is to sort the pca transformed vectors in the end, because they have different position in the array (because our body was rotated, mass points changed their positions). This makes us lose some information related to the order of the vectors but it looks inevitable.
Here is the code which checks the solution for 24 rotations. If should theoretically work in the general case as well, giving some closer values for more complicated objects rotated inside a bigger voxel:
import numpy as np
from numpy.random import randn

#np.random.seed(20)

def pca(feat, x):
    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x_c
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign
    return proj


# must be correct https://stackoverflow.com/questions/15230179/how-to-get-the-linear-index-for-a-numpy-array-sub2ind
indices = np.arange(27)
indices3d = indices.reshape((3,3,3))
voxel10 = np.random.normal(size=(3,3,3))
assert voxel10[0,1,2] == voxel10.ravel()[indices3d[0,1,2]]

rot90 = np.rot90

def rotations24(polycube):
    # imagine shape is pointing in axis 0 (up)

    # 4 rotations about axis 0
    yield from rotations4(polycube, 0)

    # rotate 180 about axis 1, now shape is pointing down in axis 0
    # 4 rotations about axis 0
    yield from rotations4(rot90(polycube, 2, axis=1), 0)

    # rotate 90 or 270 about axis 1, now shape is pointing in axis 2
    # 8 rotations about axis 2
    yield from rotations4(rot90(polycube, axis=1), 2)
    yield from rotations4(rot90(polycube, -1, axis=1), 2)

    # rotate about axis 2, now shape is pointing in axis 1
    # 8 rotations about axis 1
    yield from rotations4(rot90(polycube, axis=2), 1)
    yield from rotations4(rot90(polycube, -1, axis=2), 1)

def rotations4(polycube, axis):
    """"""List the four rotations of the given cube about the given axis.""""""
    for i in range(4):
        yield rot90(polycube, i, axis)



def rot90(m, k=1, axis=2):
    """"""Rotate an array k*90 degrees in the counter-clockwise direction around the given axis""""""
    m = np.swapaxes(m, 2, axis)
    m = np.rot90(m, k)
    m = np.swapaxes(m, 2, axis)
    return m


gen = rotations24(indices3d)

res = []

for ind2 in gen:

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    voxel1 = voxel10.copy().reshape(27,1)

    voxel2 = voxel1[ind2.reshape(27,)] #np.take(voxel10, ind2).reshape(27,1)

    basis1 += 1e-6*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1

    original_diff = np.sum(np.abs(basis1-basis2))
    a = pca(voxel1, basis1)
    t1 = a[a[:,0].argsort()]
    a = pca(voxel2, basis2)
    t2 = a[a[:,0].argsort()]
    gg= np.abs(t1-t2)
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 0.000 True


PS. I want to propose better ordering theme to take into account zero values in the voxel which might confuse previous approach when entire first column of PCA vectors is zero, etc. I propose to sort by vector norms, multiplied by the sign of the sum of elements. Here is tensorflow 2 code:

def infer_shape(x):
    x = tf.convert_to_tensor(x)

    # If unknown rank, return dynamic shape
    if x.shape.dims is None:
        return tf.shape(x)

    static_shape = x.shape.as_list()
    dynamic_shape = tf.shape(x)

    ret = []
    for i in range(len(static_shape)):
        dim = static_shape[i]
        if dim is None:
            dim = dynamic_shape[i]
        ret.append(dim)

    return ret

def merge_last_two_dims(tensor):
    shape = infer_shape(tensor)
    shape[-2] *= shape[-1]
    #shape.pop(1)
    shape = shape[:-1]
    return tf.reshape(tensor, shape)


def pca(inpt_voxel):
        patches = tf.extract_volume_patches(inpt_voxel, ksizes=[1,3,3,3,1], strides=[1, 1,1,1, 1], padding=""VALID"")
        features0 = patches[...,tf.newaxis]*basis
        # centered basises
        basis1_ = tf.ones(shape=tf.shape(patches[...,tf.newaxis]), dtype=tf.float32)*basis
        basis1 = basis1_ - tf.math.divide_no_nan(tf.reduce_sum(features0, axis=-2), tf.reduce_sum(patches, axis=-1)[...,None])[:,:,:,:,None,:]
        features = patches[...,tf.newaxis]*basis1
        features_centered_basis = features - tf.reduce_mean(features, axis=-2)[:,:,:,:,None,:]
        x = features_centered_basis
        m = tf.cast(x.get_shape()[-2], tf.float32)
        cov = tf.matmul(x,x,transpose_a=True)/(m - 1)
        e,v = tf.linalg.eigh(cov,name=""eigh"")
        proj = tf.matmul(x,v,transpose_b=False)
        asign = tf.sign(proj)
        max_ind = tf.argmax(tf.abs(proj),axis=-2)[:,:,:,:,None,:]
        sign = tf.gather(asign,indices=max_ind, batch_dims=4, axis=-2)
        sign = tf.linalg.diag_part(sign)
        proj = proj * sign
        # But we can have 1st coordinate zero. In this case,
        # other coordinates become ambiguous
        #s = tf.argsort(proj[...,0], axis=-1)
        # sort by l2 vector norms, multiplied by signs of sums
        sum_signs = tf.sign(tf.reduce_sum(proj, axis=-1))
        norms = tf.norm(proj, axis=-1)
        s = tf.argsort(sum_signs*norms, axis=-1)
        proj = tf.gather(proj, s, batch_dims=4, axis=-2)
        return merge_last_two_dims(proj)

",6,931,"Firstly, your pca function is not correct, it should be
def pca(x):
    
    x -= np.mean(x,axis=0)
    cov = np.cov(x.T)
    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    
    return x @ v

You shouldn't transpose the e_vectors[:,order] because we want each column of the v array is an eigenvector, therefore, x @ v will be projections of x on those eigenvectors.
Secondly, I think you misunderstand the meaning of rotation. It is not voxel1 that should be rotated, but the basis1. If you rotate (by taking transposition) voxel1, what you really do is to rearrange the indices of grid points, while the coordinates of the points basis1 are not changed.
In order to rotate the points (around the z axis for example), you can first define a function to calculate the rotation matrix given an angle
rotate_mat = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])

with the rotation matrix generated by this function, you can rotate the array basis1 to create another array basis2
basis2 = basis1 @ rotate_mat(np.deg2rad(angle))

Now it comes to the title of your question ""Why my PCA is not invariant to rotation and axis swap?"", from this post, the PCA result is not unique, you can actually run a test to see this
import numpy as np

np.random.seed(10)

rotate_mat = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])

def pca(x):
    
    x -= np.mean(x,axis=0)
    cov = np.cov(x.T)
    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    return x @ v


angles = np.linspace(0,90,91)
    
res = []
for angle in angles:

    voxel1 = np.random.normal(size=(3,3,3))
    voxel2 = voxel1.copy()
    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)
    
    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)])
    # basis2 = np.hstack((-basis1[:,1][:,None],basis1[:,0][:,None],-basis1[:,2][:,None]))
    basis2 = basis1 @ rotate_mat(np.deg2rad(angle))
    voxel1 = voxel1*basis1
    voxel2 = voxel2*basis2

    print(angle,np.all(np.abs(pca(voxel1) - pca(voxel2) &lt; 1e-6)))
    res.append(np.all(np.abs(pca(voxel1) - pca(voxel2) &lt; 1e-6)))
    
print()
print(np.sum(res) / len(angles))

After you run this script, you will see that in only 21% of times the two PCA results are the same.

UPDATE
I think instead of focusing on the eigenvectors of the principal components, you can instead focus on the projections. For two clouds of points, even though they are essentially the same, the eigenvectors can be drastically different. Therefore, hardcoding in order to somehow let the two sets of eigenvectors to be the same is a very difficult task.
However, based on this post, for the same cloud of points, two sets of eigenvectors can be different only up to a minus sign. Therefore, the projections upon the two sets of eigenvectors are also different only up to a minus sign. This actually offers us an elegant solution, for the projections along an eigenvector (principal axis), all we need to do is to switch the sign of the projections so that the projection with the largest absolute value along that principal axis is positive.
import numpy as np
from numpy.random import randn

#np.random.seed(20)

rotmat_z = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])
rotmat_y = lambda theta: np.array([[np.cos(theta),0.,np.sin(theta)],[0.,1.,0.],[-np.sin(theta),0.,np.cos(theta)]])
rotmat_x = lambda theta: np.array([[1.,0.,0.],[0.,np.cos(theta),-np.sin(theta)],[0.,np.sin(theta),np.cos(theta)]])
# based on https://en.wikipedia.org/wiki/Rotation_matrix
rot_mat = lambda alpha,beta,gamma: rotmat_z(alpha) @ rotmat_y(beta) @ rotmat_x(gamma)

deg2rad = lambda alpha,beta,gamma: [np.deg2rad(alpha),np.deg2rad(beta),np.deg2rad(gamma)]

def pca(feat, x):
    
    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    
    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign
    
    return proj

ref_angles = np.linspace(0.0,90.0,10)
angles = [[alpha,beta,gamma] for alpha in ref_angles for beta in ref_angles for gamma in ref_angles]


voxel1 = np.random.normal(size=(3,3,3))
res = []
for angle in angles:

    voxel2 = voxel1.copy()
    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    basis1 = basis1 + 1e-4 * randn(27,3) # perturbation
    basis2 = basis1 @ rot_mat(*deg2rad(*angle))
   
    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel1, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

As you can see by running this script, the projections on the principal axis are the same, this means we have resolved the issue of PCA results being not unique.

Reply to EDIT 3
As for the new issue you raised, I think you missed an important point, it is the projections of the cloud of points onto the principal axes that are invariant, not anything else. Therefore, if you rotate voxel1 and obtain voxel2, they are the same in the sense that their own respective projections onto the principal axes of the cloud of points are the same, it actually does not make too much sense to compare pca(voxel1,basis1) with pca(voxel2,basis1).
Furthermore, the method rotate of scipy.ndimage actually changes information, as you can see by running this script
image1 = np.linspace(1,100,100).reshape(10,10)
image2 = scipy.ndimage.rotate(image1, 45, mode='nearest', axes=(0, 1), reshape=False)
image3 = scipy.ndimage.rotate(image2, -45, mode='nearest', axes=(0, 1), reshape=False)

fig,ax = plt.subplots(nrows=1,ncols=3,figsize=(12,4))
ax[0].imshow(image1)
ax[1].imshow(image2)
ax[2].imshow(image3)

The output image is
As you can see the matrix after rotation is not the same as the original one, some information of the original matrix is changed.


Reply to EDIT 4
Actually, we are almost there, the two pca results are different because we are comparing pca components for different points.
indices = np.arange(27)
indices3d = indices.reshape((3,3,3))
# apply rotations to the indices, it is not weights yet
gen = rotations24(indices3d)

# construct the weights
voxel10 = np.random.normal(size=(3,3,3))

res = []
count = 0
for ind2 in gen:
    count += 1
    # ind2 is the array of indices after rotation
    # reindex the weights with the indices after rotation 
    voxel1 = voxel10.copy().reshape(27,1) 
    voxel2 = voxel1[ind2.reshape(27,)]

    # basis1 is the array of coordinates where the points are
    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    basis1 += 1e-4*np.random.normal(size=(27, 1))
    # reindex the coordinates with the indices after rotation
    basis2 = basis1[ind2.reshape(27,)]

    # add a slight modification to pca, return the axes also 
    pca1,v1 = pca(voxel1,basis1)
    pca2,v2 = pca(voxel2,basis2)
    # sort the principal components before comparing them 
    pca1 = np.sort(pca1,axis=0)
    pca2 = np.sort(pca2,axis=0)
    
    gg= np.abs(pca1 - pca2)
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference after pca %.3f' % ss,bl)
    res.append(bl)
    
    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

Running this script, you will find, for each rotation, the two sets of principal axes are different only up to a minus sign. The two sets of pca results are different because the indices of the cloud of points before and after rotation are different (since you apply rotation to the indices). If you sort the pca results before comparing them, you will find the two pca results are exactly the same.

Summary
The answer to this question can be divided into two parts. In the first part, the rotation is applied to the basis (the coordinates of points), while the indices and the corresponding weights are unchanged. In the second part, the rotation is applied to the indices, then the weights and the basis are rearranged with the new indices. For both of the two parts, the solution pca function is the same
def pca(feat, x):

    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]

    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign

    return proj

The idea of this function is, instead of matching the principal axes, we can match the principal components since it is the principal components that are rotationally invariant after all.
Based on this function pca, the first part of this answer is easy to understand, since the indices of the points are unchanged while we only rotate the basis. In order to understand the second part of this answer (Reply to EDIT 5), we must first understand the function rotations24. This function rotates the indices rather than the coordinates of the points, therefore, if we stay at the same position observing the points, we will feel that the positions of the points are changed.

With this in mind, it is not hard to understand Reply to EDIT 5.
Actually, the function pca in this answer can be applied to more general cases, for example (we rotate the indices)
num_of_points_per_dim = 10
num_of_points = num_of_points_per_dim ** 3

indices = np.arange(num_of_points)
indices3d = indices.reshape((num_of_points_per_dim,num_of_points_per_dim,num_of_points_per_dim))
voxel10 = 100*np.random.normal(size=(num_of_points_per_dim,num_of_points_per_dim,num_of_points_per_dim))

gen = rotations24(indices3d)

res = []
for ind2 in gen:

    voxel1 = voxel10.copy().reshape(num_of_points,1)
    voxel2 = voxel1[ind2.reshape(num_of_points,)]

    basis1 = 100*np.random.rand(num_of_points,3)
    basis2 = basis1[ind2.reshape(num_of_points,)]

    pc1 = np.sort(pca(voxel1, basis1),axis=0)
    pc2 = np.sort(pca(voxel2, basis2),axis=0)
    
    gg= np.abs(pc1-pc2)
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

",,
SciPy strange result,https://stackoverflow.com/questions/23699301,Scipy strange results using curve fitting,,6,860,,,
SciPy strange result,https://stackoverflow.com/questions/76439150,Scipy banded eigensolver much slower than standard eigensolver,"I'm observing a strange behaviour concerning the scipy.linalg.eig_banded eigensolver.
I am generating banded matrices of size N=p*f that have a specific structure. The matrices are symmetric tri-block-diagonal with p blocks of size fxf on the main diagonal and p-1 identity matrices of size f*f on the off diagonals.
Example with p=3 and f=3:
 [2 2 2 1 0 0 0 0 0]
 [2 2 2 0 1 0 0 0 0]
 [2 2 2 0 0 1 0 0 0]
 [1 0 0 3 3 3 1 0 0]
 [0 1 0 3 3 3 0 1 0]
 [0 0 1 3 3 3 0 0 1]
 [0 0 0 1 0 0 4 4 4]
 [0 0 0 0 1 0 4 4 4]
 [0 0 0 0 0 1 4 4 4]

Usually these matrices are of size p = 100, f=30, N=p*f=3000 but can easily grow much larger.
Given the structure of these matrices I was hoping that the banded eigensolver in scipy was going to be much faster than the dense eigensolver, however it seems like this is not the case.
I am benchmarking the solvers with the following code:
# Set dimension of problem
f = 50
p = 80
a = 1

print(f""p={p}, f={f}, size={f*p, f*p}"")

print(f""Matrix containing random numbers in {(-a, a)}"")
A = generate_matrix(p, f, -a, a)

# Benchmark standard eigensolver
start = time()
D, Q = linalg.eigh(A)
end = time()

# Test correctness
D = np.diag(D)
print(f""Time for dense solver {end - start}"")
print(f""||AQ - QD|| = {np.linalg.norm(A@Q - Q@D)}"")


# Convert A to banded format
A_banded = banded_format(A, upper = f)

# Benchmark banded eigensolver
start = time()
D, Q = linalg.eig_banded(A_banded)
end = time()

# Test correctness
D = np.diag(D)
print(f""Time for banded solver {end - start}"")
print(f""||AQ - QD|| = {np.linalg.norm(A@Q - Q@D)}"")


The results I get indicate that the banded eigensolver is much slower than the dense one:
p=80, f=50, size=(4000, 4000)
Matrix containing random numbers in (-1, 1)

Time for dense solver 13.475645780563354
||AQ - QD|| = 3.1334336527852233e-12

Time for banded solver 24.427151203155518
||AQ - QD|| = 1.589349711533356e-11

I have already tried storing the matrix in lower diagonal format and passing the overwrite_a_band=True option, but the performance remains the same.
Numpy configuration:
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']

Scipy configuration:
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']

I also tried running the same benchmark on a different cluster using MKL as a backend instead of OpenBLAS and I observed very similar results. Also setting the number of threads with OMP_NUM_THREADS and/or MKL_NUM_THREADS has a very small effect on performance.
Does anyone have any ideas on why this is happening?
Thanks
",5,106,"I did some digging into the source code of SciPy and the Intel MKL documentation and I have figured out why this is happening.
The scipy eig_banded solver delegates the problem to the LAPACK dsbevd routine which computes all eigenvalues and eigenvectors of a matrix in banded format using a variation of the Cuppen divide and conquer algorithm. This offers an advantage in terms of memory usage because of the banded storage format, but the actual algorithm scales in O(n^3) flops after tridiagonalization with respect to matrix size.
On the other hand, the scipy dense eigensolver delegates the problem to the dsyev routine which for real symmetric matrices calls the dsyevr routine which computes the eigenvalues and eigenvectors using the MRRR algorithm in O(n^2) flops after tridiagonalization.
I am still unsure why there is no MRRR implementation for banded matrix format in MKL.
",,
SciPy strange result,https://stackoverflow.com/questions/59406477,Correct normalization of discrete power spectral density in python for a real problem,"I am struggling with the correct normalization of the power spectral density (and its inverse).

I am given a real problem, let's say the readings of an accelerometer in the form of the power spectral density (psd) in units of Amplitude^2/Hz. I would like to translate this back into a randomized time series. However, first I want to understand the ""forward"" direction, time series to PSD.

According to [1], the PSD of a time series x(t) can be calculated by:

PSD(w) = 1/T * abs(F(w))^2 = df * abs(F(w))^2


in which T is the sampling time of x(t) and F(w) is the Fourier transform of x(t) and df=1/T is the frequency resolution in the Fourier space. However, the results I am getting are not equal to what I am getting using the scipy Welch method, see code below.

This first block of code is taken from the scipy.welch documentary:

from scipy import signal
import matplotlib.pyplot as plt

fs = 10e3
N = 1e5
amp = 2*np.sqrt(2)
freq = 1234.0
noise_power = 0.001 * fs / 2
time = np.arange(N) / fs
x = amp*np.sin(2*np.pi*freq*time)
x += np.random.normal(scale=np.sqrt(noise_power), size=time.shape)

f, Pxx_den = signal.welch(x, fs, nperseg=1024)
plt.semilogy(f, Pxx_den)
plt.ylim(\[0.5e-3, 1\])
plt.xlabel('frequency \[Hz\]')
plt.ylabel('PSD \[V**2/Hz\]')
plt.show()


First thing I noticed is that the plotted psd changes with the variable fs which seems strange to me. (Maybe I need to adjust the nperseg argument then accordingly? Why is nperseg not set to fs automatically then?)

My code would be the following: (Note that I defined my own fft_full function which already takes care of the correct fourier transform normalization, which I verified by checking Parsevals theorem).

import scipy.fftpack as fftpack

def fft_full(xt,yt):
    dt = xt[1] - xt[0]
    x_fft=fftpack.fftfreq(xt.size,dt)
    y_fft=fftpack.fft(yt)*dt
    return (x_fft,y_fft)

xf,yf=fft_full(time,x)
df=xf[1] - xf[0]
psd=np.abs(yf)**2 *df
plt.figure()
plt.semilogy(xf, psd)
#plt.ylim([0.5e-3, 1])
plt.xlim(0,)
plt.xlabel('frequency [Hz]')
plt.ylabel('PSD [V**2/Hz]')
plt.show()


Unfortunately, I am not yet allowed to post images but the two plots do not look the same!

I would greatly appreciate if someone could explain to me where I went wrong and settle this once and for all :)


  [1]: Eq. 2.82. Random Vibrations in Spacecraft Structures Design
  Theory and Applications, Authors: Wijker, J. Jaap, 2009

",5,3027,"The scipy library uses the Welch's method to estimate a PSD. This method is more complex than just taking the squared modulus of the discrete Fourier transform. In short terms, it proceeds as follows:

Let x be the input discrete signal that contains N samples.

Split x into M overlapping segments, such that each segment sm contains nperseg samples and that each two consecutive segments overlap in noverlap samples, so that nperseg = K * (nperseg - noverlap), where K is an integer (usually K = 2). Note also that:
N = nperseg + (M - 1) * (nperseg - noverlap) = (M + K - 1) * nperseg / K

From each segment sm, subtract its mean (this removes the DC component):
tm = sm - sum(sm) / nperseg

Multiply the elements of the obtained zero-mean segments tm by the elements of a suitable (nonsymmetric) window function, h (such as the Hann window):
um = tm * h

Calculate the Fast Fourier Transform of all vectors um. Before performing these transformations, we usually first append so many zeros to each vector um that its new dimension becomes a power of 2 (the nfft argument of the function welch is used for this purpose). Let us suppose that len(um) = 2p. In most cases, our input vectors are real-valued, so it is best to apply FFT for real data. Its results are then complex-valued vectors vm = rfft(um), such that len(vm) = 2p - 1 + 1.

Calculate the squared modulus of all transformed vectors:
am = abs(vm) ** 2,
or more efficiently:
am = vm.real ** 2 + vm.imag ** 2

Normalize the vectors am as follows:
bm = am / sum(h * h)
bm[1:-1] *= 2 (this takes into account the negative frequencies),
where h is a real vector of the dimension nperseg that contains the window coefficients. In case of the Hann window, we can prove that
sum(h * h) = 3 / 8 * len(h) = 3 / 8 * nperseg

Estimate the PSD as the mean of all vectors bm:
psd = sum(bm) / M
The result is a vector of the dimension len(psd) = 2p - 1 + 1. If we wish that the sum of all psd coefficients matches the mean squared amplitude of the windowed input data (rather than the sum of squared amplitudes), then the vector psd must also be divided by nperseg. However, the scipy routine omits this step. In any case, we usually present psd on the decibel scale, so that the final result is:
psd_dB = 10 * log10(psd).


For a more detailed description, please read the original Welch's paper. See also Wikipedia's page and chapter 13.4 of Numerical Recipes in C
",,
SciPy strange result,https://stackoverflow.com/questions/44710838,Calling BLAS / LAPACK directly using the SciPy interface and Cython,,4,2232,,,
SciPy strange result,https://stackoverflow.com/questions/39196057,Libraries in &quot;virtualenv&quot; much bigger than system libraries,,4,933,,,
SciPy strange result,https://stackoverflow.com/questions/36706163,python multiprocessing module: strange behaviour and processor load when using Pool,,4,871,,,
SciPy strange result,https://stackoverflow.com/questions/27004245,Numpy array bug or feature (catsing to int behind the scenes)?,,3,135,,,
SciPy strange result,https://stackoverflow.com/questions/71146140,Using RNN Trained Model without pytorch installed,"I have trained an RNN model with pytorch. I need to use the model for prediction in an environment where I'm unable to install pytorch because of some strange dependency issue with glibc. However, I can install numpy and scipy and other libraries. So, I want to use the trained model, with the network definition, without pytorch.
I have the weights of the model as I save the model with its state dict and weights in the standard way, but I can also save it using just json/pickle files or similar.
I also have the network definition, which depends on pytorch in a number of ways. This is my RNN network definition.
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import random

torch.manual_seed(1)
random.seed(1)
device = torch.device('cpu')

class RNN(nn.Module):
  def __init__(self, input_size, hidden_size, output_size,num_layers, matching_in_out=False, batch_size=1):
    super(RNN, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.num_layers = num_layers
    self.batch_size = batch_size
    self.matching_in_out = matching_in_out #length of input vector matches the length of output vector 
    self.lstm = nn.LSTM(input_size, hidden_size,num_layers)
    self.hidden2out = nn.Linear(hidden_size, output_size)
    self.hidden = self.init_hidden()
  def forward(self, feature_list):
    feature_list=torch.tensor(feature_list)
    
    if self.matching_in_out:
      lstm_out, _ = self.lstm( feature_list.view(len( feature_list), 1, -1))
      output_space = self.hidden2out(lstm_out.view(len( feature_list), -1))
      output_scores = torch.sigmoid(output_space) #we'll need to check if we need this sigmoid
      return output_scores #output_scores
    else:
      for i in range(len(feature_list)):
        cur_ft_tensor=feature_list[i]#.view([1,1,self.input_size])
        cur_ft_tensor=cur_ft_tensor.view([1,1,self.input_size])
        lstm_out, self.hidden = self.lstm(cur_ft_tensor, self.hidden)
        outs=self.hidden2out(lstm_out)
      return outs
  def init_hidden(self):
    #return torch.rand(self.num_layers, self.batch_size, self.hidden_size)
    return (torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device),
            torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device))

I am aware of this question, but I'm willing to go as low level as possible. I can work with numpy array instead of tensors, and reshape instead of view, and I don't need a device setting.
Based on the class definition above, what I can see here is that I only need the following components from torch to get an output from the forward function:

nn.LSTM
nn.Linear
torch.sigmoid

I think I can easily implement the sigmoid function using numpy. However, can I have some implementation for the nn.LSTM and nn.Linear using something not involving pytorch? Also, how will I use the weights from the state dict into the new class?
So, the question is, how can I ""translate"" this RNN definition into a class that doesn't need pytorch, and how to use the state dict weights for it?
Alternatively, is there a ""light"" version of pytorch, that I can use just to run the model and yield a result?
EDIT
I think it might be useful to include the numpy/scipy equivalent for both nn.LSTM and nn.linear. It would help us compare the numpy output to torch output for the same code, and give us some modular code/functions to use. Specifically, a numpy equivalent for the following would be great:
rnn = nn.LSTM(10, 20, 2)
input = torch.randn(5, 3, 10)
h0 = torch.randn(2, 3, 20)
c0 = torch.randn(2, 3, 20)
output, (hn, cn) = rnn(input, (h0, c0))

and also for linear:
m = nn.Linear(20, 30)
input = torch.randn(128, 20)
output = m(input)

",3,934,"You should try to export the model using torch.onnx. The page gives you an example that you can start with.
An alternative is to use TorchScript, but that requires torch libraries.
Both of these can be run without python. You can load torchscript in a C++ application https://pytorch.org/tutorials/advanced/cpp_export.html
ONNX is much more portable and you can use in languages such as C#, Java, or Javascript
https://onnxruntime.ai/ (even on the browser)
A running example
Just modifying a little your example to go over the errors I found
Notice that via tracing any if/elif/else, for, while will be unrolled
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import random

torch.manual_seed(1)
random.seed(1)
device = torch.device('cpu')

class RNN(nn.Module):
  def __init__(self, input_size, hidden_size, output_size,num_layers, matching_in_out=False, batch_size=1):
    super(RNN, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.num_layers = num_layers
    self.batch_size = batch_size
    self.matching_in_out = matching_in_out #length of input vector matches the length of output vector 
    self.lstm = nn.LSTM(input_size, hidden_size,num_layers)
    self.hidden2out = nn.Linear(hidden_size, output_size)
  def forward(self, x, h0, c0):
    lstm_out, (hidden_a, hidden_b) = self.lstm(x, (h0, c0))
    outs=self.hidden2out(lstm_out)
    return outs, (hidden_a, hidden_b)
  def init_hidden(self):
    #return torch.rand(self.num_layers, self.batch_size, self.hidden_size)
    return (torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device).detach(),
            torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device).detach())

# convert the arguments passed during onnx.export call
class MWrapper(nn.Module):
    def __init__(self, model):
        super(MWrapper, self).__init__()
        self.model = model;
    def forward(self, kwargs):
        return self.model(**kwargs)

Run an example
rnn = RNN(10, 10, 10, 3)
X = torch.randn(3,1,10)
h0,c0  = rnn.init_hidden()
print(rnn(X, h0, c0)[0])

Use the same input to trace the model and export an onnx file

torch.onnx.export(MWrapper(rnn), {'x':X,'h0':h0,'c0':c0}, 'rnn.onnx', 
                  dynamic_axes={'x':{1:'N'},
                               'c0':{1: 'N'},
                               'h0':{1: 'N'}
                               },
                  input_names=['x', 'h0', 'c0'],
                  output_names=['y', 'hn', 'cn']
                 )

Notice that you can use symbolic values for the dimensions of some axes of some inputs. Unspecified dimensions will be fixed with the values from the traced inputs. By default LSTM uses dimension 1 as batch.
Next we load the ONNX model and pass the same inputs
import onnxruntime
ort_model = onnxruntime.InferenceSession('rnn.onnx')
print(ort_model.run(['y'], {'x':X.numpy(), 'c0':c0.numpy(), 'h0':h0.numpy()}))

","Basically implementing it in numpy and copying weights from your pytorch model can do the trick.  For your usecase you will only need to do a forward pass so we just need to implement that only
#Set Parameters for a small LSTM network
input_size  = 2 # size of one 'event', or sample, in our batch of data
hidden_dim  = 3 # 3 cells in the LSTM layer
output_size = 1 # desired model output

num_layers=3
torch_lstm = RNN( input_size, 
                 hidden_dim ,
                 output_size,
                 num_layers,
                 matching_in_out=True
                 )

state = torch_lstm.state_dict() # state will capture the weights of your model

Now for LSTM in numpy these functions will be used:
got the below code from this link: https://towardsdatascience.com/the-lstm-reference-card-6163ca98ae87
### NOT MY CODE
import numpy as np 
from scipy.special import expit as sigmoid

def forget_gate(x, h, Weights_hf, Bias_hf, Weights_xf, Bias_xf, prev_cell_state):
    forget_hidden  = np.dot(Weights_hf, h) + Bias_hf
    forget_eventx  = np.dot(Weights_xf, x) + Bias_xf
    return np.multiply( sigmoid(forget_hidden + forget_eventx), prev_cell_state )

def input_gate(x, h, Weights_hi, Bias_hi, Weights_xi, Bias_xi, Weights_hl, Bias_hl, Weights_xl, Bias_xl):
    ignore_hidden  = np.dot(Weights_hi, h) + Bias_hi
    ignore_eventx  = np.dot(Weights_xi, x) + Bias_xi
    learn_hidden   = np.dot(Weights_hl, h) + Bias_hl
    learn_eventx   = np.dot(Weights_xl, x) + Bias_xl
    return np.multiply( sigmoid(ignore_eventx + ignore_hidden), np.tanh(learn_eventx + learn_hidden) )


def cell_state(forget_gate_output, input_gate_output):
    return forget_gate_output + input_gate_output

  
def output_gate(x, h, Weights_ho, Bias_ho, Weights_xo, Bias_xo, cell_state):
    out_hidden = np.dot(Weights_ho, h) + Bias_ho
    out_eventx = np.dot(Weights_xo, x) + Bias_xo
    return np.multiply( sigmoid(out_eventx + out_hidden), np.tanh(cell_state) )


We would need the sigmoid function as well so
def sigmoid(x):
    return 1/(1 + np.exp(-x))

Because pytorch stores weights in stacked manner so we need to break it up for that we would need the below function
def get_slices(hidden_dim):
    slices=[]
    breaker=(hidden_dim*4)
    slices=[[i,i+3] for i in range(0, breaker, breaker//4)]
    return slices

Now we have the functions ready for lstm, now we create an lstm class to copy the weights from pytorch class and get the output from it.
class numpy_lstm:
    def __init__( self, layer_num=0, hidden_dim=1, matching_in_out=False):
        self.matching_in_out=matching_in_out
        self.layer_num=layer_num
        self.hidden_dim=hidden_dim
        
    def init_weights_from_pytorch(self, state):
        slices=get_slices(self.hidden_dim)
        print (slices)

        #Event (x) Weights and Biases for all gates
        
        lstm_weight_ih='lstm.weight_ih_l'+str(self.layer_num)
        self.Weights_xi = state[lstm_weight_ih][slices[0][0]:slices[0][1]].numpy()  # shape  [h, x]
        self.Weights_xf = state[lstm_weight_ih][slices[1][0]:slices[1][1]].numpy()  # shape  [h, x]
        self.Weights_xl = state[lstm_weight_ih][slices[2][0]:slices[2][1]].numpy()  # shape  [h, x]
        self.Weights_xo = state[lstm_weight_ih][slices[3][0]:slices[3][1]].numpy() # shape  [h, x]

        
        lstm_bias_ih='lstm.bias_ih_l'+str(self.layer_num)
        self.Bias_xi = state[lstm_bias_ih][slices[0][0]:slices[0][1]].numpy()  #shape is [h, 1]
        self.Bias_xf = state[lstm_bias_ih][slices[1][0]:slices[1][1]].numpy()  #shape is [h, 1]
        self.Bias_xl = state[lstm_bias_ih][slices[2][0]:slices[2][1]].numpy()  #shape is [h, 1]
        self.Bias_xo = state[lstm_bias_ih][slices[3][0]:slices[3][1]].numpy() #shape is [h, 1]
        
        
        lstm_weight_hh='lstm.weight_hh_l'+str(self.layer_num)

        #Hidden state (h) Weights and Biases for all gates
        self.Weights_hi = state[lstm_weight_hh][slices[0][0]:slices[0][1]].numpy()  #shape is [h, h]
        self.Weights_hf = state[lstm_weight_hh][slices[1][0]:slices[1][1]].numpy()  #shape is [h, h]
        self.Weights_hl = state[lstm_weight_hh][slices[2][0]:slices[2][1]].numpy()  #shape is [h, h]
        self.Weights_ho = state[lstm_weight_hh][slices[3][0]:slices[3][1]].numpy() #shape is [h, h]
        
        
        lstm_bias_hh='lstm.bias_hh_l'+str(self.layer_num)

        self.Bias_hi = state[lstm_bias_hh][slices[0][0]:slices[0][1]].numpy()  #shape is [h, 1]
        self.Bias_hf = state[lstm_bias_hh][slices[1][0]:slices[1][1]].numpy()  #shape is [h, 1]
        self.Bias_hl = state[lstm_bias_hh][slices[2][0]:slices[2][1]].numpy()  #shape is [h, 1]
        self.Bias_ho = state[lstm_bias_hh][slices[3][0]:slices[3][1]].numpy() #shape is [h, 1]
    def forward_lstm_pass(self,input_data):
        h = np.zeros(self.hidden_dim)
        c = np.zeros(self.hidden_dim)
        
        output_list=[]
        for eventx in input_data:
            f = forget_gate(eventx, h, self.Weights_hf, self.Bias_hf, self.Weights_xf, self.Bias_xf, c)
            i =  input_gate(eventx, h, self.Weights_hi, self.Bias_hi, self.Weights_xi, self.Bias_xi, 
                        self.Weights_hl, self.Bias_hl, self.Weights_xl, self.Bias_xl)
            c = cell_state(f,i)
            h = output_gate(eventx, h, self.Weights_ho, self.Bias_ho, self.Weights_xo, self.Bias_xo, c)
            if self.matching_in_out: # doesnt make sense but it was as it was in main code :(
                output_list.append(h)
        if self.matching_in_out:
            return output_list
        else:
            return h


Similarly for fully connected layer,
    
    
class fully_connected_layer:
    def __init__(self,state, dict_name='fc', ):
        self.fc_Weight = state[dict_name+'.weight'][0].numpy()
        self.fc_Bias = state[dict_name+'.bias'][0].numpy() #shape is [,output_size]
        
    def forward(self,lstm_output, is_sigmoid=True):
        res=np.dot(self.fc_Weight, lstm_output)+self.fc_Bias
        print (res)
        if is_sigmoid:
            return sigmoid(res)
        else:
            return res
        

Now we would need one class to call all of them together and generalise them with respect to multiple layers
You can modify the below class if you need more Fully connected layers or want to set false condition for sigmoid etc.
        
class RNN_model_Numpy:
    def __init__(self, state, input_size, hidden_dim, output_size, num_layers, matching_in_out=True):
        self.lstm_layers=[]
        for i in range(0, num_layers):
            lstm_layer_obj=numpy_lstm(layer_num=i, hidden_dim=hidden_dim, matching_in_out=True)
            lstm_layer_obj.init_weights_from_pytorch(state) 
            self.lstm_layers.append(lstm_layer_obj)
        
        self.hidden2out=fully_connected_layer(state, dict_name='hidden2out')
        
    def forward(self, feature_list):
        for x in self.lstm_layers:
            lstm_output=x.forward_lstm_pass(feature_list)
            feature_list=lstm_output
            
        return self.hidden2out.forward(feature_list, is_sigmoid=False)

Sanity check on a numpy variable:
data = np.array(
           [[1,1],
            [2,2],
            [3,3]])



check=RNN_model_Numpy(state, input_size, hidden_dim, output_size, num_layers)
check.forward(data)

EXPLANATION:
Since we just need forward pass, we would need certain functions that are required in LSTM, for that we have the forget gate, input gate, cell gate and output gate. They are just some operations that are done on the input that you give.
For get_slices function, this is used to break down the weight matrix that we get from pytorch state dictionary (state dictionary) is the dictionary which contains the weights of all the layers that we have in our network.
For LSTM particularly have it in this order  ignore, forget, learn, output. So for that we would need to break it up for different LSTM cells.
For numpy_lstm class, we have init_weights_from_pytorch function which must be called, what it will do is that it will extract the weights from state dictionary which we got earlier from pytorch model object and then populate the numpy array weights with the pytorch weights. You can first train your model and then save the state dictionary through pickle and then use it.
The fully connected layer class just implements the hidden2out neural network.
Finally our rnn_model_numpy class is there to ensure that if you have multiple layers then it is able to send the output of one layer of lstm to other layer of lstm.
Lastly there is a small sanity check on data variable.
IMPORTANT NOTE: PLEASE NOTE THAT YOU MIGHT GET DIMENSION ERROR AS PYTORCH WAY OF HANDLING INPUT IS COMPLETELY DIFFERENT SO PLEASE ENSURE THAT YOU INPUT NUMPY IS OF SIMILAR SHAPE AS DATA VARIABLE.
Important references:
https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
https://christinakouridi.blog/2019/06/19/backpropagation-lstm/
",
SciPy strange result,https://stackoverflow.com/questions/54671752,"Why does glmnet.py throw a TypeError at cvglmnet when I provide floats, if glmnet does require floats?","I am trying to run a glm for logistic regression using Glmnet package for python following Glmnet Vignette and I am having problems with the type of my response variable 'y'.
I want to perform both the glmnet function and the cvglmnet function. The vignette says that ""For binomial logistic regression, the response variable y should be either a factor with two levels, or a two-column matrix of counts or proportions."".
I got my y values from one column of a pandas dataframe. Similarly my input matrix x is from several columns of a pandas dataframe. So my y.values is a numpy.ndarray of shape (290,) made of 1's and 2's, so an array of integers.

To check my types, if I do:

isinstance(y.values,scipy.ndarray)
True
isinstance(y.values,np.ndarray)
True
isinstance(y.values,int)
False
type(y.values[1])
numpy.int64
isinstance(y.values, float)
False


If I run

fit = glmnet(x = fold1_sp.copy(), y = y.values, family = 'binomial')


I get an error 

ValueError: y input must be a scipy float64 ndarray


I found in https://github.com/bbalasub1/glmnet_python/issues/15 that they tried to address this issue mentioning that any search for scipy float 64 ndarrays returns numpy float ndarray results because they are basically the same object type, so I convert to numpy float: 

fit = glmnet(x = fold1_sp.copy(), y = np.float64(y.values), family = 'binomial')


and it runs just fine.
But this way my y array is made of floats 1.'s and 2.'s

If I now try using the cvglmnet function, it complains it needs an integer

cvfit = cvglmnet(x = fold1_sp.copy(), y = np.float64(y.values), family = 'binomial', ptype = 'class')


throwing the following error:

TypeError: 'numpy.float64' object cannot be interpreted as an integer


which is strange at least to me since it asked for a float before, and if I use my y as integers using the original y.values, then it complains again it needs scipy float64.

So how can I overcome this TypeError problem?.

Thanks ever so much
",3,973,"I was having a similar problem but with X data, and I solve it by putting
cvglmnet(x=X.astype(np.float64), y=y,family = ""gaussian"",alpha=1)

",,
SciPy strange result,https://stackoverflow.com/questions/24824192,Redisplaying plots in Matplotlib in conjunction with PyQt,,3,313,,,
SciPy strange result,https://stackoverflow.com/questions/17753178,howto emulate 2-sample t-test in scipy,,3,4496,,,
SciPy strange result,https://stackoverflow.com/questions/10283053,Downloading a .csv file from the web (with redirects) in python,,3,9459,,,
SciPy strange result,https://stackoverflow.com/questions/76787449,SciPy&#39;s lsim not behaving as expected compared to Python Control Systems Library,"I am doing some control engineering in Python, and I have come accross a very strange discrepency between SciPy and the Python Control Systems Library for the implementation of the lsim command. To the best of my knowledge, these commands should behave identically, but it seems they are not: more particularly, the output of the SciPy command seems like nonsense. I have included a MWE below.
""""""
Minimal working example (MWE) replicating the discrepency between lsim for the SciPy module and control module
""""""

# Import modules
import numpy as np
import scipy.signal as sgnl
import matplotlib.pyplot as plt
import control as ctrl
import control.matlab as ctrl_mtl

# Number of samples
N_k = 100    # Total number of outputs

# Continous-time (unstable) open loop system dynamics
A = np.array([[0, 1],
              [-2, 3]])
B = np.array([[0],
              [1]])
n_x = A.shape[0]
n_u = B.shape[1]

# Initial state
x_0 = np.array([[1], [1]], dtype=float)

# Sampling period
h = 0.5

# Controller (full-state feedback)
K = np.array([[0, -4]])

# Closed-loop (stable) continous-time system
sys_cl_scipy = sgnl.StateSpace(A + B @ K, np.zeros((n_x, 1)), np.eye(n_x), np.zeros((n_x, 1)))
sys_cl_ctrl = ctrl.StateSpace(A + B @ K, np.zeros((n_x, 1)), np.eye(n_x), np.zeros((n_x, 1)))

# Simulate dynamics
# NOTE: The only difference is that for SciPy to work I need to add np.ravel(), as otherwise I get the error
# ""ValueError: could not broadcast input array from shape (2,1) into shape (2,)""
_, _, x_scipy = sgnl.lsim(sys_cl_scipy, None, np.linspace(0, h * (N_k - 1), N_k), X0=np.ravel(x_0), interp=False)
_, _, x_ctrl = ctrl_mtl.lsim(sys_cl_ctrl, T=np.linspace(0, h * (N_k - 1), N_k), X0=x_0)
x_scipy = x_scipy.T
x_ctrl = x_ctrl.T

# Plot the state through state-space (verification)
fig_verify, ax = plt.subplots()
ax.plot(np.ravel(x_scipy[0, :]), np.ravel(x_scipy[1, :]), label=""x SciPy"", color=""green"")
ax.plot(np.ravel(x_ctrl[0, :]), np.ravel(x_ctrl[1, :]), label=""x control"", color=""blue"")
plt.legend()
plt.title(""State trajectory"")

# Display all plots
plt.show()

This gives this as result.

The control implementation works as expected, but the SciPy implementation seems completely wrong. Is there something I am missing, or how should I interpret this? Thanks in advance.
I tried to compare both methods on an identical setup, and they produce different results, whilst they both should simulate the output of a continous-time LTI system.
",2,132,"You appear to be running into a typing problem (as in the numerical type of the variables, not hitting keys with your fingers ). Your array A is an array of integers, and lsim uses the data type of A to determine the data type of the output array.  You can fix this by ensuring that the data type of A is floating point:
A = np.array([[0.0, 1.0],
              [-2.0, 3.0]])

P.S. I'm pretty sure anyone running lsim--which simulates a continuous-time system--would expect the calculation to use floating point values, so I'd call this a bug.  I created an issue for this in the SciPy github repo: https://github.com/scipy/scipy/issues/18982
",,
SciPy strange result,https://stackoverflow.com/questions/55169294,Convert AsciiMath/MathML to SVG/PNG,"I'm looking for a way to convert my AsciiMath (or MathML) sources to SVG and/or PNG. I've found a nodeJS library for SVG conversion but calling that from Python is not very convenient and the output is not entirely satisfying.

Taking the fact I'd like to render mathematical formulas to svg/png it seems logical to look for a solution in math libraries (NumPy, SciPy, Pandas, Matplotlib, Sympy, etc...) but to no avail. All my google results combining all possible permutations of asciimath+mathml+svg+png lead to nothing which is strange.

Please recommend me either search patterns to find a solution or share your experiences/ideas to get this seemingly simple job done in Python.

All help would be highly appreciated!
",2,2498,"I just created ziamath for exactly this purpose. It comes bundled with the STIX math font, so no setup is required beyond the pip install, but it should also work with other math-enabled fonts. Pure-Python, so it does not need a Latex installation or anything else to work. This first version doesn't quite cover the full MathML specification, but the most useful parts are in there.
To bundle it into an app with something like PyInstaller, you'll need to make sure the STIX font gets included, but that should just be one line in the PyInstaller config.
",,
SciPy strange result,https://stackoverflow.com/questions/54810650,Strange behaviour in scipy.solve_ivp when using an implicit method,"I recently ran into a question about integration and encountered a strange bug. I attempt a very simple problem using solve_ivp:

from scipy.integrate import solve_ivp
import numpy as np

def f(y, t):
    return y

y0 = [1,1,1,1]
method = 'RK23'
s = solve_ivp(f, (0,1), y0, method=method, t_eval=np.linspace(0,1))


And it works fine. When I change to method='BDF' or method='Radau' I get an error:

Traceback (most recent call last):

  File ""&lt;ipython-input-222-f11c4406e92c&gt;"", line 10, in &lt;module&gt;
    s = solve_ivp(f, (0,1), y0, method=method, t_eval=np.linspace(0,1))

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\ivp.py"", line 455, in solve_ivp
    solver = method(fun, t0, y0, tf, vectorized=vectorized, **options)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\radau.py"", line 299, in __init__
    self.jac, self.J = self._validate_jac(jac, jac_sparsity)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\radau.py"", line 345, in _validate_jac
    J = jac_wrapped(t0, y0, self.f)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\radau.py"", line 343, in jac_wrapped
    sparsity)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\common.py"", line 307, in num_jac
    return _dense_num_jac(fun, t, y, f, h, factor, y_scale)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\common.py"", line 318, in _dense_num_jac
    diff = f_new - f[:, None]

IndexError: too many indices for array


I also get an error with method = 'LSODA', although different (i.e. all implicit integrators). I do not get an error with any of the explicit integrators.

I tried this in spyder with scipy version 1.0.0 and in google colab (scipy version 1.1.0), with the same results.

Is this a bug or am I missing some argument I need for implicit integrators??
",2,900,"It appears that the Radau and BDF methods do not handle single-valued RHS functions. Making the function f above output a 1-D list solves your issue. Additionally, as mentioned by Weckesser in the comments, solve_ivp expects the RHS to be f(t, y) and not f(y, t).
Like this
def f(t, y):
    return [y]

",,
SciPy strange result,https://stackoverflow.com/questions/47931749,Why is Scipy&#39;s percentileofscore returning a different result than Excel&#39;s PERCENTRANK.INC?,,2,972,,,
SciPy strange result,https://stackoverflow.com/questions/37487532,Strange Result from FFT using Scipy,"I'm trying to do an FFT of some data (a Gaussian pulse), but I'm finding a strange result. The real and imaginary components of the resultant FFT alternate in sign every index of the array. The absolute values of the arrays, however, are continuous. So, I get something that looks like this:



Does anybody have an idea on what is causing this? Thanks!
",2,212,"Alternating signs in the frequency domain corresponds to an exp(j*pi*n) complex factor which by the shift theorem corresponds to a time domain circular shift of N/2 samples. Looking at your time domain Gaussian pulse you should notice that the peak indeed appears at N/2 instead of index 0.
Shifting back your time domain Gaussian pulse with ifftshift should give you a pulse centered at 0 whose frequency domain representation does not have this sign alternation.
",,
SciPy strange result,https://stackoverflow.com/questions/20061141,pymc 3.0 Predictive Posterior Distribution,,2,1176,,,
SciPy strange result,https://stackoverflow.com/questions/17527869,Curve fit fails with exponential but zunzun gets it right,,2,1944,,,
SciPy strange result,https://stackoverflow.com/questions/16929179,Strange result with python&#39;s (scipy) curve fitting,,2,1153,,,
SciPy strange result,https://stackoverflow.com/questions/57477723,Unexpected behaviour of scipy.integrate,,1,1213,,,
SciPy strange result,https://stackoverflow.com/questions/58164513,Problems with bisplrep and bisplev from scipy.interpolate,,1,1443,,,
SciPy strange result,https://stackoverflow.com/questions/74073866,SciPy Minimize doesn&#39;t pass all guesses on?,"I am trying to minimize a function of two variables using SciPy. The function itself is a chain of multiple lambda functions (makes it complicated but unfortunately it is the easiest way to write the expressions I need).
However, when using SciPy's minimize routine, I get the error ""TypeError: () missing 1 required positional argument: 'labour'""
Strangely enough, if I pass the arguments to the function directly, there is no error, so I assume that my chaining was correct.
Here is a minimum reproducible example:
# Preliminaries 0: import packages
import numpy as np
from scipy import optimize

# Preliminaries 1: Set parameters

alpha = 0.4
gamma = 0.4
delta = 0.05
beta = 0.95

# Preliminaries 2: Define functions

production_f = lambda capital, labour : (capital** alpha) * (labour ** (1-alpha))
utility_f_uni = lambda consumption, labour : np.log(consumption) + gamma * np.log(1-labour) if (consumption &gt; 0 and labour &gt; 0 and labour &lt; 1) else -5000
law_of_motion_f = lambda current_capital, next_capital, labour : production_f(current_capital, labour) - next_capital + (1-delta) * current_capital
utility_f_multi = lambda current_capital, next_capital, labour : utility_f_uni(law_of_motion_f(current_capital, next_capital, labour), labour)

optimization_f = lambda current_capital, next_capital, labour, value_f: utility_f_multi(current_capital, next_capital, labour) + beta * value_f(next_capital)

max_capital = lambda capital : production_f(capital, 1) + (1 - delta) * capital

For those knowledgeable of Dynamic Programming, I am trying to derive the value function of a growth model using Value Function iteration, but I didn't get so far yet. The period payoff is given by utility_f_multi. The Value function guess is given by optimization_f, which takes in four arguments including the previous value function guess.
In my example, I generate an interpolation that is closer to the true value function, but for all intents and purposes the constant 0 function also suffices. I then go on to create the optimization problem given our state variable
initial_value = lambda x : 0
current_optimization_f = lambda next_capital, labour: -optimization_f(3, next_capital, labour, initial_value)

Finally, I pass on the problem to the minimize function, from which the error results:
optimized_problem = optimize.minimize(current_optimization_f, [2,0.2])

The message, as mentioned previously, is ""() missing 1 required positional argument: 'labour'""
However, if I just pass on the two arguments to the function by hand, I receive no issue
print(current_optimization_f(2, 0.3))

which returns a value without problems.
Any help on this issue would be appreciated!
",1,37,"The function that you pass to scipy.minimize must use a singular argument for all the numerical inputs.
Imaging you wrote your function like:
def current_optimization_f(next_capital, labour):
   return optimization_f(3, next_capital, labour, initial_value)

scipy will call:
current_optimization([2, 0.3])

rather than
current_optimization(2, 0.3)

You can use an additional lambda to unpack the arguments:
obj_func = lambda x: current_optimization(*x)

rtn = optimize.minimize(obj_func, [2, 0.3])

And also, unrelated to your question, you should look at CasADI to solve these types of questions
",,
SciPy strange result,https://stackoverflow.com/questions/70371720,scipy curve_fit strange result matplotlib,"inbefore my code:
def func (x,a,b):
    return a*np.exp(b*x)

xFit= np.arange(0.0, 20, 0.01)
dev_Fluenz1= np.array([68.9, 21.81, 9.38, 3.73])
dev_Fluenz2= np.array([137.68 , 42.34, 18.75, 7.47 ])
dev_Fluenz3= np.array([80.34, 23.82 , 10.06, 3.76 ])
dev_Fluenz4= np.array([203.7, 61.67 , 10.06, 10.33 ])
dev_Fluenz5= np.array([135.74, 46.23 , 19.42 , 11.21 ])


dev_Fluenz6= np.array([382.83, 112.95, 50.02, 14.95])
dev_Fluenz7= np.array([382.45 , 117.62 , 50.01, 14.95 ])
dev_Fluenz8= np.array([147.32 , 43.67 , 17.88 , 5.01 ])
dev_Fluenz9= np.array([282.91, 85.64 , 35.63 , 13.77 ])
dev_Fluenz10= np.array([150.82, 51.37 , 20.5 , 11.21 ])


dev_x2= np.array([2,5,10,20])

plt.plot(dev_x2,dev_Fluenz1, 'bo')
popt, pcov = curve_fit(func, dev_x2, dev_Fluenz1)
plt.plot(xFit, func(xFit,*popt),color='b', linestyle='--',label=f'Reales DOE Sa &lt;= 0,3 Fluenz 1 J/cm**2 \u03bcm F(x) = {round(popt[0])} * e^({round(popt[1])}*x)')

results in : enter image description here
changing xFit= np.arange(0.0, 20, 0.01) to xFit= np.arange(0.0, 200, 0.01)
and
`dev_x2= np.array([2,5,10,20])` to `dev_x2= np.array([20,50,100,200])`

results in enter image description here
Why?
Thank you for your help!
",1,54,"curve_fit takes an initial guess as a starting point of the fit, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html.
If you do not provide it, as you did not, the default is used:

p0array_like, optional
Initial guess for the parameters (length N). If None, then the initial values will all be 1 (if the number of parameters for the function can be determined using introspection, otherwise a ValueError is raised).

For your second test case that blows up, it matters because with the default initial value of the exponent b in func, for the given range of dev_x2`, you get very large values and the optimizer cannot find a good solution
So you need to provide a sensible starting value. For example, note the p0 = [... bit below
xFit= np.arange(0.0, 200, 0.01)
dev_x2= np.array([20,50,100,200])

plt.plot(dev_x2,dev_Fluenz1, 'bo')
popt, pcov = curve_fit(func, dev_x2, dev_Fluenz1, p0 = [100,  -0.01])
plt.plot(xFit, func(xFit,*popt),color='b', linestyle='--',label=f'Reales DOE Sa &lt;= 0,3 Fluenz 1 J/cm**2 \u03bcm F(x) = {round(popt[0])} * e^({round(popt[1])}*x)')


you get a sensible result:

",,
SciPy strange result,https://stackoverflow.com/questions/69420714,Python deconvolution giving unexpected result,"Below I have plotted the signal (Lifetime decay) I am trying to deconvolve from an impulse response function, i.e. the divider (IRF). So I should just get the decay a bit sharper.
Here is an example of a topic I look at that gives what I need:
Understanding scipy deconvolve

Please not for my code, I am using only the peak of the divider (IRF), not the entire array sequence as shown on the image.
I am using the following code to do that:
IRF = IRF * (max(decay)/max(IRF))
# replace 0s to avoid error message 
IRF = np.where(IRF == 0, 0.1, IRF)    
decay = np.where(decay == 0, 0.1, decay)  
# take only the quotient part of the result 
deconv = scipy.signal.deconvolve(decay, IRF)[0]
# ""padding"" the deconvolved signal so it has the same size as the original signal 
s = int((len(decay)-(len(deconv)))/2)  ## difference on each side 
deconv_res = np.zeros(len(decay))   
end = int(len(decay)-s-1)  # final index
deconv_res[s:end] = deconv
deconv = deconv_res
# convolved normalized to decay height for plotting 
deconv_n = deconv * (max(decay)/max(deconv))   

The IRF is an array of float64, the signal is an array of uint16.
I admit I'm not so familiar with the maths of deconvolution, so I am trying blindly different things, like trying different divider functions, but nothing is producing anywhere near as expected.
The last result I got looks like this (see plot of the original signal and what the signal it tried to deconvolve..)

Could anyone give me some idea if it's something in scipy.deconvolve I don't understand, what the reason could be for this strange behaviour, or even some high-level reading material that might help me out?
Or if you think this problem is more physics-y than coding-related, a better place to ask such a question?
",1,762,"The problem is that deconvolve is a sort of polynomial division, it decomposes the output signal in $conv(h, x) + r$, if your signal is noisy it may give strange results. Also if the first sample in the inpulse response is small it tends to produce the exponentially growing output.
What I would do for this problem is the division of FFTs.
N = 2**(ceil(log2(len(IRF) + len(decay)))
filtered = ifft(fft(decay, N) / fft(IRF, N))

",,
SciPy strange result,https://stackoverflow.com/questions/69077658,Doubts on what scipy.optimize.minimize is really doing,"I am trying to minimize a cost function and I got very strange results from scipy.optimize.minimize (with methods and 'SLSQP', 'L-BFGS-B').
I print the value of the cost function after each evaluation. First it performs the small perturbations before going into the supposedly right direction (ok). But then occurs something strange: it seems to change the initial cost function by something like value of the cost function at first evaluation - value of the cost function in the current evaluation and converges towards the value of the first evaluation of the cost function.
To illustrate that I created a toy function of 2 parameters (0.25 + 1000 * x1 ** 2 + 100 * x2 ** 2 + 0.1 * random()). x1 and x2 are restricted to the interval [0, 1] (bounds). X0 is set to (0.5, 0.5). Here is what i get:
cost function: 275.3414617153509 x1: 0.5 x2: 0.5
cost function: 275.34428666473536 x1: 0.5000000149011612 x2: 0.5
cost function: 275.3542128554434 x1: 0.5 x2: 0.5000000149011612
cost function: 0.2665482586461191 x1: 0.0 x2: 0.0
cost function: 68.9989043756609 x1: 0.24986835289808013 x2: 0.24986835289808013
cost function: 154.87646326641064 x1: 0.374835397734792 x2: 0.374835397734792
cost function: 210.70119869030185 x1: 0.4373600232007103 x2: 0.4373600232007103
cost function: 241.8621094503892 x1: 0.4686490613793924 x2: 0.4686490613793924
cost function: 258.36597245010955 x1: 0.4843084999840323 x2: 0.4843084999840323
cost function: 266.6807722679986 x1: 0.4921461216177911 x2: 0.4921461216177911
cost function: 270.96794190195914 x1: 0.49606891372760337 x2: 0.49606891372760337
cost function: 273.0999396362265 x1: 0.49803236262951744 x2: 0.49803236262951744
cost function: 274.23903284113646 x1: 0.4990151079476797 x2: 0.4990151079476797
cost function: 274.7564047455383 x1: 0.4995070260788122 x2: 0.4995070260788122

 fun: 274.7564047455383
 jac: array([189579.1440506 , 855714.52631378])
 message: 'Optimization terminated successfully'
nfev: 14
 nit: 1
njev: 1
status: 0
success: True
x: array([0.49950703, 0.49950703])

So I do not understand:

why the final result is 2.74.756... and not 0.2666
why it starts to converge towards X0

What makes me think that the cost function is ""modified"" (i.e., what it tries to minimize is not the cost function but initial cost function evaluation - current cost function evaluation) is that, sometimes, due the random() part of the toy function, the first guessed evaluation is a higher value than the perturbation evaluations and it also converges towards X0.
I am using Python 3.9.6 and scipy 1.6.1
Edit:
Here is the full code:
def toto(X):
   val  = 0.25 + 1000 * X[0] ** 2 + 100 * X[1] ** 2 + 0.1 * random();
   print(""cost function:"", val, 'x1:', X[0], 'x2:', X[1])
   return val

optimization = minimize(toto, [0.5, 0.5], method=SLSQP, bounds= [[0.0, 1.0], [0.0, 1.0]])
print(optimization)

Mathieu
",1,880,"Trying your code, I get basically the same results.
I can't say I have a full solution to your problem, but I can point out a few issues. One is that scipy.optimize.minimize defaults to using a very small step to compute numerical gradients (e.g. for L-BFGS-B, the default step size eps  equals 1e-8). To see why this is a problem, consider if you computed a numerical derivative from the optimal solution (0,0). The deterministic part of the derivative will be roughly 0, but what will be the stochastic part. It should the difference of the two random values divided by 1e-8. The most likely value for the difference will be 0.05 (based on the difference having a triangular distribution), so your derivative will roughly on the order of 1e6. So while the function doesn't differ much with this random noise, it has a substantial effect on the numerical derivative.
But if the gradients are so large, why is it saying it converged? Both the methods you listed also have an ftol convergence criteria, which causes convergence when the relative change in the function value between steps is below the threshold. SLSQP doesn't provide any description in it's convergence message, but L-BFGS-B at least gives a brief description of why it converged. For the cases where it was far away from (0,0), convergence was related to this ftol criteria. I don't think there is anything in the code specifically drawing it back towards your initial point; rather it just seems like a random step away from this doesn't lead to much of a change in the function value. If I ran the code repeatedly, it would converge to a lot of different solutions and not always come back to near this initial point.
You can't entirely fix this with just a numerical gradient based optimizer, but you can at least improve your result by changing the value of eps. I found that if I changed eps to 1e-4, it tended to converge to (0,0) or near it. Increasing eps doesn't entirely fix, as the gradient can still be significantly altered by random portion.
Other options are discussed in this prior post and include methods for denoising your function before evaluating gradients or evaluating the function across the range, fitting it with splines, and then optimizing the fitting function.
If your interest is in diagnosing the problems with this specific code, someone who knows more about the technical details of the scipy implementation can probably help. However, if your interest is generally about finding minima of noisy functions, I think this example makes clear that a numerical gradient based optimizer won't be sufficient for this purpose.
",,
SciPy strange result,https://stackoverflow.com/questions/65825332,Scipy Coordinate system,"i cannot find the definition of scipy's coordinate system.
i have tried several values (assuming a right hand system) but got a strange result.
for example:
from scipy.spatial.transform import Rotation as R
R.from_euler('zyx', angles=np.array([90,0,0]), degrees=True).as_matrix()
[ [ 0., -1.,  0.], [ 1.,  0.,  0.],  [ 0.,  0.,  1.]]

meaninig the counterclockwise rotation about the z axis (true for a right hand system) is inverse (meaning a left coordinate system)...
where can i find the definition??
Thanks!!!
",1,972,"The full documentation for Scipy's Rotation module can be found here. For your problem in particular, I am not sure there actually is a problem. Looking at Wikipedia, a 90-degree rotation is indeed counter-clockwise so that a vector originally aligned with the x-axis becomes aligned with the y-axis. This, I believe, is in agreement with the result of the code below.
from scipy.spatial.transform import Rotation as R

point = (5, 0, -2)
print(R.from_euler('z', angles=90, degrees=True).as_matrix() @ point)
# [0, 5, -2]

","In short, I think giving positive angle means negative rotation about the axis, since it makes sense with the result.
Normally, positive direction of rotation about z-axis is rotating from x-axis to y-axis; negative direction is from y to x.
The Documentation shows that using from_euler to initial a counter-clockwise rotation of 90 degrees about the z-axis is
R.from_euler('z', 90, degrees=True)

I guess ""the counter-clockwise rotation about z-axis"" from doc means ""negative direction about z-axis"" instead of ""positive direction about z-axis"".
",
SciPy strange result,https://stackoverflow.com/questions/61091824,scipy normal distribution with scale greater and less than 1,"I'm using the normal distribution from numpy and having a hard time understanding its documentation. Let's say I have a normal distribution with mean of 5 and standard deviation of 0.5:

import numpy as np
from matplotlib import pyplot as plt
from scipy.stats import norm

mean = 5
std = 0.25

x = np.linspace(mean - 3*std, mean + 3*std, 1000)
y = norm(loc=mean, scale=std).pdf(x)
plt.plot(x,y)




The resulting chart is the familiar bell curve but with its peak at around 1.6. How can the probability of any value exceed 1? If I multiply it by scale  then the probabilities are correct.

No such problem when std (and scale) are greater than 1 however:

mean = 5
std = 10

x = np.linspace(mean - 3*std, mean + 3*std, 1000)
y = norm(loc=mean, scale=std).pdf(x)
plt.plot(x,y)




The documentation on norm says loc is the mean and scale is the standard deviation. Why does it behave so strangely with scale greater and less than 1?

Python 3.8.2. Scipy 1.4.1
",1,2546,,,
SciPy strange result,https://stackoverflow.com/questions/60787710,"Scipy Optimization TNC method, what does scale do ? solution to &quot;unable to progress&quot;?","I am running multiple bounded optimizations with known gradient (100 to 300 Variables). Sometimes, TNC is returning ""unable to progress"".

For my objective function L-BFGS-B is much slower and outputs poor results compared to TNC. (Maybe because TNC is better when number of variables are large)
Using Basinhopping with L-BFGS-B and niter_success to 10, I am getting results close to TNC with 20x slower speed. When TNC returns ""unable to progress"", ""L-BFGS-B"" returns better results. So my current solution is to run Basinhopping when TNC fails with status 6 - ""unable to progress"".

It seems that ""unable to progress"" is returned when TNC is unable to reduce the objective function for x number of iterations. I played a little with the scale factor and inconsistently I got better results.

To my knowledge, Scale in an optimization problem let's the optimizer know which variable is more effective. I have this information and I believe this will reduce number of ""unable to progress"" I am getting. According to the docs,
https://docs.scipy.org/doc/scipy/reference/optimize.minimize-tnc.html
""The default scale array are up-low for interval bounded variables and 1+|x] fo the others"". So it's upper bound - lower bound for variables which are bounded, unable to understand how unbounded is treated. what is 1+|x]  ?

Also, I manually calculated up - low and set unbounded to 1, this is returning different results every time I run the optimization with the same input. (Strange ?)

I also tried to look into the code, how TNC is handling scale, the spicy wrapper sends an empty array or the input array to C code https://github.com/scipy/scipy/blob/master/scipy/optimize/tnc/moduleTNC.c. In the C code I am unable to find where the scale array is created or how it's used. Also could not find when ""unable to progress"" is triggered. Can someone point me where I should look into ?
",1,1201,,,
SciPy strange result,https://stackoverflow.com/questions/53000952,ImportError: cannot import name &#39;LinearNDInterpolator&#39;,,1,1576,,,
SciPy strange result,https://stackoverflow.com/questions/52027309,calculating p value from pearson r different to scipy,,1,563,,,
SciPy strange result,https://stackoverflow.com/questions/45356848,scipy curve_fit not producing smooth graph when fitting fourier function,,1,609,,,
SciPy strange result,https://stackoverflow.com/questions/44474327,Real 1D DFT in fftw,,1,1718,,,
SciPy strange result,https://stackoverflow.com/questions/40008017,scipy curve_fit strange result,,1,728,,,
SciPy strange result,https://stackoverflow.com/questions/77686694,SVD Decomposition for linear equation solving,"looking for Algo here &amp; trying to implement this code, I'm getting different l2-norms for resulting vectors of params for linear equation. Where am I mistaken in my attempt to adopt the code?
import numpy as np
from scipy import linalg

np.random.seed(123)
v = np.random.rand(4)
A = v[:,None] * v[None,:]
b = np.random.randn(4)

x = linalg.inv(A.T.dot(A)).dot(A.T).dot(b) #Usually not recommended because of Numerical Instability of the Normal Equations  https://johnwlambert.github.io/least-squares/
l2_0= linalg.norm(A.dot(x) - b)
print(""manually: "", l2_0)

x = linalg.lstsq(A, b)[0]
l2_1= linalg.norm(A.dot(x) - b)
print(""scipy.linalg.lstsq: "", l2_1)

# 2-norm of two calculations compared
print(np.allclose(l2_0, l2_1, rtol=1.3e-1))

def direct_ls_svd(x,y):
  # append a columns of 1s (these are the biases)
  x = np.column_stack([np.ones(x.shape[0]), x])

  # calculate the economy SVD for the data matrix x
  U,S,Vt = linalg.svd(x, full_matrices=False)

  # solve Ax = b for the best possible approximate solution in terms of least squares
  x_hat = Vt.T @ linalg.inv(np.diag(S)) @ U.T @ y
  #print(x_hat)

  # perform train and test inference
  #y_pred = x @ x_hat
    
  return y-x @ x_hat     #x_hat

x= direct_ls_svd(A, b)
l2_svd= linalg.norm(A.dot(x) - b)
print(""svd: "", l2_svd)

# LU
x= linalg.solve(A.T@A, A.T@b)
l2_solve= linalg.norm(A.dot(x) - b)
print(""scipy.linalg.solve: "", l2_solve)

# manually:  2.9751344995811313
# scipy.linalg.lstsq:  2.9286130558050654
# True
# svd:  6.830550019041984
# scipy.linalg.solve:  2.928613055805065

if my error is in SVD-decomposition Algorithm imlementation for solving Least-Squares problem or perhaps in Numpy relative Scipy rounding or precision differences ? How to correct svd-algo for Least-Squares to become compareable with scipy's? And will this algorithm be faster &amp; less memory-consuming than Iterative Least-Squares methods?
P.S.
SVD applications or here, SVD for PCA &amp; PLS-SVD is my final goal -- will be the algo the same as for Least-Squares approximation ? I'm confused with such a question in general (even with code examples). Can somebody add some clarity for newbie like me, please?
P.P.S.
applying such implementation - I'm getting even worse result: svd:  10.031259300735462 for l2-norm
P.P.P.S.
also I lack some understanding of svd in singular spectral decomposition if exists ref, as 1st for unsupervised dim.reduction &amp; 2nd for non-parametric TS analisys, for practice
P.P.P.P.S. ! PCA is used preliminary estimating if Multicollinearity exists, otherwise strange results can get (biased etc.)... (if no collinearity =&gt; no sensitivity to error of estimation, aka shown in small condition number of OLS analysis, - vc.vs huge cond.num.==collinearity for multivariable/multidimensional regression)
",0,134,"The most important part here is to filter out the singular vallues that are 0. For your example data S is [9.22354602e-01 3.92705914e-17 1.10667017e-17 5.55744006e-18], notice that you have one singular value close of ~9.22 and the other three are tiny (&lt; 1e-16).
If you attempt to reconstruct the solution using the small errors some elements of Vt and U, that should be about the same magnitude will get divided by these small values and will add up to a significant error to the result.
What can be done in this case is, you assume that any singular value that are small enough are an exact zero. In the following modified version of your function I am assuming all the singular values that are less than rcond times the maximum singular value should be zero. Then I compute a mask m and drop the corresponding rows and columns of the U and Vt matrices.
def direct_ls_svd(A,b,rcond=1e-7):
  # calculate the economy SVD for the data matrix x
  U,S,Vt = linalg.svd(A, full_matrices=False)
  # Mask to remove the zeroes
  m = (abs(S) / np.max(abs(S))) &gt; rcond
  U, S, Vt = U[:,m], S[m], Vt[m, :]
  assert np.allclose( U @ np.diag(S) @ Vt, A)
  # solve Ax = b for the best possible approximate solution
  # in terms of least squares
  x_hat = Vt.T @ ((U.T @ b) / S)
    
  return x_hat

An alternative solution is to set S[m]=0 then you could avoid an extra copy in the worst case, but you lose the potential savings from the reduction of the number of multiplications in the very low rank cases.
",,
SciPy strange result,https://stackoverflow.com/questions/76610841,Why does the p-value of the scipy.stats.ketest drop when the number of samples increases?,"Now days I am working on a time-series event, and want to show it is NOT randomly generated events. For this purpose, I am trying to use the kstest in scipy. By the way, I have a question about the cipy.stats.kstest for the poisson distribution as follows,
--- Code 1---
from scipy.stats import poisson, kstest

noPts = 100 # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;---------------
lambdaPoisson = 10

my_data = poisson.rvs(size = noPts, mu = lambdaPoisson)

ks_statistic, p_value = kstest(my_data, 'poisson', args=(lambdaPoisson,0))
print(ks_statistic, p_value)

0.18677614630310613 0.0015821590670650476

--- code 2 ---
from scipy.stats import poisson, kstest

noPts = 1000 #&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;--------------
lambdaPoisson = 10

my_data = poisson.rvs(size = noPts, mu = lambdaPoisson)

ks_statistic, p_value = kstest(my_data, 'poisson', args=(lambdaPoisson,0))
print(ks_statistic, p_value)

0.13477614630310608 2.6511802749311937e-16

I expected that the larger number of data points would result in a more precise statistical match between the two distributions (poisson.rvs vs. kstest(poisson). Yes, the ks_statistics is getting smaller. BUT the p-value becomes very small which rejects the null hypothesis, that is very strange because the two distributions are the same 'Poisson' with the same parameter. What am I doing wrong?
Can someone understand and explain to me the discrepancy here?
",0,55,"The documentation for kstest says it is only valid for continuous distributions. Poissons are discrete.
",,
SciPy strange result,https://stackoverflow.com/questions/75586614,Different results when computing integral analytically/with scipy VS approximating integral,"I'm trying to calculate the integral of a variant of the Hill Equation shown here. 
When I try implementing this with the integrate function in SciPy, I get the following:
from scipy import integrate

Top = 0.9015038230670139
Bottom = 0.5972679490151096
ic50 = 17561.998143066336
Coef = -1.245569789770613

def Hill_formula(X):
    return Bottom + (Top - Bottom)/(1 + ((10**np.log10(ic50))/10**X)**Coef)

integrate.quad(Hill_formula, 0.001, 40.0)

with the result coming out to (25.18116866489653, 1.3873362345430754e-08)
However, looking at the graph of this equation with 100 evenly spaced points from 0.001 to 40.0, it clearly looks nothing close to the given answer of ~25: (EDIT: It actually does look right on hindsight because I misread the y-axis of the plot. Still would appreciate help on why this discrepancy exists though) 
Checking with an approximation method for the area under the curve gives a similar result:
from scipy.integrate import simpson
points = np.linspace(0.001, 40.0, num=100)
curve_results = []
for val in points:
    curve_results.append(Hill_formula(val))

np.abs(simpson(points, curve_results))

resulting in 1.2913519756923537. Is there a reason why this discrepancy exists?
I tried calculating the analytical solution of the integral and using that resulting equation to directly obtain the area under the curve between the bounds, but ran into that same strange ~25 answer.
",0,73,"From the documentation of scipy.simpson:
scipy.integrate.simpson(y, x=None, dx=1.0, axis=-1, even='avg')

Integrate y(x) using samples along the given axis and the composite
Simpson's rule. If x is None, spacing of dx is assumed.

If there are an even number of samples, N, then there are an odd
number of intervals (N-1), but Simpson's rule requires an even number
of intervals. The parameter 'even' controls how this is handled.

Parameters
----------
y : array_like
    Array to be integrated.
x : array_like, optional
    If given, the points at which `y` is sampled.
...

That means that you've probably meant to write simpson(curve_results, points) instead of np.abs(simpson(points, curve_results)). That results in 25.18116860801739, which has the same first 9 digits as the one you've obtained and the the analytical result evaluating to 25.181168664896531....
",,
SciPy strange result,https://stackoverflow.com/questions/72985838,Scipy Gumbel Fit does not Fit - What is the correct use?,"I'm trying to fit various distributions onto my data and test (chi-squared?) which fits best. I started out by using the gumbel_r distribution of scipy, as this is the one often used in literature.
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import scipy.stats as ss

data = pd.read_csv(""data.csv"")
data

sns.histplot(data[""score""], kde=True, stat='probability')
plt.show()

x = np.linspace(0,1,101)
hist, bins = np.histogram(data[""score""], bins=x, density=True)
loc, scale = ss.gumbel_r.fit(hist)
dist = ss.gumbel_r(loc=loc,scale=scale)
plt.plot(x, dist.pdf(x))
plt.show()

Inspecting the plots yields strange results. For example my data has a peak at ~0.09 of around ~0.025. However, the plotted gumbel looks completely off.
My questions are now:

Why are the plots not looking similar? I'm also suspecting stat='probability' could be the culprit here?
What do I need to do, such that the second plot will look somewhat similar to the first one?
Optimally I would get another hist for the same bins of the fitted distribution and input into scipy.stats.chisquare to quantify how good the fit of the distribution is and see which fits best. Is that correct?

",0,815,"Don't give hist to gumbel_r.fit().  It expects the original data.  Change the line that calls fit() to
loc, scale = ss.gumbel_r.fit(data['score'].to_numpy())

Also, to get the Seaborn plot on the same scale as the plot of the PDF, change stat='probability' to stat='density' in the histplot() call.
",,
SciPy strange result,https://stackoverflow.com/questions/66856730,LAPACK&#39;s zgesvd results different than scipy.linalg&#39;s SVD,"So I'm trying to compute SVD of an NxN matrix. Strangely, for all the cases of 2x2 matrices, the SVD from both lapack and scipy match but they differ when I go for a 3x3 or 4x4 matrices.
// LAPACK (C) Case: 2x2

#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;Accelerate/Accelerate.h&gt;

/* Complex datatype */
struct _dcomplex { double re, im; };
typedef struct _dcomplex dcomplex;

/* ZGESVD prototype */
extern void zgesvd( char* jobu, char* jobvt, int* m, int* n, dcomplex* a,
                   int* lda, double* s, dcomplex* u, int* ldu, dcomplex* vt, int* ldvt,
                   dcomplex* work, int* lwork, double* rwork, int* info );
/* Auxiliary routines prototypes */
extern void print_matrix( char* desc, int m, int n, dcomplex* a, int lda );
extern void print_rmatrix( char* desc, int m, int n, double* a, int lda );

/* Parameters */
#define M 2
#define N 2
#define LDA M
#define LDU M
#define LDVT N

/* Main program */
int main() {
    /* Locals */
    int m = M, n = N, lda = LDA, ldu = LDU, ldvt = LDVT, info, lwork;
    dcomplex wkopt;
    dcomplex* work;
    /* Local arrays */
    /* rwork dimension should be at least max( 1, 5*min(m,n) ) */
    double s[M], rwork[5*M];
    dcomplex u[LDU*M], vt[LDVT*N];
//    dcomplex a[LDA*N] = {
//        {0, 0}, {0, 0}, {1,  0},
//        {-0.36599657,  -0.27449743}, {-0.27449743,  0.36599657}, {0.76249285, 0},
//        {-0.36599657, 0.27449743}, {-0.27449743, -0.36599657}, {0.76249285, 0},
//    };
    dcomplex a[LDA*N] = {
        {0.70710678, 0}, {0, -0.70710678},
        {0.70710678,  0}, {0,  0.70710678},
    };
    /* Executable statements */
    printf( "" ZGESVD Example Program Results\n"" );
    /* Query and allocate the optimal workspace */
    lwork = -1;
    zgesvd( ""All"", ""All"", &amp;m, &amp;n, a, &amp;lda, s, u, &amp;ldu, vt, &amp;ldvt, &amp;wkopt, &amp;lwork,
           rwork, &amp;info );
    lwork = (int)wkopt.re;
    work = (dcomplex*)malloc( lwork*sizeof(dcomplex) );
    /* Compute SVD */
    zgesvd( ""All"", ""All"", &amp;m, &amp;n, a, &amp;lda, s, u, &amp;ldu, vt, &amp;ldvt, work, &amp;lwork,
           rwork, &amp;info );
    /* Check for convergence */
    if( info &gt; 0 ) {
        printf( ""The algorithm computing SVD failed to converge.\n"" );
        exit( 1 );
    }
    /* Print singular values */
    print_rmatrix( ""Singular values"", 1, m, s, 1 );
    /* Print left singular vectors */
    print_matrix( ""Left singular vectors (stored columnwise)"", m, m, u, ldu );
    /* Print right singular vectors */
    print_matrix( ""Right singular vectors (stored rowwise)"", m, n, vt, ldvt );
    /* Free workspace */
    free( (void*)work );
    exit( 0 );
} /* End of ZGESVD Example */

/* Auxiliary routine: printing a matrix */
void print_matrix( char* desc, int m, int n, dcomplex* a, int lda ) {
    int i, j;
    printf( ""\n %s\n"", desc );
    for( i = 0; i &lt; m; i++ ) {
        for( j = 0; j &lt; n; j++ )
        printf( "" (%6.2f,%6.2f)"", a[i+j*lda].re, a[i+j*lda].im );
        printf( ""\n"" );
    }
}

/* Auxiliary routine: printing a real matrix */
void print_rmatrix( char* desc, int m, int n, double* a, int lda ) {
    int i, j;
    printf( ""\n %s\n"", desc );
    for( i = 0; i &lt; m; i++ ) {
        for( j = 0; j &lt; n; j++ ) printf( "" %6.2f"", a[i+j*lda] );
        printf( ""\n"" );
    }
}

Yields
&lt;!-- language: lang-bash --&gt;
ZGESVD Example Program Results

 Singular values
   1.00   1.00

 Left singular vectors (stored columnwise)
 ( -0.71,  0.00) ( -0.71,  0.00)
 ( -0.00,  0.71) (  0.00, -0.71)

 Right singular vectors (stored rowwise)
 ( -1.00, -0.00) ( -0.00, -0.00)
 ( -0.00, -0.00) ( -1.00, -0.00)

and scipy.linalg.SVD yields
&lt;!-- language: lang-bash --&gt;
Singular values: [1. 1.]
Left singular vectors: [[-0.70710678+0.j         -0.70710678+0.j        ]
                        [ 0.        +0.70710678j  0.        -0.70710678j]]
Right singular vectors: [[-1.+0.j -0.+0.j]
                         [-0.+0.j -1.+0.j]]

So far so good. Now when I try to input a 3x3 or NxN matrix, the results are like
// LAPACK C Case: 3x3

#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;Accelerate/Accelerate.h&gt;

/* Complex datatype */
struct _dcomplex { double re, im; };
typedef struct _dcomplex dcomplex;

/* ZGESVD prototype */
extern void zgesvd( char* jobu, char* jobvt, int* m, int* n, dcomplex* a,
                   int* lda, double* s, dcomplex* u, int* ldu, dcomplex* vt, int* ldvt,
                   dcomplex* work, int* lwork, double* rwork, int* info );
/* Auxiliary routines prototypes */
extern void print_matrix( char* desc, int m, int n, dcomplex* a, int lda );
extern void print_rmatrix( char* desc, int m, int n, double* a, int lda );

/* Parameters */
#define M 3
#define N 3
#define LDA M
#define LDU M
#define LDVT N

/* Main program */
int main() {
    /* Locals */
    int m = M, n = N, lda = LDA, ldu = LDU, ldvt = LDVT, info, lwork;
    dcomplex wkopt;
    dcomplex* work;
    /* Local arrays */
    /* rwork dimension should be at least max( 1, 5*min(m,n) ) */
    double s[M], rwork[5*M];
    dcomplex u[LDU*M], vt[LDVT*N];
    dcomplex a[LDA*N] = {
        {0, 0}, {0, 0}, {1,  0},
        {-0.36599657,  -0.27449743}, {-0.27449743,  0.36599657}, {0.76249285, 0},
        {-0.36599657, 0.27449743}, {-0.27449743, -0.36599657}, {0.76249285, 0},
    };
//    dcomplex a[LDA*N] = {
//        {0.70710678, 0}, {0, -0.70710678},
//        {0.70710678,  0}, {0,  0.70710678},
//    };
    /* Executable statements */
    printf( "" ZGESVD Example Program Results\n"" );
    /* Query and allocate the optimal workspace */
    lwork = -1;
    zgesvd( ""All"", ""All"", &amp;m, &amp;n, a, &amp;lda, s, u, &amp;ldu, vt, &amp;ldvt, &amp;wkopt, &amp;lwork,
           rwork, &amp;info );
    lwork = (int)wkopt.re;
    work = (dcomplex*)malloc( lwork*sizeof(dcomplex) );
    /* Compute SVD */
    zgesvd( ""All"", ""All"", &amp;m, &amp;n, a, &amp;lda, s, u, &amp;ldu, vt, &amp;ldvt, work, &amp;lwork,
           rwork, &amp;info );
    /* Check for convergence */
    if( info &gt; 0 ) {
        printf( ""The algorithm computing SVD failed to converge.\n"" );
        exit( 1 );
    }
    /* Print singular values */
    print_rmatrix( ""Singular values"", 1, m, s, 1 );
    /* Print left singular vectors */
    print_matrix( ""Left singular vectors (stored columnwise)"", m, m, u, ldu );
    /* Print right singular vectors */
    print_matrix( ""Right singular vectors (stored rowwise)"", m, n, vt, ldvt );
    /* Free workspace */
    free( (void*)work );
    exit( 0 );
} /* End of ZGESVD Example */

/* Auxiliary routine: printing a matrix */
void print_matrix( char* desc, int m, int n, dcomplex* a, int lda ) {
    int i, j;
    printf( ""\n %s\n"", desc );
    for( i = 0; i &lt; m; i++ ) {
        for( j = 0; j &lt; n; j++ )
        printf( "" (%6.2f,%6.2f)"", a[i+j*lda].re, a[i+j*lda].im );
        printf( ""\n"" );
    }
}

/* Auxiliary routine: printing a real matrix */
void print_rmatrix( char* desc, int m, int n, double* a, int lda ) {
    int i, j;
    printf( ""\n %s\n"", desc );
    for( i = 0; i &lt; m; i++ ) {
        for( j = 0; j &lt; n; j++ ) printf( "" %6.2f"", a[i+j*lda] );
        printf( ""\n"" );
    }
}


Yeilds
&lt;!-- language: lang-bash --&gt;
 ZGESVD Example Program Results

 Singular values
   1.55   0.65   0.42

 Left singular vectors (stored columnwise)
 (  0.26,  0.00) (  0.49, -0.34) ( -0.75,  0.00)
 (  0.20, -0.00) ( -0.66,  0.46) ( -0.57,  0.00)
 ( -0.94, -0.00) (  0.00,  0.00) ( -0.33,  0.00)

 Right singular vectors (stored rowwise)
 ( -0.61, -0.00) ( -0.56, -0.00) ( -0.56, -0.00)
 (  0.00,  0.00) (  0.41, -0.58) ( -0.41,  0.58)
 ( -0.79, -0.00) (  0.43, -0.00) (  0.43, -0.00)

&lt;!-- language: lang-bash --&gt;
# Python
Singular values: [1.55161905 0.64699664 0.41698163]
Left singular vectors: [[ 0.26480555-9.68622857e-18j  0.57973136-1.54633603e-01j
  -0.75490266+2.76133169e-17j]
 [ 0.19860416+1.15286199e-17j -0.77297515+2.06178138e-01j
  -0.56617699-3.28655711e-17j]
 [-0.94362832+0.00000000e+00j  0.        +0.00000000e+00j
  -0.33100694+0.00000000e+00j]]
Right singular vectors: [[-0.60815722+0.j         -0.5613131 +0.j         -0.5613131 +0.j        ]
 [ 0.        +0.j          0.18223745-0.68321996j -0.18223745+0.68321996j]
 [-0.7938166 +0.j          0.43003209+0.j          0.43003209+0.j        ]]

Now for a fact I know that the results computed by Scipy are perfect as the purpose that I'm using SVD for is point perfect and gives perfect results and my goal is to generate results like scipy. Now I know that Scipy also uses LAPACK's Drivers but why the difference then? Where am I messing it up.
",0,155,"The conditions you should be testing is not that the output matrices are the same.
The condition u @ np.diag(s) @ vh = A with u, s, vh = np.linalg.svd(A), is where you should focus.
Also pay atention to the order of the singular values, if you have two set of singular values s1 and s2 that are the same when sorted, you can construct a matrix P such that P @ np.diag(s1) @ P.T = s2 if you are lucky you can then use u1 @ P.inv() = u2 and P.T.inv() @ vh1 = vh2 since (u1 @ P.inv()) @ P @ np.diag(s1) @ P.T @ (P.T.inv() @ vh1) = u1 @ np.diag(1) @ vh1.
",,
SciPy strange result,https://stackoverflow.com/questions/65678500,"How to use print() command and make the shape of a numpy array consistent, during integration using scipy?","I tried examining how spicy.integrate.ode works. The code below is simple code to do this.
def func(t, z, p):
    x = z[0]
    y = z[1]
    print('x :', x)
    print('x.shape :', x.shape)
    print('y :', y)
    print('y.shape :', y.shape)
    return [x*0, y*0]

t_ini = 0
t_fin = 1
x_ini = np.array([[2, 2]])
y_ini = np.array([[2, 2]])

solver = ode(func)
solver.set_integrator('dopri5')
solver.set_initial_value([x_ini, y_ini], t_ini)
solver.set_f_params([0])
solver.integrate(t_fin)
x_fin, y_fin = solver.y
print('x_fin :', x_fin)
print('y_fin :', y_fin)

However,
print('x :', x)
print('x.shape :', x.shape)
print('y :', y)
print('y.shape :', y.shape)
return [x*0, y*0]

didn't work. The result of the code was
x_fin : [[2. 2.]]
y_fin : [[2. 2.]]

.
Interestingly, when I changed x_ini and y_ini into
x_ini = np.array([[2]])
y_ini = np.array([[2]])

, the print() command worked and the result of the code was the repetition of
x : 2.0
x.shape : ()
y : 2.0
y.shape : ()

with the two lines after the repetition which are
x_fin : [[2.]]
y_fin : [[2.]]

.
It was strange that even if I put x_ini and y_ini having (1, 1) shape, both print(x.shape) and print(y.shape) showed ().
So the questions are:

Why the print() didn't worked for x_ini = y_ini = np.array([[2, 2]]) and what I should to to make them work?
Why the shape of the numpy arrays which are x and y became () instead of (1, 1).
How to make the shape of the numpy arrays which are x and y be (1, 1) during the integration using scipy. What should I do if the shape of both x_ini and y_ini is (2, 2) and I want to make the shape consistent during the integration using scipy.

Is there any guys who know about these?
",0,70,"I get a warning when using your initial value array:
In [9]: x_ini = np.array([[2, 2]])
   ...: y_ini = np.array([[2, 2]])
In [10]: solver.set_initial_value([x_ini, y_ini], 0)
Out[10]: &lt;scipy.integrate._ode.ode at 0x7f1ab8953d60&gt;
In [11]: solver.integrate(.1)
/usr/local/lib/python3.8/dist-packages/scipy/integrate/_ode.py:1181: UserWarning: dopri5: input is not consistent
  warnings.warn('{:s}: {:s}'.format(self.__class__.__name__,
Out[11]: 
array([[[2., 2.]],

       [[2., 2.]]])

The output is the same as the input
In [12]: np.array([x_ini, y_ini])
Out[12]: 
array([[[2, 2]],

       [[2, 2]]])

With
x_ini = np.array([[2]])
y_ini = np.array([[2]])

The initial value is a (2,1,1) array
In [18]: np.array([x_ini, y_ini])
Out[18]: 
array([[[2]],

       [[2]]])

That does run, but the values passed to your function are 0d arrays
x : 2.0
x.shape : ()
y : 2.0
y.shape : ()

===
Let's simplify the func:
In [20]: def func(t, z, p):
    ...:     print(type(z), z.shape, z)
    ...:     return z*0
    ...: 
In [21]: solver = ode(func)
    ...: solver.set_integrator('dopri5')
Out[21]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [22]: solver.set_f_params([0])
Out[22]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [23]: solver.set_initial_value([1,2], 0)
Out[23]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [24]: solver.integrate(.1)
&lt;class 'numpy.ndarray'&gt; (2,) [1. 2.]
...

If I change the initial value to a (2,1,1), func gets the same inputs:
In [27]: solver.set_initial_value([[[1]],[[2]]], 0)
Out[27]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [28]: solver.integrate(.1)
&lt;class 'numpy.ndarray'&gt; (2,) [1. 2.]

Change the input to a 3 element array:
In [31]: solver.set_initial_value([1,2,3], 0)
Out[31]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [32]: solver.integrate(.1)
&lt;class 'numpy.ndarray'&gt; (3,) [1. 2. 3.]

From the docs:
f : callable ``f(t, y, *f_args)``
    Right-hand side of the differential equation. t is a scalar,
    ``y.shape == (n,)``.
    ``f_args`` is set by calling ``set_f_params(*args)``.
    `f` should return a scalar, array or list (not a tuple).

f returns dy/dt.  The y will be a 1d array, and it's supposed to return a like size array.  Note the y.shape requirement.
The y that ode passes to the function is derived from the initial value array.  A (2,1,1) input is flattened to (2,).  A (2,1,2) produces the warning.
",,
SciPy strange result,https://stackoverflow.com/questions/64392302,"Scipy, Numpy, Django, Docker Issue","Having issues with scipy and numpy.
This code, part of a Django app, runs perfectly when run on my windows 10 system:
try:
    # sparse_load is a scipy.sparse.csr_matrix
    sparse_load = scipy.sparse.load_npz(cache)
    logger.info('Got sparse_load')
    concept_alias_tfidfs = sparse_load.astype(numpy.float32)
except:
    logger.exception('Something went wrong!' ) 
# code continues here ...

It also runs perfectly when running inside a docker container deployed on a Linux server.
The issue is that the type conversion (sparse_load.astype(numpy.float32)) crashes my app when running in a docker container deployed on Docker Desktop running on Windows 10.  The strange thing is that logger.exception is never executed!  I've tried other type conversions with the same result and also tried removing the astype altogether which resulted in another crash further down in the code (again w/o hitting the exception handler placed around that piece of code.
Thoughts?
",0,146,"Even though OOMKIlled is false (Killed due to Out of memory), I increased the memory from 2 to 8 GB on docker desktop and voila my app worked!
",,
SciPy strange result,https://stackoverflow.com/questions/62479689,Butterworth filters look very strange as increasing order,"I'm trying to design a simple Butterworth bandpass filter in SciPy, and I'm running into some strange results. 

import scipy.signal as signal
import numpy as np
import matplotlib.pyplot as plt

def butter_bandpass(lowcut, highcut, fs, freqs,order=3, label=None):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    sos = signal.butter(order, [low, high], btype='band', output='sos')

    w, h = signal.sosfreqz(sos,worN=freqs,whole=True,fs=fs)

    return w,h

freqs = 650

for i in np.arange(1,10):
    w,h = butter_bandpass(0.01, 0.1, fs=1/0.68, freqs=freqs, order=i)
    plt.plot(h)


This is giving strange results, as can be seen from the image below (Butterworth filters from order 1-10). I thought the filter was supposed to become increasingly rectangular as the order increased? 



Does anyone know how to design a simple Butterworth filter in SciPy?
",0,413,"The frequency response of the Butterworth filter is not real-valued. When plotting the complex-valued response using plt.plot(), only the real component is shown. You should see a warning:

ComplexWarning: Casting complex values to real discards the imaginary part


To examine the filter's gain, plot the magnitude of the frequency response:

plt.plot(np.abs(h))


You will see an increasingly square response, as expected:


",,
SciPy strange result,https://stackoverflow.com/questions/54369255,Signal processing - Why a signal is not completely filtered out at my cutoff frequency?,,0,355,,,
SciPy strange result,https://stackoverflow.com/questions/53187760,scipy.linalg.sparse.eigsh returns negative and non-consistent eigenvalues for positive semi-definite matrix,,0,765,,,
SciPy strange result,https://stackoverflow.com/questions/40540229,Python Scipy Optimize Error &quot;ValueError: Lengths must match to compare&quot;,,0,2280,,,
SciPy strange result,https://stackoverflow.com/questions/17543432,"Matlab calling python, returning vector, str2num not working on returned vector",,0,494,,,
SciPy strange issue,https://stackoverflow.com/questions/39350557,Running scipy.integrate.ode in multiprocessing Pool results in huge performance hit,"I'm using python's scipy.integrate to simulate a 29-dimensional linear system of differential equations. Since I need to solve several problem instances, I thought I could speed it up by doing computations in parallel using multiprocessing.Pool. Since there is no shared data or synchronization necessary between threads (the problem is embarrassingly parallel), I thought this should obviously work. After I wrote the code to do this, however, I got very strange performance measurements:


Single-threaded, without jacobian: 20-30 ms per call
Single-threaded, with jacobian: 10-20 ms per call
Multi-threaded, without jacobian: 20-30 ms per call
Multi-threaded, with jacobian: 10-5000 ms per call


What's shocking is that what I thought should be the fastest setup, was actually the slowest, and the variability was two orders of magnitude. It's a deterministic computation; computers aren't supposed to work this way. What could possibly be causing this?

Effect seems system-dependent

I tried the same code on another computer and I didn't see this effect. 

Both machines were using Ubuntu 64 bit, Python 2.7.6, scipy version 0.18.0, and numpy version 1.8.2. I didn't see the variability with an Intel(R) Core(TM) i5-5300U CPU @ 2.30GHz processor. I did see the issue with an Intel(R) Core(TM) i7-2670QM CPU @ 2.20GHz.

Theories

One thought was that there might be a shared cache among processors, and by running it in parallel I can't fit two instances of the jacobian matrix in the cache, so they constantly battle each other for the cache slowing each other down compared with if they are run serially or without the jacobian. But it's not a million variable system. The jacobian is a 29x29 matrix, which takes up 6728 bytes. The level 1 cache on the processor is 4 x 32 KB, much larger. Are there any other shared resources between processors that might be to blame? How can we test this?

Another thing I noticed is that each python process seems to take several hundred percent of the CPU as it's running. This seems to mean that the code is already parallelized at some point (perhaps in the low-level library). This could mean that further parallelization wouldn't help, but I wouldn't expect such a dramatic slowdown.

Code

It would be good to try out the on more machines to see if (1) other people can experience the slowdown at all and (2) what are the common features of systems where the slowdown occurs. The code does 10 trials of two parallel computations using a multiprocessing pool of size two, printing out the time per scipy.ode.integrate call for each of the 10 trials. 

'odeint with multiprocessing variable execution time demonsrtation'

from numpy import dot as npdot
from numpy import add as npadd
from numpy import matrix as npmatrix
from scipy.integrate import ode
from multiprocessing import Pool
import time

def main():
    ""main function""

    pool = Pool(2) # try Pool(1)
    params = [0] * 2

    for trial in xrange(10):
        res = pool.map(run_one, params)
        print ""{}. times: {}ms, {}ms"".format(trial, int(1000 * res[0]), int(1000 * res[1]))

def run_one(_):
    ""perform one simulation""

    final_time = 2.0
    init_state = [0.1 if d &lt; 7 else 0.0 for d in xrange(29)]
    (a_matrix, b_vector) = get_dynamics()

    derivative = lambda dummy_t, state: npadd(npdot(a_matrix, state), b_vector)
    jacobian = lambda dummy_t, dummy_state: a_matrix
    #jacobian = None # try without the jacobian

    #print ""jacobian bytes:"", jacobian(0, 0).nbytes

    solver = ode(derivative, jacobian)
    solver.set_integrator('vode')
    solver.set_initial_value(init_state, 0)

    start = time.time()
    solver.integrate(final_time)
    dif = time.time() - start

    return dif

def get_dynamics():
    ""return a tuple (A, b), which are the system dynamics x' = Ax + b""

    return \
    (
        npmatrix([
        [0, 0, 0, 0.99857378006, 0.053384274244, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 1, -0.003182219341, 0.059524655342, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, -11.570495605469, -2.544637680054, -0.063602626324, 0.106780529022, -0.09491866827, 0.007107574493, -5.20817921341, -23.125876742495, -4.246931301528, -0.710743697134, -1.486697327603, -0.044548215175, 0.03436637817, 0.022990248611, 0.580153205353, 1.047552018229, 11.265023544535, 2.622275290571, 0.382949404795, 0.453076470454, 0.022651889536, 0.012533628369, 0.108399390974, -0.160139432044, -6.115359574845, -0.038972389136, 0, ],
        [0, 0, 0.439356565475, -1.998182296753, 0, 0.016651883721, 0.018462046981, -0.001187470742, -10.778778281386, 0.343052863546, -0.034949331535, -3.466737362551, 0.013415853489, -0.006501746896, -0.007248032248, -0.004835912875, -0.152495086764, 2.03915052839, -0.169614300211, -0.279125393264, -0.003678218266, -0.001679708185, 0.050812027754, 0.043273505033, -0.062305315646, 0.979162836629, 0.040401368402, 0.010697028656, 0, ],
        [0, 0, -2.040895462036, -0.458999156952, -0.73502779007, 0.019255757332, -0.00459562242, 0.002120360732, -1.06432932386, -3.659159530947, -0.493546966858, -0.059561101143, -1.953512259413, -0.010939065041, -0.000271004496, 0.050563886711, 1.58833954495, 0.219923768171, 1.821923233098, 2.69319056633, 0.068619628466, 0.086310028398, 0.002415425662, 0.000727041422, 0.640963888079, -0.023016712545, -1.069845542887, -0.596675149197, 0, ],
        [-32.103607177734, 0, -0.503355026245, 2.297859191895, 0, -0.021215811372, -0.02116791904, 0.01581159234, 12.45916782984, -0.353636907076, 0.064136531117, 4.035326800046, -0.272152744884, 0.000999589868, 0.002529691904, 0.111632959213, 2.736421830861, -2.354540136198, 0.175216915979, 0.86308171287, 0.004401276193, 0.004373406589, -0.059795009475, -0.051005479746, 0.609531777761, -1.1157829788, -0.026305051933, -0.033738880627, 0, ],
        [0.102161169052, 32.057830810547, -2.347217559814, -0.503611564636, 0.83494758606, 0.02122657001, -0.037879735231, 0.00035400386, -0.761479736492, -5.12933410588, -1.131382179292, -0.148788337148, 1.380741054924, -0.012931029503, 0.007645723855, 0.073796656681, 1.361745395486, 0.150700793731, 2.452437244444, -1.44883919298, 0.076516270282, 0.087122640348, 0.004623192159, 0.002635233443, -0.079401941141, -0.031023369979, -1.225533436977, 0.657926151362, 0, ],
        [-1.910972595215, 1.713829040527, -0.004005432129, -0.057411193848, 0, 0.013989634812, -0.000906753354, -0.290513515472, -2.060635522957, -0.774845915178, -0.471751979387, -1.213891560083, 5.030515136324, 0.126407660877, 0.113188603433, -2.078420624662, -50.18523312358, 0.340665548784, 0.375863242926, -10.641168797333, -0.003634153255, -0.047962774317, 0.030509705209, 0.027584169642, -10.542357589006, -0.126840767097, -0.391839285172, 0.420788121692, 0, ],
        [0.126296110212, -0.002898250629, -0.319316070797, 0.785201711657, 0.001772374259, 0.00000584372, 0.000005233812, -0.000097899495, -0.072611454126, 0.001666291957, 0.195701043078, 0.517339177294, 0.05236528267, -0.000003359731, -0.000003009077, 0.000056285381, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [-0.018114066432, 0.077615035084, 0.710897211118, 2.454275059389, -0.012792968774, 0.000040510624, 0.000036282541, -0.000678672106, 0.010414324729, -0.044623231468, 0.564308412696, -1.507321670112, 0.066879720068, -0.000023290783, -0.00002085993, 0.000390189123, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [-0.019957254425, 0.007108972111, 122.639137999354, 1.791704310155, 0.138329792976, 0.000000726169, 0.000000650379, -0.000012165459, -8.481152717711, -37.713895394132, -93.658221074435, -4.801972165378, -2.567389718833, 0.034138340146, -0.038880106034, 0.044603217363, 0.946016722396, 1.708172458034, 18.369114490772, 4.275967542224, 0.624449778826, 0.738801257357, 0.036936909247, 0.020437742859, 0.176759579388, -0.261128576436, -9.971904607075, -0.063549647738, 0, ],
        [0.007852964982, 0.003925745426, 0.287856349997, 58.053471054491, 0.030698062827, -0.000006837601, -0.000006123962, 0.000114549925, -17.580742026275, 0.55713614874, 0.205946900184, -43.230778067404, 0.004227082975, 0.006053854501, 0.006646690253, -0.009138926083, -0.248663457912, 3.325105302428, -0.276578605231, -0.455150962257, -0.005997822569, -0.002738986905, 0.082855748293, 0.070563187482, -0.101597078067, 1.596654829885, 0.065879787896, 0.017442923517, 0, ],
        [0.011497315687, -0.012583019909, 13.848373855148, 22.28881517216, 0.042287331657, 0.000197558695, 0.000176939544, -0.003309689199, -1.742140233901, -5.959510415282, -11.333020298294, -14.216479234895, -3.944800806497, 0.001304578929, -0.005139259078, 0.08647432259, 2.589998222025, 0.358614863147, 2.970887395829, 4.39160430183, 0.111893402319, 0.140739944934, 0.003938671797, 0.001185537435, 1.045176603318, -0.037531801533, -1.744525005833, -0.972957942438, 0, ],
        [-16.939142002537, 0.618053512295, 107.92089190414, 204.524147386814, 0.204407545189, 0.004742101706, 0.004247169746, -0.079444150933, -2.048456967261, -0.931989524708, -66.540858220883, -116.470289129818, -0.561301215495, -0.022312225275, -0.019484747345, 0.243518778973, 4.462098610572, -3.839389874682, 0.285714413078, 1.40736916669, 0.007176864388, 0.007131419303, -0.097503691021, -0.083171197416, 0.993922379938, -1.819432085819, -0.042893874898, -0.055015718216, 0, ],
        [-0.542809857455, 7.081822285872, -135.012404429101, 460.929268260027, 0.036498617908, 0.006937238413, 0.006213200589, -0.116219147061, -0.827454697348, 19.622217613195, 78.553728334274, -283.23862765888, 3.065444785639, -0.003847616297, -0.028984525722, 0.187507140282, 2.220506417769, 0.245737625222, 3.99902408961, -2.362524402134, 0.124769923797, 0.142065016461, 0.007538727793, 0.004297097528, -0.129475392736, -0.050587718062, -1.998394759416, 1.072835822585, 0, ],
        [-1.286456393795, 0.142279456389, -1.265748910581, 65.74306027738, -1.320702989799, -0.061855995532, -0.055400100872, 1.036269854556, -4.531489334771, 0.368539277612, 0.002487097952, -42.326462719738, 8.96223401238, 0.255676968878, 0.215513465742, -4.275436802385, -81.833676543035, 0.555500345288, 0.612894852362, -17.351836610113, -0.005925968725, -0.078209662789, 0.049750119549, 0.044979645917, -17.190711833803, -0.206830688253, -0.638945907467, 0.686150823668, 0, ],
        [0, 0, 0, 0, 0, -0.009702263896, -0.008689641059, 0.162541456323, 0, 0, 0, 0, 0, 0, 0, 0, -0.012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [-8.153162937544, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.005, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, -3.261265175018, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.005, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 0, 0.17441246156, -3.261265175018, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.01, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, -3.261265175018, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -8.5, -18, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 0, -8.153162937544, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -8.5, -18, 0, 0, 0, 0, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 0.699960862226, 0.262038222227, 0.159589891262, 0.41155156501, -1.701619176699, -0.0427567124, -0.038285155304, 0.703045934017, 16.975651534025, -0.115788018654, -0.127109026104, 3.599544290134, 0.001229743857, 0.016223661959, -0.01033400498, -0.00934235613, -6.433934989563, 0.042639567847, 0.132540852847, -0.142338323726, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, -37.001496211974, 0.783588795613, -0.183854784348, -11.869599790688, -0.106084318011, -0.026306590251, -0.027118088888, 0.036744952758, 0.76460150301, 7.002366574508, -0.390318898363, -0.642631203146, -0.005701671024, 0.003522251111, 0.173867535377, 0.147911422248, 0.056092715216, -6.641979472328, 0.039602243105, 0.026181724138, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 1.991401999957, 13.760045912368, 2.53041689113, 0.082528789604, 0.728264862053, 0.023902766734, -0.022896554363, 0.015327568208, 0.370476566397, -0.412566245022, -6.70094564846, -1.327038338854, -0.227019235965, -0.267482033427, -0.008650986307, -0.003394359441, 0.098792645471, 0.197714179668, -6.369398456151, -0.011976840769, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 1.965859332057, -3.743127938662, -1.962645156793, 0.018929412474, 11.145046656101, -0.03600197464, -0.001222148117, 0.602488409354, 11.639787952728, -0.407672972316, 1.507740702165, -12.799953897143, 0.005393102236, -0.014208764492, -0.000915158115, -0.000640326416, -0.03653528842, 0.012458973237, -0.083125038259, -5.472831842357, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        ])
    , 
        npmatrix([1.0 if d == 28 else 0.0 for d in xrange(29)])
    )



if __name__ == ""__main__"":
    main()


Example Output

Here's an example of the output that demonstrates the problem (each run is slightly different). Notice the large variability in execution times (over two orders of magnitude!). Again, this all goes away if I either use a pool of size 1 (or run the code without a pool), or if I don't use an explicit jacobian in the call to integrate.


  
  times: 5847ms, 5760ms
  times: 4177ms, 3991ms
  times: 229ms, 36ms
  times: 1317ms, 1544ms
  times: 87ms, 100ms
  times: 113ms, 102ms
  times: 4747ms, 5077ms
  times: 597ms, 48ms
  times: 9ms, 49ms
  times: 135ms, 109ms
  

",14,2467,"Based on the variability of the execution times you posted for the machine showing the problem, I wonder what else that computer is doing at the time you are running your test.  Here are times I saw when I ran your code on an AWS r3.large server (2 cores, 15 GB of RAM) that normally runs interactive R sessions but is currently mostly idle:


  
  times: 11ms, 11ms
  times: 9ms, 9ms
  times: 9ms, 9ms
  times: 9ms, 9ms
  times: 10ms, 10ms
  times: 10ms, 10ms
  times: 10ms, 10ms
  times: 11ms, 10ms
  times: 11ms, 10ms
  times: 9ms, 9ms
  


Is it possible your machine is swapping and you do not know it?  vmstat 5 will give you a lot of information about swap in and outs, but not about cache evictions.

Intel makes some very nice tools for monitoring--two at a time--thousands of different types of operations and errors going on in a processor--including L2 cache evictions--but they are a bit of a firehose: there is information generated every microsecond--or more frequently--and you have to decide what you are going to monitor and how often you want an interrupt to deliver the numbers into your software.  Likely it will take many runs just to narrow down the stats you want to track and you still have to filter out the noise generated by the operating system and everything else running at the time.  It is a time consuming process, but if you follow it to the end and run many different tests you will come to understand what is going on.  

But is this--shared cache resources in a processor--really your question?  It seems more like you just want to figure out why you have variable run times on one machine and, second, why multi-threaded is slower on both machines than single threaded.  Do I have it right?  If not I will edit my answer and we can talk about processor cache, cache snooping and cache coherency.

So, as to the variability on the i7-2670QM CPU machine, I would start with htop, vmstat 5 and iostat 5 to see if the machine is doing something you didn't realize.  That much variability says the executable is getting stalled because the processor is busy doing something else: going off to the network and not finding a share it expects, unable to connect to a DNS server, getting kerbios failures: it could be a lot of things including hardware failures from a hard disk that is being continually reset.  Oh, and move your program to /dev/shm and cd there before you start it.  That won't help you if there are Python libraries in a bad place on a disk, but at least you won't have issues with your local directory.  Report back what you find and we can make further suggestions.

Your second question as I see it, which is perhaps where you started, is why is your program slower when run multi-threaded than single-threaded.  This is a big subject that will come a lot more in focus if we can see how you multi-threaded it.  But even before we do you have to realize that there are several things that can cause a multi-threaded program to run slower than a single-threaded program, and it can have as much to do with the support infrastructure around your program--libraries and operating system calls you make--as your program.  Just because you do not need mutexes does not mean the libraries and operating system do not need them when they are being called from a multi-threaded application.  Locking a mutex is an expensive operation, especially as different threads are rotated between different cores.  

On top of that, since the vode is not re-entrant, if you called it from multiple threads it is possible that it is having trouble finding convergence and having to recalculate the same values many times before it ""gets lucky"" and has enough processor time to complete an iteration before it is swapped out and intermediate results are overwritten.  Give us the code you are using for your multi-threaded runs and I will add to this answer.
","This is intended as a formatted comment regarding the mathematical background raised in a comment by @Dietrich. As it doesn't address the programming question, I intend to delete this answer in a little while until the bounty blows over.

As @Dietrich noted, you can solve your ODE exactly, since if

x' = A*x,


then the exact solution is

x(t) = exp(A*t)*x0


Already I'd say that an exact solution is always superior than a numerical approximation, but this can indeed be faster than a numerical integration. As you noted in a comment, you're worried about efficiency. So don't compute the matrix exponential for each t: compute the eigensystem of A only once:

A*v_i = L_i*v_i


then

x(t) = sum_i c_i*v_i*exp(L_i*t),


and the coefficients c_i can be determined from the linear equations

x0 = sum_i c_i*v_i.


Now, having an inhomogeneous term doesn't change much, as long as your matrix is not singular:

x' = A*x + b
(x - A^(-1)*b)' = A*(x - A^(-1)*b)


so we can solve the homogeneous equation for y = x - A^(-1)*b and in a final step recover x = y + A^(-1)*b.

This all works nicely while the matrix is regular, but in your specific case it's singular. But it turns out that this is due to your final dimension:

&gt;&gt;&gt; np.linalg.det(A)
0.0
&gt;&gt;&gt; np.linalg.det(A[:-1,:-1])
1920987.0461154305


And also note that the final row of A is all zeros (this is the reason for the singularity of A). So the last dimension of x is constant (or changes linearly due to b).

I suggest eliminating this variable, rewriting your equation for the rest of the variables, and solving the non-singular inhomogeneous linear system of ODEs using the above procedure, exactly. It should be faster and precise.



The following will be a bit speculative, see also the caveat at the end.

In case of user-input A and b, things might get trickier. Finding a zero row/column in your matrix would be easy, but A can be singular even though none of its rows/columns are fully zero. I'm not an expert in the subject, but I think your best bet is using something akin to principal component analysis: transforming your system of equations according to the eigensystem of A. My following thoughts will still assume that A is diagonalizable, but mostly because I'm unfamiliar with singular value decomposition. In realistic cases I'd expect your matrices to be diagonalizable, even if singular.

So I'll assume that the matrix A can be decomposed as

A = V * D * V^(-1),


where D is a diagonal matrix containing the eigenvalues of A, and columns of V are the eigenvectors of A corresponding to each respective eigenvalue. The very same decomposition can be obtained in numpy using

DD,V = np.linalg.eig(A)
D = np.asmatrix(np.diag(DD))


I usually prefer using ndarrays instead of matrices, but this way V*D*np.linalg.inv(V) would really correspond to the matrix product of the three matrices, rather than calling np.dot twice.

Now, rewrite your equation again:

x' = A*x + b
x' = V*D*V^(-1)*x + b
V^(-1)*x' = D*V^(-1)*x + V^(-1)*b


By defining the auxiliary variables

X = V^(-1)*x
B = V^(-1)*b


we obtain

X' = D*X + B


i.e. the usual inhomogeneous form, but now D is a diagonal matrix containing the eigenvalues of A in the diagonal.

Since A is singular, some of the eigenvalues are zero. Look for zero elements in D (well, you can do that already with DD from eig()), and you'll know that they behave trivially during time-evolution. The remaining variables behave well, although at this point we see that the equations for X are decoupled due to D being diagonal, so you could integrate each independently and analytically. For this you need to first go from your initial condition x0 to X0 = np.linalg.inv(V)*x0, then after solving the equations, back to x = V*X.

Caveat: as I said, I'm not an expert in this subject. I can easily imagine that the inversions involved in the diagonalization can be a numerical issue in practical applications. So I'd first test if the matrix is singular, and only carry on with this procedure if it is (or nearly is). It's possible that the above carries a lot of error, in which case numerical integration might be better (I really can't tell).
","on my compiled linux kernel :


times: 8ms, 7ms
times: 5ms, 4ms
times: 4ms, 4ms
times: 8ms, 8ms
times: 4ms, 4ms
times: 5ms, 4ms
times: 4ms, 8ms
times: 8ms, 8ms
times: 8ms, 8ms
times: 4ms, 5ms


Intel(R) Core(TM) i5-4300U CPU @ 1.90GHz

be sure your processor runs at fixed speed, noswap.
/tmp is mounted in RAM.
"
SciPy strange issue,https://stackoverflow.com/questions/65585639,Eigenvalues in Python: A Bug?,,10,1316,"As far as I know, assumption 1 is correct, but assumption 2 is not.
A Real Symmetric matrix produces eigenvalues and eigenvectors that are real only.
However, for a given eigenvalue, the associated eigenvector isn't necessarily unique.
Furthermore, round-off error shouldn't be so significant for a matrix that actually isn't that big, or contain numbers that aren't very small.
For comparison, I ran your test matrix through a JavaScript version of RG.F (Real General, from the EISPACK Library):  Eigenvalues and Eigenvectors Calculator
Here is the output:
Eigenvalues:
   20
   12
   20
   20
   20
   20
   20
   20

Eigenvectors:
 0.9354143466934854     0.35355339059327395     -0.021596710639534     -0.021596710639534     -0.021596710639534     -0.021596710639534     -0.021596710639533997     -0.021596710639533997
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999622     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999622

No imaginary components.
To confirm, or deny, the validity of results, you could always write a small program that plugs the results back into the original equation. Simple matrix and vector multiplication. Then you'd know for sure whether or not the outputs are correct. Or, if they are wrong, how far away from correct answers they are.
",,
SciPy strange issue,https://stackoverflow.com/questions/36706163,python multiprocessing module: strange behaviour and processor load when using Pool,,4,871,"According to here, there is an issue when OpenBLAS is used together with joblib.

Similar issues occur when MKL is used (see here).
The solution given here, also worked for me:
Adding

import os
os.environ['MKL_NUM_THREADS'] = '1'


at the beginning of my python script solves the issue.
",,
SciPy strange issue,https://stackoverflow.com/questions/28056404,MATLAB and SciPy give different results for &#39;buttord&#39; function,"I'm trying to design an analog Butterworth filter using the buttord function (actually, I'm porting a program where this function is called from MATLAB to Python). 

My parameters are:

Passband frequency (Fp) = 10 Hz, giving Wp = 2*pi*10 Hz

Stopband frequency (Fs) = 100 Hz, giving Ws = 2*pi*100 Hz

The passband and stopband losses/attenuations (Rp, Rs) are 3 and 80 dB respectively.

In MATLAB I use this code:

Wp = 2 * pi * 10
Ws = 2 * pi * 100
Rp = 3
Rs = 80
[N, Wn] = buttord(Wp, Ws, Rp, Rs, 's')


that gives me N = 5, Wn = 99.581776302.

In SciPy I tried to do the same:

from numpy import pi
from scipy import signal
Wp = 2 * pi * 10
Ws = 2 * pi * 100
Rp = 3
Rs = 80
(N, Wn) = signal.buttord(Wp, Ws, Rp, Rs, analog=True)


and I get N = 5 and Wn = 62.861698649592753. Wn is different than the value that MATLAB gives, and is strangely close to Wp. What is wrong here?

Digging into SciPy's sources and issues, I found this pull request which might explain things: turns out MATLAB and SciPy have different design goals (MATLAB tries to optimize for matching the stopband frequency and SciPy tries to optimize for matching the passband frequency).

I'm using MATLAB R2013a, Python 3.4.2 and SciPy 0.15.0 if that matters.
",4,928,"(I also posted the following on the scipy mailing list.)

When you design a Butterworth filter with buttord, there aren't enough
degrees of freedom to meet all the design constraints exactly.  So there
is a choice of which end of the transition region hits the constraints
and which end is ""over-designed"".  A change made in scipy 0.14.0 switched that choice from the stop-band edge to the pass-band edge.

A picture will make it clear.  The script below generates the following plot.  (I changed Rp from 3 to 1.5.  -3 dB coincides with the gain at Wn, that's why your Wn was the same as Wp.)  The filters generated using either the old or new convention both satisfy the design constraints.  With the new convention, the response just bumps against the constraint at the end of the pass-band.



import numpy as np
from scipy.signal import buttord, butter, freqs
import matplotlib.pyplot as plt


# Design results for:
Wp = 2*np.pi*10
Ws = 2*np.pi*100
Rp = 1.5      # instead of 3
Rs = 80

n_old = 5
wn_old = 99.581776302787929

n_new, wn_new = buttord(Wp, Ws, Rp, Rs, analog=True)

b_old, a_old = butter(n_old, wn_old, analog=True)
w_old, h_old = freqs(b_old, a_old)

b_new, a_new = butter(n_new, wn_new, analog=True)
w_new, h_new = freqs(b_new, a_new)


db_old = 20*np.log10(np.abs(h_old))
db_new = 20*np.log10(np.abs(h_new))

plt.semilogx(w_old, db_old, 'b--', label='old')
plt.axvline(wn_old, color='b', alpha=0.25)
plt.semilogx(w_new, db_new, 'g', label='new')
plt.axvline(wn_new, color='g', alpha=0.25)

plt.axhline(-3, color='k', ls=':', alpha=0.5, label='-3 dB')

plt.xlim(40, 1000)
plt.ylim(-100, 5)

xbounds = plt.xlim()
ybounds = plt.ylim()
rect = plt.Rectangle((Wp, ybounds[0]), Ws - Wp, ybounds[1] - ybounds[0],
                     facecolor=""#000000"", edgecolor='none', alpha=0.1, hatch='//')
plt.gca().add_patch(rect)
rect = plt.Rectangle((xbounds[0], -Rp), Wp - xbounds[0], 2*Rp,
                     facecolor=""#FF0000"", edgecolor='none', alpha=0.25)
plt.gca().add_patch(rect)
rect = plt.Rectangle((Ws, ybounds[0]), xbounds[1] - Ws, -Rs - ybounds[0],
                     facecolor=""#FF0000"", edgecolor='none', alpha=0.25)
plt.gca().add_patch(rect)

plt.annotate(""Pass"", (0.5*(xbounds[0] + Wp), Rp+0.5), ha='center')
plt.annotate(""Stop"", (0.5*(Ws + xbounds[1]), -Rs+0.5), ha='center')
plt.annotate(""Don't Care"", (0.1*(8*Wp + 2*Ws), -Rs+10), ha='center')

plt.legend(loc='best')
plt.xlabel('Frequency [rad/s]')
plt.ylabel('Gain [dB]')
plt.show()

",,
SciPy strange issue,https://stackoverflow.com/questions/57484399,Issue importing scikit-learn: module &#39;scipy&#39; has no attribute &#39;_lib&#39;,,3,12402,"I encountered the same error after letting my PC sit for 4 days unattended. Restarting the kernel solved it.
This probably won't work for everyone, but it might save someone a little agony.
","I ended up fixing this by uninstalling my current version of Anaconda and installing a version from a few months ago. I didn't get the ""ordinal 242"" error nor the issues with scikit-learn.
","had a similar problem on google collaboratory, simply uninstalling and reinstalling the scipy module alone solved it for me

and the simple fact that collab ai suggested this move emboldened me
"
SciPy strange issue,https://stackoverflow.com/questions/71146140,Using RNN Trained Model without pytorch installed,,3,934,"You should try to export the model using torch.onnx. The page gives you an example that you can start with.
An alternative is to use TorchScript, but that requires torch libraries.
Both of these can be run without python. You can load torchscript in a C++ application https://pytorch.org/tutorials/advanced/cpp_export.html
ONNX is much more portable and you can use in languages such as C#, Java, or Javascript
https://onnxruntime.ai/ (even on the browser)
A running example
Just modifying a little your example to go over the errors I found
Notice that via tracing any if/elif/else, for, while will be unrolled
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import random

torch.manual_seed(1)
random.seed(1)
device = torch.device('cpu')

class RNN(nn.Module):
  def __init__(self, input_size, hidden_size, output_size,num_layers, matching_in_out=False, batch_size=1):
    super(RNN, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.num_layers = num_layers
    self.batch_size = batch_size
    self.matching_in_out = matching_in_out #length of input vector matches the length of output vector 
    self.lstm = nn.LSTM(input_size, hidden_size,num_layers)
    self.hidden2out = nn.Linear(hidden_size, output_size)
  def forward(self, x, h0, c0):
    lstm_out, (hidden_a, hidden_b) = self.lstm(x, (h0, c0))
    outs=self.hidden2out(lstm_out)
    return outs, (hidden_a, hidden_b)
  def init_hidden(self):
    #return torch.rand(self.num_layers, self.batch_size, self.hidden_size)
    return (torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device).detach(),
            torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device).detach())

# convert the arguments passed during onnx.export call
class MWrapper(nn.Module):
    def __init__(self, model):
        super(MWrapper, self).__init__()
        self.model = model;
    def forward(self, kwargs):
        return self.model(**kwargs)

Run an example
rnn = RNN(10, 10, 10, 3)
X = torch.randn(3,1,10)
h0,c0  = rnn.init_hidden()
print(rnn(X, h0, c0)[0])

Use the same input to trace the model and export an onnx file

torch.onnx.export(MWrapper(rnn), {'x':X,'h0':h0,'c0':c0}, 'rnn.onnx', 
                  dynamic_axes={'x':{1:'N'},
                               'c0':{1: 'N'},
                               'h0':{1: 'N'}
                               },
                  input_names=['x', 'h0', 'c0'],
                  output_names=['y', 'hn', 'cn']
                 )

Notice that you can use symbolic values for the dimensions of some axes of some inputs. Unspecified dimensions will be fixed with the values from the traced inputs. By default LSTM uses dimension 1 as batch.
Next we load the ONNX model and pass the same inputs
import onnxruntime
ort_model = onnxruntime.InferenceSession('rnn.onnx')
print(ort_model.run(['y'], {'x':X.numpy(), 'c0':c0.numpy(), 'h0':h0.numpy()}))

","Basically implementing it in numpy and copying weights from your pytorch model can do the trick.  For your usecase you will only need to do a forward pass so we just need to implement that only
#Set Parameters for a small LSTM network
input_size  = 2 # size of one 'event', or sample, in our batch of data
hidden_dim  = 3 # 3 cells in the LSTM layer
output_size = 1 # desired model output

num_layers=3
torch_lstm = RNN( input_size, 
                 hidden_dim ,
                 output_size,
                 num_layers,
                 matching_in_out=True
                 )

state = torch_lstm.state_dict() # state will capture the weights of your model

Now for LSTM in numpy these functions will be used:
got the below code from this link: https://towardsdatascience.com/the-lstm-reference-card-6163ca98ae87
### NOT MY CODE
import numpy as np 
from scipy.special import expit as sigmoid

def forget_gate(x, h, Weights_hf, Bias_hf, Weights_xf, Bias_xf, prev_cell_state):
    forget_hidden  = np.dot(Weights_hf, h) + Bias_hf
    forget_eventx  = np.dot(Weights_xf, x) + Bias_xf
    return np.multiply( sigmoid(forget_hidden + forget_eventx), prev_cell_state )

def input_gate(x, h, Weights_hi, Bias_hi, Weights_xi, Bias_xi, Weights_hl, Bias_hl, Weights_xl, Bias_xl):
    ignore_hidden  = np.dot(Weights_hi, h) + Bias_hi
    ignore_eventx  = np.dot(Weights_xi, x) + Bias_xi
    learn_hidden   = np.dot(Weights_hl, h) + Bias_hl
    learn_eventx   = np.dot(Weights_xl, x) + Bias_xl
    return np.multiply( sigmoid(ignore_eventx + ignore_hidden), np.tanh(learn_eventx + learn_hidden) )


def cell_state(forget_gate_output, input_gate_output):
    return forget_gate_output + input_gate_output

  
def output_gate(x, h, Weights_ho, Bias_ho, Weights_xo, Bias_xo, cell_state):
    out_hidden = np.dot(Weights_ho, h) + Bias_ho
    out_eventx = np.dot(Weights_xo, x) + Bias_xo
    return np.multiply( sigmoid(out_eventx + out_hidden), np.tanh(cell_state) )


We would need the sigmoid function as well so
def sigmoid(x):
    return 1/(1 + np.exp(-x))

Because pytorch stores weights in stacked manner so we need to break it up for that we would need the below function
def get_slices(hidden_dim):
    slices=[]
    breaker=(hidden_dim*4)
    slices=[[i,i+3] for i in range(0, breaker, breaker//4)]
    return slices

Now we have the functions ready for lstm, now we create an lstm class to copy the weights from pytorch class and get the output from it.
class numpy_lstm:
    def __init__( self, layer_num=0, hidden_dim=1, matching_in_out=False):
        self.matching_in_out=matching_in_out
        self.layer_num=layer_num
        self.hidden_dim=hidden_dim
        
    def init_weights_from_pytorch(self, state):
        slices=get_slices(self.hidden_dim)
        print (slices)

        #Event (x) Weights and Biases for all gates
        
        lstm_weight_ih='lstm.weight_ih_l'+str(self.layer_num)
        self.Weights_xi = state[lstm_weight_ih][slices[0][0]:slices[0][1]].numpy()  # shape  [h, x]
        self.Weights_xf = state[lstm_weight_ih][slices[1][0]:slices[1][1]].numpy()  # shape  [h, x]
        self.Weights_xl = state[lstm_weight_ih][slices[2][0]:slices[2][1]].numpy()  # shape  [h, x]
        self.Weights_xo = state[lstm_weight_ih][slices[3][0]:slices[3][1]].numpy() # shape  [h, x]

        
        lstm_bias_ih='lstm.bias_ih_l'+str(self.layer_num)
        self.Bias_xi = state[lstm_bias_ih][slices[0][0]:slices[0][1]].numpy()  #shape is [h, 1]
        self.Bias_xf = state[lstm_bias_ih][slices[1][0]:slices[1][1]].numpy()  #shape is [h, 1]
        self.Bias_xl = state[lstm_bias_ih][slices[2][0]:slices[2][1]].numpy()  #shape is [h, 1]
        self.Bias_xo = state[lstm_bias_ih][slices[3][0]:slices[3][1]].numpy() #shape is [h, 1]
        
        
        lstm_weight_hh='lstm.weight_hh_l'+str(self.layer_num)

        #Hidden state (h) Weights and Biases for all gates
        self.Weights_hi = state[lstm_weight_hh][slices[0][0]:slices[0][1]].numpy()  #shape is [h, h]
        self.Weights_hf = state[lstm_weight_hh][slices[1][0]:slices[1][1]].numpy()  #shape is [h, h]
        self.Weights_hl = state[lstm_weight_hh][slices[2][0]:slices[2][1]].numpy()  #shape is [h, h]
        self.Weights_ho = state[lstm_weight_hh][slices[3][0]:slices[3][1]].numpy() #shape is [h, h]
        
        
        lstm_bias_hh='lstm.bias_hh_l'+str(self.layer_num)

        self.Bias_hi = state[lstm_bias_hh][slices[0][0]:slices[0][1]].numpy()  #shape is [h, 1]
        self.Bias_hf = state[lstm_bias_hh][slices[1][0]:slices[1][1]].numpy()  #shape is [h, 1]
        self.Bias_hl = state[lstm_bias_hh][slices[2][0]:slices[2][1]].numpy()  #shape is [h, 1]
        self.Bias_ho = state[lstm_bias_hh][slices[3][0]:slices[3][1]].numpy() #shape is [h, 1]
    def forward_lstm_pass(self,input_data):
        h = np.zeros(self.hidden_dim)
        c = np.zeros(self.hidden_dim)
        
        output_list=[]
        for eventx in input_data:
            f = forget_gate(eventx, h, self.Weights_hf, self.Bias_hf, self.Weights_xf, self.Bias_xf, c)
            i =  input_gate(eventx, h, self.Weights_hi, self.Bias_hi, self.Weights_xi, self.Bias_xi, 
                        self.Weights_hl, self.Bias_hl, self.Weights_xl, self.Bias_xl)
            c = cell_state(f,i)
            h = output_gate(eventx, h, self.Weights_ho, self.Bias_ho, self.Weights_xo, self.Bias_xo, c)
            if self.matching_in_out: # doesnt make sense but it was as it was in main code :(
                output_list.append(h)
        if self.matching_in_out:
            return output_list
        else:
            return h


Similarly for fully connected layer,
    
    
class fully_connected_layer:
    def __init__(self,state, dict_name='fc', ):
        self.fc_Weight = state[dict_name+'.weight'][0].numpy()
        self.fc_Bias = state[dict_name+'.bias'][0].numpy() #shape is [,output_size]
        
    def forward(self,lstm_output, is_sigmoid=True):
        res=np.dot(self.fc_Weight, lstm_output)+self.fc_Bias
        print (res)
        if is_sigmoid:
            return sigmoid(res)
        else:
            return res
        

Now we would need one class to call all of them together and generalise them with respect to multiple layers
You can modify the below class if you need more Fully connected layers or want to set false condition for sigmoid etc.
        
class RNN_model_Numpy:
    def __init__(self, state, input_size, hidden_dim, output_size, num_layers, matching_in_out=True):
        self.lstm_layers=[]
        for i in range(0, num_layers):
            lstm_layer_obj=numpy_lstm(layer_num=i, hidden_dim=hidden_dim, matching_in_out=True)
            lstm_layer_obj.init_weights_from_pytorch(state) 
            self.lstm_layers.append(lstm_layer_obj)
        
        self.hidden2out=fully_connected_layer(state, dict_name='hidden2out')
        
    def forward(self, feature_list):
        for x in self.lstm_layers:
            lstm_output=x.forward_lstm_pass(feature_list)
            feature_list=lstm_output
            
        return self.hidden2out.forward(feature_list, is_sigmoid=False)

Sanity check on a numpy variable:
data = np.array(
           [[1,1],
            [2,2],
            [3,3]])



check=RNN_model_Numpy(state, input_size, hidden_dim, output_size, num_layers)
check.forward(data)

EXPLANATION:
Since we just need forward pass, we would need certain functions that are required in LSTM, for that we have the forget gate, input gate, cell gate and output gate. They are just some operations that are done on the input that you give.
For get_slices function, this is used to break down the weight matrix that we get from pytorch state dictionary (state dictionary) is the dictionary which contains the weights of all the layers that we have in our network.
For LSTM particularly have it in this order  ignore, forget, learn, output. So for that we would need to break it up for different LSTM cells.
For numpy_lstm class, we have init_weights_from_pytorch function which must be called, what it will do is that it will extract the weights from state dictionary which we got earlier from pytorch model object and then populate the numpy array weights with the pytorch weights. You can first train your model and then save the state dictionary through pickle and then use it.
The fully connected layer class just implements the hidden2out neural network.
Finally our rnn_model_numpy class is there to ensure that if you have multiple layers then it is able to send the output of one layer of lstm to other layer of lstm.
Lastly there is a small sanity check on data variable.
IMPORTANT NOTE: PLEASE NOTE THAT YOU MIGHT GET DIMENSION ERROR AS PYTORCH WAY OF HANDLING INPUT IS COMPLETELY DIFFERENT SO PLEASE ENSURE THAT YOU INPUT NUMPY IS OF SIMILAR SHAPE AS DATA VARIABLE.
Important references:
https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
https://christinakouridi.blog/2019/06/19/backpropagation-lstm/
",
SciPy strange issue,https://stackoverflow.com/questions/54671752,"Why does glmnet.py throw a TypeError at cvglmnet when I provide floats, if glmnet does require floats?",,3,973,"I was having a similar problem but with X data, and I solve it by putting
cvglmnet(x=X.astype(np.float64), y=y,family = ""gaussian"",alpha=1)

","Does this work? 

cvglmnet(x = fold1_sp.copy(), y = np.int64(y.values), family = 'binomial', ptype = 'class')

",
SciPy strange issue,https://stackoverflow.com/questions/20061141,pymc 3.0 Predictive Posterior Distribution,,2,1176,"There has been a lot of development activity on PyMC3 recently, so I thought I'd post a little answer to your question, even though you figured this out yourself over a year ago.

First, the pymc3.find_MAP function now tries to handle discrete variables, so to find a MAP you should just have to do this:

import pymc3 as pm

m = pm.Model()
with m:
    p        = pm.Beta('p',2,2)
    surv_sim = pm.Binomial('surv_sim', n=20, p=p)
    surv     = pm.Binomial('surv', n=20, p=p, observed=15)

    map_est = pm.find_MAP()

map_est


For me, this results in the following

{'p': array(0.6190476210094416), 'surv_sim': 10.0} 


which does not seem to be correct.  Perhaps this is worthy of a bug report.

Sampling from the posterior distribution now requires selecting a step method explicitly, and for a discrete variable pymc3.step_methods.Metropolis is an acceptable choice. It works fine for the continuous variable in this example as well:

with m:
    step1 = pm.step_methods.Metropolis(vars=[p])
    step2 = pm.step_methods.Metropolis(vars=[surv_sim])
    trace = pm.sample(10000, [step1, step2])

",,
SciPy strange issue,https://stackoverflow.com/questions/47931749,Why is Scipy&#39;s percentileofscore returning a different result than Excel&#39;s PERCENTRANK.INC?,,2,972,"The Excel function PERCENTILERANK.INC excludes the max value (in my case 45). Which is why it shows 0 versus 6.25 like scipy does.

To rectify this, I modified my function to remove the max values of the array like so:

array = list(filter(lambda a: a != max(array), array))

return 100 - int(stats.percentileofscore(array, score, kind='strict'))


This gave me the correct results, and all my other tests passed.

Additional information based on Brian Pendleton's comment. Here is a link to the Excel functions explaining PERCENTILERANK.INC as well as other ranking functions. Thanks for this.
",,
SciPy strange issue,https://stackoverflow.com/questions/20600527,Python deadlock related to packaging and mayavi?,"I have a curious problem that I hope someone can shed some light on.

I have a complex piece of code, that started as a directory full of scripts, that I decided to rework into a package. This code change appears to have been the trigger for some strange deadlocks to appear.

Below is an attempt at canonical reproduction of the problem; which fails, in the sense that this code runs as expected. Actually reproducing the issue may require a lot of code; but I cannot for the life of my imagine what is different for the offending code snippet in-context.

import numpy as np
from scipy.sparse import csr_matrix
from threading import Thread

def dummy():
    print 'this is printed'
    I = np.eye(3)
    print 'all is still fine'
    csr_matrix(I)
    print 'this is never printed; csr_matrix appears to be a trigger for deadlock'
    print np.ones(4)
    print 'same problem; somehow, printing ndarrays is no longer cool either'

thr = Thread(target=dummy)
thr.start()


Perhaps this terse comment in the docs is related? I am not sure I fully appreciate what is being said here

http://docs.python.org/2/library/threading#importing-in-threaded-code

Firstly, other than in the main module, an import should not have the side effect of spawning a new thread and then waiting for that thread in any way. Failing to abide by this restriction can lead to a deadlock if the spawned thread directly or indirectly attempts to import a module.

Some context: I am using python 2.7, numpy 1.8, where I try to spawn this new thread from within a mayavi/traitsui thread (which I dont see why it should be relevant, and which worked fine before the package structure, but ok). Also, there is a boatload of numpy/scipy code in my spawned thread that executes perfectly fine; its just printing ndarrays and creating sparse matrices which so far have proven to be triggers for deadlock.

I am suspecting some funky interaction with mayavi, since closing the mayavi window causes all deadlocked threads to start running again. Perhaps these specific statement trigger the python thread to yield back to the mayavi thread, but they somehow fail to gain focus again?

Any hints that lead to further narrowing down on this mystery are much appreciated!
",2,209,"From your comments, it looks like you start up the UI event loop at the top level of one of your subsidiary modules. This is not a good idea because it causes exactly the same problems that the documentation alludes to. import foo should never start a UI event loop. The problem is that the main thread grabs the import lock to handle the import of the module. This module starts up the UI event loop before completing the import. This is essentially the same situation as the waiting for the other threads to finish; you are waiting for the UI loop to finish. If your UI starts up other threads, the code that is running in the other threads will not be able to import anything (both csr_matrix() and ndarray.__repr__() import other modules) because the main thread is still holding onto the import lock.
",,
SciPy strange issue,https://stackoverflow.com/questions/6968219,scipy on os x lion,"I'm am trying to get my python/numpy/scipy working environment running on OS X Lion.

I've already managed to compile numpy and scipy from their latest sourcecode versions with the usual

python setup.py build
python setup.py install


I do run the Python 2.7.2 version downloaded from python.org:

tobi-mbp:~ tobi$ python
Python 2.7.2 (v2.7.2:8527427914a2, Jun 11 2011, 15:22:34) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt;


If i than import numpy it shows:

&gt;&gt;&gt; import numpy
&gt;&gt;&gt; numpy.__version__
'1.5.1'
&gt;&gt;&gt; numpy.__file__
'/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/__init__.pyc'
&gt;&gt;&gt; 


the system instalation of numpy. Whereas

&gt;&gt;&gt; import scipy
&gt;&gt;&gt; scipy.__version__
'0.10.0.dev'
&gt;&gt;&gt; scipy.__file__
'/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/__init__.pyc'
&gt;&gt;&gt;


gives the installed version of scipy. That means the scipy.test() fails and actually gets stuck.

I did try pip and easy_install as well but the fail to compile scipy and with numpy the same strange bahavior occours. Same for the precompiled binarys provided on sourcefourge.

As this seems to be an issue with PYTHONPATH:

&gt;&gt;&gt; import sys, os, pprint
&gt;&gt;&gt; pprint.pprint(sys.path)
['',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/distribute-0.6.19-py2.7.egg',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pip-1.0.2-py2.7.egg',
 '/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python',
 '/Library/Python/2.7/site-packages/nose-1.1.2-py2.7.egg',
 '/Library/Python/2.7/site-packages/pip-1.0.2-py2.7.egg',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python27.zip',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-darwin',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac/lib-scriptpackages',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-old',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages',
 '/Library/Python/2.7/site-packages']
&gt;&gt;&gt; pprint.pprint([p for p in os.environ.items() if p[0].startswith('PYTHON')])
[]
&gt;&gt;&gt; 


And a list of the contents of site-packages

tobi-mbp:site-packages tobi$ ls -l
total 752
-rw-rw-r--   1 root  admin     119 12 Jun 00:25 README
drwxr-xr-x  10 root  admin     340  6 Aug 19:47 distribute-0.6.19-py2.7.egg
-rw-r--r--   1 tobi  admin     237  6 Aug 19:48 easy-install.pth
drwxrwxr-x  48 tobi  admin    1632  6 Aug 18:19 numpy
drwxrwxr-x   6 tobi  admin     204 20 Jul 20:44 numpy-1.6.1-py2.7.egg-info
-rw-r--r--   1 root  admin    1670  6 Aug 18:19 numpy-2.0.0.dev_26aa3cf-py2.7.egg-info
drwxr-xr-x   4 root  admin     136  6 Aug 19:48 pip-1.0.2-py2.7.egg
drwxr-xr-x   8 tobi  admin     272  6 Aug 19:48 readline-6.2.0-py2.7.egg-info
-rwxr-xr-x   1 tobi  admin  357048  6 Aug 19:48 readline.so
drwxrwxr-x  42 tobi  admin    1428  6 Aug 18:21 scipy
-rw-r--r--   1 root  admin    1768  6 Aug 18:21 scipy-0.10.0.dev-py2.7.egg-info
drwxrwxr-x   6 tobi  admin     204 27 Feb 14:00 scipy-0.9.0-py2.7.egg-info
-rw-r--r--   1 tobi  admin     144  6 Aug 19:47 setuptools-0.6c11-py2.7.egg-info
-rw-r--r--   1 tobi  admin      30  6 Aug 19:47 setuptools.pth

tobi-mbp:site-packages tobi$ more easy-install.pth
import sys; sys.__plen = len(sys.path)
./distribute-0.6.19-py2.7.egg
./pip-1.0.2-py2.7.egg
import sys; new=sys.path[sys.__plen:]; del sys.path[sys.__plen:]; p=getattr(sys,'__egginsert',0); sys.path[p:p]=new; sys.__egginsert = p+len(new)

tobi-mbp:site-packages tobi$ more setuptools.pth 
./distribute-0.6.19-py2.7.egg


I do see the System Extras folder in the path, now the question is why and how I can get rid of it.

thanks for your help

cheers Tobi
",2,2260,"There appears to be a path issue somewhere. Perhaps you are setting PYTHONPATH?  When you are running the python.org 2.7.2, the Apple-supplied Python 2.7.1's Extras directory should not be on sys.path.  To help figure out what's going on, launch the Python 2.7.2 and examine the following:

&gt;&gt;&gt; import sys, os, pprint
&gt;&gt;&gt; pprint.pprint(sys.path)
&gt;&gt;&gt; pprint.pprint([p for p in os.environ.items() if p[0].startswith('PYTHON')])


Also the contents of the site-packages directory:

$ cd /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/
$ ls -l
$ more easy-install.pth


and the contents of any other .pth files there.  

If it is still not clear what is going on, edit your answer to show the results of the above.

UPDATE:

Thanks for providing the requested additional info.  It confirms that sys.path does include the Extras directory from the Apple-supplied system Python 2.7.  There is no obvious reason why that should be happening.  Without more information, I can only speculate.  As unlikely as it may seem, my best guess at this point is that you accidentally copied some files from the one Python to the other.  The Apple-supplied Python has a patch in site.py to add the Extras directory:

$ cd /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7
$ ls -l site.py
-rw-r--r--  1 root  wheel  19929 Jun 16 17:03 site.py
$ grep Extras site.py
            sitepackages.append(os.path.join(prefix, ""Extras"", ""lib"", ""python""))


You should not see that in the python.org Python:

$ cd /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7
$ ls -l site.py
-rw-rw-r--  1 root  admin  20288 Jun 11 15:25 site.py
$ grep Extras site.py
$


You could also do a more complete recursive search of all of the directories on sys.path for pth files but that seems unlikely to find anything.

You also appear to have installed two versions each of numpy and scipy in your site-packages, one the latest released version and the other cutting-edge version.   I don't know whether you took steps to clear out the numpy and scipy directories between installs or whether that can cause problems but it might.

At this point, without more information, I would recommend that you completely delete the 2.7.2 installation and start over:

$ sudo rm -rf /Library/Frameworks/Python.framework/Versions/2.7


You might consider using another approach like the binary numpy/scipy installers or install using MacPorts.  A better source of advice might be found on one of the project's mailing lists.  Good luck!
",,
SciPy strange issue,https://stackoverflow.com/questions/31685156,Normalizing vector produces nan in Numpy,,1,5875,"It looks like this is a bug in numpy.  I can reproduce the problem if the data type of the array is np.int16:

In [1]: np.__version__
Out[1]: '1.9.2'

In [2]: x = np.array([ -91, -119, -137, -132], dtype=np.int16)

In [3]: x
Out[3]: array([ -91, -119, -137, -132], dtype=int16)

In [4]: np.linalg.norm(x)
/Users/warren/anaconda/lib/python2.7/site-packages/numpy/linalg/linalg.py:2061: RuntimeWarning: invalid value encountered in sqrt
  return sqrt(sqnorm)
Out[4]: nan


The problem also occurs in the master branch of the development version of numpy.  I created an issue here: https://github.com/numpy/numpy/issues/6128

If p_frame is, in fact, a 16 bit integer array, a simple work-around is something like:

x = np.asarray(p_frame, dtype=np.float64)
pf = x / norm(x)

","Following one of Warren's links, I get this warning:

In [1016]: np.linalg.norm(100000*np.ones(2).astype('int16'))
/usr/local/lib/python2.7/site-packages/numpy/linalg/linalg.py:2051: RuntimeWarning: invalid value encountered in sqrt
  return sqrt(add.reduce((x.conj() * x).real, axis=None))


For this x2, the inner expression is negative - the result of overflow in a small dtype.

In [1040]: x2=100000*np.ones(2).astype('int16')
In [1041]: np.add.reduce((x2.conj()*x2).real,axis=None)
Out[1041]: -1474836480


similarly with an x1:

In [1042]: x1
Out[1042]: array([ -9100, -11900, -13700, -13200], dtype=int16)
In [1043]: np.add.reduce((x1.conj()*x1).real,axis=None)
Out[1043]: -66128


If the sum of the 'dot' becomes too large for the dtype, it can be negative, producing a nan when passed through sqrt.

(I'm using 1.8.2 and 1.9.0 under linux).
",
SciPy strange issue,https://stackoverflow.com/questions/74073866,SciPy Minimize doesn&#39;t pass all guesses on?,,1,37,"The function that you pass to scipy.minimize must use a singular argument for all the numerical inputs.
Imaging you wrote your function like:
def current_optimization_f(next_capital, labour):
   return optimization_f(3, next_capital, labour, initial_value)

scipy will call:
current_optimization([2, 0.3])

rather than
current_optimization(2, 0.3)

You can use an additional lambda to unpack the arguments:
obj_func = lambda x: current_optimization(*x)

rtn = optimize.minimize(obj_func, [2, 0.3])

And also, unrelated to your question, you should look at CasADI to solve these types of questions
",,
SciPy strange issue,https://stackoverflow.com/questions/41049834,Global Modules are not available in VirtualEnv,"I'm using WinPython to work with libraries such as Scipy. I can not install those inside my virtual environment because blas/lapack is not available. The solution was to use the modules (theano/scipy/numpy) which are already installed on my system inside my virtual environment. 
Here is what I tried:

open console (cmd)

python
&gt;&gt;&gt; import numpy


this works, now creating the virtual environment

set VIRTUALENV_PYTHON=C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\python.exe
set VIRTUALENV_EXTRA_SEARCH_DIR=""C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\libs C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\Lib C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\DLLs C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\ C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\Scripts""
virtualenv --system-site-packages -v --always-copy venv


or simply:

virtualenv --system-site-packages venv


but no matter what I try I can't import numpy when entering the venv.

venv\Scripts\activate
python
&gt;&gt;&gt; import numpy
ImportError: No module named 'numpy'


Strange. So I compared the sys.path inside the venv:

&gt;&gt;&gt; sys.path
['', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\Scripts', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\DLLs', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\Lib', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\libs', 'C:\\Users\\user\\venv\\Scripts\\python35.zip', 'C:\\Users\\user\\venv\\DLLs', 'C:\\Users\\user\\venv\\lib', 'C:\\Users\\user\\venv\\Scripts', 'C:\\Users\\user\\venv', 'C:\\Users\\user\\venv\\lib\\site-pack
ages']


with the original one (outside):

&gt;&gt;&gt; sys.path
['', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\Scripts', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\DLLs', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\Lib', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\libs', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\python35.zip', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\lib\\site-packages', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\lib\\site-packages\\win32', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\lib\\site-packages\\win32\\lib', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\lib\\site-packages\\Pythonwin']


as you can see, not all the entries got copied over. Why is that and how can I fix it?

And yes I read virtualenv --system-site-packages not using system site packages and deleted my PYTHONPATH variable but the issue remains.
",1,957,"Figured it out eventually. I did not restart after removing my PYTHONPATH variable. So if you have the same issue:


Delete your PYTHONPATH env. variable. 
Reboot!

",,
SciPy strange issue,https://stackoverflow.com/questions/18705141,TypeError when calling scipy griddata,"I'm having an issue with scipy.interpolate's griddata function. I developed a quick interpolator using griddata in a sandbox, then once I had the interpolator how I liked it, a copied the function into a larger model I had developed. Here is the function:

def windGrid(*sensors):
    """"""Creates grids of wind speed components""""""
    xb = [0, 0, num_x, num_x]
    yb = [0, num_y, num_y, 0]

    xs = [s.lng for s in sensors]
    ys = [s.lat for s in sensors]
    us = [s.u for s in sensors]
    vs = [s.v for s in sensors]

    ub, vb = boundaryWeighting(*sensors)

    x = xb+xs
    y = yb+ys
    u = ub+us
    v = vb+vs

    x_grid, y_grid = np.mgrid[0:num_x, 0:num_y]

    zx = griddata((x, y), u, (x_grid, y_grid))
    zy = griddata((x, y), v, (x_grid, y_grid))

   return zx, zy


The boundaryWeighting() function simply returns wind speed components of the corners of the grid so that interpolations can be made within. Now here is the strange part... When I call griddata in the sandbox (i.e. without other independent functions around it), it works fine. However, when called in the python file to which it was copied, it returns this error:

TypeError: griddata() takes at least 5 arguments (3 given)


According to the Scipy documentation, griddata takes 5 arguments, but the last two are optional. I tried inserting the optional arguments (i.e. method and fill_val), but then I got this error:

TypeError: griddata() got an unexpected keyword argument 'method'


So it seems that the python interpreter is referencing a different version/function of griddata in the different python files.

Here is why this doesn't make sense:
1. Both files are in the same directory, and are using the same interpreter
2. Both files reference the same Scipy version, 0.12.0

I've checked all my variable/function assignments and there are no overlaps. Any clues as to why this would be happening?

Thanks
",1,2393,"Do you have something like from matplotlib.pylab import * in one of your files?  You might be getting a namespace collision with the matplotlib version of griddata.
",,
SciPy strange issue,https://stackoverflow.com/questions/64392302,"Scipy, Numpy, Django, Docker Issue",,0,146,"Even though OOMKIlled is false (Killed due to Out of memory), I increased the memory from 2 to 8 GB on docker desktop and voila my app worked!
",,
SciPy strange issue,https://stackoverflow.com/questions/77434232,Scipy sosfilt ValueError ndarray is not C-contiguous,"sosfilt from the scipy=1.9.3 library is giving me a strange ValueError when inputting a numpy=1.23.4 array.
MWE:
import numpy as np
from scipy.signal import sosfilt

fs=48000

rng = np.random.default_rng()
signal = rng.normal(size=(fs))

b_0k = [1.01589602025559, 0.958943219304445, 0.961371976333197,
        2.22580350360974, 0.471735128494163, 0.115267139824401,
        0.988029297230954, 1.95223768730136]
b_1k = [-1.92529887777608, -1.80608801184949, -1.76363215433825,
        -1.43465048479216, -0.366091796830044, 0.0, -1.91243380293387,
        0.162319983017519]
b_2k = [0.922118060364679, 0.876438777856084, 0.821787991845146,
        -0.498204282194628, 0.244144703885020, -0.115267139824401,
        0.926131550180785, -0.667994113035186]
a_0k = np.ones(len(b_0k))
a_1k = [-1.92529887777608, -1.80608801184949, -1.76363215433825,
        -1.43465048479216, -0.366091796830044, -1.79600256669201,
        -1.91243380293387, 0.162319983017519]
a_2k = [0.938014080620272, 0.835381997160530, 0.783159968178343,
        0.727599221415107, -0.284120167620817, 0.805837815618546,
        0.914160847411739, 0.284243574266175]

sos = np.array([b_0k, b_1k, b_2k, a_0k, a_1k, a_2k]).T

signalFiltered = sosfilt(sos, signal, axis=0)

gives me
signalFiltered = sosfilt(sos, signal, axis=0)
Traceback (most recent call last):
Cell In[102], line 1
signalFiltered = sosfilt(sos, signal, axis=0)
File C:\ProgramData\Miniconda3\envs\devenv\lib\site-packages\scipy\signal_signaltools.py:4247 in sosfilt
_sosfilt(sos, x, zi)
File _sosfilt.pyx:81 in scipy.signal._sosfilt._sosfilt
File stringsource:660 in View.MemoryView.memoryview_cwrapper
File stringsource:350 in View.MemoryView.memoryview.cinit
ValueError: ndarray is not C-contiguous
However
signal.flags

shows
C_CONTIGUOUS : True
F_CONTIGUOUS : True
OWNDATA : True
WRITEABLE : True
ALIGNED : True
WRITEBACKIFCOPY : False
I have also tried this on other signals imported from wav files, with the same error. Having looked around, there does not seem to be any known issue or other examples of this error using this function (although there are some machine learning package bugs discussed elsewhere)
What is the problem with Scipy?
",0,59,"It's the transpose that's giving problems, making it F contiguous. Add a .copy().
Look at the array flags, before and after transpose and copy.
transpose works by reversing the strides and shape. It's a view, a cheap way of doing the job, but messes with the  continuity.
",,
SciPy strange issue,https://stackoverflow.com/questions/76218384,solve_ivp gets stuck and I can&#39;t figure out why,"I am using SciPy's solve_ivp to simulate a model consisting of 5 equations:
Upsilon = 0.305
Alpha = 0.00000055
r = 0.176
b = 0.0000000000005

def model(t, y0, Beta, Eta, Rmin, p1, p2, p3, A, a, Xi, Epsilon, Lambda, Theta, Mu, Delta, Gamma):

    CD, CT, CM, CE, T, TOTAL = y0

    def k_t(t):
        return Rmin + (p1 / (1 + (p2 * t) ** p3))

    def F_t(T):
        return T / (A + T)

    def f_t(CF, T):
        return (CF / T) / (Upsilon + ((a + CF) / T))

    dCD_dt = -(Beta + Eta) * CD
    dCT_dt = (Eta * CD) + (k_t(t) * F_t(T) * CT) - ((Xi + Epsilon + Lambda) * CT) + (Theta * T * CM) - (Alpha * T * CT)
    dCM_dt = (Epsilon * CT) - (Theta * T * CM) - (Mu * CM)
    dCE_dt = (Lambda * CT) - (Delta * CE)
    dT_dt = ((r * T) * (1 - (b * T))) - (Gamma * f_t((CD + CT), T) * T)

    TOTAL = dCD_dt + dCT_dt + dCM_dt + dCE_dt

    print(t)
    return [dCD_dt, dCT_dt, dCM_dt, dCE_dt, dT_dt, TOTAL]


I am performing sensitivity analysis, so I am running the model about 37,000 times with variations in the input parameters. When I run the model, I do so with the following code:
#p_values is a numpy array

final = 100
t_range = (0, final)
t_eval = np.arange(0, final, 0.1)

# run model for each parameter combination
for i in range(len(p_values)):
    CD = p_values[i][15]
    CT, CM, CE = 0, 0, 0
    T = 10 ** 7
    TOTAL = CD
    y0 = [CD, CT, CM, CE, T, TOTAL]

    solution = solve_ivp(model, t_range, y0, t_eval=t_eval, args=(p_values[i][0], p_values[i][1], p_values[i][2], p_values[i][3], p_values[i][4], p_values[i][5], p_values[i][6], p_values[i][7], p_values[i][8], p_values[i][9], p_values[i][10], p_values[i][11], p_values[i][12], p_values[i][13], p_values[i][14])).y

But at some point while it's running it will just... get stuck. Sometimes it happens early (within the first thousand solutions) and sometimes it can get to 10,000 solutions or more before it gets stuck.
To try and solve the problem, I started printing t for each loop of the model, and I noticed something very strange - when it gets stuck, it seems that t stays stagnant, or at least grows extremely slowly. This doesn't make sense to me, as I assume it should always increase by 0.1 at each step. Whenever this happens t is quite high (above 95), but I don't know if this is a coincidence or not. If not, I suppose it could be an issue with the function k(t) in the model producing a number that is far too small, but I don't see why this should be an issue.
If anyone with more experience than me has a suggestion to try and fix it, that would obviously be much appreciated. In lieu of that, I would also be okay with having some way to skip over that run of the model and move onto the next when the running time gets too long, but I don't know how to implement something like this without making the runs very slow.
",0,82,"You could possibly try using the optional method parameter in your solve_ivp, and set it to method = ""BDF"". It might have a little more accuracy.
According to
https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html :

BDF: Implicit multi-step variable-order (1 to 5) method based on a
backward differentiation formula for the derivative approximation [5].
The implementation follows the one described in [6]. A quasi-constant
step scheme is used and accuracy is enhanced using the NDF
modification. Can be applied in the complex domain.

",,
SciPy strange issue,https://stackoverflow.com/questions/64130416,How to remove hidden utdated scipy file so it won&#39;t import with python in VS Code OSX,"I am currently trying to run a program with Scipy, and I want to use the load_npz module.
Whenever I tried to run it, the compiler would say that that module doesn't exist.
I ran scipy.__version__ and got 0.13.0b1, which makes sense as to why it couldn't find the module as it doesn't exist in that version, but I am confused as I have 1.5.2 installed in both pip and brew yet it keeps defaulting to the oldest version which is very frustrating.
Does anybody know how to get rid of this version? I have tried uninstalling from pip and brew, along with finding the path of the imported scipy with the outdated version yet it still is causing issues.
I do have a lot of packages installed (numpy, matplotlib, etc.) so could it be a dependency that keeps reinstalling an old version?
Strangely, even if I delete scipy from both brew and pip, it will still show the old version but throw an error on a different local file that also uses scipy saying the module does not exist (which is expected as I deleted it).
",0,25,"I figured it out, I just deleted all my possible scipy locations and then just downloaded Anaconda and I'm using that as my python interpreter.
","write this python to find out the location of the imported scipy
import scipy
print(scipy.__file__)

",
SciPy strange issue,https://stackoverflow.com/questions/58558248,Why does gstreamer capture image incorrectly?,"Hello,

I have a Jetson Nano device with a Raspberry Pi v2.1-camera and use gstreamer and OpenCV to capture images with it. Still, I thought this question would be better preserved here because I mainly think it is a software issue.
The point is: I use following Python-script to capture my image:

import cv2
import numpy as np
import matplotlib.pyplot as plt
from scipy.misc import imshow

def gstreamer_pipeline (capture_width=3280, capture_height=2464, display_width=1280, display_height=720, framerate=21, flip_method=0) :   
    return ('nvarguscamerasrc ! ' 
    'video/x-raw(memory:NVMM), '
    'width=(int)%d, height=(int)%d, '
    'format=(string)NV12, framerate=(fraction)%d/1 ! '
    'nvvidconv flip-method=%d ! '
    'video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! '
    'videoconvert ! '
    'video/x-raw, format=(string)BGR ! appsink'  % (capture_width,capture_height,framerate,flip_method,display_width,display_height))


if __name__ == '__main__':
    cap = cv2.VideoCapture(gstreamer_pipeline(flip_method=0), cv2.CAP_GSTREAMER)
    if cap.isOpened():
        ret_val, img = cap.read()
        img = np.flipud(img)
        img = np.fliplr(img)
        imshow(img)
        cv2.imwrite(""test.jpg"", img)
    else:
        print(""Unable to open camera."")


My problem is I am not sure if it captures color values correctly. When I use imshow(img) from SciPy to display the image I just took, it looks like this:
(I do not have enough reputation to insert it, so please see this link)

That is strange because it is very blue-toned. After I use cv2.imwrite(""test.jpg"", img) to save it, it looks normal again: (Here the another image)

I do not know if I defined something wrong in the gstreamer-pipeline or what else could cause the error. It is important for my program to capture colors correctly and since I do not want to save an image and then load it just to get the colors right (bad in terms of speed and CPU-power), I would be very glad if someone could help me out.

Thanks for the answers in advance!
",0,1057,"As mentioned by HansHirse your color space is wrong. 

You could alter your gstreamer pipeline to output RGB

format=(string)RGB    


Or convert from BGR to RGB within oopencv 

im_rgb = cv2.cvtColor(im_cv, cv2.COLOR_BGR2RGB)

",,
