search_keywords,issue_link,issue_title,issue_body,issue_score,issue_views,answer_1,answer_2,answer_3
scikit-learn unexpected behavior,https://stackoverflow.com/questions/76876536,Unexpected Feature Names in scikit-learn TfidfVectorizer,"I'm trying to use the TfidfVectorizer from scikit-learn to convert a list of words into a TF-IDF matrix. However, I'm encountering some unexpected behavior where the feature names generated by the vectorizer don't match the words in my input text.
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import brown
import re

txt = brown.words()
txt_str = "" "".join(txt)

def custom_tokenizer(text):
    tokens = re.findall(r'\b\w+\b', text.lower())
    return tokens

documents = [txt_str]

vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)
tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

print(feature_names)

I've noticed that some feature names in the output are neither present in my txt nor in the tokens generated by the custom_tokenizer. Additionally, some feature names seem to be numeric values or arbitrary characters and words.
I've tried various approaches, including using a custom tokenizer to ensure accurate tokenization, updating scikit-learn. It works on smaller texts as well as when the max_features is set to low enough value. However, for my project I do need all the features so I am trying to figure out what is going on.
Why I'm getting these unexpected feature names and how I can ensure that the feature names correspond accurately to the words in my input text?
some of the features are zwei' 'zworykin'.
",1,52,"Have you some examples of unexpected feature names ?
From my side, I don't think there is any problem extracting features.
The words not present in feature names, are words with special characters like -  ' or $. WHen you extract the text with the function custom_tokenizer, you get two tokens for example polyether-type, gives ""polyether"" and ""type"" tokens
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import brown
import re

txt = brown.words()
txt_str = "" "".join(txt)

def custom_tokenizer(text):
    tokens = re.findall(r'\b\w+\b', text.lower())
    return tokens

documents = [txt_str]

vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)
tfidf_matrix = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names_out()

print(f""Nb features                     : { len(feature_names)}"")
print(f""Nb words in brown               : { len(txt) }"")

# nb uniq lowercase words in brown
mylist = list(dict.fromkeys(txt))
mylist = [ x.lower() for x in mylist]
print(f""Nb uniq lowercase words in brown: {len(mylist)}"")

# in li1 and not in li2
def Diff(li1, li2):
    set_dif = set(li1).difference(set(li2))
    temp3 = list(set_dif)
    return temp3
li3 = Diff(mylist,feature_names)

def info(text):
    custom_tokenizer(text)
    try:
        s=list(feature_names).index(text)
        print(f""The text {text} was found in features at position : {s}"")
    except:
        print(f""The text {text} was not found in features"")    
    

print(f""10 first words not in feature_names : \n { li3[0:10]} "")

info(""season"")
info(""mosquito"")

Nb features                     : 42432
Nb words in brown               : 1161192
Nb uniq lowercase words in brown: 56057
10 first words not in feature_names :
[""season's"", 'non-farm', ""chip-o's"",
'polyether-type', 'take-off', '1-0', 'spring-back', '$50',
'mosquito-plagued', 'anti-submarine']
The text season was found in features at position : 33478
The text mosquito was found in features at position : 24918
",,
scikit-learn unexpected result,https://stackoverflow.com/questions/41949101,scikit learn LDA giving unexpected results,"I am attempting to classify some data with the scikit learn LDA classifier.  I'm not entirely sure what to ""expect"" from it, but what I am getting is weird.  Seems like a good opportunity to learn about either a shortcoming of the technique, or a way in which I am applying it wrong.  I understand that no line could completely separate this data, but it seems that there are much ""better"" lines than the one it is finding.  I'm just using the default options.  Any thoughts on how to do this better?  I'm using LDA because it is linear in the size of my dataset.  Although I think a linear SVM has a similar complexity.  Perhaps it would be better for such data?  I will update when I have tested other possibilities.

The picture: (light blue is what my LDA classifier predicts will be dark blue)



The code:

import numpy as np
from numpy import array
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
import itertools

X = array([[ 0.23125754,  0.79170351],
       [ 0.78021491, -0.24999486],
       [ 0.00856446,  0.41452734],
       [ 0.66381753, -0.09872504],
       [-0.03178685,  0.04876317],
       [ 0.65574645, -0.68214948],
       [ 0.14290684,  0.38256002],
       [ 0.05156987,  0.11094875],
       [ 0.06843403,  0.19110019],
       [ 0.24070898, -0.07403764],
       [ 0.03184353,  0.4411446 ],
       [ 0.58708124, -0.38838008],
       [-0.00700369,  0.07540799],
       [-0.01907816,  0.07641038],
       [ 0.30778608,  0.30317186],
       [ 0.55774143, -0.38017325],
       [-0.00957214, -0.03303287],
       [ 0.8410637 ,  0.158594  ],
       [-0.00294113, -0.00380608],
       [ 0.26577841,  0.07833684],
       [-0.32249375,  0.49290502],
       [ 0.11313078,  0.35697211],
       [ 0.41153679, -0.4471876 ],
       [-0.00313315,  0.30065913],
       [ 0.14344143, -0.19127107],
       [ 0.04857767,  0.01339191],
       [ 0.5865007 ,  0.71209886],
       [ 0.08157439,  0.40909955],
       [ 0.72495202,  0.29583866],
       [-0.09391461,  0.17976605],
       [ 0.06149141,  0.79323099],
       [ 0.52208024, -0.2877661 ],
       [ 0.01992141, -0.00435266],
       [ 0.68492617, -0.46981335],
       [-0.00641231,  0.29699622],
       [ 0.2369677 ,  0.140319  ],
       [ 0.6602586 ,  0.11200433],
       [ 0.25311836, -0.03085372],
       [-0.0895014 ,  0.45147252],
       [-0.18485667,  0.43744524],
       [ 0.94636701,  0.16534406],
       [ 0.01887734, -0.07702135],
       [ 0.91586801,  0.17693792],
       [-0.18834833,  0.31944796],
       [ 0.20468328,  0.07099982],
       [-0.15506378,  0.94527383],
       [-0.14560083,  0.72027034],
       [-0.31037647,  0.81962815],
       [ 0.01719756, -0.01802322],
       [-0.08495304,  0.28148978],
       [ 0.01487427,  0.07632112],
       [ 0.65414479,  0.17391618],
       [ 0.00626276,  0.01200355],
       [ 0.43328095, -0.34016614],
       [ 0.05728525, -0.05233956],
       [ 0.61218382,  0.20922571],
       [-0.69803697,  2.16018536],
       [ 1.38616732, -1.86041621],
       [-1.21724616,  2.72682759],
       [-1.26584365,  1.80585403],
       [ 1.67900048, -2.36561699],
       [ 1.35537903, -1.60023078],
       [-0.77289615,  2.67040114],
       [ 1.62928969, -1.20851808],
       [-0.95174264,  2.51515935],
       [-1.61953649,  2.34420531],
       [ 1.38580104, -1.9908369 ],
       [ 1.53224512, -1.96537012]])

y = array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.])

classifier = LDA()
classifier.fit(X,y)

xx = np.array(list(itertools.product(np.linspace(-4,4,300), np.linspace(-4,4,300))))
yy = classifier.predict(xx)
b_colors = ['salmon' if yyy==0 else 'deepskyblue' for yyy in yy]
p_colors = ['r' if yyy==0 else 'b' for yyy in y]
plt.scatter(xx[:,0],xx[:,1],s=1,marker='o',edgecolor=b_colors,c=b_colors)
plt.scatter(X[:,0], X[:,1], marker='o', s=5, c=p_colors, edgecolor=p_colors)
plt.show()


UPDATE: Changing from using sklearn.discriminant_analysis.LinearDiscriminantAnalysis to sklearn.svm.LinearSVC also using the default options gives the following picture:



I think using the zero-one loss instead of the hinge loss would help, but sklearn.svm.LinearSVC doesn't seem to allow custom loss functions.

UPDATE: The loss function to sklearn.svm.LinearSVC approaches the zero-one loss as the parameter C goes to infinity.  Setting C = 1000 gives me what I was originally hoping for.  Not posting this as an answer, because the original question was about LDA.

picture:


",3,315,"LDA models each class as a Gaussian, so the model for each class is determined by the class' estimated mean vector and covariance matrix.
Judging by the eye only, your blue and red classes have approximately the same mean and same covariance, which means the 2 Gaussians will 'sit' on top of each other, and the discrimination will be poor. Actually it also means that the separator (the blue-pink border) will be noisy, that is it will change a lot between random samples of your data.

Btw your data is clearly not linearly-separable, so every linear model will have a hard time discriminating the data.

If you must use a linear model, try using LDA with 3 components, such that the top-left blue blob is classified as '0', the bottom-right blue blob as '1', and the red as '2'. This way you will get a much better linear model. You can do it by preprocessing the blue class with a clustering algorithm with K=2 classes.
",,
scikit-learn unexpected result,https://stackoverflow.com/questions/30066268,Provide Starting Positions to t-distributed Stochastic Neighbor Embedding (TSNE) in scikit-learn,"I've been looking at using scikit learns' TSNE method to visualize high dimensional data in 2D. However, I have some idea of where the starting positions should be in 2D space but I don't see any way of specifying this information. Any ideas how I might be able to provide the starting coordinates?

I see that there's an ""init"" parameter but it appears to only take ""random"" or ""pca"" as options. What I'm looking for is a way to specify exactly what the initial coordinates are.

Update

I'm trying to increase the distance between my data points (~6,500 points) but they are quite tightly clustered and overlapping. With default parameters, I get:

model = sklearn.manifold.TSNE(n_components=2, random_state=0)




A slight increase in n_iter and early_exaggeration didn't produce significantly different results as the points (within clusters) are still overlapping.

model = sklearn.manifold.TSNE(n_components=2, random_state=0, n_iter=10000, early_exaggeration=10)




However, increasing the early_exaggeration from 10 to 100 (which, according to the docs, should increase the distance between clusters) produced some unexpected results (I ran this twice and it was the same result):

model = sklearn.manifold.TSNE(n_components=2, random_state=0, n_iter=10000, early_exaggeration=100)




This link: https://beta.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm Provides an example (three quarters of the way down the page) for how to monkey patch the gradient_descent function to save coordinates/positions. 

Added Issue to scikit-learn
",2,906,"It is currently not possible, but it would be a two-line change.
I think it would be a good addition, and we do support init=array for things like k-means. So PR welcome.
",,
scikit-learn unexpected result,https://stackoverflow.com/questions/36865542,Adjusted Mutual Information (scikit-learn),"I have implemented a clustering algorithm for summarizing log files, and am currently testing it against ground-truth data with the Adjusted Rand index and the Adjusted Mutual Information index. 

Input to my algorithm is a list of log entries, and output is a list of integers (the cluster label that each item belongs to). The ground truth is similarly a list of integers where each integer represent the true cluster the item belongs to. For most of my test cases I receive normal/expected results, but one file is giving me unexpected output. I have enclosed the two lists, the ground-truth clustering as well as that of my algorithm's:

Ground truth list:
http://pastebin.com/9Y5TE6b7

Own clustering:
http://pastebin.com/hJz1M4sf

These two lists are fed into scikit-learn functions to get the ARI and AMI. The ARI score looks roughly correct, but AMI is above 1, which according to the documentation and definition of AMI should not be possible if I understand it correctly. This data set is highly unbalanced, but many of my other files are similarly balanced. I cannot figure this out. For reference, the scores I get for ARI and AMI is:

ARI: 0.99642743999922712

AMI: 1.0190170466324
",2,813,"This has been fixed in the development version.
",,
scikit-learn unexpected result,https://stackoverflow.com/questions/12358965,scikit-learn RandomForestClassifier produces &#39;unexpected&#39; results,"I'm trying to use sk-learn's RandomForestClassifier for a binary classification task (positive and negative examples). My training data contains 1.177.245 examples with 40 features, in SVM-light format (sparse vectors) which I load using sklearn.dataset's load_svmlight_file. It produces a sparse matrix of 'feature values' (1.177.245 * 40) and one array of 'target classes' (1s and 0s, 1.177.245 of them). I don't know whether this is worrysome, but the trainingdata has 3552 positives and the rest are all negative.

As the sk-learn's RFC doesn't accept sparse matrices, I convert the sparse matrix to a dense array (if I'm saying that right? Lots of 0s for absent features) using .toarray(). I print the matrix before and after converting to arrays and that seems to be going all right.

When I initiate the classifier and start fitting it to the data, it takes this long:

[Parallel(n_jobs=40)]: Done   1 out of  40 | elapsed: 24.7min remaining: 963.3min
[Parallel(n_jobs=40)]: Done  40 out of  40 | elapsed: 27.2min finished


(is that output right? Those 963 minutes take about 2 and a half...)

I then dump it using joblib.dump.
When I re-load it:

RandomForestClassifier: RandomForestClassifier(bootstrap=True, compute_importances=True,
        criterion=gini, max_depth=None, max_features=auto,
        min_density=0.1, min_samples_leaf=1, min_samples_split=1,
        n_estimators=1500, n_jobs=40, oob_score=False,
        random_state=&lt;mtrand.RandomState object at 0x2b2d076fa300&gt;,
        verbose=1)


And test it on real trainingdata (consisting out of 750.709 examples, exact same format as training data) I get ""unexpected"" results. To be exact; only one of the examples in the testingdata is classified as true. When I train on half the initial trainingdata and test on the other half, I get no positives at all.

Now I have no reason to believe anything is wrong with what's happening, it's just that I get weird results, and furthermore I think it's all done awfully quick. It's probably impossible to make a comparison, but training a RFClassifier on the same data using rt-rank (also with 1500 iterations, but with half the cores) takes over 12 hours...

Can anyone enlighten me whether I have any reason to believe something is not working the way it's supposed to? Could it be the ratio of positives to negatives in the training data? Cheers.
",1,2607,"Indeed this dataset is very very imbalanced. I would advise you to subsample the negative examples (e.g. pick n_positive_samples of them at random) or to oversample the positive example (the latter is more expensive and but might yield better models).

Also are you sure that all your features are numerical features (larger values means something in real life)? If some of them are categorical integer markers, those feature should be exploded as one-of-k boolean encodings instead as scikit-learn implementation of random forest s cannot directly deal with categorical data.
",,
scikit-learn unexpected result,https://stackoverflow.com/questions/53215887,unexpected result for inheritance in scikit-learn,"I am getting an unexpected result when trying to create a simple modification of the class KNeighborsClassifier:

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)

class my_Classifier(KNeighborsClassifier):  
  """"""My Nearest Neighbour classifier""""""
  def __init__(self, gamma=0):
    def my_dist(x, y):  # squared distance
      return np.sum((abs(x-y))**gamma)
    KNeighborsClassifier.__init__(self, n_neighbors=1, metric=my_dist)
    self.gamma = gamma
    print(gamma)
  def fit(self, X, y):
    KNeighborsClassifier.fit(self, X, y)
    return self
  def predict(self, X, y=None):
    return KNeighborsClassifier.predict(self, X)
  def score(self, X, y):
    return KNeighborsClassifier.score(self, X, y)


I have made the metric dependent on a parameter gamma&gt;=0.  If gamma=2, this is just the squared Euclidean metric, and if gamma=0, this is a useless metric (essentially a constant).  First it works as expected: for

knn = my_Classifier(gamma=2)
knn.fit(X_train, y_train)
knn.score(X_test,y_test)


the output is

2
0.9736842105263158


(the accuracy is good) and for

knn = my_Classifier(gamma=0)
knn.fit(X_train, y_train)
knn.score(X_test,y_test)


the output is

0
0.34210526315789475


(the accuracy is hopeless).  I am also printing the value of gamma used.

However, when I try

from sklearn.model_selection import GridSearchCV
param_grid = {'gamma': [0,2]}
grid_search = GridSearchCV(my_Classifier(), param_grid)
grid_search.fit(X_train, y_train)
grid_search.score(X_test, y_test)


the result is unexpected: 

0
0
0
0
0
0
0
0
0.34210526315789475


Why is the value gamma=0 used every time?  And gamma=2 (producing a much better result) is never tried.  I know I am making some silly mistake but can't see where.
",1,72,"The value gamma = 0 is not used every time !

What you see when you get this:

0
0
0
0
0
0
0
0
0.34210526315789475


Is the score



To see the gamma use this:

grid_search.cv_results_ 

",,
scikit-learn unexpected result,https://stackoverflow.com/questions/69916926,Unexpected error when trying to install conda environment from .yaml file,"I am trying to install a conda environment in WSL2 from a .yaml file, the instructions for the installation saying the first step is to run the following command:
conda env create -f devtools/conda-envs/ael-test.yaml

After the WSL2 terminal collects the package metadata and solves the environment, it encounters an error, which I don't really know how to solve. I run the command above from the base environment, from the directory where /devtools/conda-ens/ael-test.yaml is located.
Collecting package metadata (repodata.json): done
Solving environment: done
Preparing transaction: failed

# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

    Traceback (most recent call last):
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/exceptions.py"", line 1079, in __call__
        return func(*args, **kwargs)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/cli/main.py"", line 80, in do_call
        exit_code = getattr(module, func_name)(args, parser)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/cli/main_create.py"", line 141, in execute
        result[installer_type] = installer.install(prefix, pkg_specs, args, env)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/installers/conda.py"", line 59, in install
        unlink_link_transaction.execute()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 245, in execute
        self.verify()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 222, in verify
        self.prepare()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 211, in prepare
        grps = self._prepare(self.transaction_context, stp.target_prefix,
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 288, in _prepare
        packages_info_to_link = tuple(read_package_info(prec, pcrec)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 288, in &lt;genexpr&gt;
        packages_info_to_link = tuple(read_package_info(prec, pcrec)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/gateways/disk/read.py"", line 89, in read_package_info
        package_metadata = read_package_metadata(epd)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/gateways/disk/read.py"", line 144, in read_package_metadata
        data = json.loads(f.read())
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/__init__.py"", line 357, in loads
        return _default_decoder.decode(s)
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/decoder.py"", line 337, in decode
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/decoder.py"", line 355, in raw_decode
        raise JSONDecodeError(""Expecting value"", s, err.value) from None
    json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

`$ /home/gheorghe/anaconda3/bin/conda-env create -f devtools/conda-envs/ael-test.yaml`

  environment variables:
                 CIO_TEST=&lt;not set&gt;
  CONDA_AUTO_UPDATE_CONDA=false
        CONDA_DEFAULT_ENV=base
                CONDA_EXE=/home/gheorghe/anaconda3/bin/conda
             CONDA_PREFIX=/home/gheorghe/anaconda3
    CONDA_PROMPT_MODIFIER=(base)
         CONDA_PYTHON_EXE=/home/gheorghe/anaconda3/bin/python
               CONDA_ROOT=/home/gheorghe/anaconda3
              CONDA_SHLVL=1
           CURL_CA_BUNDLE=&lt;not set&gt;
                     PATH=/home/gheorghe/anaconda3/bin:/home/gheorghe/.vscode-server/bin/f4af3cb
                          f5a99787542e2a30fe1fd37cd644cc31f/bin:/home/gheorghe/anaconda3/bin:/ho
                          me/gheorghe/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbi
                          n:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Program Files
                          /WindowsApps/Microsoft.WindowsTerminal_1.11.2921.0_x64__8wekyb3d8bbwe:
                          /mnt/c/Program Files/copasi.org/COPASI 4.29.228/bin:/mnt/c/Windows/sys
                          tem32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/Syste
                          m32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Prog
                          ram Files (x86)/Intel/Intel(R) Management Engine
                          Components/DAL:/mnt/c/Program Files/Intel/Intel(R) Management Engine
                          Components/DAL:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA
                          NvDLISR:/mnt/c/Program Files (x86)/NVIDIA
                          Corporation/PhysX/Common:/mnt/c/Program Files/Mullvad VPN/resources:/m
                          nt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt
                          /c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/Ope
                          nSSH:/mnt/c/Program Files (x86)/Common Files/Propellerhead
                          Software/ReWire:/mnt/c/Program Files/Common Files/Propellerhead
                          Software/ReWire:/mnt/c/Program Files/NCBI/blast-2.12.0+/bin:/mnt/c/Use
                          rs/gheor/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/ghe
                          or/AppData/Local/Microsoft/WindowsApps:/snap/bin
       REQUESTS_CA_BUNDLE=&lt;not set&gt;
            SSL_CERT_FILE=&lt;not set&gt;

     active environment : base
    active env location : /home/gheorghe/anaconda3
            shell level : 1
       user config file : /home/gheorghe/.condarc
 populated config files : /home/gheorghe/.condarc
          conda version : 4.10.3
    conda-build version : 3.21.5
         python version : 3.8.11.final.0
       virtual packages : __linux=5.10.16.3=0
                          __glibc=2.31=0
                          __unix=0=0
                          __archspec=1=x86_64
       base environment : /home/gheorghe/anaconda3  (writable)
      conda av data dir : /home/gheorghe/anaconda3/etc/conda
  conda av metadata url : None
           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /home/gheorghe/anaconda3/pkgs
                          /home/gheorghe/.conda/pkgs
       envs directories : /home/gheorghe/anaconda3/envs
                          /home/gheorghe/.conda/envs
               platform : linux-64
             user-agent : conda/4.10.3 requests/2.26.0 CPython/3.8.11 Linux/5.10.16.3-microsoft-standard-WSL2 ubuntu/20.04.3 glibc/2.31
                UID:GID : 1000:1000
             netrc file : None
           offline mode : False


An unexpected error has occurred. Conda has prepared the above report.

If submitted, this report will be used by core maintainers to improve
future releases of conda.

Edit:
This is the .yaml file:
channels:
  - conda-forge
  - pytorch
dependencies:
  - python
  - pip

  - numpy
  - scipy
  - scikit-learn

  - qcelemental
  - openbabel

  - matplotlib
  - seaborn

  - mlflow

  - pytorch
  - torchvision

  - lark-parser
  - cython

  - pip:
    - torchani
    - ""git+https://github.com/RMeli/mdanalysis.git@develop#egg=MDAnalysis&amp;subdirectory=package""

  - black
  - flake8
  - mypy
  - isort

  - pytest
  - pytest-xdist
  - pytest-cov
  - codecov

",1,915,"There is no problem in your environment, I managed to install it, getting
Successfully built MDAnalysis
Installing collected packages: msgpack, tqdm, mrcfile, mmtf-python, gsd, fasteners, biopython, GridDataFormats, torchani, MDAnalysis
Successfully installed GridDataFormats-1.0.1 MDAnalysis-2.6.0.dev0 biopython-1.81 fasteners-0.18 gsd-3.1.1 mmtf-python-1.1.3 mrcfile-1.4.3 msgpack-1.0.5 torchani-2.2.3 tqdm-4.66.1

done
#
# To activate this environment, use
#
#     $ conda activate ael

in the end.
The problem you encountering is probably related to https://github.com/conda/conda/issues/9590
which has a workaround
sudo rm -r ~/.condarc

or editing the ~/.condarc like https://github.com/conda/conda/issues/9590#issuecomment-1003211237
",,
scikit-learn unexpected result,https://stackoverflow.com/questions/17794313,Unexpected results when using scikit-learn&#39;s SVM classification algorithm (RBF kernel),"Using the example on this page 
http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html,
I created my own graphs using some normally distributed data with a standard deviation of 10 instead of the iris data.

My graph turned out to be like this:


Notice how the RBF kernel graph is very different from the the one from the example. The entire area is classified to be yellow except the red and blue bits. In other words there are too many support vectors. I have tried changing C and degree but they didn't help. The code I used to produce this graph is shown below. 

Please note I need to use RBF kernel because polynomial kernels run significantly slower than RBF kernels.

import numpy as np
import pylab as pl
from sklearn import svm, datasets

FP_SIZE = 50
STD = 10

def gen(fp):

  data = []
  target = []

  fp_count = len(fp)

  # generate rssi reading for monitors / fingerprint points
  # using scikit-learn data structure
  for i in range(0, fp_count):
    for j in range(0,FP_SIZE):
      target.append(i)
      data.append(np.around(np.random.normal(fp[i],STD)))

  data = np.array(data)
  target = np.array(target)

  return data, target

fp = [[-30,-70],[-58,-30],[-60,-60]]

data, target = gen(fp)

# import some data to play with
# iris = datasets.load_iris()
X = data[:, :2]  # we only take the first two features. We could
                      # avoid this ugly slicing by using a two-dim dataset
Y = target

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, Y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)
lin_svc = svm.LinearSVC(C=C).fit(X, Y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# title for the plots
titles = ['SVC with linear kernel',
          'SVC with RBF kernel',
          'SVC with polynomial (degree 3) kernel',
          'LinearSVC (linear kernel)']


for i, clf in enumerate((svc, rbf_svc, poly_svc, lin_svc)):
    # Plot the decision boundary. For that, we will asign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    pl.subplot(2, 2, i + 1)
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    pl.contourf(xx, yy, Z, cmap=pl.cm.Paired)
    pl.axis('off')

    # Plot also the training points
    pl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)

    pl.title(titles[i])

pl.show()

",0,3941,"DId you use any other measure of correctness aside from what you get in the point.

Usually SVMs need to be run using a grid search, specially if you have  an RBF, C only will take care of the regularization, which will do little if your data is not sparse to begin with.

You need to run a grid search over gamma and C, they have a really good example of that here:

http://scikit-learn.org/0.13/auto_examples/grid_search_digits.html#example-grid-search-digits-py

Also, their library already takes care of the cross validation.

Remember that those examples are good for the toy datasets, the moment you enter with a new dataset, there is no reason to believe is going to behave anything like the one in the example.
",,
scikit-learn unexpected issue,https://stackoverflow.com/questions/59830510,Defining distance parameter (V) in knn crossval grid search (seuclidean/mahalanobis distance metrics),"I am trying to carry out a k-fold cross-validation grid search using the KNN algorithm using python sklearn, with parameters in the search being number of neighbors K and distance metric. I am including mahalanobis and seuclidean as distance metrics, and understand these have a parameter which needs to be specified, namely V or VI (covariance matrix of features or inverse of this). 

Below is my code: 

X_train, X_test, y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=10,stratify=y)

knn=KNeighborsClassifier()

grid_param={'n_neighbors':np.arange(1,51),'metric':['euclidean','minkowski','mahalanobis','seuclidean'],'metric_params':[{'V': np.cov(X_train)}]} 

knn_gscv=GridSearchCV(knn,grid_param,cv=5)

knn_gscv.fit(X_train,y_train) (*)


The (*) line throws this error when executed: 

TypeError: __init__() got an unexpected keyword argument 'V'

I have also tried VI instead of V but getting same error. 

I have come across potential solutions below but these don't help. 

https://github.com/scikit-learn/scikit-learn/issues/6915

Scikit-learn: How do we define a distance metric's parameter for grid search

Any help appreciated!

This is also my first question, so any feedback would be helpful also with this regard. 
",3,2128,"grid_params = [
    {'n_neighbors': np.arange(1, 51), 'metric': ['euclidean', 'minkowski']},
    {'n_neighbors': np.arange(1, 51), 'metric': ['mahalanobis', 'seuclidean'],
     'metric_params': [{'V': np.cov(X_train)}]}
]


The issue is that euclidean and minkowski metrics do not accepts V parameter. So you need to separate them. 
",,
scikit-learn unexpected issue,https://stackoverflow.com/questions/38966075,How to restrict anaconda from upgrading the module being installed if its a higher level dependency,"I'm trying to use continuum io anaconda packing system to package python-2.7.10 with other dependent modules for our environment. I want to automate the pack distribution to simply be a single installation of python with the modules we require.

The issue I'm having is when I specify the modules under the build parameter in meta.yaml it will upgrade the version of python being installed despite the fact that it is python-2.7.10. This will cause an error in the build process.

Is there a way to pin the version of python being installed so that if there is a dependency it will hard fail, or use an earlier version of the package?

meta.yaml, ive tried not pinning the version of the modules as well.

package:
  name: python
  version: 2.7.10

source:
  fn: Python-2.7.10.tgz
  url: https://www.python.org/ftp/python/2.7.10/Python-2.7.10.tgz
  md5: d7547558fd673bd9d38e2108c6b42521

build:
  no_link: bin/python

requirements:
  build:
    - bzip2 [unix]
    - zlib [unix]
    - sqlite [unix]
    - readline [unix]
    - tk [unix]
    - openssl [unix]
    - system [linux]
    - ipython 5.0.0
    - numpy 1.11.1
    - cython 0.24.1
    - scipy 0.18.0
    - pandas 0.18.1
    - patsy 0.4.1
    - statsmodels 0.6.1
    - matplotlib 1.5.2
    - ggplot 0.9.4
    - scikit-learn 0.17.1
    - distribute 0.6.45
    - backports.ssl-match-hostname 3.5.0.1
    - certifi 14.05.14
    - nose_parameterized 0.5.0
    - pyparsing 2.1.4
    - python-dateutil 2.5.3
    - pytz 2016.6.1
    - pyzmq 15.3.0
    - simplejson 3.3.3
    - six 1.10.0
    - sympy 1.0
    - tornado 4.4.1
    - virtualenv 13.0.1
    - wsgiref 0.1.2
    - python-swiftclient 2.7.0
    #- python-ceilometerclient #issue
    #- python-heatclient #issue
    #- python-keystoneclient 1.6.0
    #- python-novaclient 2.26.0
    #- python-troveclient #issue
    - python-cinderclient 1.1.2
    - python-glanceclient 0.17.2
    - python-neutronclient 2.4.0
    - networkx 1.11
    - pysal 1.11.1
    - pyyaml 3.11
    - shapely 1.5.13
    - beautifulsoup4 4.4.1
    - nltk 3.2.1
    - requests 2.10.0
    - seaborn 0.5.0
    - h5py 2.6.0
    - xlrd 1.0.0
    - markupsafe 0.23
    - crypto 1.1.0
    - jinja2 2.8
    - openpyxl 2.3.2
    - jaro_winkler 1.0.2
    - bokeh 0.12.1
    - numexpr 2.6.1
    - pytables 3.2.3.1
    - pycurl 7.43.0
    - mgrs 1.1.0
    - psutil 4.3.0
    - biopython 1.67
    - enaml 0.9.8
    - mdp 3.5
    - bitarray 0.8.1
    - clusterpy 0.9.9
    - pyside 1.2.1
    - pyqt 4.11.4
    - parsedatetime 1.4
    - pymysql 0.6.7
    - pyodbc 3.0.10
    - tabulate 0.7.2

  run:
    - zlib [unix]
    - sqlite [unix]
    - readline [unix]
    - tk [unix]
    - openssl [unix]
    - system [linux]

test:
  commands:
    - python -V [unix]
    - 2to3 -h [unix]
    - python-config --help [unix]

about:
  home: http://www.python.org/
  summary: general purpose programming language
  license: PSF


The output with the error:

$ conda build .
Removing old build environment
BUILD START: python-2.7.10-0
    (actual version deferred until further download or env creation)
Using Anaconda Cloud api site https://api.anaconda.org

The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    geos-3.5.0                 |                0        16.9 MB  defaults
    libgcc-4.8.5               |                1         922 KB  r
    pixman-0.32.6              |                0         2.4 MB  defaults
    unixodbc-2.3.4             |                0         688 KB  defaults
    yaml-0.1.6                 |                0         246 KB  defaults
    curl-7.49.0                |                1         543 KB  defaults
    glib-2.43.0                |                2         7.4 MB  r
    hdf5-1.8.17                |                1         1.9 MB  defaults
    atom-0.3.10                |           py27_0         676 KB  defaults
    backports_abc-0.4          |           py27_0           5 KB  defaults
    beautifulsoup4-4.4.1       |           py27_0         116 KB  defaults
    bitarray-0.8.1             |           py27_0          89 KB  defaults
    et_xmlfile-1.0.1           |           py27_0          15 KB  defaults
    future-0.15.2              |           py27_0         616 KB  defaults
    jaro_winkler-1.0.2         |           py27_0          24 KB  auto
    jdcal-1.2                  |           py27_1           9 KB  defaults
    kiwisolver-0.1.3           |           py27_0         571 KB  defaults
    markupsafe-0.23            |           py27_2          31 KB  defaults
    mgrs-1.1.0                 |           py27_0          48 KB  auto
    mpmath-0.19                |           py27_1         873 KB  defaults
    nltk-3.2.1                 |           py27_0         1.7 MB  defaults
    parsedatetime-1.2          |           py27_0          39 KB  auto
    ply-3.8                    |           py27_0          71 KB  defaults
    psutil-4.3.0               |           py27_0         224 KB  defaults
    pycurl-7.43.0              |           py27_0         128 KB  defaults
    pymysql-0.7.6              |           py27_0         116 KB  defaults
    pyodbc-3.0.10              |           py27_0         146 KB  defaults
    pyyaml-3.11                |           py27_4         297 KB  defaults
    pyzmq-15.4.0               |           py27_0         705 KB  defaults
    requests-2.10.0            |           py27_0         611 KB  defaults
    shapely-1.5.16             |           py27_0         494 KB  defaults
    tabulate-0.7.2             |           py27_0          18 KB  auto
    wsgiref-0.1.2              |           py27_0          943 B  auto
    xlrd-1.0.0                 |           py27_0         181 KB  defaults
    biopython-1.67             |      np111py27_0         2.2 MB  defaults
    clusterpy-0.9.9            |           py27_1         101 KB  conda-forge
    cmd2-0.6.7                 |           py27_0          33 KB  auto
    h5py-2.6.0                 |      np111py27_2         2.4 MB  defaults
    jinja2-2.8                 |           py27_1         264 KB  defaults
    jsonschema-2.5.1           |           py27_0          55 KB  defaults
    mdp-3.5                    |           py27_0         477 KB  defaults
    networkx-1.11              |           py27_0         1.1 MB  defaults
    numexpr-2.6.1              |      np111py27_0         347 KB  defaults
    openpyxl-2.3.2             |           py27_0         248 KB  defaults
    rsa-3.4.2                  |           py27_0          50 KB  conda-forge
    singledispatch-3.4.0.3     |           py27_1          17 KB  r
    ssl_match_hostname-3.4.0.2 |           py27_1           6 KB  defaults
    cliff-1.10.1               |           py27_0          36 KB  gus
    crypto-1.1.0               |           py27_0           3 KB  auto
    pysal-1.11.1               |           py27_0        11.2 MB  defaults
    pytables-3.2.3.1           |      np111py27_0         3.4 MB  defaults
    tornado-4.4.1              |           py27_0         552 KB  defaults
    bokeh-0.12.1               |           py27_0         3.2 MB  defaults
    harfbuzz-0.9.35            |                6         1.1 MB  r
    ipython-5.1.0              |           py27_0         936 KB  defaults
    pyopenssl-16.0.0           |           py27_0          66 KB  defaults
    pango-1.36.8               |                3         796 KB  r
    qt-4.8.7                   |                4        32.7 MB  defaults
    python-neutronclient-2.4.0 |           py27_0         222 KB  gus
    shiboken-1.2.1             |           py27_0         883 KB  defaults
    enaml-0.9.8                |           py27_1         944 KB  defaults
    pyside-1.2.1               |           py27_1         5.7 MB  defaults
    seaborn-0.7.1              |           py27_0         272 KB  defaults
    ------------------------------------------------------------
                                           Total:       107.8 MB

The following NEW packages will be INSTALLED:

    atom:                         0.3.10-py27_0       defaults
    babel:                        2.3.3-py27_0        defaults
    backports:                    1.0-py27_0          defaults
    backports.ssl-match-hostname: 3.5.0.1-py27_0      getpantheon
    backports_abc:                0.4-py27_0          defaults
    beautifulsoup4:               4.4.1-py27_0        defaults
    biopython:                    1.67-np111py27_0    defaults
    bitarray:                     0.8.1-py27_0        defaults
    bokeh:                        0.12.1-py27_0       defaults
    brewer2mpl:                   1.4.1-py27_1        conda-forge
    bzip2:                        1.0.6-3             defaults
    cairo:                        1.12.18-6           defaults
    certifi:                      2016.2.28-py27_0    defaults
    cffi:                         1.6.0-py27_0        defaults
    cliff:                        1.10.1-py27_0       gus
    clusterpy:                    0.9.9-py27_1        conda-forge
    cmd2:                         0.6.7-py27_0        auto
    crypto:                       1.1.0-py27_0        auto
    cryptography:                 1.4-py27_0          defaults
    curl:                         7.49.0-1            defaults
    cycler:                       0.10.0-py27_0       defaults
    cython:                       0.24.1-py27_0       defaults
    decorator:                    4.0.10-py27_0       defaults
    distribute:                   0.6.45-py27_1       defaults
    enaml:                        0.9.8-py27_1        defaults
    enum34:                       1.1.6-py27_0        defaults
    et_xmlfile:                   1.0.1-py27_0        defaults
    fontconfig:                   2.11.1-6            defaults
    freetype:                     2.5.5-1             defaults
    functools32:                  3.2.3.2-py27_0      defaults
    future:                       0.15.2-py27_0       defaults
    futures:                      3.0.5-py27_0        defaults
    geos:                         3.5.0-0             defaults
    get_terminal_size:            1.0.0-py27_0        defaults
    ggplot:                       0.11.1-py27_1       conda-forge
    glib:                         2.43.0-2            r
    h5py:                         2.6.0-np111py27_2   defaults
    harfbuzz:                     0.9.35-6            r
    hdf5:                         1.8.17-1            defaults
    idna:                         2.1-py27_0          defaults
    ipaddress:                    1.0.16-py27_0       defaults
    ipython:                      5.1.0-py27_0        defaults
    ipython_genutils:             0.1.0-py27_0        defaults
    iso8601:                      0.1.11-py27_0       defaults
    jaro_winkler:                 1.0.2-py27_0        auto
    jdcal:                        1.2-py27_1          defaults
    jinja2:                       2.8-py27_1          defaults
    jsonpatch:                    1.3-py27_0          auto
    jsonpointer:                  1.2-py27_0          auto
    jsonschema:                   2.5.1-py27_0        defaults
    kiwisolver:                   0.1.3-py27_0        defaults
    libffi:                       3.2.1-0             defaults
    libgcc:                       4.8.5-1             r
    libgfortran:                  3.0.0-1             defaults
    libpng:                       1.6.22-0            defaults
    libsodium:                    1.0.10-0            defaults
    libxml2:                      2.9.2-0             defaults
    markupsafe:                   0.23-py27_2         defaults
    matplotlib:                   1.5.1-np111py27_0   defaults
    mdp:                          3.5-py27_0          defaults
    mgrs:                         1.1.0-py27_0        auto
    mkl:                          11.3.3-0            defaults
    mpmath:                       0.19-py27_1         defaults
    msgpack-python:               0.4.7-py27_0        defaults
    netaddr:                      0.7.18-py27_0       conda-forge
    netifaces:                    0.10.4-py27_0       conda-forge
    networkx:                     1.11-py27_0         defaults
    nltk:                         3.2.1-py27_0        defaults
    nose_parameterized:           0.5.0-py27_0        conda-forge
    numexpr:                      2.6.1-np111py27_0   defaults
    numpy:                        1.11.1-py27_0       defaults
    openpyxl:                     2.3.2-py27_0        defaults
    openssl:                      1.0.2h-1            defaults
    oslo.config:                  1.9.3-py27_0        gus
    oslo.i18n:                    1.5.0-py27_0        gus
    oslo.serialization:           1.4.0-py27_0        gus
    oslo.utils:                   1.4.0-py27_0        gus
    pandas:                       0.18.1-np111py27_0  defaults
    pango:                        1.36.8-3            r
    parsedatetime:                1.2-py27_0          auto
    path.py:                      8.2.1-py27_0        defaults
    pathlib2:                     2.1.0-py27_0        defaults
    patsy:                        0.4.1-py27_0        defaults
    pbr:                          0.11.0-py27_0       defaults
    pexpect:                      4.0.1-py27_0        defaults
    pickleshare:                  0.7.3-py27_0        defaults
    pip:                          8.1.2-py27_0        defaults
    pixman:                       0.32.6-0            defaults
    ply:                          3.8-py27_0          defaults
    prettytable:                  0.7.2-py27_0        conda-forge
    prompt_toolkit:               1.0.3-py27_0        defaults
    psutil:                       4.3.0-py27_0        defaults
    ptyprocess:                   0.5.1-py27_0        defaults
    pyasn1:                       0.1.9-py27_0        defaults
    pycairo:                      1.10.0-py27_0       defaults
    pycparser:                    2.14-py27_1         defaults
    pycurl:                       7.43.0-py27_0       defaults
    pygments:                     2.1.3-py27_0        defaults
    pymysql:                      0.7.6-py27_0        defaults
    pyodbc:                       3.0.10-py27_0       defaults
    pyopenssl:                    16.0.0-py27_0       defaults
    pyparsing:                    2.1.4-py27_0        defaults
    pyqt:                         4.11.4-py27_4       defaults
    pysal:                        1.11.1-py27_0       defaults
    pyside:                       1.2.1-py27_1        defaults
    pytables:                     3.2.3.1-np111py27_0 defaults
    python:                       2.7.12-1            defaults
    python-cinderclient:          1.1.2-py27_0        gus
    python-dateutil:              2.5.3-py27_0        defaults
    python-glanceclient:          0.17.2-py27_0       gus
    python-keystoneclient:        1.3.2-py27_0        gus
    python-neutronclient:         2.4.0-py27_0        gus
    python-swiftclient:           2.7.0-py27_0        chenghlee
    pytz:                         2016.6.1-py27_0     defaults
    pyyaml:                       3.11-py27_4         defaults
    pyzmq:                        15.4.0-py27_0       defaults
    qt:                           4.8.7-4             defaults
    readline:                     6.2-2               defaults
    requests:                     2.10.0-py27_0       defaults
    rsa:                          3.4.2-py27_0        conda-forge
    scikit-learn:                 0.17.1-np111py27_2  defaults
    scipy:                        0.18.0-np111py27_0  defaults
    seaborn:                      0.7.1-py27_0        defaults
    setuptools:                   25.1.6-py27_0       defaults
    shapely:                      1.5.16-py27_0       defaults
    shiboken:                     1.2.1-py27_0        defaults
    simplegeneric:                0.8.1-py27_1        defaults
    simplejson:                   3.8.2-py27_0        defaults
    singledispatch:               3.4.0.3-py27_1      r
    sip:                          4.18-py27_0         defaults
    six:                          1.10.0-py27_0       defaults
    sqlite:                       3.13.0-0            defaults
    ssl_match_hostname:           3.4.0.2-py27_1      defaults
    statsmodels:                  0.6.1-np111py27_1   defaults
    stevedore:                    1.3.0-py27_0        gus
    sympy:                        1.0-py27_0          defaults
    system:                       5.8-2               defaults
    tabulate:                     0.7.2-py27_0        auto
    tk:                           8.5.18-0            defaults
    tornado:                      4.4.1-py27_0        defaults
    traitlets:                    4.2.2-py27_0        defaults
    unixodbc:                     2.3.4-0             defaults
    virtualenv:                   13.0.1-py27_0       defaults
    warlock:                      1.3.0-py27_0        conda-forge
    wcwidth:                      0.1.7-py27_0        defaults
    wheel:                        0.29.0-py27_0       defaults
    wsgiref:                      0.1.2-py27_0        auto
    xlrd:                         1.0.0-py27_0        defaults
    yaml:                         0.1.6-0             defaults
    zeromq:                       4.1.4-0             defaults
    zlib:                         1.2.8-3             defaults

Source cache directory is: /opt/app/anaconda2/conda-bld/src_cache
Found source in cache: Python-2.7.10.tgz
Extracting download
BUILD START: python-2.7.10-0
python is installed as a build dependency. Removing.
An unexpected error has occurred, please consider sending the
following traceback to the conda GitHub issue tracker at:

    https://github.com/conda/conda-build/issues

Include the output of the command 'conda info' in your report.


Traceback (most recent call last):
  File ""/opt/app/anaconda2/bin/conda-build"", line 5, in &lt;module&gt;
    sys.exit(main())
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 152, in main
    args_func(args, p)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 415, in args_func
    args.func(args, p)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/main_build.py"", line 358, in execute
    debug=args.debug)
  File ""/opt/app/anaconda2/lib/python2.7/site-packages/conda_build/build.py"", line 561, in build
    assert not plan.nothing_to_do(actions), actions
AssertionError: defaultdict(&lt;type 'list'&gt;, {'op_order': ('RM_FETCHED', 'FETCH', 'RM_EXTRACTED', 'EXTRACT', 'UNLINK', 'LINK', 'SYMLINK_CONDA'), 'PREFIX': '/opt/app/anaconda2/envs/_build_placehold_placehold_placehold_placehold_placehold'})

",3,577,"Unless there's a specific reason you need to compile python yourself, I think what you're actually going after is conda bundle (http://conda.pydata.org/docs/commands/conda-bundle.html).  Unfortunately we've removed it in conda 4.2 which will be coming out soon, intending to move it to conda-build.  Since that hasn't happened yet, and if it ends up actually being useful to people, we can add it back.



You could also try this using conda-build...

Remove the whole source block in your meta.yaml file. Also remove all of the build requirements that are also not run requirements.  Then in your build.sh file

conda install --yes --quiet \
    python=2.7.10 \
    ipython=5.0.0 \
    numpy=1.11.1 \
    cython=0.24.1 \
    scipy=0.18.0 \
    pandas=0.18.1 \
    patsy=0.4.1 \
    statsmodels=0.6.1 \
    matplotlib=1.5.2 \
    ggplot=0.9.4 \
    scikit-learn=0.17.1 \
    distribute=0.6.45 \
    backports.ssl-match-hostname=3.5.0.1 \
    certifi=14.05.14 \
    nose_parameterized=0.5.0 \
    pyparsing=2.1.4 \
    python-dateutil=2.5.3 \
    pytz=2016.6.1 \
    pyzmq=15.3.0 \
    simplejson=3.3.3 \
    six=1.10.0 \
    sympy=1.0 \
    tornado=4.4.1 \
    virtualenv=13.0.1 \
    wsgiref=0.1.2 \
    python-swiftclient=2.7.0 \
    python-cinderclient=1.1.2 \
    python-glanceclient=0.17.2 \
    python-neutronclient=2.4.0 \
    networkx=1.11 \
    pysal=1.11.1 \
    pyyaml=3.11 \
    shapely=1.5.13 \
    beautifulsoup4=4.4.1 \
    nltk=3.2.1 \
    requests=2.10.0 \
    seaborn=0.5.0 \
    h5py=2.6.0 \
    xlrd=1.0.0 \
    markupsafe=0.23 \
    crypto=1.1.0 \
    jinja2=2.8 \
    openpyxl=2.3.2 \
    jaro_winkler=1.0.2 \
    bokeh=0.12.1 \
    numexpr=2.6.1 \
    pytables=3.2.3.1 \
    pycurl=7.43.0 \
    mgrs=1.1.0 \
    psutil=4.3.0 \
    biopython=1.67 \
    enaml=0.9.8 \
    mdp=3.5 \
    bitarray=0.8.1 \
    clusterpy=0.9.9 \
    pyside=1.2.1 \
    pyqt=4.11.4 \
    parsedatetime=1.4 \
    pymysql=0.6.7 \
    pyodbc=3.0.10 \
    tabulate=0.7.2


The big difference: by listing all of those packages as build requirements, you're actually ensuring that they won't be in your final conda package.  Think of build requirements more like a compiler, or something that's necessary when you're building the package, but not when you're actually running it.
",,
scikit-learn unexpected issue,https://stackoverflow.com/questions/76309535,Quantile Forest error &quot;predict() got an unexpected keyword argument &#39;quantiles&#39;&quot;,"I continue to run into errors when run any form of quantile forest models with the prediction and quantile phases. I am following this example but with my own X and y. I have trained many a random forest and other derivations of tree models with this dataset, so I'm fairly certain it's not the input data issue.
https://github.com/zillow/quantile-forest
And have created an environment that supposedly follows all the installation requirements. I can provide my list of versions upon request. Time and time again, the RandomForestQuantileRegressor will work, but when I want to plot and see the quantiles, I get the error

**""TypeError: predict() got an unexpected keyword argument 'quantiles'""
**

Here is an example when I set up the environment:

Collecting quantile-forest
Downloading quantile_forest-1.1.2-cp310-cp310-macosx_10_9_x86_64.whl (188 kB)
 188.6/188.6 kB 2.7 MB/s eta 0:00:00
Requirement already satisfied: numpy&gt;=1.23 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from quantile-forest) (1.24.3)
Requirement already satisfied: scipy&gt;=1.4 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from quantile-forest) (1.10.1)
Requirement already satisfied: scikit-learn&gt;=1.0 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from quantile-forest) (1.2.2)
Requirement already satisfied: joblib&gt;=1.1.1 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from scikit-learn&gt;=1.0-&gt;quantile-forest) (1.2.0)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in ./opt/anaconda3/envs/uq_rf/lib/python3.10/site-packages (from scikit-learn&gt;=1.0-&gt;quantile-forest) (3.1.0)
Installing collected packages: quantile-forest
Successfully installed quantile-forest-1.1.2

This code works:
X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=0.5, random_state=0)

qrf = RandomForestQuantileRegressor(q=[0.05, 0.50, 0.95])
qrf.fit(X_train, y_train)

y_pred_5, y_pred_median, y_pred_95 = qrf.predict(X_test)
qrf.score(X_test, y_test)

But I cannot get any code that calls ""predict"" where I specify the quantiles to work without that type error.
",0,180,"I notice that the code you've provided is an example from the sklearn-quantile package. Perhaps confusingly, both packages -- sklearn-quantile and quantile-forest -- provide a RandomForestQuantileRegressor class, but the packages have different ways of passing the quantiles to the class methods. As a result, the RandomForestQuantileRegressor classes from the two packages are not currently interchangeable.
In the code snippet you've provided, it's not clear what imports are being used, but it appears that you may be using the RandomForestQuantileRegressor class from the sklearn-quantile package. This class expects the quantiles to be passed to the initialization function instead of the predict function and would lead to the error you've presented. If this is correct, then you can fix this by importing the RandomForestQuantileRegressor class from the quantile-forest package and passing the quantiles to the predict function instead of the initialization function.
If the above is not helpful or you still are running into errors, you're welcome to create an issue in the quantile-forest repository here for additional troubleshooting.
",,
scikit-learn unexpected issue,https://stackoverflow.com/questions/40877501,Error using conda to update a package,"I launched on AWS EC2 an instance using ubuntu as AMI.

Then I ran a script to install the jupyter notebook and be able to access to it with the IP address and the specific port.
The code I wrote was this one :

sudo apt-get install git

git clone https://gist.github.com/rashmibanthia/5a1e4d7e313d6832f2ff nb

. nb/jupyter_notebook_ec2.sh 

cd;mkdir notebook;cd notebook

tmux new -s nb

jupyter notebook --certfile=~/certs/mycert.pem --keyfile ~/certs/mycert.key


Now I am in the notebook. Here comes my problem. When I try to import some packages and class, I get an error. For example with this line:

from sklearn.gaussian_process import GaussianProcessRegressor


I get this:


  ImportError                               Traceback (most recent call
  last)  in ()
  ----&gt; 1 from sklearn.gaussian_process import GaussianProcessRegressor
  
  ImportError: cannot import name 'GaussianProcessRegressor'


So I tried to update the scikit learn package with conda with conda update scikit-learn but I have another error:


  Fetching package metadata ...An unexpected error has occurred. Please
  consider posting the following information to the conda GitHub issue
  tracker at:

https://github.com/conda/conda/issues



Current conda install:

           platform : linux-64
      conda version : 4.2.13
   conda is private : False
  conda-env version : 4.2.13
conda-build version : 1.18.2
     python version : 3.5.2.final.0
   requests version : 2.12.1
   root environment : /home/ubuntu/anaconda3  (writable)
default environment : /home/ubuntu/anaconda3
   envs directories : /home/ubuntu/anaconda3/envs
      package cache : /home/ubuntu/anaconda3/pkgs
       channel URLs : https://repo.continuum.io/pkgs/free/linux-64
                      https://repo.continuum.io/pkgs/free/noarch
                      https://repo.continuum.io/pkgs/pro/linux-64
                      https://repo.continuum.io/pkgs/pro/noarch
        config file : None
       offline mode : False




$ /home/ubuntu/anaconda3/bin/conda update scikit-learn

Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/exceptions.py"", line 479, in conda_exception_handler
    return_value = func(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/cli/main.py"", line 145, in _main
    exit_code = args.func(args, p)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/cli/main_update.py"", line 65, in execute
    install(args, parser, 'update')
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/cli/install.py"", line 238, in install
    prefix=prefix)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/api.py"", line 24, in get_index
    index = fetch_index(channel_urls, use_cache=use_cache, unknown=unknown)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 300, in fetch_index
    repodatas = [(u, f.result()) for u, f in zip(urls, futures)]
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 300, in &lt;listcomp&gt;
    repodatas = [(u, f.result()) for u, f in zip(urls, futures)]
  File ""/home/ubuntu/anaconda3/lib/python3.5/concurrent/futures/_base.py"", line 405, in result
    return self.__get_result()
  File ""/home/ubuntu/anaconda3/lib/python3.5/concurrent/futures/_base.py"", line 357, in __get_result
    raise self._exception
  File ""/home/ubuntu/anaconda3/lib/python3.5/concurrent/futures/thread.py"", line 55, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 75, in func
    res = f(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/conda/fetch.py"", line 117, in fetch_repodata
    timeout=(6.1, 60))
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 501, in get
    return self.request('GET', url, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 488, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/sessions.py"", line 609, in send
    r = adapter.send(request, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/adapters.py"", line 423, in send
    timeout=timeout
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 594, in urlopen
    chunked=chunked)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 350, in _make_request
    self._validate_conn(conn)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 835, in _validate_conn
    conn.connect()
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/connection.py"", line 330, in connect
    cert = self.sock.getpeercert()
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 324, in getpeercert
    'subjectAltName': get_subj_alt_name(x509)
  File ""/home/ubuntu/anaconda3/lib/python3.5/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 171, in get_subj_alt_name
    ext = cert.extensions.get_extension_for_class(
AttributeError: 'Extensions' object has no attribute 'get_extension_for_class'

",0,1164,"As kalefranz comments here 'https://github.com/conda/conda/issues/3898' it might be because pyopenssl and cryptography are out of sync and this should fix it:

CONDA_SSL_VERIFY=false conda update pyopenssl

In windows to set ssl_verify the following command can be used:

conda config --set ssl_verify False

As it is said in Problems with updating anaconda and installing new packages
",,
scikit-learn strange behavior,https://stackoverflow.com/questions/44729373,GaussianProcess regression results correct...up to a scale factor?,"I am running GaussianProcess regressions over some very noisy data. When I scatter plot predictions (which are, I know, predictions of means) vs actuals, I get a beautiful only slightly noisy y=x line.

Only one problem: the slope is completely wrong. Is there any way I can address this without building a second-stage linear regressor?

I regret I cannot share my data, but my model is fairly basic. X is a matrix with 10 columns, y a matrix with 1 column. I am using 1,000 examples to train and plot.

added: The below plot is plotting predicted versus actual. Given that I am using a nonlinear kernel, I find it strange that the GP regressor can find a relationship which is accurate up to a multiplier (slope).



kernel = (
    GP.kernels.RationalQuadratic(
        length_scale=.8,
        length_scale_bounds=(1e-3,1e3),
        alpha=.8,
        alpha_bounds=(1e-3,1e3),
        )
    + GP.kernels.WhiteKernel()
    )

gp = Pipeline( [
    ('scale',preproc.StandardScaler()),
    ('gp',GP.GaussianProcessRegressor(kernel=kernel)),
    ] )
gp.fit( X, y )


added: I'm a bit embarrassed, but I'm new to the GP world in particular and, really, regression as a ML problem in general. I had not plotted the model's performance over a test set, which revealed a strong overfit. Additionally, I've added an idiom to my code to deal with scikit-learn's default GP behavior, i.e., optimization makes me sad when I give it significant quantities of data, by ""pretraining"" on a small quantity of data, using the optimizer to find reasonable values for the kernel parameters, then ""training"" a much larger quantity of data. This allowed me to widen the parameter search and use multiple restarts on the optimizer, finding a much more generalizable model...which was almost all noise. Which was what I was expecting, really.

kernel = (
    GP.kernels.RationalQuadratic(
        length_scale=1,
        alpha=.5,
        )
    + GP.kernels.WhiteKernel(
        noise_level=1,
        )
    )*GP.kernels.ConstantKernel()

gp = Pipeline( [
    ('scale',preproc.StandardScaler()),
    ('gp',GP.GaussianProcessRegressor(
        kernel=kernel,
        n_restarts_optimizer=3,
        alpha=0,
        )),
    ] )
print(""pretraining model for target %s..."" % c)
x_pre = X_s.values[:500,:]
y_pre = y_s_scl[:500,:]
gp.fit( x_pre, y_pre )

gp = Pipeline( [
    ('scale',preproc.StandardScaler()),
    ('gp',GP.GaussianProcessRegressor(
        kernel=kernel,
        optimizer=None,
        alpha=0,
        )),
    ] )
print(""training model for target %s..."" % c)

",2,404,"EDIT: Have you tried centering your data before doing the regression? (subtracting the mean of all the output values from each output). I know the Gp Toolbox in Matlab doesn't need the data to be centered, but I am not sure about the GP in sklearn. See:
https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia

OLD COMMENT:
Your initial values for the hyperparameters in the kernel function (i.e. length-scale and alpha) are very important. During the fit(), the hyperparameters are optimized and local maximum of hyperpareters can be found, which could in turn affect your result. Depending on the bounds you set for these hyperparameters, many local maximum can be found depending on the initial conditions. 
On the sklearn site it says:
""As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying n_restarts_optimizer.""
You may try using the RBF function as it is a very traditional kernel function for the GP. 
",,
scikit-learn strange behavior,https://stackoverflow.com/questions/44527713,Python script works from one folder but not on subfolders.,"Good evening everyone, 

I am trying to do some machine learning with python so I imported the module scikit-learn:

from sklearn.preprocessing import MinMaxScaler


And it gives me this strange error: 

Traceback (most recent call last):

  File ""&lt;ipython-input-22-b55a4eaccb0b&gt;"", line 1, in &lt;module&gt;
    from sklearn.preprocessing import MinMaxScaler

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\preprocessing\__init__.py"", line 6, in &lt;module&gt;
    from ._function_transformer import FunctionTransformer

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\preprocessing\_function_transformer.py"", line 2, in &lt;module&gt;
    from ..utils import check_array

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\utils\__init__.py"", line 18, in &lt;module&gt;
    from ..externals.joblib import cpu_count

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\externals\joblib\__init__.py"", line 128, in &lt;module&gt;
    from .parallel import Parallel

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 24, in &lt;module&gt;
    from ._multiprocessing_helpers import mp

  File ""C:\Users\Francesco\Anaconda3\lib\site-packages\sklearn\externals\joblib\_multiprocessing_helpers.py"", line 24, in &lt;module&gt;
    _sem = mp.Semaphore()

AttributeError: module 'multiprocessing' has no attribute 'Semaphore'


The errors are copied from Anaconda (Spider) but they are present even by starting a python session from the command line. The folder where I run the script is: 

C:\Users\Francesco\Desktop\script_python


The very strange thing is that if I run the same script from the Desktop, just a folder up, it works!

C:\Users\Francesco\Desktop\ &lt;-- Here it works!!


I found out that the problem is relative to the multiprocessing module, here a snapshot of this strange behavior: 


",1,403,"You probably have a module multiprocessing in the directory where you are running the script from.

Generally, in Python, the path where you run your script has precedence over the Python env, so it will try to pick that one first. This means that when sklearn imports multiprocessing, it uses your module, and not the built-in multiprocessing.

When this type of error appears, one way to identify the error is to run

python -c ""import multiprocessing; print(multiprocessing.__dir__)""


to double check which module is being used.
",,
scikit-learn strange behavior,https://stackoverflow.com/questions/48263740,strange memory consumption behaviour when running PCA in python,"At the moment I am trying to find the reason for a specific behavior in Python. 

First I want to describe my use case. The idea is a performance analysis on Python. Therefore I want to analyze the memory usage and runtime regarding the ""Principle Component Analysis""-algorithm. For that, I use scikit-learn (http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). 

To see, how the algorithm behaves for different datasets, I generated several randomly uniform distributed datasets. The strange thing is, there is an increase in memory usage and a faster runtime regarding two datasets.

Now I want to explain, how I measure the memory-usage and runtime shortly. 
Memory measurement (package: https://pypi.python.org/pypi/memory_profiler)


disable the garbage collection
Use the @profile decoration to find the increments of the function
Write the output into a log file
Use another python program to find the increments of the log files which belong to a specific dataset
sum up the increments - the result is the total memory allocation for the complete function
(I tested it with enabled garbage collection, too)


Time measurement (function: https://docs.python.org/2/library/timeit.html)


Use ""timeit.repeat()"" to get a list of many runtimes of the function
Write the list of those measurements to a CSV


After I had performed about 100 memory measurements and even more time measurements, I recognized the aforementioned behavior.
Hopefully, you can help me, find a reason for this behavior.

Here is the function, I measured:

@profile 
def pcaTrain(dataset):
 model = sklearn_pca.fit(dataset)
 model.variance = np.var(model.transform(dataset), axis=0, ddof=1)
 return model 


And here are some diagrams of the memory usage and the time measurements:

Memory measurements for dataset 4-17

Runtime measurements for dataset 4-17

Set up:


Python-version: 3.5.2
Memory: &gt; 100 GB


List of datasets:


dset04 -&gt; 1.000.000 rows per column (10 features x 1.000.000)
dset05 -&gt; 1.000.000 rows per column (12 features x 1.000.000)
dset06 -&gt; 1.000.000 rows per column (14 features x 1.000.000)
dset07 -&gt; 1.000.000 rows per column (16 features x 1.000.000)
dset08 -&gt; 1.000.000 rows per column (18 features x 1.000.000)
dset09 -&gt; 1.000.000 rows per column (20 features x 1.000.000)
dset10 -&gt; 1.000.000 rows per column (22 features x 1.000.000)
dset11 -&gt; 1.000.000 rows per column (24 features x 1.000.000)
dset12 -&gt; 1.000.000 rows per column (26 features x 1.000.000)
dset13 -&gt; 1.000.000 rows per column (28 features x 1.000.000)
dset14 -&gt; 1.000.000 rows per column (30 features x 1.000.000)
dset15 -&gt; 1.000.000 rows per column (35 features x 1.000.000)
dset16 -&gt; 1.000.000 rows per column (40 features x 1.000.000)
dset17 -&gt; 1.000.000 rows per column (45 features x 1.000.000)

",1,300,"The important question is, how many of the features you are extractiong.
Since version 0.18 of Scikit-learn, the svd_solver flag of the PCA algorithm determines which algorithm to use.  The default behavior is to select the ""best"" choice, which is described in detail in the official documentation which you mentioned. 
Possibly, one of those selections hits your performance, depending on the number of components. Otherwise, I would suggest you mention this behavior in the official GitHub of scikit-learn, since this could be interesting to them as well.
",,
scikit-learn strange behavior,https://stackoverflow.com/questions/53630329,Watershed analysis of coins - wrong output,"For the counting of round objects in an image I want to use the watershed algorithm.
In order to learn how it works and how I can use it for my needs, I have searched some working examples in python (https://docs.opencv.org/3.1.0/d3/db4/tutorial_py_watershed.html ; http://scikit-image.org/docs/dev/auto_examples/segmentation/plot_label.html)

I finaly found a working solution, which works more or less out-of-the-box for my own purposes (How to define the markers for Watershed in OpenCV?)

With this code I get nice results, both with the example file as with my own images.
I do get a strange behavior though after the watershed analysis. For some reason, the watershed step also adds a border around the image. So next to the objects that are detected, also the whole edge of the image gets detected and colored.

My guess is that I should change a parameters in the code to stop this from happening, but so far I'm unable to find what I should do.

this is the code:

import cv2
import numpy as np
from scipy.ndimage import label
def segment_on_dt(a, img):
    border = cv2.dilate(img, None, iterations=3)
    border = border - cv2.erode(border, None)
    dt = cv2.distanceTransform(img, cv2.DIST_L2, 3)
    dt = ((dt - dt.min()) / (dt.max() - dt.min()) * 255).astype(np.uint8)
    _, dt = cv2.threshold(dt, 200, 255, cv2.THRESH_BINARY)
    lbl, ncc = label(dt)
    # Completing the markers now. 
    lbl[border == 255] = 255 
    lbl = lbl.astype(np.int32)
    cv2.watershed(a, lbl)
    lbl[lbl == -1] = 0
    lbl = lbl.astype(np.uint8)
    return 255 - lbl

# Load image file
img = cv2.imread('coins.jpg')
# Pre-processing.
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img_gray = cv2.GaussianBlur(img_gray,(5,5),0)  
width, height = img_gray.shape
_, img_bin = cv2.threshold(img_gray, 0,  255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
img_bin = cv2.morphologyEx(img_bin, cv2.MORPH_OPEN,np.ones((5, 5),     dtype=int))
result = segment_on_dt(img, img_bin)
result[result != 255] = 0
result = cv2.dilate(result, None)
img[result == 255] = (0, 0, 255)
cv2.imwrite('Img_output.png',img)


Running this code will give this result (at least on my pc)



The result for detecting the coins is good enough for my purposes, but I'm a bit puzzled about the image edge that is also detected. From what I see during debugging, the watershed adds this edge, but it is unclear to me why this happens.
",1,514,"You can fix this by adding a background label using the tutorial provided by openCV. 
https://docs.opencv.org/3.1.0/d3/db4/tutorial_py_watershed.html

They added an extra step to insert sure background and sure foreground region to help the watershed algorithm to properly segment the coin regions.

*********** edit**************

After reading your code again. I found that your original code has no problem. 
Background label was set using the variable border.

You will probably get the same result by executing the code found in OpenCV tutorial. The problem is in the way your draw the results. Since this is a display problem, there are many ways we can tackle the problem. One of the many is to use the information of the sure-background

Here are the modification to the function segment_on_dt

def segment_on_dt(a, img):
    sure_background = cv2.dilate(img, None, iterations=3)
    border = sure_background - cv2.erode(sure_background, None)


    dt = cv2.distanceTransform(img, cv2.DIST_L2, 3)
    dt = ((dt - dt.min()) / (dt.max() - dt.min()) * 255).astype(np.uint8)
    _, dt = cv2.threshold(dt, 200, 255, cv2.THRESH_BINARY)
    lbl, ncc = label(dt)


    # Completing the markers now. 
    lbl[border == 255] = 255 


    lbl = lbl.astype(np.int32)
    cv2.watershed(a, lbl)
    lbl[lbl == -1] = 0
    # Only draw red line if its not in sure background
    lbl[sure_background == 0] = 255

    lbl = lbl.astype(np.uint8)
    cv2.imshow('lbl_2',lbl)

    return 255 - lbl


I have added a new condition for the red lines to be drawn. The line are only drawn if its not in sure background region.

Your final result should look like this.


",,
scikit-learn strange behavior,https://stackoverflow.com/questions/28726548,Text Documents Clustering - Non Uniform Clusters,"I have been trying to cluster a set of text documents. I have a sparse TFIDF matrix with around 10k documents (subset of a large dataset), and I try to run the scikit-learn k-means algorithm with different sizes of clusters (10,50,100). Rest all the parameters are default values.

I get a very strange behavior that no matter how many clusters I specify or even if I change the number of iterations, there would be 1 cluster in the lot which would contain most of the documents in itself and there will be many clusters which would have just 1 document in them. This is highly non-uniform behavior 

Does anyone know what kind of problem am I running into? 
",0,215,"Here are the possible things that might be going ""wrong"":


Your k-means cluster initialization points are chosen as the same set of points in each run. I recommend using the 'random' for the init parameter of k-means http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. If that doesn't work then supply to k-means your own set of random initial cluster centers. Remember to initialize your random generator using its seed() method as the current date and time. https://docs.python.org/2/library/random.html uses current date-time as the default value. 
Your distance function, i.e. euclidean distance might be the culprit. This is less likely but it is always good to run k-means using cosine similarity especially when you are using it for document similarity. scikits doesn't have this functionality at present but you should look here: Is it possible to specify your own distance function using scikit-learn K-Means Clustering?


These two combined should give you good clusters.
","I noticed with the help of above answers and comments that there was a problem with outliers and noise in original space. For this, we should use a dimensionality reduction method which eliminates the unwanted noise in the data. I tried random projections first but it failed to work with text data, simply because the problem was still not solved.
Then using Truncated Singular Value Decomposition, I was able to get perfect uniform clusters. Hence, the Truncated SVD is the way to go with textual data in my opinion.
",
scikit-learn strange behavior,https://stackoverflow.com/questions/62777025,GridSearchCV fails for own model class,"I'm trying to use a regression model I have implemented in combination with the GridSearchCV class of scikit-learn to optimize the hyper-parameters of my model. My modelclass is nicely build following the suggestions of the scikit-api:
class FOO(BaseEstimator, RegressorMixin):
def __init__(self,...)
    *** initialisation of all the parameters and hyperparameters (including the kernelfunction)***

def fit(self,X,y)
    *** implementation of fit: just takes input and performs fit of parameters.

def predict(self,X)
    *** implementation of predict: just takes input and calculates the result

The regression-class works as it should, but strangely enough, when I study the behavior of the hyperparameters, I tend to get inconsistencies. It appears one hyper-parameter is correctly applied by GridSearchCV, but the other one is clearly not.
So I am wondering, can someone explain to me how gridsearchCV is working (from the technical perspective)? How does it initialise the estimator, how does it run it over the grid?
My current assumption of the workings and required use of GridsearchCV is this:

Create a GridSearchCV instance  (CVmodel=GridSearchCV(MyRegressor,param_grid=Myparamgrid,...)
Fit the hyperparameter(s) via: CVmodel.fit(X,y). Which naively would work like this:

&gt; Loop over Parameter-values
&gt;          - create esimator instance with parameter value(and defaults for the other params)
&gt;          - estimator.fit
&gt;          - result[parameter-value]=estimator.predict

However, experience shows me this naive idea is quite wrong, as the hyper-parameter associated with the kernel-function of my regressor is not correctly initialized.
Can anyone provide some insight into what GridSearchCV is truly doing?
",0,184,"After quite some digging I discovered, scikit-learn does not create new instances (as would be expected in OOP) but rather updates the properties of the object via the set_params method. In my case, this worked fine for the hyperparameter which is directly defined by the same keyword in the __ init __ method, however, it breaks down when the hyperparameter is a property of the static method set during the __ init __ method. Overriding the set_params method (which many tutorials advise against) to deal with this fixes the problem.
For those interested in more details, I wrote this all up in a tutorial myself.
",,
scikit-learn strange behavior,https://stackoverflow.com/questions/63692895,Is there a way to build a logistic regression model even if there is only one class?,"Is there a way to build a scikit-learn logistic regression model for only 1 class? Obviously this model would predict the same class every time, regardless of the input data. My models are currently using liblinear as the solver, I'm not sure if there's another solver that would allow for this?
I realize this is a very strange question for ML but I am building many hierarchical models and in my situation it is easier to have a model for every case even if it predicts the same class every time.
Background: I have a hierarchical prediction task where I'm trying to predict three parts of a 9 digit code (e.g. for a code = 001010424, part 1 = 001, part 2 = 01, part 3= 0424). To do this I'm building hierarchical models. Using the input data we first predict part 1, then using the highest confidence decision for part 1 we use the input data again in a model for part 2 that is specific to the part 1 code. So for example, if I run the part 1 model and get a prediction that part 1 = 001 I then go to the part 2 model for 001 which then (is trained on and) predicts part 2 given part 1 = 001. This hierarchical behavior is repeated for part 3.
",0,1123,"Scitkit learn needs samples of at least two classes.
  import numpy as np
  from sklearn.linear_model import LogisticRegression

  x = np.random.rand(5,2)
  y = np.ones(5).astype(int)
  model = LogisticRegression().fit(x, y)

This yields the error:
  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1

You are probably better off having your training algorithm check if there is only one y label, and if there is, just have your code memorize that.  It seems that such an implementation would be straightforward to implement and much easier to understand for anyone looking at the code later.
",,
scikit-learn strange behavior,https://stackoverflow.com/questions/68155575,RandomForestClassifier takes less time to train when in Pipeline?,"I am experiencing strange behavior in Scikit-learn.
When I train a RandomForestClassifier standalone (not in a pipeline), the model takes 280 seconds to train, while the training process only takes 18 seconds in a pipeline.
Am I doing something wrong ?

",-1,223,"In 1st block, you have 6 lines of code. You are importing libraries, fitting your model, getting scores, and doing everything for the first time. Whereas in 2nd block you are importing the libraries like Scikit-learn which is already imported, so it won't take time. And, in 3rd block there is just one line of code. Another reason is that, In 1st block, you are getting some kind of warning which might be affecting the speed of running the code.
",,
scikit-learn strange result,https://stackoverflow.com/questions/22054964,OLS Regression: Scikit vs. Statsmodels?,"Short version: I was using the scikit LinearRegression on some data, but I'm used to p-values so put the data into the statsmodels OLS, and although the R^2 is about the same the variable coefficients are all different by large amounts. This concerns me since the most likely problem is that I've made an error somewhere and now I don't feel confident in either output (since likely I have made one model incorrectly but don't know which one).

Longer version: Because I don't know where the issue is, I don't know exactly which details to include, and including everything is probably too much. I am also not sure about including code or data. 

I am under the impression that scikit's LR and statsmodels OLS should both be doing OLS, and as far as I know OLS is OLS so the results should be the same.

For scikit's LR, the results are (statistically) the same whether or not I set normalize=True or =False, which I find somewhat strange.

For statsmodels OLS, I normalize the data using StandardScaler from sklearn. I add a column of ones so it includes an intercept (since scikit's output includes an intercept). More on that here: http://statsmodels.sourceforge.net/devel/examples/generated/example_ols.html (Adding this column did not change the variable coefficients to any notable degree and the intercept was very close to zero.) StandardScaler didn't like that my ints weren't floats, so I tried this: https://github.com/scikit-learn/scikit-learn/issues/1709
That makes the warning go away but the results are exactly the same.

Granted I'm using 5-folds cv for the sklearn approach (R^2 are consistent for both test and training data each time), and for statsmodels I just throw it all the data.

R^2 is about 0.41 for both sklearn and statsmodels (this is good for social science). This could be a good sign or just a coincidence.

The data is observations of avatars in WoW (from http://mmnet.iis.sinica.edu.tw/dl/wowah/) which I munged about to make it weekly with some different features. Originally this was a class project for a data science class.

Independent variables include number of observations in a week (int), character level (int), if in a guild (Boolean), when seen (Booleans on weekday day, weekday eve, weekday late, and the same three for weekend), a dummy for character class (at the time for the data collection, there were only 8 classes in WoW, so there are 7 dummy vars and the original string categorical variable is dropped), and others.

The dependent variable is how many levels each character gained during that week (int).

Interestingly, some of the relative order within like variables is maintained across statsmodels and sklearn. So, rank order of ""when seen"" is the same although the loadings are very different, and rank order for the character class dummies is the same although again the loadings are very different.

I think this question is similar to this one: Difference in Python statsmodels OLS and R's lm

I am good enough at Python and stats to make a go of it, but then not good enough to figure something like this out. I tried reading the sklearn docs and the statsmodels docs, but if the answer was there staring me in the face I did not understand it. 

I would love to know:


Which output might be accurate? (Granted they might both be if I missed a kwarg.)
If I made a mistake, what is it and how to fix it?
Could I have figured this out without asking here, and if so how?


I know this question has some rather vague bits (no code, no data, no output), but I am thinking it is more about the general processes of the two packages. Sure, one seems to be more stats and one seems to be more machine learning, but they're both OLS so I don't understand why the outputs aren't the same.

(I even tried some other OLS calls to triangulate, one gave a much lower R^2, one looped for five minutes and I killed it, and one crashed.)

Thanks!
",31,34799,"It sounds like you are not feeding the same matrix of regressors X to both procedures (but see below). Here's an example to show you which options you need to use for sklearn and statsmodels to produce identical results.

import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression

# Generate artificial data (2 regressors + constant)
nobs = 100 
X = np.random.random((nobs, 2)) 
X = sm.add_constant(X)
beta = [1, .1, .5] 
e = np.random.random(nobs)
y = np.dot(X, beta) + e 

# Fit regression model
sm.OLS(y, X).fit().params
&gt;&gt; array([ 1.4507724 ,  0.08612654,  0.60129898])

LinearRegression(fit_intercept=False).fit(X, y).coef_
&gt;&gt; array([ 1.4507724 ,  0.08612654,  0.60129898])


As a commenter suggested, even if you are giving both programs the same X, X may not have full column rank, and they sm/sk could be taking (different) actions under-the-hood to make the OLS computation go through (i.e. dropping different columns).

I recommend you use pandas and patsy to take care of this:

import pandas as pd
from patsy import dmatrices

dat = pd.read_csv('wow.csv')
y, X = dmatrices('levels ~ week + character + guild', data=dat)


Or, alternatively, the statsmodels formula interface:

import statsmodels.formula.api as smf
dat = pd.read_csv('wow.csv')
mod = smf.ols('levels ~ week + character + guild', data=dat).fit()


Edit: This example might be useful: http://statsmodels.sourceforge.net/devel/example_formulas.html
","If you use statsmodels, I would highly recommend using the statsmodels formula interface instead.  You will get the same old result from OLS using the statsmodels formula interface as you would from sklearn.linear_model.LinearRegression, or R, or SAS, or Excel.  

smod = smf.ols(formula ='y~ x', data=df)
result = smod.fit()
print(result.summary())


When in doubt, please 


try reading the source code
try a different language for benchmark, or 
try OLS from scratch, which is basic linear algebra. 

","i just wanted to add here, that in terms of sklearn, it does not use OLS method for linear regression under the hood. Since sklearn comes from the data-mining/machine-learning realm, they like to use Steepest Descent Gradient algorithm. This is a numerical method that is sensitive to initial conditions etc, while the OLS is an analytical closed form approach, so one should expect differences. So statsmodels comes from classical statistics field hence they would use OLS technique. So there are differences between the two linear regressions from the 2 different libraries
"
scikit-learn strange result,https://stackoverflow.com/questions/10407266,scikits learn and nltk: Naive Bayes classifier performance highly different,"I am comparing two Naive Bayes classifiers: one from NLTK and and one from scikit-learn. I'm dealing with a multi-class classification problem (3 classes: positive (1), negative (-1), and neutral (0)). 

Without performing any feature selection (that is, using all features available), and using a training dataset of 70,000 instances (noisy-labeled, with an instance distribution of  17% positive, 4%  negative and 78% neutral), I train two classifiers, the first one is a nltk.NaiveBayesClassifier, and the second one is a sklearn.naive_bayes.MultinomialNB (with fit_prior=True).

After training, I evaluated the classifiers on my test set of 30,000 instances and I get the following results:

**NLTK's NaiveBayes**
accuracy: 0.568740
class: 1
     precision: 0.331229
     recall: 0.331565
     F-Measure: 0.331355
class: -1
     precision: 0.079253 
     recall: 0.446331 
     F-Measure: 0.134596 
class: 0
     precision: 0.849842 
     recall: 0.628126 
     F-Measure: 0.722347 


**Scikit's MultinomialNB (with fit_prior=True)**
accuracy: 0.834670
class: 1
     precision: 0.400247
     recall: 0.125359
     F-Measure: 0.190917
class: -1
     precision: 0.330836
     recall: 0.012441
     F-Measure: 0.023939
class: 0
     precision: 0.852997
     recall: 0.973406
     F-Measure: 0.909191

**Scikit's MultinomialNB (with fit_prior=False)**
accuracy: 0.834680
class: 1
     precision: 0.400380
     recall: 0.125361
     F-Measure: 0.190934
class: -1
     precision: 0.330836
     recall: 0.012441
     F-Measure: 0.023939
class: 0
     precision: 0.852998
     recall: 0.973418
     F-Measure: 0.909197


I have noticed that while Scikit's classifier has better overall accuracy and precision, its recall is very low compared to the NLTK one, at least with my data. Taking into account that they might be (almost) the same classifiers, isn't this strange?
",14,5547,"Naive Bayes classifier usually means a Bayesian classfier over binary features that are assumed to be independent. This is what NLTK's Naive Bayes classifier implements. The corresponding scikit classifier is BernoulliNB classifier.

The restriction to boolean valued features is not actually necessary,
it is just the simplest to implement.
A naive Bayes classifier can be defined for (assumed) independent features from any parametric distribution.

MultinomialNB is for data with integer valued input features that are assumed to be multinomially distributed.

Sckit also has GaussianNB that for continuous valued features that are assumed to idependently Gaussian distributed.
",,
scikit-learn strange result,https://stackoverflow.com/questions/13301986,predict_proba or decision_function as estimator &quot;confidence&quot;,"I'm using LogisticRegression as a model to train an estimator in scikit-learn. The features I use are (mostly) categorical; and so are the labels. Therefore, I use a DictVectorizer and a LabelEncoder, respectively, to encode the values properly. 

The training part is fairly straightforward, but I'm having problems with the test part. The simple thing to do is to use the ""predict"" method of the trained model and get the predicted label. However, for the processing I need to do afterwards, I need the probability for each possible label (class) for each particular instance. I decided to use the ""predict_proba"" method. However, I get different results for the same test instance, whether I use this method when the instance is by itself or accompanied by others. 

Next, is a code that reproduces the problem.

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import LabelEncoder


X_real = [{'head': u'n\xe3o', 'dep_rel': u'ADVL'}, 
          {'head': u'v\xe3o', 'dep_rel': u'ACC'}, 
          {'head': u'empresa', 'dep_rel': u'SUBJ'}, 
          {'head': u'era', 'dep_rel': u'ACC'}, 
          {'head': u't\xeam', 'dep_rel': u'ACC'}, 
          {'head': u'import\xe2ncia', 'dep_rel': u'PIV'}, 
          {'head': u'balan\xe7o', 'dep_rel': u'SUBJ'}, 
          {'head': u'ocupam', 'dep_rel': u'ACC'}, 
          {'head': u'acesso', 'dep_rel': u'PRED'}, 
          {'head': u'elas', 'dep_rel': u'SUBJ'}, 
          {'head': u'assinaram', 'dep_rel': u'ACC'}, 
          {'head': u'agredido', 'dep_rel': u'SUBJ'}, 
          {'head': u'pol\xedcia', 'dep_rel': u'ADVL'}, 
          {'head': u'se', 'dep_rel': u'ACC'}] 
y_real = [u'AM-NEG', u'A1', u'A0', u'A1', u'A1', u'A1', u'A0', u'A1', u'AM-ADV', u'A0', u'A1', u'A0', u'A2', u'A1']

feat_encoder =  DictVectorizer()
feat_encoder.fit(X_real)

label_encoder = LabelEncoder()
label_encoder.fit(y_real)

model = LogisticRegression()
model.fit(feat_encoder.transform(X_real), label_encoder.transform(y_real))

print ""Test 1...""
X_test1 = [{'head': u'governo', 'dep_rel': u'SUBJ'}]
X_test1_encoded = feat_encoder.transform(X_test1)
print ""Features Encoded""
print X_test1_encoded
print ""Shape""
print X_test1_encoded.shape
print ""decision_function:""
print model.decision_function(X_test1_encoded)
print ""predict_proba:""
print model.predict_proba(X_test1_encoded)

print ""Test 2...""
X_test2 = [{'head': u'governo', 'dep_rel': u'SUBJ'}, 
           {'head': u'atrav\xe9s', 'dep_rel': u'ADVL'}, 
           {'head': u'configuram', 'dep_rel': u'ACC'}]

X_test2_encoded = feat_encoder.transform(X_test2)
print ""Features Encoded""
print X_test2_encoded
print ""Shape""
print X_test2_encoded.shape
print ""decision_function:""
print model.decision_function(X_test2_encoded)
print ""predict_proba:""
print model.predict_proba(X_test2_encoded)


print ""Test 3...""
X_test3 = [{'head': u'governo', 'dep_rel': u'SUBJ'}, 
           {'head': u'atrav\xe9s', 'dep_rel': u'ADVL'}, 
           {'head': u'configuram', 'dep_rel': u'ACC'},
           {'head': u'configuram', 'dep_rel': u'ACC'},]

X_test3_encoded = feat_encoder.transform(X_test3)
print ""Features Encoded""
print X_test3_encoded
print ""Shape""
print X_test3_encoded.shape
print ""decision_function:""
print model.decision_function(X_test3_encoded)
print ""predict_proba:""
print model.predict_proba(X_test3_encoded)


Following is the output obtained:

Test 1...
Features Encoded
  (0, 4)    1.0
Shape
(1, 19)
decision_function:
[[ 0.55372615 -1.02949707 -1.75474347 -1.73324726 -1.75474347]]
predict_proba:
[[ 1.  1.  1.  1.  1.]]
Test 2...
Features Encoded
  (0, 4)    1.0
  (1, 1)    1.0
  (2, 0)    1.0
Shape
(3, 19)
decision_function:
[[ 0.55372615 -1.02949707 -1.75474347 -1.73324726 -1.75474347]
 [-1.07370197 -0.69103629 -0.89306092 -1.51402163 -0.89306092]
 [-1.55921001  1.11775556 -1.92080112 -1.90133404 -1.92080112]]
predict_proba:
[[ 0.59710757  0.19486904  0.26065002  0.32612646  0.26065002]
 [ 0.23950111  0.24715931  0.51348452  0.3916478   0.51348452]
 [ 0.16339132  0.55797165  0.22586546  0.28222574  0.22586546]]
Test 3...
Features Encoded
  (0, 4)    1.0
  (1, 1)    1.0
  (2, 0)    1.0
  (3, 0)    1.0
Shape
(4, 19)
decision_function:
[[ 0.55372615 -1.02949707 -1.75474347 -1.73324726 -1.75474347]
 [-1.07370197 -0.69103629 -0.89306092 -1.51402163 -0.89306092]
 [-1.55921001  1.11775556 -1.92080112 -1.90133404 -1.92080112]
 [-1.55921001  1.11775556 -1.92080112 -1.90133404 -1.92080112]]
predict_proba:
[[ 0.5132474   0.12507868  0.21262531  0.25434403  0.21262531]
 [ 0.20586462  0.15864173  0.4188751   0.30544372  0.4188751 ]
 [ 0.14044399  0.3581398   0.1842498   0.22010613  0.1842498 ]
 [ 0.14044399  0.3581398   0.1842498   0.22010613  0.1842498 ]]


As can be seen, the values obtained with ""predict_proba"" for the instance in ""X_test1"" change when that same instance is with others in X_test2. Also, ""X_test3"" just reproduces the ""X_test2"" and adds one more instance (that is equal to the last in ""X_test2""), but the probability values for all of them change. Why does this happen?
Also, I find it really strange that ALL the probabilities for ""X_test1"" are 1, shouldn't the sum of all be 1?

Now, if instead of using ""predict_proba"" I use ""decision_function"", I get the consistency in the values obtained that I need. The problem is that I get negative coefficients, and even some of the positives ones are greater than 1. 

So, what should I use? Why do the values of ""predict_proba"" change that way? Am I not understanding correctly what those values mean?

Thanks in advance for any help you could give me.

UPDATE

As suggested, I changed the code so as to also print the encoded ""X_test1"", ""X_test2"" and ""X_test3"", as well as their shapes. This doesn't appear to be the problem, as the encoding is consistant for the same instances between the test sets. 
",12,3268,,,
scikit-learn strange result,https://stackoverflow.com/questions/33937532,Use one attribute only once in scikit-learn decision tree in python,"I am using scikit-learn to create a decision tree, and its working like a charm. I would like to achieve one more thing: to make the tree to split on an attribute only once.
The reason behind this is because of my very strange dataset. I use a noisy dataset, and i am really interested in the noise as well. My class outcomes are binary let say [+,-]. I have a bunch of attributes with numbers mostly in the range of (0,1).
When scikit-learn creates the tree it splits on attributes multiple times, to make the tree ""better"". I understand that in this way the leaf nodes become more pure, but thats not the case i would like to achieve.
The thing i did was to define cutoffs for every attribute by counting the the information gain in different cutoffs, and choosing the maximum. In this way with ""leave-one-out"" and ""1/3-2/3"" cross validation techniques i get better results then the original tree.
The problem is that when i try to automatize this, i run into a problem  around the lower and upper bound e.g. around 0 and 1 because most of the elements will be under/upper that and  i get really high informational gain, cause one of the sets are pure, even if it only contains 1-2% of the full data.
All in all i would like to do something to make scikit-learn to only split on an attribute once.
If it cannot be done, do you guys have any advice how to generate those cutoffs in a nice way?
",11,4162,"To answer your question briefly, no, there is no built-in parameter to do this in sklearn. I tried to do the same a year ago, so I opened an issue requesting the addition of this feature.
sklearn builds nodes by randomly picking max_features features from the training dataset and searching for the cutoff that reduces the loss function the most. This exact same process is ran iteratively until some stopping criteria is met (max_depth, min_samples_leaf, etc.).
Hence, every feature always has the same probability of being picked, regardless of whether or not it has been used before.
If you're up for it, you can edit the source code of the classifier. In essence, all you need to do is drop the feature that minimizes the loss function after it has been chosen to build a node. That way, the algorithm will be unable to pick that feature again when taking a new sample of max_features features.
","I am not giving a method to directly deal with stopping the Classifier from using a feature multiple times. (Although you could do it by defining your own splitter and wiring it in, it is a lot of work.)

I would suggest making sure that you are balancing your classes in the first place, take a look at the class_weight parameter for details. That should help a lot in your issue. But if that does not work you can still enforce that there are no leafs having too small weight in them using the min_weight_fraction_leaf or similar parameters as suggested by maxymoo.
",
scikit-learn strange result,https://stackoverflow.com/questions/28076232,Is Apache Spark less accurate than Scikit Learn?,"I've recently been trying to get to know Apache Spark as a replacement for Scikit Learn, however it seems to me that even in simple cases, Scikit converges to an accurate model far faster than Spark does.
For example I generated 1000 data points for a very simple linear function (z=x+y) with the following script:

from random import random

def func(in_vals):
    '''result = x (+y+z+w....)'''
    result = 0
    for v in in_vals:
        result += v
    return result

if __name__ == ""__main__"":
    entry_count = 1000
    dim_count = 2
    in_vals = [0]*dim_count
    with open(""data_yequalsx.csv"", ""w"") as out_file:
        for entry in range(entry_count):
            for i in range(dim_count):
                in_vals[i] = random()
            out_val = func(in_vals)
            out_file.write(','.join([str(x) for x in in_vals]))
            out_file.write("",%s\n"" % str(out_val))


I then ran the following Scikit script:

import sklearn
from sklearn import linear_model

import numpy as np

data = []
target = []
with open(""data_yequalsx.csv"") as inFile:
    for row in inFile:
        vals = row.split("","")
        data.append([float(x) for x in vals[:-1]])
        target.append(float(vals[-1]))

test_samples= len(data)/10

train_data = [0]*(len(data) - test_samples)
train_target = [0]*(len(data) - test_samples)
test_data = [0]*(test_samples)
test_target = [0]*(test_samples)
train_index = 0
test_index = 0
for j in range(len(data)):
    if j &gt;= test_samples:
        train_data[train_index] = data[j]
        train_target[train_index] = target[j]
        train_index += 1
    else:
        test_data[test_index] = data[j]
        test_target[test_index] = target[j]
        test_index += 1

model = linear_model.SGDRegressor(n_iter=100, learning_rate=""invscaling"", eta0=0.0001, power_t=0.5, penalty=""l2"", alpha=0.0001, loss=""squared_loss"")
model.fit(train_data, train_target)
print(model.coef_)
print(model.intercept_)

result = model.predict(test_data)
mse = np.mean((result - test_target) ** 2)
print(""Mean Squared Error = %s"" % str(mse))


And then this Spark script: (with spark-submit , no other arguments)

from pyspark.mllib.regression import LinearRegressionWithSGD, LabeledPoint
from pyspark import SparkContext

sc = SparkContext (appName=""mllib_simple_accuracy"")

raw_data = sc.textFile (""data_yequalsx.csv"", minPartitions=10) #MinPartitions doesnt guarantee that you get that many partitions, just that you wont have fewer than that many partitions
data = raw_data.map(lambda line: [float(x) for x in line.split ("","")]).map(lambda entry: LabeledPoint (entry[-1], entry[:-1])).zipWithIndex()
test_samples= data.count()/10

training_data = data.filter(lambda (entry, index): index &gt;= test_samples).map(lambda (lp,index): lp)
test_data = data.filter(lambda (entry, index): index &lt; test_samples).map(lambda (lp,index): lp)

model = LinearRegressionWithSGD.train(training_data, step=0.01, iterations=100, regType=""l2"", regParam=0.0001, intercept=True)
print(model._coeff)
print(model._intercept)

mse = (test_data.map(lambda lp: (lp.label - model.predict(lp.features))**2 ).reduce(lambda x,y: x+y))/test_samples;
print(""Mean Squared Error: %s"" % str(mse))

sc.stop ()


Strangely though, the error given by spark is an order of magnitude larger than that given by Scikit (0.185 and 0.045 respectively) despite the two models having a nearly identical setup (as far as I can tell)
I understand that this is using SGD with very few iterations and so the results may differ but I wouldn't have thought that it would be anywhere near such a large difference or such a large error, especially given the exceptionally simple data.



Is there something I'm misunderstanding in Spark? Is it not correctly configured? Surely I should be getting a smaller error than that?
",11,4567,,,
scikit-learn strange result,https://stackoverflow.com/questions/39550118,Cross Validation function for logistic regression in R,"I Come from a predominantly python + scikit learn background, and I was wondering how would one obtain the cross validation accuracy for a logistic regression model in R? I was searching and surprised that there's no easy way to this. I'm looking for the equivalent:

import pandas as pd
from sklearn.cross_validation import cross_val_score
from sklearn.linear_model import LogisticRegression

## Assume pandas dataframe of dataset and target exist.

scores = cross_val_score(LogisticRegression(),dataset,target,cv=10)
print(scores)


For R: I have:

model = glm(df$Y~df$X,family=binomial')
summary(model) 


And now I'm stuck. Reason being, the deviance for my R model is 1900, implying its a bad fit, but the python one gives me 85% 10 fold cross validation accuracy.. which means its good. Seems a bit strange... So i wanted to run cross val in R to see if its the same result.

Any help is appreciated!
",7,35743,"R version using caret package:

library(caret)

# define training control
train_control &lt;- trainControl(method = ""cv"", number = 10)

# train the model on training set
model &lt;- train(target ~ .,
               data = train,
               trControl = train_control,
               method = ""glm"",
               family=binomial())

# print cv scores
summary(model)

","Below I took an answer from here and made a few changes. 

The changes I made were to make it a logit (logistic) model, add modeling and prediction, store the CV's results, and to make it a fully working example.

Also note that there are many packages and functions you could use, including cv.glm() from boot.

data(ChickWeight)

df                    &lt;- ChickWeight
df$Y                  &lt;- 0
df$Y[df$weight &gt; 100] &lt;- 1
df$X                  &lt;- df$Diet 

df     &lt;- df[sample(nrow(df)),]
folds  &lt;- cut(seq(1,nrow(df)),breaks=10,labels=FALSE)
result &lt;- list()

for(i in 1:10){
  testIndexes &lt;- which(folds==i,arr.ind=TRUE)
  testData    &lt;- df[testIndexes, ]
  trainData   &lt;- df[-testIndexes, ]
  model       &lt;- glm(Y~X,family=binomial,data=trainData)
  result[[i]] &lt;- predict(model, testData) 
}
result


You could add a line to calculate accuracy within the loop or just do it after the loop completes.
",
scikit-learn strange result,https://stackoverflow.com/questions/49416716,LabelBinarizer yields different result in multiclass example,"When executing the multiclass example in the scikit-learn tutorial


http://scikit-learn.org/stable/tutorial/basic/tutorial.html#multiclass-vs-multilabel-fitting


I came across a slight oddity.

&gt;&gt;&gt; import sklearn
&gt;&gt;&gt; sklearn.__version__
0.19.1

&gt;&gt;&gt; from sklearn.svm import SVC
&gt;&gt;&gt; from sklearn.multiclass import OneVsRestClassifier
&gt;&gt;&gt; from sklearn.preprocessing import LabelBinarizer

&gt;&gt;&gt; X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]
&gt;&gt;&gt; y = [0, 0, 1, 1, 2] # Three classes

&gt;&gt;&gt; clf = OneVsRestClassifier(estimator=SVC(random_state=0))
&gt;&gt;&gt; clf.fit(X, y).predict(X)
array([0, 0, 1, 1, 2])


This is all fine. Now with one-hot encoding:

&gt;&gt;&gt; y = LabelBinarizer().fit_transform(y)
&gt;&gt;&gt; y
array([[1, 0, 0],
       [1, 0, 0],
       [0, 1, 0],
       [0, 1, 0],
       [0, 0, 1]])


I would expect the label binarizer to only encode the target, but not having an influence on the classifier. However it yields a different result:

&gt;&gt;&gt; clf.fit(X, y).predict(X)
array([[1, 0, 0],
       [1, 0, 0],
       [0, 1, 0],
       [0, 0, 0],
       [0, 0, 0]])


Notebook on Google Colab (where the same code yields yet a different error, strangely):


https://drive.google.com/file/d/13dZ2aVbKTMgPOxj2SLsas2U2mOoKng2M/view?usp=sharing

",5,1334,"OneVsRestClassifier is applying  LabelBinarizer itself under the hood (the source code in sklearn/multiclass.py):

def fit(self, X, y):
  ...
  self.label_binarizer_ = LabelBinarizer(sparse_output=True)
  Y = self.label_binarizer_.fit_transform(y)
  Y = Y.tocsc()
  self.classes_ = self.label_binarizer_.classes_


So extra manual conversion is unnecessary. In fact, it's interpreting your one-hot encoded y as multi-label input. From the documentation:


  y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]
  
  Multi-class targets. An indicator matrix turns on multilabel
  classification.

",,
scikit-learn strange result,https://stackoverflow.com/questions/59240556,LatentDirichletAllocation was not installed in decomposition module of SciKit-Learn,"I got some strange problem at SciKit-Learn package. 
There is ""decomposition"" module inside SciKit-Learn package, which should contain LatentDirichletAllocation([]) function. See documentation here:
""https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition""

When I tried to import ""decomposition"" module: 
from sklearn import as decomposition
it gives error:

Traceback (most recent call last):
  File ""tf_1_day_scikit_dnn.py"", line 12, in &lt;module&gt;
    from sklearn import decomposition
  File ""/home/developer1/.local/lib/python3.6/site-packages/sklearn/decomposition/__init__.py"", line 19, in &lt;module&gt;
    from ._online_lda import LatentDirichletAllocation
ImportError: cannot import name 'LatentDirichletAllocation'


Command: 
ls -al ~/.local/lib/python3.6/site-packages/sklearn/decomposition shows:

drwxr-xr-x  4 developer1 developer1   4096 Dec  9 00:45 .
drwxr-xr-x 33 developer1 developer1   4096 Dec  9 00:45 ..
-rw-r--r--  1 developer1 developer1   5490 Dec  9 00:44 _base.py
-rw-r--r--  1 developer1 developer1    480 Dec  9 00:44 base.py
-rwxr-xr-x  1 developer1 developer1 179440 Dec  9 00:44 _cdnmf_fast.cpython-36m-x86_64-linux-gnu.so
-rwxr-xr-x  1 developer1 developer1 175344 Dec  3 00:09 cdnmf_fast.cpython-36m-x86_64-linux-gnu.so
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 cdnmf_fast.py
-rw-r--r--  1 developer1 developer1  54528 Dec  9 00:44 _dict_learning.py
-rw-r--r--  1 developer1 developer1    507 Dec  9 00:44 dict_learning.py
-rw-r--r--  1 developer1 developer1  12572 Dec  9 00:44 _factor_analysis.py
-rw-r--r--  1 developer1 developer1    513 Dec  9 00:44 factor_analysis.py
-rw-r--r--  1 developer1 developer1  20866 Dec  9 00:44 _fastica.py
-rw-r--r--  1 developer1 developer1    490 Dec  9 00:44 fastica_.py
-rw-r--r--  1 developer1 developer1  14076 Dec  9 00:44 _incremental_pca.py
-rw-r--r--  1 developer1 developer1    513 Dec  9 00:44 incremental_pca.py
-rw-r--r--  1 developer1 developer1   1401 Dec  9 00:44 __init__.py
-rw-r--r--  1 developer1 developer1  13597 Dec  9 00:44 _kernel_pca.py
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 kernel_pca.py
-rw-r--r--  1 developer1 developer1  47255 Dec  9 00:44 _nmf.py
-rw-r--r--  1 developer1 developer1    477 Dec  9 00:44 nmf.py
-rwxr-xr-x  1 developer1 developer1  62056 Dec  3 00:09 _online_lda.cpython-36m-x86_64-linux-gnu.so
-rwxr-xr-x  1 developer1 developer1  62064 Dec  9 00:44 _online_lda_fast.cpython-36m-x86_64-linux-gnu.so
-rw-r--r--  1 developer1 developer1    513 Dec  9 00:44 online_lda_fast.py
-rw-r--r--  1 developer1 developer1  30471 Dec  9 00:44 _online_lda.py
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 online_lda.py
-rw-r--r--  1 developer1 developer1  22807 Dec  9 00:44 _pca.py
-rw-r--r--  1 developer1 developer1    477 Dec  9 00:44 pca.py
drwxr-xr-x  2 developer1 developer1   4096 Dec  9 00:45 __pycache__
-rw-r--r--  1 developer1 developer1    855 Dec  9 00:44 setup.py
-rw-r--r--  1 developer1 developer1  13654 Dec  9 00:44 _sparse_pca.py
-rw-r--r--  1 developer1 developer1    498 Dec  9 00:44 sparse_pca.py
drwxr-xr-x  3 developer1 developer1   4096 Dec  9 00:45 tests
-rw-r--r--  1 developer1 developer1   8346 Dec  9 00:44 _truncated_svd.py
-rw-r--r--  1 developer1 developer1    507 Dec  9 00:44 truncated_svd.py


Most of the functions are here but there are no traces of the ""LatentDirichletAllocation"" function. Yet I did see LatentDirichletAllocation class defined in the _online_lda.py file.

Command python3 -c ""import sklearn; sklearn.show_versions()"" prints all versions required:

System:
    python: 3.6.8 (default, Oct  7 2019, 12:59:55)  [GCC 8.3.0]
executable: /usr/bin/python3
   machine: Linux-4.15.0-65-generic-x86_64-with-Ubuntu-18.04-bionic

Python dependencies:
       pip: 9.0.1
setuptools: 42.0.2
   sklearn: 0.22
     numpy: 1.17.4
     scipy: 1.3.3
    Cython: None
    pandas: 0.25.3
matplotlib: 3.1.2
    joblib: 0.14.0

Built with OpenMP: True


So I assume all requirements were met here. (BTW SciKit-Learn worked fine till I tried to import ""decomposition"" module).

I installed the SciKit-Learn package as it was described here: 
""https://scikit-learn.org/stable/install.html""
using command pip3 install -U scikit-learn
I did reinstall that several times but got same result. 

What do I do wrong? Do I need to add missing ""LatentDirichletAllocation"" function into the ""decomposition"" module? If so - how? Should I install the whole package somehow different way? 

Thanks.
",3,1271,"I had the same problem today. I solved it by going back to the previous version 0.21 of scikit-learn:

pip3 install scikit-learn==0.21


EDIT: I think the answer from glemaitre (https://stackoverflow.com/a/59328446/10429267) shows a better solution.
","The validated ""answered"" is not the right fix since it only downgrades scikit-learn. You will not be able to benefit from new features and bug fixes.

It will be helpful instead to report and give feedback in the issue tracker: https://github.com/scikit-learn/scikit-learn/issues/15884

It will allow us to find the root of the problem and propose the proper fix which seems to affect several persons.

EDIT:
After some investigation, you need to remove the following files:


~/.local/lib/python3.6/site-packages/sklearn/decomposition/_online_lda.cpython-36m-x86_64-linux-gnu.so
~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/_hashing.cpython-36m-x86_64-linux-gnu.so
~/.local/lib/python3.6/site-packages/sklearn/datasets/_svmlight_format.cpython-36m-x86_64-linux-gnu.so


Apparently, the so files do not get removed when updating to scikit-learn. This might be due to an old pip version.
","Try:



For Python 3

pip3 uninstall scikit-learn
pip3 install -U scikit-learn==0.21.3




For Python 2

pip uninstall scikit-learn
pip install -U scikit-learn==0.20.4

"
scikit-learn strange result,https://stackoverflow.com/questions/33317896,scikit learn TSNE transform returns strange results when applied to word vectors,"I am getting weird results when applying scikit learn's manifold t-sne implementation on word embeddings generated by word2vec.
Normally, the distance and direction of (king-queen) should be the same as (man-woman) as stated in plenty of examples on the internet.

I am sure that my word2vec model is trained correctly as I am using the Google News data set and the query of most_similar(king-man+woman) also returns queen as it is supposed to.

The plotting of the transformed 2d-vectors is also done correctly. Therefore, it must be related to the t-sne transformation from scikit learn.

I am using the following python code for transformation:

vecs = np.concatenate(vecs)
np_vecs = np.array(vecs, dtype='float')
ts = TSNE(2)
reduced_vecs = ts.fit_transform(np_vecs)




I am I doing anything wrong or is the library configuration wrong?
",3,1287,"I would like to add to iceui2's answer. Original TSNE algorithm, is a non-linear dimension reduction technique, which tries to make sure that distribution of distances between neighbours is preserved in lower and higher dimension (neighbours in high dimension will be neighbours in lower dimension). 

However, your directions are likely to get distorted, as the manifold in which points used to lie has changed drastically.

To preserve directions, I recommend using PCA for dimension reduction and then plotting. 
",,
scikit-learn strange result,https://stackoverflow.com/questions/21855410,Number of clusters increased with the increase of MinPts in scikit-learn DBSCAN,"I use DBSCAN implementation from scikit-learn library and I got strange results.
The number of estimated clusters increased with the increase of parameter MinPts (min_samples) and from my understanding of algorithm this should not happend. 

Here are my results:

Estimated number of clusters:34 eps=0.9 min_samples=13.0
Estimated number of clusters:35 eps=0.9 min_samples=12.0
Estimated number of clusters:42 eps=0.9 min_samples=11.0 &lt;- strange result here
Estimated number of clusters:37 eps=0.9 min_samples=10.0   
Estimated number of clusters:53 eps=0.9 min_samples=9.0
Estimated number of clusters:63 eps=0.9 min_samples=8.0


I use scikit-learn like this:

X = StandardScaler().fit_transform(X)
db = DBSCAN(eps=eps, min_samples=min_samples, algorithm='kd_tree').fit(X)


and X is an array that contains ~200k 12-dimensional points.

What can be the problem here?
",3,2858,,,
scikit-learn strange result,https://stackoverflow.com/questions/60673168,Weird linear regression learning curve,"I'm trying to build a prediction model for apartments price. I use python scikit-learn toolset. I'm using a dataset having total floor area and location of the apartment, which I have converted to dummy features. So the dataset looks like this:

Then I build a learning curve to see how the model is doing. 
I build the learning curve this way:

from matplotlib import pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import learning_curve

model = LinearRegression()
training_sizes, training_scores, validation_scores = learning_curve(
    estimator = model,
    X = X_train,
    y = y_train,
    train_sizes = np.linspace(5, len(X_train) * 0.8, dtype = int), 
    cv = 5
)
line1, line2 = plt.plot(
    training_sizes, training_scores.mean(axis = 1), 'g', 
    training_sizes, validation_scores.mean(axis = 1), 'r')
plt.legend((line1, line2), ('Training', 'Cross-validation'))


The picture I see is somewhat confusing:

Anomalies I see here are:


Huge error on cross-validation set
Error not steadily decreasing on training examples number growth. 


Is it normal? 

Learning curve of training set only is also not so smooth but at least the error isn't that huge:


Also I tried to add to add polynomial features of 2nd degree. But this didn't make the model perform any different. And because I have a lot of categorical features (total 106) it takes quite long even for 2nd degree polynomial. So I didn't try for higher degrees. 

Also I tried to build a model using as simple cost function and gradient descent as possible using Octave. The result with weird error was same.  

Update:
Thanks to tolik I made several amendments:

Data preparation:
Categorical data are independent. So I can't combine them into one feature.
Features were scaled using StandardScaler(). Thank you for that.

Feature extraction:
After features transformation with PCA I found out one new feature has explained variance ratio over 99%. Though it's strange I used only this one. That also allowed to increase polynomial degree though it didn't increase performance.

Model selection:
I tried several different models but none seem to perform better than LinearRegression. Interesting thing - all models perform worse on full data set. Probably it's because I sorted by price and higher prices are rather outliers. So when I start training sets on 1000 samples and go to the maximum, I get this picture (for nearly all models):

",2,1020,"My explanation have 3 steps: The data preparation, feature extraction, and model selection.

Data preparation:


In this dataset there are lots of Categorical and Ordinal values. If
the column has several non related categories it's ok to one-hot it.
but if the column has categories with order like
""bad"",""normal"",""good"" you can convert it to numerical as
{Good:1,Normal:0.5,Bad:0}.
Value ranges: the value ranges for each feature differs from the other, therefore the best thing to do is to normalise each feature along itself between 0:1.


Feature Extraction:


Your goal is to maximise the score so I guess you don't care about which feature is more important. Use PCA (has an implementation in scikit-learn library) , this algorithm convert your feature vectors into different features that each of them is a linear combination of the other features.  These new features are ordered by their explained variance. The first features describes the data better than the last one. You select the first features that their explained_variance_ sums to 99%. Now you have weigh less features.


Model Selection: 
You don't really know what is a good model, because No Free Lunch Theory but in this problem the best results that don't use deep learning , use these: XGBoost-Regressor , Random-Forest-Regressor ,Ada-Boost.

The most important thing is the Data Preparation!!!
",,
scikit-learn strange result,https://stackoverflow.com/questions/53218341,Up Sampling imbalanced dataset&#39;s minor classes,"i am using scikit-learn to classify my data, at the moment i am running a simple DecisionTree classifier.
I have three classes with a big imbalanced problem. The classes are 0,1 and 2. The minor classes are 1 and 2.

To give you an idea about the number of samples of the classes:

0 = 25.000 samples
1 = 15/20 less or more
2 = 15/20 less or more


so minor classes are about 0.06% of the dataset.
The approach that i am following to solve the imbalance problem is the UPSAMPLING of the minor classes. Code:

from sklearn.utils import resample,
resample(data, replace=True, n_samples=len_major_class, random_state=1234)


Now comes the problem. I did two tests:


If I upsample the minor classes and then divide my dataset in two groups one for training and one for testing... the accuracy is:



             precision    recall  f1-score   support

          0       1.00      1.00      1.00     20570
          1       1.00      1.00      1.00     20533
          2       1.00      1.00      1.00     20439

avg / total       1.00      1.00      1.00     61542



very good result.


If I ONLY upsample the training data and leave the original data for testing, the result is:



             precision    recall  f1-score   support

          0       1.00      1.00      1.00     20570
          1       0.00      0.00      0.00        15
          2       0.00      0.00      0.00        16

avg / total       1.00      1.00      1.00     20601



as you can see the global accuracy is high, but the accuracy of the class 1 and 2 is zero.

I am creating the classifier in this way:

DecisionTreeClassifier(max_depth=20, max_features=0.4, random_state=1234, criterion='entropy')


I also have tried adding the class_weight with balanced value, but it makes no difference.

I should only upsample the training data, why am i getting this strange problem?
",2,1572,"The fact that you obtain that behavior is quite normal when you do the re-sampling before the splitting; you are inducing a bias in your data.

If you oversample the data and then split, the minority samples in the test won't be anymore independent from the samples in the training set because they are generated together. In your case they are exact copies of the samples in the training set. Your accuracy is 100% because the classifier is classifying samples that have already been seen in the training.

Since your problem is strongly umbalanced I would suggest to use an ensemble of classifiers to handle it. 1) Split your dataset in training set and test set. Given the size of the dataset you can sample 1-2 samples from the minority class for test and leave the other for training. 2) From the training you generate N datasets containing all the remaining samples of the minority class and under-samples from the majority class (i would say 2*number of samples in the minority class). 3) For each one of the dataset obtained you train a model. 4) Use the test set to obtain the prediction; the final prediction will be the results of a majority vote of all the predictions of the classifiers. 

To have robust metrics perform different iterations with different initial splitting test/training. 
","You should not split the dataset after upsampling. You can do the upsampling within the training data.

Basically, you are leaking the test data into the training data. 
","I have a function that resamples the dataset for each class to have the same amount of instance.
from sklearn.utils import resample
import pandas as pd

def make_resample(_df, column):

  dfs_r = {}
  dfs_c = {}
  bigger = 0
  ignore = """"
  for c in _df[column].unique():
    dfs_c[c] = _df[df[column] == c]
    if dfs_c[c].shape[0] &gt; bigger:
      bigger = dfs_c[c].shape[0]
      ignore = c

  for c in dfs_c:
    if c == ignore:
      continue
    dfs_r[c] = resample(dfs_c[c], 
                        replace=True,
                        n_samples=bigger - dfs_c[c].shape[0],
                        random_state=0)
  return pd.concat([dfs_r[c] for c in dfs_r] + [_df])

"
scikit-learn strange result,https://stackoverflow.com/questions/46046360,Why is sklearn faster on CPU than Theano on GPU?,"I've compared processing time with theano(CPU), theano(GPU) and Scikit-learn(CPU) using Python.
But, I got strange result.
Here look at the graph that I plot.

Processing Time Comparison:



you can see the result of scikit-learn that is faster than theano(GPU).
The program that I checked its elapsed time is to compute euclidean distance matrix from a matrix which have n * 40 elements.

Here is the part of code.

points = T.fmatrix(""points"")
edm = T.zeros_like(points)

def get_point_to_points_euclidean_distances(point_id):
    euclideans = (T.sqrt((T.sqr(points- points[point_id, : ])).sum(axis=1)))

    return euclideans

def get_EDM_CPU(points):
    EDM = np.zeros((points.shape[0], points.shape[0])).astype(np.float32)
    for row in range(points.shape[0]):
        EDM[row, :] = np.sqrt(np.sum((points - points[row, :])**2, axis=1))

    return EDM

def get_sk(points):
    EDM = sk.pairwise_distances(a, metric='l2')

    return EDM

seq = T.arange(T.shape(points)[0])
(result, _) = theano.scan(fn = get_point_to_points_euclidean_distances, \
outputs_info = None , \
sequences = seq)

get_EDM_GPU = theano.function(inputs = [points], outputs = result, allow_input_downcast = True)


I thought that the reason why GPU is slower than sci-kit learn is probably transfer time. So I did profiling GPU with nvprof command. then I got this.

==27105== NVPROF is profiling process 27105, command: python ./EDM_test.py
Using gpu device 0: GeForce GTX 580 (CNMeM is disabled, cuDNN not available)
data shape :  (10000, 40)
get_EDM_GPU elapsed time :  1.84863090515 (s)
get_EDM_CPU elapsed time :  8.09937691689 (s)
get_EDM_sk elapsed time :  1.10968112946 (s)
ratio :  4.38128395145
==27105== Profiling application: python ./EDM_test.py
==27105== Warning: Found 9 invalid records in the result.
==27105== Warning: This could be because device ran out of memory when profiling.
==27105== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 71.34%  1.28028s      9998  128.05us  127.65us  128.78us  kernel_reduce_01_node_316e2e1cbfbe8cfb8e4a101f329ffeec_0(int, int, float const *, int, int, float*, int)
 19.95%  357.97ms      9997  35.807us  35.068us  36.948us  kernel_Sub_node_bc41b3f8f12c93d29f2c4360ad445d80_0_2(unsigned int, int, int, float const *, int, int, float const *, int, int, float*, int, int)
  7.32%  131.38ms         2  65.690ms  1.2480us  131.38ms  [CUDA memcpy DtoH]
  1.25%  22.456ms      9996  2.2460us  2.1140us  2.8420us  kernel_Sqrt_node_23508f8f49d12f3e8369d543f5620c15_0_Ccontiguous(unsigned int, float const *, float*)
  0.12%  2.1847ms         1  2.1847ms  2.1847ms  2.1847ms  [CUDA memset]
  0.01%  259.73us         5  51.946us     640ns  250.36us  [CUDA memcpy HtoD]
  0.00%  17.086us         1  17.086us  17.086us  17.086us  kernel_reduce_ccontig_node_97496c4d3cf9a06dc4082cc141f918d2_0(unsigned int, float const *, float*)
  0.00%  2.0090us         1  2.0090us  2.0090us  2.0090us  void copy_kernel&lt;float, int=0&gt;(cublasCopyParams&lt;float&gt;)


The transfer [CUDA memcpy DtoH] was performed twice { 1.248 [us],  131.38 [ms] }

The transfer [CUDA memcpy HtoD] was performed 5x { min: 640 [ns], max: 250.36 [us] }  

The transfer time is about 131.639 ms (131.88 ms + 259.73 us).
but the gap between GPU and scikit-learn is about 700ms (1.8 s - 1.1 s) So, the gap is over the transfer time.

does it compute only upper triangular matrix from symmetric matrix?

what makes scikit-learn so fast?
",2,2264,"What makes scikit-learn ( on pure CPU-side ) so fast?

My initial candidates would be a mix of:  


highly efficient use of available CPU-cores' L1-/ L2- sizes within the fastest [ns]-distances  
smart numpy vectorised execution being friendly to CPU cache-lines  
dataset so small, it can completely remain non-evicted from cache ( test to scale the dataset-under-review way above the L2-/L3-cache sizes to see the DDRx-memory-cost effects on the observed performance ( details are in the URL below ) )  
might enjoy even better timing on numpy, if avoiding .astype() conversions ( test it )


Facts on the GPU-side


auto-generated GPU-kernels do not have much chance to get ultimate levels of global memory latency-masking, compared to manually tweaked kernel-designs, tailor fit to respective GPU-silicon-architecture / latencies observed in-vivo  
data-structures larger than just a few KB remain paying GPU-SM/GDDR-MEM distances of ~ large hundreds of [ns], nearly [us] -v/s- compared to small units ~ small tens of [ns] at CPU/L1/L2/L3/DDRx ) ref. timing details in &gt;&gt;&gt; https://stackoverflow.com/a/33065382  
not being able to enjoy much of the GPU/SMX power, due to this task's obvious low-reuse of data points and dataset size beyond the GPU/SM-silicon limits, that causes and must cause GPU/SM-register capacity spillovers in any kind of GPU-kernel design attempts and tweaking  
the global task is not having a minimum reasonable amount of asynchronous, isolated ( non-communicating islands ) mathematically-dense, yet SMX-local, GPU-kernel processing steps ( there is not much to compute so as to adjust for the add-on overheads and expensive SMX/GDDR memory costs )  


GPU-s can lovely exhibit it's best-performance, if sufficiently enough densely-convoluted re-processing operations take place -- like in large-scale/high-resolution image-processing -- on [m,n,o]-convolution-kernel matrices so small, so as that all these m*n*o constant values can reside local to SM, inside an available set of SMX-SM_registers and if the GPU-kernel-launchers are optimally tweaked by the 3D-tblock/grid processing-layout geometries, so that the global memory access latencies are at its best-masked performance, having all the GPU-threads enforced within the hardware WARP-aligned SMx:WarpScheduler RoundRobin thread-scheduling capabilites ( the first swap from Round-Robin into Greedy-WarpSchedule mode loses the whole battle in case of divergent execution-paths in GPU-kernel-code ).
",,
scikit-learn strange result,https://stackoverflow.com/questions/39366120,Numpy Cosine Similarity difference over big collections,"I need to use the Scikit-learn sklearn.metric.pairwise.cosine_similarity over big matrixes. 
For some optimizations i need to compute only some rows of the matrixes, and so i tried different methods.

I found that in some cases the results were different depending on the size of the vectors, and I saw this strange behaviour on this test case (big vectors, transpose and estimate cosine):

from sklearn.metrics.pairwise import cosine_similarity
from scipy import spatial
import numpy as np
from scipy.sparse import csc_matrix

size=200
a=np.array([[1,0,1,0]]*size)
sparse_a=csc_matrix(a.T)
#standard cosine similarity between the whole transposed matrix, take only the first row
res1=cosine_similarity(a.T,a.T)[0]
#take the row obtained by the multiplication of the first row of the transposed matrix with transposed matrix itself (optimized for the first row calculus only)
res2=cosine_similarity([a.T[0]],a.T)[0]
#sparse matrix implementation with the transposed, which should be faster
res3=cosine_similarity(sparse_a,sparse_a)[0]
print(""res1: "",res1)
print(""res2: "",res2)
print(""res3: "",res3)
print(""res1 vs res2: "",res1==res2)
print(""res1 vs res3: "",res1==res3)
print(""res2 vs res3: "", res2==res3)


If ""size"" is set to 200 I got this result, that is ok:

res1:  [ 1.  0.  1.  0.]
res2:  [ 1.  0.  1.  0.]
res3:  [ 1.  0.  1.  0.]
res1 vs res2:  [ True  True  True  True]
res1 vs res3:  [ True  True  True  True]
res2 vs res3:  [ True  True  True  True]


But if ""size"" is set to 2000 or more, some strange things happen:

res1:  [ 1.  0.  1.  0.]
res2:  [ 1.  0.  1.  0.]
res3:  [ 1.  0.  1.  0.]
res1 vs res2:  [False  True False  True]
res1 vs res3:  [False  True False  True]
res2 vs res3:  [ True  True  True  True]


Does anybody know what am I missing?

Thanks in advance
",2,471,"In order to compare numpy.array you have to use np.isclose instead of equality operator. Try:

from sklearn.metrics.pairwise import cosine_similarity
from scipy import spatial
import numpy as np
from scipy.sparse import csc_matrix

size=2000
a=np.array([[1,0,1,0]]*size)
sparse_a=csc_matrix(a.T)
#standard cosine similarity between the whole transposed matrix, take only the first row
res1=cosine_similarity(a.T,a.T)[0]
#take the row obtained by the multiplication of the first row of the transposed matrix with transposed matrix itself (optimized for the first     row calculus only)
res2=cosine_similarity([a.T[0]],a.T)[0]
#sparse matrix implementation with the transposed, which should befaster
res3=cosine_similarity(sparse_a,sparse_a)[0]
print(""res1: "",res1)
print(""res2: "",res2)
print(""res3: "",res3)
print(""res1 vs res2: "", np.isclose(res1, res2))
print(""res1 vs res3: "", np.isclose(res1, res3))
print(""res2 vs res3: "", np.isclose(res2, res2))


The results are:

res1:  [ 1.  0.  1.  0.]
res2:  [ 1.  0.  1.  0.]
res3:  [ 1.  0.  1.  0.]
res1 vs res2:  [ True  True  True  True]
res1 vs res3:  [ True  True  True  True]
res2 vs res3:  [ True  True  True  True]


as expected.
",,
scikit-learn strange result,https://stackoverflow.com/questions/73491673,Strange results when scaling data using scikit learn,"I have an input dataset that has 4 time series with 288 values for 80 days. So the actual shape is (80,4,288). I would like to cluster differnt days. I have 80 days and all of them have 4 time series: outside temperature, solar radiation, electrical demand, electricity prices. What I want is to group similar days with regard to these 4 time series combined into clusters. Days belonging to the same cluster should have similar time series.
Before clustering the days using k-means or Ward's method, I would like to scale them using scikit learn. For this I have to transform the data into a 2 dimensional shape array with the shape (80, 4*288) = (80, 1152), as the Standard Scaler of scikit learn does not accept 3-dimensional input. The Standard Scaler just standardizes features by removing the mean and scaling to unit variance.
Now I scale this data using sckit learn's standard scaler:
import numpy as np
from sklearn.preprocessing import StandardScaler
import pandas as pd

data_Unscaled = pd.read_csv(""C:/Users/User1/Desktop/data_Unscaled.csv"", sep="";"")
scaler = StandardScaler()
data_Scaled = scaler.fit_transform(data_Unscaled)
np.savetxt(""C:/Users/User1/Desktop/data_Scaled.csv"", data_Scaled, delimiter="";"")

When I now compare the unscaled and scaled data e.g. for the first day (1 row) and the 4th time series (columns 864 - 1152 in the csv file), the results look quite strange as you can see in the following figure:

As far as I see it, they are not in line with each other. For example in the timeslots between 111 and 201 the unscaled data does not change at all whereas the scaled data fluctuates. I can't explain that. Do you have any idea why this is happening and why they don't seem to be in line?
Here is the unscaled input data with shape (80,1152): https://filetransfer.io/data-package/CfbGV9Uk#link
and here the scaled output of the scaling with shape (80,1152): https://filetransfer.io/data-package/23dmFFCb#link
",1,469,"You have two issues here: scaling and clustering. As the question title refers to scaling, I'll handle that one in detail. The clustering issue is probably better suited for CrossValidated.
You don't say it, but it seems natural that all temperatures, be it on day 1 or day 80, are measured on a same scale. The same holds for the other three variables. So, for the purpose of scaling you essentially have four time series.
StandardScaler, like basically everything in sklearn, expects your observations to be organised in rows and variables in columns. It treats each column separately, deducting its mean from all the values in the column and dividing the resulting values by their standard deviation.
I reckon from your data that the first 288 entries in each row correspond to one variable, the next 288 to the second one etc. You need to reshape these data to form 288*80=23040 rows and 4 columns, one for each variable.
You apply StandardScaler on that array and reformat the data into the original shape, with 80 rows and 4*288=1152 columns. The code below should do the trick:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

data_Unscaled = pd.read_csv(""C:/Users/User1/Desktop/data_Unscaled.csv"", sep="";"", header=None)

X = data_Unscaled.to_numpy()
X_narrow = np.array([X[:, i*288:(i+1)*288].ravel() for i in range(4)]).T
scaler = StandardScaler()
X_narrow_scaled = scaler.fit_transform(X_narrow)
X_scaled = np.array([X_narrow_scaled[i*288:(i+1)*288, :].T.ravel() for i in range(80)])

# Plot the original data:
i=3
j=0
plt.plot(X[j, i*288:(i+1)*288])
plt.title('TimeSeries_Unscaled')
plt.show()

# plot the scaled data:
plt.plot(X_scaled[j, i*288:(i+1)*288])
plt.title('TimeSeries_Scaled')
plt.show()

resulting in the following graphs:


The line
X_narrow = np.array([X[:, i*288:(i+1)*288].ravel() for i in range(4)]).T

uses list comprehension to generate the four columns of the long, narrow array X_narrow. Basically, it is just a shorthand for a for-loop over your four variables. It takes the first 288 columns of X, flattens them into a vector, which it then puts into the first column of X_narrow. Then it does the same for the next 288 columns, X[:, 288:576], and then for the third and the fourth block of the 288 observed values per day. This way, each column in X_narrow contains a long time series, spanning 80 days (and 288 observations per day), of exactly one of your variables (outside temperature, solar radiation, electrical demand, electricity prices).
Now, you might try to cluster X_scaled using K-means, but I doubt it will work. You have just 80 points in a 1152-dimensional space, so the curse of dimensionality will almost certainly kick in. You'll most probably need to perform some kind of dimensionality reduction, but, as I noted above, that's a different question.
",,
scikit-learn strange result,https://stackoverflow.com/questions/59140556,Can / should I use past (e.g. monthly) label columns from a database as features in an ML prediction (no time-series!)?,"The question: Is it normal / usual / professional to use the past of the labels as features?
I could not find anything reliable on this, although it is a basic question.
Edited: Please mind, this is not a time-series question, I have deleted the time-series tag now and I changed the question. This question is about features that change regularly over time, yes! But we do not create a time-series from this, as there are many other features as well which are not like the label and are also important features in the model. Now please think of using past labels as normal features without a time-series approach.
I try to predict a certain month of data that is available monthly, thus a time-series, but I am not using it as a time-series, it is just monthly avaiable data of various different features.
It is a classification model, and now I want to predict a label column of a selected month of that time-series. The previous months before the selected label month are now the point of the question.
I do not want to just drop the past months of the label just because they are ""almost"" a label (or in other words: they were just the label columns of the preceding models in time). I know the past of the label, why not considering it as features as well?
My predictions are of course much better when adding the past labels of the time-series of labels to the features. This is logical as the labels usually do not change so much from one month to the other and thus can be predicted very well if you have fed the data with the past of the label. It would be strange not to use such ""past labels"" as features, as any simple time-series regression would then be better than the ml model.
Example: Let's say I predict the IQ test result of a person, and I use her past IQ test results as features in addition to other normal ""non-label"" features like age, education aso. I use the first 11 months of ""past labels"" of a year as features in addition to my normal ""non-label"" features. I predict the label of the 12th month.
Predicting the label of the 12th month works much better if you add the past of the labels to the features - obviously. This is because the historical labels, if there are any, are of course better indicators of the final outcome than normal columns like age and education.
Possibly related p.s.:
p.s.1: In auto-regressive models, the past of the dependent variable can well be used as independent variable, see: https://de.wikipedia.org/wiki/Regressionsanalyse
p.s.2: In ML you can perhaps just try any features and take what gives you the best results, a bit like &gt;Good question, try them [feature selection methods] all and see what works best&lt; in https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/ &gt;If the features are relevant to the outcome, the model will figure out how to use them. Or most models will.&lt; The same is said in Does the feature selection matter for learning algorithm with regularization?
p.s.3: Also probably relevant is the problem of multicollinearity: https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/ though multicollinearity is said to be no issue for the prediction: &gt;Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you dont need to understand the role of each independent variable, you dont need to reduce severe multicollinearity.
",1,1210,"It is perfectly possible and also good practice to include past label columns as features, though it depends on your question: do you want to explain the label only with other features (on purpose), or do you want to consider other and your past label columns to get the next label predicted, as a sort of adding a time-series character to the model without using a time-series?
The sequence in time is not even important, as long as all of such monthly columns are shifted in time consistently by the same time when going over to the predicting set. The model does not care if it is just January and February of the same column type, for the model, every feature is isolated.
Example: You can perfectly run a random forest model on various features, including their past label columns that repeat the same column type again and again, only representing different months. Any month's column can be dealt with as an independent new feature in the ml model, the only importance is to shift all of those monthly columns by the exactly same period to reach a consistent predicting set. In other words, obviously you should avoid replacing January with March column when you go from a training set January-June to a predicting set February-July, instead you must replace January with February of course.
Update 202301: model name is ""walk-forward""
This model setup is called ""walk-forward"", see Why isnt out-of-time validation more ubiquitous? --&gt; option 3 almost at the bottom of the page.
I got this from a comment at Splitting Time Series Data into Train/Test/Validation Sets.
In the following, it shows only training and testing set. It writes ""validation set"", but it is known that this gets mixed up all over the place, see What is the Difference Between Test and Validation Datasets?, and it must be meant as the testing set in the default understanding of it.

Thus, with the right wording, it is:

This should be the best model for labels that become features in time.
validation set in a ""walk-forward"" model?
As you can see in the model, no validation set is needed since the test data must be biased ""forward"" in time, that is the whole idea of predicting the ""step forward in time"", and any validation set would have to be in that same biased artificial future - which is already the past at the time of training, but the model does not know this.
The validation happens by default, without a needed dataset split, during the walk-forward, when the model learns again and again to predict the future and the output metrics can be put against each other. As the model is to predict the time-biased future, there is no need to prove that or how the artificial future is biased and sort of ""overtrained by time"". It is the aim of the model to have the validation in the artificial future and predict the real future as a last step only.
But then, why not still having a validation set on top of this, at least if it is just a small k-fold validation? It could play a role if the testing set has a few strong changes that happen in small time windows but which are still important to be predicted, or at least hinted at, but should also not be overtrained within each training step. The validation set would hit some of these time windows and might show whether the model can handle them well enough. Any other method than k-fold would shrink the power of the model too much. The more you take away from the testing set during training, the less it can predict the future.
Wrap up:
Try it out, and in doubt, leave the validation aside and judge upon the model by checking its metrics over time, during the ""walk-forward"". This model is not like the others.
Thus, in the end, you can, but you do not have to, split a k-fold validation from the testing set. That would look like:

After predicting a lot of known futures, the very last step in time is then the prediction of the unknown future.
This also answers Does the training+testing set have to be different from the predicting set (so that you need to apply a time-shift to ALL columns)?.
",,
scikit-learn strange result,https://stackoverflow.com/questions/42265434,significant test using scikit-learn&#39;s permutation test results in the same p-value for all classifiers,"I'm trying to find out the significance of the results using scikit-learn's permutation test as in:

score, permutation_scores, pvalue = permutation_test_score(clf.best_estimator_, X_train, Y_train, cv=10, n_jobs=10, n_permutations=100, scoring='accuracy')


where the clf.best_estimator is the result of cross-validation.

I use it for several classifiers (several independent clf.best_estimator_) but the p-values for all of them is the same 0.00990099009901.

I have no idea why this happens. The strange thing is that this is the same number that is reported in the linked code in scikit-learn user guide.
",1,690,"I asked the same question in scikit-learn's issues and the answer was: for most of the good classifiers if the random classifier is better than the trained classifier in 1 test out of 100, this magic number would be the result.

so there's nothing wrong with this magic number.
",,
scikit-learn strange result,https://stackoverflow.com/questions/39865557,sentiment analysis joint list,"im doing the sentiment analysis with scikit-learn python, now I'm using the nltk to do the words lemmatization in order to increase processing speed, for example:

I get the following arrays after nltk processing:

array([ ['Really', 'a', 'terrible', 'course', u'lecture', u'be', 'so', 'boring', 'i', u'contemplate', 'suicide', 'on', 'numerous', u'occasion', 'and', 'the', 'tutes', u'go', 'for', 'two', u'hour', 'and', u'be', 'completely'], ['Management', 'accounting', u'require', 'sufficient', 'practice', 'to', 'get', 'a', 'hang', 'of', 'Made', 'easier', 'with', 'a', 'great', 'lecturer']], dtype=object)


but the scklearn require the array is 

array([ 'Really a terrible course  lectures were so boring i contemplated suicide on numerous occasions and the tutes went for two hours and were completely ', 'Management accounting requires sufficient practice to get a hang of  Made easier with a great lecturer '],dtype=object)


so what is the best way to convert this array into the right form? I try to use joint list but the result is strange
",1,68,"You would do:

second_array = [' '.join(each) for each in first_array]


Alternatively you can tell sklearn.CountVectorizer to just use your tokens:

vect = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)
X = vect.fit_transform(first_array)

",,
scikit-learn strange result,https://stackoverflow.com/questions/23914472,strange F1 score result using scikit learn,"I am doing some classification and I was looking at the f1 score and noticed something strange.

When I do: 

""f1:"" + str(f1_score(y_test_bin, target, average=""weighted""))


I get :

f1:0.444444444444


When I do:

print ""f1:"" + str(f1_score(y_test_bin, target,pos_label=0, average=""weighted""))


I get:

f1:0.823529411765


Which is stange since I set the average to be 'weighted'. This should give me the weighted average of those two scores. Which is independent of the ""True label""

I can also see this in the classification report:

         precision    recall  f1-score   support

      0       0.76      0.90      0.82        39
      1       0.60      0.35      0.44        17

avg / total       0.71      0.73      0.71        56


In the classification report I get the weighted average, but not when I use the f1 score function. Why is this ?
",1,2729,"I was struggling with this problem as well and found a solution after reading eickenberg's answer on this thread which is definitely worth a read for the background on this.

In short, sklearn automatically overrides the averaging to take the positive class score when it interprets the data as binary. It does this automatically or when you specify a pos_label. The solution then is to redefine the pos_label as None.

print ""f1:"" + str(f1_score(y_test_bin, target, pos_label=None, average=""weighted""))


Hope this helps!
",,
scikit-learn strange result,https://stackoverflow.com/questions/65856432,Using Perceptron sklearn.ensemble.AdaBoostClassifier() gives an error,"I have the problem of using perceptrons for AdaBoost classifier.
The training and testing data from here
should be turned to 0 and 1 in the last column (""Poker Hand""), (it is from 1 to 9 inclusively originally), then both Decision Tree Classifier and AdaBoost Classifier with the total of 15 weak perceptron classifiers should be implemented in the data. I try to use scikit-learn libraries, but while my Decision Tree Classifier provides good results, AdaBoost Classifier throws error:
ValueError: BaseClassifier in AdaBoostClassifier ensemble is worse than random, ensemble can not be fit.
Here, the crucial parts of the code.
import pandas as pd
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import Perceptron
from sklearn import metrics

if __name__ == ""__main__"":
   
    data_train = pd.read_csv(""poker-hand-testing.data"",header=None)
    data_test = pd.read_csv(""poker-hand-training-true.data"",header=None)
    

    for value in range(0, len(data_train)):
        if data_train[10][value] != 0:
            data_train[10][value] = 1
    
    for value in range(0, len(data_test)):
        if data_test[10][value] != 0:
            data_test[10][value] = 1

    col=['Suit of card #1','Rank of card #1',
     'Suit of card #2','Rank of card #2',
     'Suit of card #3','Rank of card #3',
     'Suit of card #4','Rank of card #4',
     'Suit of card #5','Rank of card #5',
     'Poker Hand']
    
    data_train.columns=col
    data_test.columns=col
    
    y_train=data_train['Poker Hand']
    y_test=data_test['Poker Hand']
    
    x_train=data_train.drop('Poker Hand',axis=1)
    x_test=data_test.drop('Poker Hand',axis=1)
    
#The problematic part
    classifier = AdaBoostClassifier(base_estimator=Perceptron(), n_estimators=15, algorithm='SAMME')
    classifier = classifier.fit(x_train, y_train)
    y_pred = classifier.predict(x_test)
    
    print(""Accuracy of AdaBoost:"", metrics.accuracy_score(y_test, y_pred))

The strange thing is that this error occurs only once per 9-10 times when I don't change values to binary ones, while binary values almost always gives an error. Also, changing Perceptron() to SGDClassifier(loss=""perceptron"", eta0=1, learning_rate=""constant"", penalty=None) also throws such errors.
My questions are:

What is the solution with the possibility of using scikit-learn library?

Is there any way to handle such an exception? For example, if it gives error, execute it again until desired results?

Are there any other alternatives where I can use both Decision Tree and AdaBoost with Perceptron, if it is couldn't be solved in scikit-learn library?


",0,421,"The problematic part could be solved by try-catch block. For example,
#The problematic part solution
AdaBoost_accuracy = 0

while AdaBoost_accuracy == 0:
    try:
        classifier = AdaBoostClassifier(base_estimator=Perceptron(), n_estimators=15, algorithm='SAMME')
        classifier = classifier.fit(x_train, y_train)
        y_pred = classifier.predict(x_test)
        AdaBoost_accuracy = metrics.accuracy_score(y_test, y_pred)
    except:
        print(""Let me reclassify AdaBoost again"")

print(""Accuracy of AdaBoost:"", AdaBoost_accuracy)

",,
scikit-learn strange result,https://stackoverflow.com/questions/37907775,Scikitklearns TfidfTransformer makes my pipeline predict just one label,"I have a pandas dataframe containing texts and labels, and I'm trying to predict the labels using scikit-learn's CountVectorizer, TfidfTransformer and MultinomialNB. Here's what the dataframe looks like:

                                                text party
0  Herr lderspresident! Att vara talman i Sverig...     S
1  Herr lderspresident! rade ledamter av Sveri...     M
2  Herr lderspresident! Som fretrdare fr Alli...     M
3  Val av andre vice talman Herr lderspresident!...    SD
4  Herr lderspresident! Vnsterpartiet vill utny...     V


When I construct a pipeline with the three estimators mentioned above, I only get a ~35% accuracy in my predictions, but when I remove the TfidfTransformer the accuracy is bumped up to a more reasonable ~75% accuracy.

text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()), # problematic row
                     ('clf', MultinomialNB()),
                   ])

text_clf = text_clf.fit(df.text.values, df.party.values)

test = df.sample(500, random_state=42)
docs_test = test.text.values
predicted = text_clf.predict(docs_test)
np.mean(predicted == test.party.values)
# Out: either 0.35 or 0.75 depending on whether I comment out the problematic row above


When I get 0.35 and inspect predicted I see that it almost exclusively contains one label ('S'). This is the most common label in the original dataset, but that shouldn't impact the predictions, right? Any ideas on why I get these strange results?

EDIT: Link to data where anforandetext and parti are the relevant columns.
",0,392,"The reason that you are getting so much difference is because of smoothing. If you checkout the documentation of MultinomialNB class, checkout the alpha parameter. The default value for that is 1.0. This means that it implements Plus One smoothing by default. Plus one smoothing is a very common technique used with relative frequency estimates to account for unseen data. In Plus One smoothing, we add 1 to all raw counts to account for unseen terms and the sparsity of the document-term matrix.

However, when you end up using TF-IDF weights, the numbers that you get are very small and mostly between 0 - 1. To illustrate, if I use your data and only convert it into TF-IDF weights, this is the small snapshot of the TF-IDF weights that I obtain.

  (0, 80914)    0.0698184481033
  (0, 80552)    0.0304609466459
  (0, 80288)    0.0301759343786
  (0, 80224)    0.103630302925
  (0, 80204)    0.0437500703747
  (0, 80192)    0.0808649191625


You can see that these are really small numbers and adding 1 to them for smoothing will have a drastic effect on the calculations that Multinomial Naive Bayes makes. By adding 1 to these numbers, you completely change their scale for classification and hence your estimates mess up. I am assuming, you have a good idea about how Multinomial Naive Bayes works. If not, then definitely see this video. The video and my answer will be sufficient to understand what is going wrong over here. 

You should either use a small value of alpha in TF-IDF case or you should build TF-IDF weights after doing smoothing on the raw counts. Also on a secondary note, please use cross-validation to get any accuracy estimates. By testing the model on a sample of the training data, your accuracy numbers will be extremely biased. I would recommend using cross-validation or a separate hold-out set to evaluate your model.

Hope that helps.
",,
scikit-learn strange result,https://stackoverflow.com/questions/35232804,Scikit-learn: precision_recall_fscore_support returns strange results,"I am doing some text minining/classification and attempt to evaluate performance with the precision_recall_fscore_support function from the sklearn.metrics module. I am not sure how I can create a really small example reproducing the problem, but maybe somebody can help even so because it is something obvious I am missing. 

The aforementioned function returns among other things the support for each class. The documentation states


  support: int (if average is not None) or array of int, shape = [n_unique_labels] :
  The number of occurrences of each label in y_true.


But in my case, the number of classes for which support is returned is not the same as the number of different classes in the testing data.     

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)

classifier = svm.SVC(kernel=""linear"")
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
prec, rec, fbeta, supp = precision_recall_fscore_support(y_test, y_pred)

print(len(classifier.classes_)) # prints 18
print(len(supp))                # prints 19
print(len(np.unique(y_test)))   # prints 18


How can this be? How can there be support for a class which is not in the data?
",0,1943,"I am not sure what the problem is, but in my case there seems to be a mismatch between the classes learned by the classifier and the ones occurring in the test data. One can force the the function to compute the performance measures for the right classes by explicitly naming them.

prec, rec, fbeta, supp = precision_recall_fscore_support(y_test, y_pred, labels=classifier.classes_)

",,
scikit-learn strange result,https://stackoverflow.com/questions/54166529,How to fix strange prediction results in scikit-learn,"I have a simple example in scikit-learn for prediction. Here is my data file (data.csv):

first second third target1 target2
 800   400    240    400     25
 400   200    120    200     50
 200   100    60     100     100
 100   50     30     50      200
 50    25     15     25      400


The features include : first ,second and third 

The targets include : target1 and target2

Now I want to provide new values for the features and predict target1 and target2. As you can see, there is a trend between the values of features and the values of the target1 and target2:

By doubling the features values the value of target1 also doubles while the value of target2 cuts in half 

For example if I provide the values : 1600 , 800 and 480 for the first, second and the third features, I expect to get 800 and 12.5 for the target1 and target2 respectively. Here is the code:

import pandas as pd
from sklearn.model_selection import train_test_split
from collections import *
from sklearn.linear_model import LinearRegression

features = pd.read_csv('data.csv')

features.head()
features_name = ['first' , 'second' , 'third']
target_name = ['target1','target2']

X = features[features_name]
y = features[target_name]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.155, random_state = 42)

linear_regression_model = LinearRegression()
linear_regression_model.fit(X_train,y_train)

new_data  = OrderedDict([('first',1600) ,('second',800),('third',480) ])
new_data = pd.Series(new_data).values.reshape(1,-1)
ss = linear_regression_model.predict(new_data)

print (ss)


Here is the output:

[[ 800.         -284.58549223]]


As you can see the predicted value for the target1 is exactly what is expected while the predicted value for the target2 (-284.58) is far away from what is expected (12.5)

Does anybody know what is wrong here and how I can fix this issue? Thanks in advance for your help. 
",-1,270,"Edit: there is no linear correlation between your inputs (any of them, they are essentially all the same) and your target2. It seems to be an exponential decay. Try transforming your feature to another feature (1/exp(x)) and then you can do a linear regression on that feature

Edit2: my mistake, it is simply 1/x

Also.. (original post below)
I'm not an expert, but it appears that your input features are not only highly correlated, they are actually perfectly correlated (linear correlation that is) - this generally makes regression breaks.
The intuitive reason is that because they are correlated, there exists an infinite amount of possible coefficients, where they counter one another, and the prediction would still be exactly the same (imagine in your case first-second is exactly the same as 2first-3second or 0first-1second etc).
I also think this causes the normal equation of linear regression to generate a matrix that cannot be inverted..
",,
scikit-learn strange issue,https://stackoverflow.com/questions/57484399,Issue importing scikit-learn: module &#39;scipy&#39; has no attribute &#39;_lib&#39;,"I'm new to Python and am using Anaconda on Windows 10 to learn how to implement machine learning. Running this code on Spyder:

import sklearn as skl


Originally got me this:

Traceback (most recent call last):

  File ""&lt;ipython-input-1-7135d3f24347&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/julia/.spyder-py3/temp.py', wdir='C:/Users/julia/.spyder-py3')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/julia/.spyder-py3/temp.py"", line 3, in &lt;module&gt;
    from sklearn.family import Model

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\__init__.py"", line 76, in &lt;module&gt;
    from .base import clone

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\base.py"", line 16, in &lt;module&gt;
    from .utils import _IS_32BIT

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\__init__.py"", line 20, in &lt;module&gt;
    from .validation import (as_float_array,

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py"", line 21, in &lt;module&gt;
    from .fixes import _object_dtype_isnan

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\fixes.py"", line 289, in &lt;module&gt;
    from scipy.sparse.linalg import lsqr as sparse_lsqr

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\__init__.py"", line 114, in &lt;module&gt;
    from .isolve import *

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\isolve\__init__.py"", line 6, in &lt;module&gt;
    from .iterative import *

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\isolve\iterative.py"", line 10, in &lt;module&gt;
    from . import _iterative

ImportError: DLL load failed: The specified module could not be found.


I then went to the command line and did

pip uninstall scipy
pip install scipy

pip uninstall scikit-learn
pip install scikit-learn


and got no errors when doing so, with scipy 1.3.1 (along with numpy 1.17.0) and scikit-learn 0.21.3 being installed according to the command line.

However, now when I try to import sklearn I get a different error:

 File ""&lt;ipython-input-2-7135d3f24347&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/julia/.spyder-py3/temp.py', wdir='C:/Users/julia/.spyder-py3')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/julia/.spyder-py3/temp.py"", line 3, in &lt;module&gt;
    from sklearn.family import Model

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\__init__.py"", line 76, in &lt;module&gt;
    from .base import clone

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\base.py"", line 16, in &lt;module&gt;
    from .utils import _IS_32BIT

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\__init__.py"", line 20, in &lt;module&gt;
    from .validation import (as_float_array,

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py"", line 21, in &lt;module&gt;
    from .fixes import _object_dtype_isnan

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\fixes.py"", line 289, in &lt;module&gt;
    from scipy.sparse.linalg import lsqr as sparse_lsqr

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\__init__.py"", line 113, in &lt;module&gt;
    from .isolve import *

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\isolve\__init__.py"", line 6, in &lt;module&gt;
    from .iterative import *

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\linalg\isolve\iterative.py"", line 136, in &lt;module&gt;
    def bicg(A, b, x0=None, tol=1e-5, maxiter=None, M=None, callback=None, atol=None):

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\_lib\_threadsafety.py"", line 59, in decorator
    return lock.decorate(func)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\_lib\_threadsafety.py"", line 47, in decorate
    return scipy._lib.decorator.decorate(func, caller)

AttributeError: module 'scipy' has no attribute '_lib'


Any suggestions? I've uninstalled and reinstalled Anaconda and I'm still getting the same issue.

EDIT: When I do 

conda list --show-channel-urls


I get

# packages in environment at C:\ProgramData\Anaconda3:
#
# Name                    Version                   Build  Channel
_ipyw_jlab_nb_ext_conf    0.1.0                    py37_0    defaults
alabaster                 0.7.12                   py37_0    defaults
anaconda-client           1.7.2                    py37_0    defaults
anaconda-navigator        1.9.7                    py37_0    defaults
asn1crypto                0.24.0                   py37_0    defaults
astroid                   2.2.5                    py37_0    defaults
attrs                     19.1.0                   py37_1    defaults
babel                     2.7.0                      py_0    defaults
backcall                  0.1.0                    py37_0    defaults
backports                 1.0                        py_2    defaults
backports.functools_lru_cache 1.5                        py_2    defaults
backports.tempfile        1.0                        py_1    defaults
backports.weakref         1.0.post1                  py_1    defaults
beautifulsoup4            4.7.1                    py37_1    defaults
blas                      1.0                         mkl    defaults
bleach                    3.1.0                    py37_0    defaults
bzip2                     1.0.8                he774522_0    defaults
ca-certificates           2019.5.15                     1    defaults
certifi                   2019.6.16                py37_1    defaults
cffi                      1.12.3           py37h7a1dbc1_0    defaults
chardet                   3.0.4                 py37_1003    defaults
click                     7.0                      py37_0    defaults
cloudpickle               1.2.1                      py_0    defaults
clyent                    1.2.2                    py37_1    defaults
colorama                  0.4.1                    py37_0    defaults
conda                     4.7.11                   py37_0    defaults
conda-build               3.18.8                   py37_0    defaults
conda-env                 2.6.0                         1    defaults
conda-package-handling    1.3.11                   py37_0    defaults
conda-verify              3.4.2                      py_1    defaults
console_shortcut          0.1.1                         3    defaults
cryptography              2.7              py37h7a1dbc1_0    defaults
decorator                 4.4.0                    py37_1    defaults
defusedxml                0.6.0                      py_0    defaults
docutils                  0.15.1                   py37_0    defaults
entrypoints               0.3                      py37_0    defaults
filelock                  3.0.12                     py_0    defaults
freetype                  2.9.1                ha9979f8_1    defaults
future                    0.17.1                   py37_0    defaults
glob2                     0.7                        py_0    defaults
icc_rt                    2019.0.0             h0cc432a_1    defaults
icu                       58.2                 ha66f8fd_1    defaults
idna                      2.8                      py37_0    defaults
imagesize                 1.1.0                    py37_0    defaults
intel-openmp              2019.4                      245    defaults
ipykernel                 5.1.1            py37h39e3cac_0    defaults
ipython                   7.7.0            py37h39e3cac_0    defaults
ipython_genutils          0.2.0                    py37_0    defaults
ipywidgets                7.5.1                      py_0    defaults
isort                     4.3.21                   py37_0    defaults
jedi                      0.13.3                   py37_0    defaults
jinja2                    2.10.1                   py37_0    defaults
joblib                    0.13.2                   py37_0    defaults
jpeg                      9b                   hb83a4c4_2    defaults
json5                     0.8.5                      py_0    defaults
jsonschema                3.0.1                    py37_0    defaults
jupyter_client            5.3.1                      py_0    defaults
jupyter_core              4.5.0                      py_0    defaults
jupyterlab                1.0.2            py37hf63ae98_0    defaults
jupyterlab_server         1.0.0                      py_1    defaults
keyring                   18.0.0                   py37_0    defaults
lazy-object-proxy         1.4.1            py37he774522_0    defaults
libarchive                3.3.3                h0643e63_5    defaults
libiconv                  1.15                 h1df5818_7    defaults
liblief                   0.9.0                ha925a31_2    defaults
libpng                    1.6.37               h2a8f88b_0    defaults
libsodium                 1.0.16               h9d3ae62_0    defaults
libtiff                   4.0.10               hb898794_2    defaults
libxml2                   2.9.9                h464c3ec_0    defaults
lz4-c                     1.8.1.2              h2fa13f4_0    defaults
lzo                       2.10                 h6df0209_2    defaults
m2w64-gcc-libgfortran     5.3.0                         6    defaults
m2w64-gcc-libs            5.3.0                         7    defaults
m2w64-gcc-libs-core       5.3.0                         7    defaults
m2w64-gmp                 6.1.0                         2    defaults
m2w64-libwinpthread-git   5.0.0.4634.697f757               2    defaults
markupsafe                1.1.1            py37he774522_0    defaults
mccabe                    0.6.1                    py37_1    defaults
menuinst                  1.4.16           py37he774522_0    defaults
mistune                   0.8.4            py37he774522_0    defaults
mkl                       2019.4                      245    defaults
mkl-service               2.0.2            py37he774522_0    defaults
mkl_fft                   1.0.12           py37h14836fe_0    defaults
mkl_random                1.0.2            py37h343c172_0    defaults
msys2-conda-epoch         20160418                      1    defaults
navigator-updater         0.2.1                    py37_0    defaults
nbconvert                 5.5.0                      py_0    defaults
nbformat                  4.4.0                    py37_0    defaults
notebook                  6.0.0                    py37_0    defaults
numpy                     1.17.0                   pypi_0    pypi
numpy-base                1.16.4           py37hc3f5095_0    defaults
numpydoc                  0.9.1                      py_0    defaults
olefile                   0.46                     py37_0    defaults
openssl                   1.1.1c               he774522_1    defaults
packaging                 19.0                     py37_0    defaults
pandas                    0.25.0           py37ha925a31_0    defaults
pandoc                    2.2.3.2                       0    defaults
pandocfilters             1.4.2                    py37_1    defaults
parso                     0.5.0                      py_0    defaults
pickleshare               0.7.5                    py37_0    defaults
pillow                    6.1.0            py37hdc69c19_0    defaults
pip                       19.2.2                   pypi_0    pypi
pkginfo                   1.5.0.1                  py37_0    defaults
powershell_shortcut       0.0.1                         2    defaults
prometheus_client         0.7.1                      py_0    defaults
prompt_toolkit            2.0.9                    py37_0    defaults
psutil                    5.6.3            py37he774522_0    defaults
py-lief                   0.9.0            py37ha925a31_2    defaults
pycodestyle               2.5.0                    py37_0    defaults
pycosat                   0.6.3            py37hfa6e2cd_0    defaults
pycparser                 2.19                     py37_0    defaults
pyflakes                  2.1.1                    py37_0    defaults
pygments                  2.4.2                      py_0    defaults
pylint                    2.3.1                    py37_0    defaults
pyopenssl                 19.0.0                   py37_0    defaults
pyparsing                 2.4.0                      py_0    defaults
pyqt                      5.9.2            py37h6538335_2    defaults
pyrsistent                0.14.11          py37he774522_0    defaults
pysocks                   1.7.0                    py37_0    defaults
python                    3.7.3                h8c8aaf0_1    defaults
python-dateutil           2.8.0                    py37_0    defaults
python-libarchive-c       2.8                     py37_13    defaults
pytz                      2019.1                     py_0    defaults
pywin32                   223              py37hfa6e2cd_1    defaults
pywinpty                  0.5.5                 py37_1000    defaults
pyyaml                    5.1.1            py37he774522_0    defaults
pyzmq                     18.0.0           py37ha925a31_0    defaults
qt                        5.9.7            vc14h73c81de_0    defaults
qtawesome                 0.5.7                    py37_1    defaults
qtconsole                 4.5.2                      py_0    defaults
qtpy                      1.8.0                      py_0    defaults
requests                  2.22.0                   py37_0    defaults
rope                      0.14.0                     py_0    defaults
ruamel_yaml               0.15.46          py37hfa6e2cd_0    defaults
scikit-learn              0.21.3                   pypi_0    pypi
scipy                     1.3.0                    pypi_0    pypi
send2trash                1.5.0                    py37_0    defaults
setuptools                41.0.1                   py37_0    defaults
sip                       4.19.8           py37h6538335_0    defaults
six                       1.12.0                   py37_0    defaults
snowballstemmer           1.9.0                      py_0    defaults
soupsieve                 1.9.2                    py37_0    defaults
sphinx                    2.1.2                      py_0    defaults
sphinxcontrib-applehelp   1.0.1                      py_0    defaults
sphinxcontrib-devhelp     1.0.1                      py_0    defaults
sphinxcontrib-htmlhelp    1.0.2                      py_0    defaults
sphinxcontrib-jsmath      1.0.1                      py_0    defaults
sphinxcontrib-qthelp      1.0.2                      py_0    defaults
sphinxcontrib-serializinghtml 1.1.3                      py_0    defaults
spyder                    3.3.6                    py37_0    defaults
spyder-kernels            0.5.1                    py37_0    defaults
sqlite                    3.29.0               he774522_0    defaults
terminado                 0.8.2                    py37_0    defaults
testpath                  0.4.2                    py37_0    defaults
tk                        8.6.8                hfa6e2cd_0    defaults
tornado                   6.0.3            py37he774522_0    defaults
tqdm                      4.32.1                     py_0    defaults
traitlets                 4.3.2                    py37_0    defaults
urllib3                   1.24.2                   py37_0    defaults
vc                        14.1                 h0510ff6_4    defaults
vs2015_runtime            14.15.26706          h3a45250_4    defaults
wcwidth                   0.1.7                    py37_0    defaults
webencodings              0.5.1                    py37_1    defaults
wheel                     0.33.4                   py37_0    defaults
widgetsnbextension        3.5.0                    py37_0    defaults
win_inet_pton             1.1.0                    py37_0    defaults
wincertstore              0.2                      py37_0    defaults
winpty                    0.4.3                         4    defaults
wrapt                     1.11.2           py37he774522_0    defaults
xz                        5.2.4                h2fa13f4_4    defaults
yaml                      0.1.7                hc54c509_2    defaults
zeromq                    4.3.1                h33f27b4_3    defaults
zlib                      1.2.11               h62dcd97_3    defaults
zstd                      1.3.7                h508b16e_0    defaults


with the version of scipy not matching up with the version that pip installed. Not sure how significant it is but it seemed strange to me.

EDIT 2:
Doing pip list returns

Package                       Version
----------------------------- ---------
-cipy                         1.3.0
alabaster                     0.7.12
anaconda-client               1.7.2
anaconda-navigator            1.9.7
asn1crypto                    0.24.0
astroid                       2.2.5
attrs                         19.1.0
Babel                         2.7.0
backcall                      0.1.0
backports.functools-lru-cache 1.5
backports.tempfile            1.0
backports.weakref             1.0.post1
beautifulsoup4                4.7.1
bleach                        3.1.0
certifi                       2019.6.16
cffi                          1.12.3
chardet                       3.0.4
Click                         7.0
cloudpickle                   1.2.1
clyent                        1.2.2
colorama                      0.4.1
conda                         4.7.11
conda-build                   3.18.8
conda-package-handling        1.3.11
conda-verify                  3.4.2
cryptography                  2.7
decorator                     4.4.0
defusedxml                    0.6.0
docutils                      0.15.1
entrypoints                   0.3
filelock                      3.0.12
future                        0.17.1
glob2                         0.7
idna                          2.8
imagesize                     1.1.0
ipykernel                     5.1.1
ipython                       7.7.0
ipython-genutils              0.2.0
ipywidgets                    7.5.1
isort                         4.3.21
jedi                          0.13.3
Jinja2                        2.10.1
joblib                        0.13.2
json5                         0.8.5
jsonschema                    3.0.1
jupyter-client                5.3.1
jupyter-core                  4.5.0
jupyterlab                    1.0.2
jupyterlab-server             1.0.0
keyring                       18.0.0
lazy-object-proxy             1.4.1
libarchive-c                  2.8
MarkupSafe                    1.1.1
mccabe                        0.6.1
menuinst                      1.4.16
mistune                       0.8.4
mkl-fft                       1.0.12
mkl-random                    1.0.2
mkl-service                   2.0.2
navigator-updater             0.2.1
nbconvert                     5.5.0
nbformat                      4.4.0
notebook                      6.0.0
numpy                         1.17.0
numpydoc                      0.9.1
olefile                       0.46
packaging                     19.0
pandas                        0.25.0
pandocfilters                 1.4.2
parso                         0.5.0
pickleshare                   0.7.5
Pillow                        6.1.0
pio                           0.0.3
pip                           19.2.2
pkginfo                       1.5.0.1
prometheus-client             0.7.1
prompt-toolkit                2.0.9
psutil                        5.6.3
pycodestyle                   2.5.0
pycosat                       0.6.3
pycparser                     2.19
pyflakes                      2.1.1
Pygments                      2.4.2
pylint                        2.3.1
pyOpenSSL                     19.0.0
pyparsing                     2.4.0
pyrsistent                    0.14.11
PySocks                       1.7.0
python-dateutil               2.8.0
pytz                          2019.1
pywin32                       223
pywinpty                      0.5.5
PyYAML                        5.1.1
pyzmq                         18.0.0
QtAwesome                     0.5.7
qtconsole                     4.5.2
QtPy                          1.8.0
requests                      2.22.0
rope                          0.14.0
ruamel-yaml                   0.15.46
scikit-learn                  0.21.3
scipy                         1.3.1
Send2Trash                    1.5.0
setuptools                    41.0.1
six                           1.12.0
snowballstemmer               1.9.0
soupsieve                     1.9.2
Sphinx                        2.1.2
sphinxcontrib-applehelp       1.0.1
sphinxcontrib-devhelp         1.0.1
sphinxcontrib-htmlhelp        1.0.2
sphinxcontrib-jsmath          1.0.1
sphinxcontrib-qthelp          1.0.2
sphinxcontrib-serializinghtml 1.1.3
spyder                        3.3.6
spyder-kernels                0.5.1
terminado                     0.8.2
testpath                      0.4.2
tornado                       6.0.3
tqdm                          4.32.1
traitlets                     4.3.2
urllib3                       1.24.2
wcwidth                       0.1.7
webencodings                  0.5.1
wheel                         0.33.4
widgetsnbextension            3.5.0
win-inet-pton                 1.1.0
wincertstore                  0.2
wrapt                         1.11.2


pip list says scipy is version 1.3.1, while conda list says it's version 1.3.0. Again, not sure how relevant it is, but seems strange

EDIT 3: I got this error after putting the following lines (suggested by @Brennan) in my command prompt then running the file

pip uninstall scikit-learn
pip uninstall scipy
conda uninstall scikit-learn
conda uninstall scipy

conda update --all
conda install scipy
conda install scikit-learn


This is the new error I get when trying to import sklearn:

Traceback (most recent call last):

  File ""&lt;ipython-input-15-7135d3f24347&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/julia/.spyder-py3/temp.py', wdir='C:/Users/julia/.spyder-py3')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/julia/.spyder-py3/temp.py"", line 2, in &lt;module&gt;
    import sklearn as skl

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\__init__.py"", line 76, in &lt;module&gt;
    from .base import clone

  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\base.py"", line 13, in &lt;module&gt;
    import numpy as np

  File ""C:\ProgramData\Anaconda3\lib\site-packages\numpy\__init__.py"", line 140, in &lt;module&gt;
    from . import _distributor_init

  File ""C:\ProgramData\Anaconda3\lib\site-packages\numpy\_distributor_init.py"", line 34, in &lt;module&gt;
    from . import _mklinit

ImportError: DLL load failed: The specified module could not be found.


A possible cause of this might be me deleting the mkl_rt.dll file from my Anaconda/Library/bin after encountering the error described here: https://github.com/ContinuumIO/anaconda-issues/issues/10182

This puts me in a predicament, because reinstalling Anaconda to repair this will get me the same ""ordinal 242 could not be located"" error that I faced earlier, but not repairing it will continue the issue with sklearn...

FINAL EDIT: Solved by installing old version of Anaconda. Will mark as solved when I am able to (2 days)
",3,12402,"I encountered the same error after letting my PC sit for 4 days unattended. Restarting the kernel solved it.
This probably won't work for everyone, but it might save someone a little agony.
","I ended up fixing this by uninstalling my current version of Anaconda and installing a version from a few months ago. I didn't get the ""ordinal 242"" error nor the issues with scikit-learn.
","had a similar problem on google collaboratory, simply uninstalling and reinstalling the scipy module alone solved it for me

and the simple fact that collab ai suggested this move emboldened me
"
scikit-learn strange issue,https://stackoverflow.com/questions/73491958,Installation always stuck on PyCaret 2.2.2 + Package problems,"I'm stuck on an issue that I can't seem to solve. I was fine using PyCaret on my other PC and had recently got a new desktop.
I was working on one dataset on my old PC and had no problems with setup() and PyCaret preprocessed my data without any issues. When I worked on my the same dataset with my new desktop and Jupyter newly installed, I noticed I ran into an ValueError: Setting a random_state has no effect since shuffle is False. You should leave random_state to its default (None), or set shuffle=True. I thought it was strange but went on to set fold_shuffle=True to get through this.
Next, I encountered AttributeError: 'Simple_Imputer' object has no attribute 'fill_value_categorical'. It seems I'm getting failures at every step of setup(). I went through the forums and found a thread where at the bottom of it, @eddygeek mentioned that PyCaret was set up to fail if the sklearn version is wrong. This got me looking into the packages I have that may meet dependencies between packages.
I noticed the following issues:

I get several errors:
ERROR: Command errored out with exit status 1: C:\Users\%%USER%%\anaconda3\python.exe'
Ignoring numpy: markers 'python_version &gt;= ""3.8"" and platform_system == ""AIX""' don't match your environment
ERROR: Could not find a version that satisfies the requirement scikit-learn==0.23.2
Screenshot of more errors attached

Jupyter Notebook fails to launch because of Pandas Profiling Import Error: cannot import name 'soft_unicode' from 'markupsafe'. I got around this by installing markupsafe===2.0.1 but this leads to incompatibility warning by pandas-profiling 3.2.0 saying it needs markupsafe 2.1.1

PyCaret keeps getting installed as 2.2.2 version. I think that's why it keeps looking for scikit-learn 0.23.2 when the latest PyCaret 2.3.10 works with scikit-learn &gt;=1.0. I've tried uninstalling and reinstalling PyCaret several times but it's still the same.


What I've done
I'm on Python 3.9.12 that was installed together with Anaconda3. My PyCaret was installed with pip install pycaret[full] --user on Anaconda Prompt.
In my pip list, I have:

scikit-learn 1.1.2
markupsafe 2.1.1
pandas-profiling 3.2.0
pycaret 2.2.2

I've added C:\Users\%%USER%%\AppData\Roaming\Python\Python39\Scripts to PATH
I'm really at my wits end so I hope I can get some advice on this. Thank you.
",1,977,"You are using a very old version of pycaret which does not work in Python 3.9. Please install the latest version in a fresh (conda) environment. Make sure it is a new environment in order to avoid any package issues.
# This installs the pre-release 3.0.0 release which has reduced dependencies.
pip install --pre pycaret

","I've encountered the very same issues and solved as follows.
According to the documentation, there are a few problems with your setup:

PyCaret is not yet compatible with sklearn&gt;=0.23.2

PyCaret is tested and supported on the following 64-bit systems:
Python 3.6  3.8
Python 3.9 for Ubuntu only


So if you're using python 3.9 on Windows, I'd start with that.
I went into a rabbit hole of downgrading the packages and getting one error after another.
Long story short, the setup that finally worked was:
sklearn 0.23.1
scipy 1.5.2

Both installed on a virtual conda environment but at the end I had to run:
pip3 install pycaret[full]

Notice pip3 intead of pip because I was getting permission errors.
",
scikit-learn strange issue,https://stackoverflow.com/questions/44490337,Import module works in Jupyter notebook but not in IDLE,"I dont understand what Im doing wrong. I re-installed my windows last week; after that I got python (3.6), and all the libraries I need, but when I try to import them in the IDLE returns an error (ModuleNotFoundError: No module named), and when I check in the command window with pip list or conda list, the package is already there. The strange thing is when I tried to import them from a Jupyter notebook worked without any problem. 

Im working with a laptop Windows 7, 64 bits. No previous version of python was installed before. I am having issues with scikit-learn, pandas-datareader and beautifulsoup4. I tried to install the packages with pip, conda and the wheel file. Nothing seems to work.  (since Im in my laptop, I only download miniconda, and Im installing the modules I need).

Any suggestions? The safety of my laptop is in your hands. :D 
",1,3390,"The reason is that your pip/conda installed library paths are not accessible by python IDLE.    You have to add those library paths to your environment variable(PATH). To do this open my computer &gt; properties &gt; advanced system settings &gt; system.

Under environment variables look for PATH and at the end add the location of installed libraries. Refer this for more details on how to add locations in path variable. Once you do these you will be able to import the libraries. In order to know what locations python searches for libraries you can use 

import sys 
print sys.path


This will give you a list of locations where python searches for libraries. Once you edit the PATH variable those locations will be reflected here.

Refer this also in order to know how to add python library path.

Note: The tutorial is a reference on how to edit PATH variable. I encourage you to find the location of installed libraries and follow the steps to edit the same.
",,
scikit-learn strange issue,https://stackoverflow.com/questions/66023482,Sklearn and Pymesh import causes conflicts,"I am working on processing 3D meshes using PyMesh. pip install pymesh2 has build error. Hence I installed from another release from the official github page. I am using pymesh2-0.3-cp36-cp36m-linux_x86_64.whl.
I installed scikit-learn using pip install. Both installed successfully in my system.
While importing them, I observed something strange.
While importing Pymesh and Sklearn. Pymesh imported correctly. Sklearn is not

While importing Sklearn and Pymesh. Sklearn imported correctly. Pymesh is not

Seems like these two packages are conflicting each other!
Can someone help me what is the issue and any possible resolution?
",0,140,"As suggested in the comments, I used conda instead of virtual env and pip installed the pymesh using the wheel file from the github. That worked well for me!
",,
scikit-learn strange issue,https://stackoverflow.com/questions/37165344,python - sklearn 0.17 and 0.18 - how to retrieve only the first value of an StratifiedKFold object?,"I have a python problem.
I use scikit-learn 0.17
Someone give me an example code that I have to run.
The code was in python 2, and scikit-learn 0.18, the dev version.
I transformed the code in python 3 without issue.
But he used the function :

sklearn.model_selection.StratifiedKFold(n_folds=3, shuffle=False, random_state=None)


In scikit-learn 0.17, I have function that looks like this one : 

sklearn.cross_validation.StratifiedKFold(y, n_folds=3, shuffle=False, random_state=None)


As you can see, they seem very similar : there is only the ""y"" that change.

So I have two solution : update to 0.18 (I'm using windows, and I can't use linux) I tried this one, but I don't succeed to compile sources.

The second solution is to use the StratifiedKFold of 0.17.
I succeed this solution. (I don't know if it doing the things it suppose to do, however x) ).

So, there is my problem (finally) :

I have to adapt this two line : 

skf = StratifiedKFold(n_folds=5)
train_idx, test_idx = skf.split(patch_arr, labels).next()


for the moment, I wrote this (cover your eyes):

skf = StratifiedKFold(labels, n_folds=5)
for train_idx, test_idx in skf:
    break 


And the program run, but my code is awful. Is there any other method to retrieve only the first couple of (train_idx, test_idx) ?
I tried with skf[0] and skf.next(), but none of this works.

(for your information, train_idx and test_idx are two arrays)

Have you got any idea ?
I find strange that I can't use patch_arr in my solution too, but if I can avoid to spend more hours trying to update scikit-image, I will be very happy :)
",0,331,"The following should suffice.

train_idx, test_idx = next(iter(skf))

",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/5941234,Profiling SQL query,"I'm using sqlalchemy (expression language, not full ORM) with MySQL and experiencing some unexpected slowness.  Particularly, the time spent performing a select query in by sqlalchemy is ten times greater than the time spent performing the same query from the mysql command line.

Output from cprofile:

ncalls  tottime  percall  cumtime  percall filename:lineno(function)
100  206.703    2.067  206.703    2.067 {method 'query' of '_mysql.connection' objects}


MySQL time:  0.26 seconds

The consensus seems to be that there is some overhead using sqlalchemy, but not nearly this much.  Any suggestions as to what could cause behavior like this?

The queries are generally of the form:

SELECT fieldnames.minage, fieldnames.maxage, fieldnames.race,    
fieldnames.sex, sum( pop.population ) AS pop, pop.zip5
FROM pop
INNER JOIN fieldnames ON fieldnames.fieldname = pop.fieldname_id
WHERE fieldnames.race IN (""White alone"")
AND fieldnames.sex IN (""Female"")
AND fieldnames.maxage &gt;=101
AND fieldnames.minage &lt;=107
GROUP BY fieldnames.minage, fieldnames.maxage

",6,852,"One possible reason for slowness - Does sql alchemy use prepared statements? If yes, then a reason why you may be experiencing a difference in performance is because the mysql optimizer has different information when creating the two query plans.

When you run the query from the command-line, the mysql optimizer has the complete query with all where clause values filled in (as you showed above3), thus can optimize explicitly for these values.

When you run from sql alchemy, the mysql optimizer may only see this (perhaps fieldnames.race and fieldnames.sex are parameterized as well):

SELECT fieldnames.minage, fieldnames.maxage, fieldnames.race,    
fieldnames.sex, sum( pop.population ) AS pop, pop.zip5
FROM pop
INNER JOIN fieldnames ON fieldnames.fieldname = pop.fieldname_id
WHERE fieldnames.race IN (""White alone"")
AND fieldnames.sex IN (""Female"")
AND fieldnames.maxage &gt;= ?
AND fieldnames.minage &lt;= ?
GROUP BY fieldnames.minage, fieldnames.maxage


Thus the optimizer has to make a guess on what values you might use then optimize around that. Unfortunately, it may make a bad guess, and thus in a worst case create a query plan that makes the query run significantly slower than you expect.
",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/28901050,Default value doesn&#39;t work in SQLAlchemy + PostgreSQL + aiopg + psycopg2,"I've found an unexpected behavior in SQLAlchemy. I'm using the following versions:


SQLAlchemy (0.9.8)
PostgreSQL (9.3.5)
psycopg2 (2.5.4)
aiopg (0.5.1)


This is the table definition for the example:

import asyncio
from aiopg.sa import create_engine
from sqlalchemy import (
  MetaData,
  Column,
  Integer,
  Table,
  String,
)
metadata = MetaData()

users = Table('users', metadata,
  Column('id_user', Integer, primary_key=True, nullable=False),
  Column('name', String(20), unique=True),
  Column('age', Integer, nullable=False, default=0),
)


Now if I try to execute a simple insert to the table just populating the id_user and name, the column age should be auto-generated right? Lets see...

@asyncio.coroutine
def go():
  engine = yield from create_engine('postgresql://USER@localhost/DB')
  data = {'id_user':1, 'name':'Jimmy' }
  stmt = users.insert(values=data, inline=False)
  with (yield from engine) as conn:
    result = yield from conn.execute(stmt)


loop = asyncio.get_event_loop()
loop.run_until_complete(go())


This is the resulting statement with the corresponding error:

INSERT INTO users (id_user, name, age) VALUES (1, 'Jimmy', null);

psycopg2.IntegrityError: null value in column ""age"" violates not-null constraint


I didn't provide the age column, so where is that age = null value coming from? I was expecting something like this:

INSERT INTO users (id_user, name) VALUES (1, 'Jimmy');


Or if the default flag actually works should be:

INSERT INTO users (id_user, name, Age) VALUES (1, 'Jimmy', 0);


Could you put some light on this?
",4,1565,"This issue has been confirmed has an aiopg bug. Seems like at the moment it's ignoring the default argument on data manipulation. 

I've fixed the issue using server_default instead:

users = Table('users', metadata,
          Column('id_user', Integer, primary_key=True, nullable=False),
          Column('name', String(20), unique=True),
          Column('age', Integer, nullable=False, server_default='0'))

","I think you need to use inline=True in your insert. This turns off 'pre-execution'.
Docs are a bit cryptic on what exactly this 'pre-execution' entails, but they mentions default parameters:

    :param inline:
      if True, SQL defaults present on :class:`.Column` objects via
      the ``default`` keyword will be compiled 'inline' into the statement
      and not pre-executed.  This means that their values will not
      be available in the dictionary returned from
      :meth:`.ResultProxy.last_updated_params`.


This piece of docstring is from Update class, but they have a shared behavior with Insert. 

Besides, that's the only way they test it: 
https://github.com/zzzeek/sqlalchemy/blob/rel_0_9/test/sql/test_insert.py#L385
",
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/17959320,Flask-Login &amp; Flask-Principle authenticated user drops to flask_login.AnonymousUserMixin,"I'm getting two problems


My authenticated user constantly drops to flask_login.AnonymousUserMixin
I get unexpected signalling using Flask-Login &amp; Flask-Principal


Trying to get /projects/10 URL which is protected with
@admin_permission.require(http_exception=403)

This is my console output:

127.0.0.1 - - [30/Jul/2013 16:22:58] ""GET /projects/10 HTTP/1.1"" 302 -
127.0.0.1 - - [30/Jul/2013 16:22:58] ""GET /login HTTP/1.1"" 200 -


Getting on login form (All good so far). Typing valid login &amp; password and getting crazy signalling and behavior is not what I expect:

127.0.0.1 - - [30/Jul/2013 16:24:06] ""POST /login HTTP/1.1"" 302 -
&lt;Employee('103','Dmitry Semenov')&gt;
&lt;Identity id=""103"" auth_type=""None"" provides=set([Need(method='role', value='manager'), Need(method='id', value=103L), Need(method='role', value='admin')])&gt;
&lt;flask_login.AnonymousUserMixin object at 0x03258790&gt;
&lt;Identity id=""103"" auth_type=""None"" provides=set([])&gt;
127.0.0.1 - - [30/Jul/2013 16:24:06] ""GET /projects/10 HTTP/1.1"" 302 -
&lt;flask_login.AnonymousUserMixin object at 0x03342AF0&gt;
&lt;Identity id=""103"" auth_type=""None"" provides=set([])&gt;
127.0.0.1 - - [30/Jul/2013 16:24:06] ""GET /login HTTP/1.1"" 200 -
&lt;flask_login.AnonymousUserMixin object at 0x03342E90&gt;
&lt;Identity id=""103"" auth_type=""None"" provides=set([])&gt;


As you see I got current_user pointing to valid Employee instance (class) and identity id=103, but then immediately it becamse flask_login.AnonymousUserMixin for some reason and then the auth system passes that user and don't allow me to open /projects/10 URL.

Any Ideas what is wrong? And Why I'm getting that many signals - according to the code they should happen only on successfull login. What do I miss?

Source code:

# flask-principal
principals = Principal()
normal_role = RoleNeed('normal')
normal_permission = Permission(normal_role)
admin_permission  = Permission(RoleNeed('admin'))
principals._init_app(app)

login_manager    = LoginManager()
login_manager.init_app(app)

@login_manager.user_loader
def load_user(userid):
    return mysqlsess.query(Employee).get(userid)


@app.route(""/"")
@app.route(""/dashboard"")
def vDashboard():
    return render_template('dashboard.html')

@app.route('/projects')
def vPojects():
    return ""Projects""


@app.route('/projects/&lt;ID&gt;')
@admin_permission.require(http_exception=403)
def vProject(ID):
    return current_user.roles[1]

# somewhere to login    
@app.route('/login', methods=['GET', 'POST'])
def login():
    # A hypothetical login form that uses Flask-WTF
    form = LoginForm()

    # Validate form input
    if form.validate_on_submit():
        # Retrieve the user from the hypothetical datastore
        user = mysqlsess.query(Employee).get(form.email.data)

        # Compare passwords (use password hashing production)
        if form.password.data == str(user.ID):
            # Keep the user info in the session using Flask-Login
            login_user(user)
            # Tell Flask-Principal the identity changed
            identity_changed.send(app,
                                  identity=Identity(user.ID))
            return redirect(session['redirected_from'] or '/')
        else:
            return abort(401)

    return render_template('login.html', form=form)


# somewhere to logout
@app.route(""/logout"")
def logout():
    logout_user()

    for key in ['identity.name', 'identity.auth_type', 'redirected_from']:
        try:
            del session[key]
        except:
            pass
    return Response('&lt;p&gt;Logged out&lt;/p&gt;')


# handle login failed
@app.errorhandler(401)
def page_not_found(e):
    return Response('&lt;p&gt;Login failed&lt;/p&gt;')


@app.errorhandler(403)
def page_not_found(e):
    session['redirected_from'] = request.url
    return redirect(url_for('login'))


@identity_loaded.connect_via(app)
def on_identity_loaded(sender, identity):
        identity.user = current_user
        print identity.user


        if hasattr(current_user, 'ID'):
            identity.provides.add(UserNeed(current_user.ID))

        if hasattr(current_user, 'roles'):
            for role in current_user.roles:
                identity.provides.add(RoleNeed(role))

        print identity


class LoginForm(Form):
    email = TextField()
    password = PasswordField()

if __name__ == ""__main__"":
    app.run()


And my Employee SQLAlchemy class

class Employee(Base):

__tablename__  = ""Employees""

# Properties
ID           = Column(BigInteger,   primary_key=True)
name         = Column(VARCHAR(255), nullable=False)
created      = Column(DateTime,     nullable=False, default=datetime.now())
updated      = Column(DateTime)
deleted      = Column(DateTime)
branchID     = Column(BigInteger,   ForeignKey('Branches.ID'),    nullable=False)
departmentID = Column(BigInteger,   ForeignKey('Departments.ID'), nullable=False)
utilization  = Column(SmallInteger, nullable=False, default=1)
statusID     = Column(Enum('active', 'fired', 'vacation'), default='active')
birthday     = Column(Date)

# Relationships
Branch       = relationship(""Branch"")
Department   = relationship(""Department"")
ProjectStat  = relationship(""ProjectStat"",  lazy=""dynamic"")

roles        = [""admin"", ""manager""]

# Methods
def zzz(self):
    session = object_session(self)

    stats = self.ProjectStat.filter(and_(ProjectStat.metricID=='hb', ProjectStat.metricValue&gt;=6)).all()
    for s in stats:
        print s.metricValue

# Constructor
def __init__(self, ID, name):
    self.ID   = ID
    self.name = name

# Friendly Print
def __repr__(self):
    return ""&lt;Employee('%s','%s')&gt;"" % (self.ID, self.name)

def is_active(self):
    return True

def get_id(self):
    return unicode(self.ID)

def is_authenticated(self):
    return True

def is_anonymous(self):
    return False

",3,2099,"You need to instantiate principle after login.

This is a repeat question, see here Flask Login and Principal - current_user is Anonymous even though I'm logged in
",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/48462859,"SQLAlchemy selects give different results on SQLite table, raw sql versus selectable","While reading SQLite tables using pandas and dask, I came across some unexpected behavior of SQLAlchemy when selecting from SQLite tables with datetimes (ISO formatted strings) stored as NUMERIC data type. An SQLAlchemy raw SQL query works fine, but a query using a selectable constructed from reflection fails. The two queries appear to be equivalent.

I have pasted an example below, along with the traceback. Can someone explain what is wrong with the third query in the example?

Set up table with NUMERIC datetime:

import sqlalchemy as sa
from sqlalchemy import text

connString = ""sqlite:///c:\\temp\\test.db""
engine = sa.create_engine(connString)
conn = engine.connect()
conn.execute(""create table testtable (uid INTEGER Primary Key, datetime NUMERIC)"")
conn.execute(""insert into testtable values (1, '2017-08-03 01:11:31')"")
print(conn.execute('PRAGMA table_info(testtable)').fetchall())
# [(0, 'uid', 'INTEGER', 0, None, 1), (1, 'datetime', 'NUMERIC', 0, None, 0)]


Query with raw SQL works:

resultList1 = conn.execute(""SELECT testtable.uid, testtable.datetime \nFROM testtable"").fetchall()
print(resultList1)
# [(1, '2017-08-03 01:11:31')]


Query with this selectable works:

resultList2 = conn.execute(sa.sql.select(columns=[text('uid'),text('datetime')]).select_from(text('testtable'))).fetchall() 
print(resultList2)
# [(1, '2017-08-03 01:11:31')]


Query with this selectable fails:

m = sa.MetaData()
table = sa.Table('testtable', m, autoload=True, autoload_with=engine)
selectble = sa.sql.select(table.columns).select_from(table)
print(selectble.compile().string)
#  note: same raw sql query as above
# ""SELECT testtable.uid, testtable.datetime \nFROM testtable""

resultList3 = conn.execute(sa.sql.select(table.columns).select_from(table)).fetchall()
# SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively...
print(resultList3)

conn.close()


The error:

Traceback (most recent call last):

  File ""&lt;ipython-input-20-188c84a35d95&gt;"", line 1, in &lt;module&gt;
    print(resultList3)

  File ""c:\program files\python36\lib\site-packages\sqlalchemy\engine\result.py"", line 156, in __repr__
    return repr(sql_util._repr_row(self))

  File ""c:\program files\python36\lib\site-packages\sqlalchemy\sql\util.py"", line 329, in __repr__
    "", "".join(trunc(value) for value in self.row),

TypeError: must be real number, not str

",3,1302,"SQLite has a very different type system from most SQL databases: it uses dynamic typing, and after conversion the typename you give a column determines its affinity, such as NUMERIC:


  A column with NUMERIC affinity may contain values using all five storage classes. When text data is inserted into a NUMERIC column, the storage class of the text is converted to INTEGER or REAL (in order of preference) if such conversion is lossless and reversible. For conversions between TEXT and REAL storage classes, SQLite considers the conversion to be lossless and reversible if the first 15 significant decimal digits of the number are preserved. If the lossless conversion of TEXT to INTEGER or REAL is not possible then the value is stored using the TEXT storage class. No attempt is made to convert NULL or BLOB values.


Since you've inserted values for which a (lossless) conversion to INTEGER or REAL1 is not possible, your values use the TEXT storage class, and SQLAlchemy/pysqlite is unhappy since it on the other hand expected values that it can convert to float, which fails.

The typing system causes other similar issues, such as when reflecting the resulting table from a CREATE TABLE ... AS against a SELECT from a table using DATETIME typename, which is converted to NUMERIC affinity.

A shorter code example that demonstrates the issue:

In [2]: foo = Table('foo', metadata, Column('bar', NUMERIC))

In [3]: foo.create(engine)
CREATE TABLE foo (
        bar NUMERIC
)

In [4]: engine.execute(""insert into foo values ('not really a number, no')"")
Out[4]: &lt;sqlalchemy.engine.result.ResultProxy at 0x7fbcd7ee8f98&gt;

In [5]: foo.select().execute().fetchall()
Out[5]: ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
  ...
~/Work/SO/lib/python3.6/site-packages/sqlalchemy/sql/util.py in __repr__(self)
    327         trunc = self.trunc
    328         return ""(%s%s)"" % (
--&gt; 329             "", "".join(trunc(value) for value in self.row),
    330             "","" if len(self.row) == 1 else """"
    331         )

TypeError: must be real number, not str


1 Probably the reason why the sqlite+pysqlite dialect does not support Decimal natively  neither does SQLite
",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/62170836,How do I fix this Elastic Beanstalk error when deploying a Flask app: Python 3.7.0 was not found on your system,"I am trying to deploy my Flask app using the EB CLI following this official AWS tutorial. I receive the error 'Create environment operation is complete, but with errors'. Observing the environment logs, the server seems to be lacking the correct Python version.

Below are the EB environment details, my PipFile, the EB CLI output and the error in the deploy logs.

How can I fix this problem?

Environment details:

 Application name: server_eb
  Region: eu-west-2
  Deployed Version: XXX
  Environment ID: XXX
  Platform: arn:aws:elasticbeanstalk:eu-west-2::platform/Python 3.7 running on 64bit Amazon Linux 2/3.0.1
  Tier: WebServer-Standard-1.0
  CNAME: UNKNOWN
  Updated: 2020-06-02 16:53:10.321000+00:00


PipFile:

[[source]]
name = ""pypi""
url = ""https://pypi.org/simple""
verify_ssl = true

[dev-packages]

[packages]
flask = ""~=1.1.1""
python-dotenv = ""~=0.13.0""
psycopg2 = ""~=2.8.5""
flask-sqlalchemy = ""~=2.4.1""
flask-migrate = ""~=2.5.3""
flask-dance = {extras = [""sqla""],version = ""~=3.0.0""}
flask-login = ""~=0.4.1""
blinker = ""~=1.4""
flask-session = ""==0.3.1""
redis = ""==3.4.1""

[scripts]
migrate=""flask db upgrade""
start=""flask run --cert=cert.pem --key=key.pem""

[requires]
python_version = ""3.7.0""


CLI output:

2020-06-02 16:54:55    ERROR   [Instance: XXX] Command failed on instance. An unexpected error has occurred [ErrorCode: 0000000001].
2020-06-02 16:54:55    INFO    Command execution completed on all instances. Summary: [Successful: 0, Failed: 1].
2020-06-02 16:55:57    ERROR   Create environment operation is complete, but with errors. For more information, see troubleshooting documentation.

ERROR: ServiceError - Create environment operation is complete, but with errors. For more information, see troubleshooting documentation.


Logs: 

2020/06/02 16:54:55.030996 [ERROR] An error occurred during execution of command [app-deploy] - [SetUpPythonEnvironment]. Stop running the command. Error: fail to install Gunicorn with error Command /bin/sh -c python3 -m pipenv install gunicorn --skip-lock failed with error exit status 1. Stderr:Warning: the environment variable LANG is not set!
We recommend setting this in ~/.profile (or equivalent) for proper expected behavior.
Warning: Python 3.7.0 was not found on your system
You can specify specific versions of Python with:
  $ pipenv --python path/to/python

",1,993,"Based on the comments, the issue was that the EB uses Python 3.7.6, while the OP's dependencies required version 3.7.0. 

The solution was to change the dependency to Python 3.7.6.

The current python version running on EB can be found here:


  64bit Amazon Linux 2 v3.0.1 running Python 3.7 - Python 3.7.6

",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/66656355,How to access only filtered objects when filtering on joined tables in sqlalchemy,"I am using sqlalchemy for the first time and I have an unexpected behavior in reading results of a query.
Let's say these are my database models:
class Parent(Base):
    __table__ = ""parents""
    id = Column(Integer, primary_key=True)
    children = relationship(""Child"", back_populates=""parent"")

class Child(Base):
    __table__ = ""children""
    id = Column(Integer, primary_key=True)
    parent_id = Column(Integer, ForeignKey(""parents.id""))
    did_homework = Column(Boolean, default=False)
    parent = relationship(""Parent"", back_populates=""children"")

When I query the database like this:
parents = session.query(Parent).join(Parent.children).filter(Child.did_homework == True).all()
good_children = parents[0].children

I expected all good_children to have did_homework set to True.
But it seems the query returns all parents which have at least one Child with did_homework set to True and then when I access parents[0].children I get all children of that parent, not just the ones that satisfy my criteria.
I have tried querying like this:
result = (
        db.query(Parent, Child)
        .filter(Parent.id == Child.parent_id)
        .filter(Child.did_homework == True)
        .all()
    )

which gives the expected tuples as the result (every Child in the tuple has did_homework == True), but every Parent in the tuple still has all children, not just the ones that did their homework.
Question:
Is there a way of writing a query that would assign only the children which are not filtered out to the parent they belong to?
So that when I write: good_children = parents[0].children I could be sure all objects in good_children have did_homework set to True.
",1,202,"After posting this I found the same question here with an answer that didn't work for me, but with a small modification I managed to do it:
parents = (
        db.query(Parent)
        .join(Child)
        .filter(Child.did_homework == True)
        .options(contains_eager(""children""))
        .all()
    )

",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/53726041,SQLAlchemy not committing update,"I am get some unexpected behavior when trying to commit an update with SQLalchemy..

I have an object like so..

updatedINfo = {'id': 1, 'type': 'NewData'}
# The ID is an id from the table, the 'type' is a column from that table
# The 'NewData' is what needs to replace whatever is currently stored in
# 'type' column.


Because I want this function to be reusble and not have to hardcode the column names, I figured I could do something like this;

# Find the event first
the_event = db.query(EventsPending).filter(EventsPending.id == updatedInfo.get('id')).one_or_none()

# If it returns None, something went wrong
if not the_event:
    return {'Error': 'Something went wrong..'}

# Take the id out of the JSON post
del updatedInfo['id']

# Turn our db query into a dict
event_dict = the_event.as_dict()

# Set the keys for cross checking
for key in event_dict.keys():
    if key in updatedInfo.keys():
        the_event.__dict__[key] = updatedInfo.get(key)
        print(the_event.type)
        # THIS PRINTS THE UPDATED TYPE
        print(the_event.as_dict())
        # THIS ALSO PRINTS THE UPDATED TYPE

       db.commit()
       print(the_event.type)
       # HOWEVER THIS PRINTS THE OLD TYPE AFTER THE COMMIT
       # The commit does not make it to the database
       # I see no errors, and logging shows no useful info


Obviously I can work around this by coding in the different types but really seems ugly and inefficient. Any help is appreciated.
",0,211,"For anyone running into this.. You need

setattr(the_event, key, data)
# the_event being the sqlalchemy object
# key being the column name
# data being the new value


Hope this helps someone.
",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/76536770,SQLAlchemy: Understanding Select statements within transactions,"I'm trying to understand the behavior of SQLAlchemy Sessions and their relations to PostgreSQL transactions, as they don't seem to behave quite the same. I've created a simple program to insert, update, and subsequently update a row:
engine = sa.create_engine(f""postgresql+psycopg2://{pg_user}:@{pg_host}:{pg_port}/{pg_db}"", echo=True)
Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)
    name = Column(String)

Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine, expire_on_commit=True)
session = Session()

# Add a user
u1 = User(id=1, name=""u1"")
session.add(u1)
session.commit()

# Modify user's name
session.query(User).filter_by(id=1).update({""name"": ""u2""})
session.commit()

# Get user's name
user = session.query(User).filter_by(id=1).one()
print(user.name) # prints ""u2"" as expected

However, if I now remove the session.commit() statement between the modification and select, I get an unexpected result:
# ...

# Modify user's name
session.query(User).filter_by(id=1).update({""name"": ""u2""})

# Get user's name
user = session.query(User).filter_by(id=1).one()
print(user.name) # prints ""u1"" -- NOT expected!

Presumably this is because the session is querying the database, in which the user's name has not been updated (because the transaction has not yet been committed). However, I would have thought that because I'm using the same session object to query the user, it should recognize the changes that have occurred on the User object within the transaction.
What am I not understanding here?
",0,106,"PostgreSQL uses ""Read Committed"" isolation level. This means that the each query being executed sees data committed earlier.
So, when you are making changes without ""session.commit()"", the changes are not made in the database, so the queries does not see those changes. Once you have committed the changes, the changes are made in database and upcoming queries within same session will see updated data.
To avoid ""session.commit()"" after every query, you can set ""autocommit"" to true.
",,
SQLAlchemy unexpected behavior,https://stackoverflow.com/questions/77024615,Redshift Sqlalchemy transactions and session management,"Database - Redshift
SQLAlchemy version - 1.4.2
I'm trying to create a re-usable query using the SQLAlchemy ORM. What I do is something like this.
def get_page(query, page_size, page_number):
    return query.limit(page_size).offset((page_number - 1) * page_size).all()

def my_query(self):
    with Session(engine) as session:
        query = session.query(my_model).order_by(
            my_model.id
        )
        return query

query = my_query()

while True:
    page_number = 1
    results = get_page(query, 100, page_number)
    if not results:
        break
    for result in results:
        print(result)
        
    page_number+=1

I'm experiencing unexpected behavior using this method on very large data sets. Where it will miss a bunch of entries, or will repeat some of the entries it has already done.  (entry A could appear on page 1 and 4 for some reason)
I'm assuming my problem is that every time i get a page and call .all() it is performing a brand new query so if new data is put onto the table it will now effect my results.
This is a watered down example of my real world use case.
The real scenarios query is a left join on two tables.
Table X (a join table) and table Y (The data to act on)
I join table Y to table X and find all of Y that does not have a corresponding X entry.
I iterate through my results using the method above and after 500 entries are processed i create 500 join entries on table X that reference the processed table Y entry.(keeping track of what data has been processed this way)
What I want to have happen is have all the results from my query -&gt; Stream them by chunks of 100, process them, and be able to perform bulk inserts onto the join table without effecting my initial query results or committing my query session(resulting in me requerying and ending up with different results). (meaning if the data changes it is not changed in my results of the initial query)
Tell me what is wrong with my method?
",0,84,"For this to be happening these different queries would need to be in different transactions (xid).  Can you confirm this is what is happening?  (I say this b/c if they happen in the same transaction you should get the same version of the tables.  And it sounds like from your description that this isn't just result ordering differences.)
So the first solution would be to have everything happen in a single transaction (if possible).  The best way to run this would be with a cursor to store the results between fetches.  See: https://docs.aws.amazon.com/redshift/latest/dg/declare.html and https://docs.aws.amazon.com/redshift/latest/dg/fetch.html
If it isn't possible to be in a single transaction, then can it be in a single session?  This way you could keep the results in a temp table and read from there.
If these queries absolutely need to be made across separate connections then you will need to set up a perm table to hold results.  This has the disadvantage of needing to be cleaned up when the process ends.  You will want to add some per iteration random chars to the table name so that no iteration collisions can happen on the table name.
I'd start by examining the system tables for these queries on Redshift and note the pid and xid of each.  This will give you more specific information on how SQLAlchemy is interacting with Redshift and what changes are needed.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/14719507,Unit tests for Query in SQLAlchemy,"How does one go about testing queries in SQLAlchemy? For example suppose we have this models.py

from sqlalchemy import (
        Column,
        Integer,
        String,
)
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class Panel(Base):
    __tablename__ = 'Panels'

    id = Column(Integer, primary_key=True)
    category = Column(Integer, nullable=False)
    platform = Column(String, nullable=False)
    region = Column(String, nullable=False)

    def __init__(self, category, platform, region):
        self.category = category
        self.platform = platform
        self.region = region


    def __repr__(self):
        return (
            ""&lt;Panel('{self.category}', '{self.platform}', ""
            ""'{self.region}')&gt;"".format(self=self)
        )


and this tests.py

import unittest

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from models import Base, Panel


class TestQuery(unittest.TestCase):

    engine = create_engine('sqlite:///:memory:')
    Session = sessionmaker(bind=engine)
    session = Session()

    def setUp(self):
        Base.metadata.create_all(self.engine)
        self.session.add(Panel(1, 'ion torrent', 'start'))
        self.session.commit()

    def tearDown(self):
        Base.metadata.drop_all(self.engine)

    def test_query_panel(self):
        expected = [Panel(1, 'ion torrent', 'start')]
        result = self.session.query(Panel).all()
        self.assertEqual(result, expected)


When we try running the test, it fails, even though the two Panels look identical.

$ nosetests
F
======================================================================
FAIL: test_query_panel (tests.TestQuery)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/clasher/tmp/tests.py"", line 31, in test_query_panel
    self.assertEqual(result, expected)
AssertionError: Lists differ: [&lt;Panel('1', 'ion torrent', 's... != [&lt;Panel('1', 'ion torrent', 's...

First differing element 0:
&lt;Panel('1', 'ion torrent', 'start')&gt;
&lt;Panel('1', 'ion torrent', 'start')&gt;

  [&lt;Panel('1', 'ion torrent', 'start')&gt;, &lt;Panel('2', 'ion torrent', 'end')&gt;]

----------------------------------------------------------------------
Ran 1 test in 0.063s

FAILED (failures=1)


One solution I've found is to make a query for every single instance I expect to find in the query:

class TestQuery(unittest.TestCase):

    ...

    def test_query_panel(self):
        expected = [
            (1, 'ion torrent', 'start'),
            (2, 'ion torrent', 'end')
        ]
        successful = True
        # Check to make sure every expected item is in the query
        try:
            for category, platform, region in expected:
                self.session.query(Panel).filter_by(
                        category=category, platform=platform,
                        region=region).one()
        except (NoResultFound, MultipleResultsFound):
            successful = False
        self.assertTrue(successful)
        # Check to make sure no unexpected items are in the query
        self.assertEqual(self.session.query(Panel).count(),
                         len(expected))


This strikes me as pretty ugly, though, and I'm not even getting to the point where I have a complex filtered query that I'm trying to test. Is there a more elegant solution, or do I always have to manually make a bunch of individual queries?
",40,48388,"your original test is on the right track, you just have to do one of two things: either make sure that two Panel objects of the same primary key identity compare as True:
import unittest

from sqlalchemy import create_engine
from sqlalchemy.orm import Session

from database.models import Base

class Panel(Base):
    # ...

    def __eq__(self, other):
        return isinstance(other, Panel) and other.id == self.id

or you can organize your test such that you make sure you're checking against the same Panel instance (because here we take advantage of the identity map):
class TestQuery(unittest.TestCase):
    def setUp(self):
        self.engine = create_engine('sqlite:///:memory:')
        self.session = Session(self.engine)
        Base.metadata.create_all(self.engine)
        self.panel = Panel(1, 'ion torrent', 'start')
        self.session.add(self.panel)
        self.session.commit()

    def tearDown(self):
        Base.metadata.drop_all(self.engine)

    def test_query_panel(self):
        expected = [self.panel]
        result = self.session.query(Panel).all()
        self.assertEqual(result, expected)

as far as the engine/session setup/teardown, I'd go for a pattern where you use a single engine for all tests, and assuming your schema is fixed, a single schema for all tests, then you make sure the data you work with is performed within a transaction that can be rolled back.   The Session can be made to work this way, such that calling commit() doesn't actually commit the ""real"" transaction, by wrapping the whole test within an explicit Transaction.  The example at https://docs.sqlalchemy.org/en/latest/orm/session_transaction.html#joining-a-session-into-an-external-transaction-such-as-for-test-suites illustrates this usage.    Having a "":memory:"" engine on every test fixture will take up a lot of memory and not really scale out to other databases besides SQLite.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/42681231,SQLAlchemy: unexpected results when using `and` and `or`,"I have a declarative base class News:

class News(Base):
    __tablename__ = ""news""
    id = Column(Integer, primary_key = True)
    title = Column(String)
    author = Column(String)
    url = Column(String)
    comments = Column(Integer)
    points = Column(Integer)
    label = Column(String)


I also have a function f(title), that gets a string and returns one of 3 variants of strings: 'good', 'maybe' or 'never'. 
I try to get filtered rows:

rows = s.query(News).filter(News.label == None and f(News.title) == 'good').all()


But the program fails, raising this error:

raise TypeError(""Boolean value of this clause is not defined"")


How can I resolve it? 
",25,32145,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/70104873,how to access relationships with async sqlalchemy?,"import asyncio

from sqlalchemy import Column
from sqlalchemy import DateTime
from sqlalchemy import ForeignKey
from sqlalchemy import func
from sqlalchemy import Integer
from sqlalchemy import String
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.ext.asyncio import create_async_engine
from sqlalchemy.future import select
from sqlalchemy.orm import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.orm import selectinload
from sqlalchemy.orm import sessionmaker

engine = create_async_engine(
        ""postgresql+asyncpg://user:pass@localhost/db"",
        echo=True,
    )


# expire_on_commit=False will prevent attributes from being expired
# after commit.
async_session = sessionmaker(
    engine, expire_on_commit=False, class_=AsyncSession
)


Base = declarative_base()

class A(Base):
    __tablename__ = ""a""

    id = Column(Integer, primary_key=True)
    name = Column(String, unique=True)
    data = Column(String)
    create_date = Column(DateTime, server_default=func.now())
    bs = relationship(""B"")

    # required in order to access columns with server defaults
    # or SQL expression defaults, subsequent to a flush, without
    # triggering an expired load
    __mapper_args__ = {""eager_defaults"": True}


class B(Base):
    __tablename__ = ""b""
    id = Column(Integer, primary_key=True)
    a_id = Column(ForeignKey(""a.id""))
    data = Column(String)
    
    
       

async with engine.begin() as conn:
    await conn.run_sync(Base.metadata.drop_all)
    await conn.run_sync(Base.metadata.create_all)


async with async_session() as session:
    async with session.begin():
        session.add_all(
            [
                A(bs=[B(), B()], data=""a1""),
                A(bs=[B()], data=""a2""),
            ]
        )


async with async_session() as session:
    result = await session.execute(select(A).order_by(A.id))
    a1 = result.scalars().first()

    # no issue: 
    print(a1.name, a1.data)

    # throws error:
    print(a1.bs)
    

Trying to access a1.bs  gives this error:
     59     current = greenlet.getcurrent()
     60     if not isinstance(current, _AsyncIoGreenlet):
---&gt; 61         raise exc.MissingGreenlet(
     62             ""greenlet_spawn has not been called; can't call await_() here. ""
     63             ""Was IO attempted in an unexpected place?""

MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/14/xd2s)



",18,11578,"This is how:
from sqlalchemy.orm import selectinload

async with async_session() as session:
    result = await session.execute(select(A).order_by(A.id)
                                            .options(selectinload(A.bs)))
    a = result.scalars().first()

    print(a.bs)


key is using the selectinload method to prevent implicit IO
UPDATE
There are a few alternatives to selectinload like joinedload, lazyload. I am still trying to understand the differences.
","muons answer is correct if you want eager loading (which is better).
But if for some reason you already have loaded your model and later want to load a relationship, there is a way starting with SQLAlchemy 2.0.4:
Using session.refresh, you can tell it to load a1.bs:
await session.refresh(a1, attribute_names=[""bs""])
print(a1.bs)  # This works

From the docs:

New in version 2.0.4: Added support for AsyncSession.refresh() and the underlying Session.refresh() method to force lazy-loaded relationships to load, if they are named explicitly in the Session.refresh.attribute_names parameter.

","There is an additional way of accessing the relationship attributes in SQLAlchemy v2, for the case where you have already loaded your model and later want to load a relationship.
As the documentation says, you can use AsyncAttrs: ""when added to a specific class or more generally to the Declarative Base superclass, provides an accessor AsyncAttrs.awaitable_attrs which delivers any attribute as an awaitable"". So you can create a Declarative Base class like this:
class Base(AsyncAttrs, DeclarativeBase):
    pass

And then use awaitable_attrs to access the relationship that must be previously loaded:
a1_bs = await a1.awaitable_attrs.bs
print(a1_bs)

"
SQLAlchemy unexpected result,https://stackoverflow.com/questions/40681371,Bug in SQLAlchemy Rollback after DB Exception?,,18,1040,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/68195361,How to properly handle many to many in async sqlalchemy?,"I was trying to implement many to many relationship between tables.
When I use backpopulates all tags for a specific user must be in the tags field.
The tables are successfully created.
Users and tags are added.
Link table too.
import asyncio
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.util import await_only, greenlet_spawn

from sqlalchemy import Column, Table, ForeignKey
from sqlalchemy.orm import declarative_base, relationship
from sqlalchemy.dialects.postgresql import VARCHAR, INTEGER

Base = declarative_base()

user_tag = Table('user_tag', Base.metadata,
                 Column('user_id', INTEGER, ForeignKey('users.id')),
                 Column('tag_id', INTEGER, ForeignKey('tags.id'))
                 )


class User(Base):
    __tablename__ = 'users'
    id = Column(INTEGER, primary_key=True)
    name = Column(VARCHAR(32), nullable=False, unique=True)
    tags = relationship(""Tag"",
                        secondary=user_tag,
                        back_populates=""users"")


class Tag(Base):
    __tablename__ = 'tags'
    id = Column(INTEGER, primary_key=True)
    tag = Column(VARCHAR(255), nullable=False, unique=True)
    users = relationship(""User"",
                         secondary=user_tag,
                         back_populates=""tags"")


async def main():
    engine = create_async_engine(
        ""postgresql+asyncpg://postgres:pgs12345@localhost/test"",
        echo=False,
    )

    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)
        await conn.run_sync(Base.metadata.create_all)

    users = [User(name=""p1""), User(name=""p2""), User(name=""p3"")]
    tags = [Tag(tag=""tag1""), Tag(tag=""tag2""), Tag(tag=""tag3"")]

    async with AsyncSession(engine) as session:
        async with session.begin():
            session.add_all(users)
            session.add_all(tags)

        for user in users:
            await session.refresh(user)
        for tag in tags:
            await session.refresh(tag)

        for user in users:
            for i in range(3, user.id - 1, -1):
                await session.execute(user_tag.insert().values(user_id=user.id, tag_id=i))
        await session.commit()

        for user in users:
            await session.refresh(user)
        for tag in tags:
            await session.refresh(tag)

        tags = await greenlet_spawn(users[0].tags)
        print(tags)


loop = asyncio.get_event_loop()
loop.run_until_complete(main())

When I run the program, it crashes with:
 File ""C:\Sources\asyncSQLAl test\main.py"", line 48, in &lt;module&gt;
    loop.run_until_complete(main())
  File ""C:\Users\Stanislav\AppData\Local\Programs\Python\Python39\lib\asyncio\base_events.py"", line 
642, in run_until_complete
    return future.result()
  File ""C:\Sources\asyncSQLAl test\main.py"", line 41, in main
    tags = await greenlet_spawn(await users[0].tags)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\attributes.py"", line 480, in __get__
    return self.impl.get(state, dict_)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\attributes.py"", line 931, in get
    value = self.callable_(state, passive)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\strategies.py"", line 879, in _load_for_state
    return self._emit_lazyload(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\strategies.py"", line 1036, 
in _emit_lazyload
    result = session.execute(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1689, in 
execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1582, in 
_execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\sql\lambdas.py"", line 481, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1451, in 
_execute_clauseelement
    ret = self._execute_context(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1813, in 
_execute_context
    self._handle_dbapi_exception(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1998, in 
_handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1770, in 
_execute_context
    self.dialect.do_execute(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 717, in do_execute
    cursor.execute(statement, parameters)
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py"", line 449, in execute
    self._adapt_connection.await_(
  File ""C:\Sources\asyncSQLAl test\venv\lib\site-packages\sqlalchemy\util\_concurrency_py3k.py"", line 60, in await_only
    raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)      
sys:1: RuntimeWarning: coroutine 'AsyncAdapt_asyncpg_cursor._prepare_and_execute' was never awaited

I don't quite understand how greenlet_spawn works here and where it should be used in this example.
For example, same program, but in sync style
from sqlalchemy import Column, Table, ForeignKey
from sqlalchemy.orm import declarative_base, relationship, sessionmaker
from sqlalchemy import create_engine
from sqlalchemy.dialects.postgresql import VARCHAR, INTEGER

Base = declarative_base()

user_tag = Table('user_tag', Base.metadata,
                 Column('user_id', INTEGER, ForeignKey('users.id')),
                 Column('tag_id', INTEGER, ForeignKey('tags.id'))
                 )


class User(Base):
    __tablename__ = 'users'
    id = Column(INTEGER, primary_key=True)
    name = Column(VARCHAR(32), nullable=False, unique=True)
    tags = relationship(""Tag"",
                        secondary=user_tag,
                        back_populates=""users"")


class Tag(Base):
    __tablename__ = 'tags'
    id = Column(INTEGER, primary_key=True)
    tag = Column(VARCHAR(255), nullable=False, unique=True)
    users = relationship(""User"",
                         secondary=user_tag,
                         back_populates=""tags"")
    
    def __str__(self):
        return self.tag


def main():
    engine = create_engine(
        ""postgresql+psycopg2://postgres:pgs12345@localhost/test"",
        echo=False,
    )

    Base.metadata.drop_all(engine)
    Base.metadata.create_all(engine)

    Session = sessionmaker(bind=engine)
    session = Session()

    users = [User(name=""p1""), User(name=""p2""), User(name=""p3"")]
    tags = [Tag(tag=""tag1""), Tag(tag=""tag2""), Tag(tag=""tag3"")]

    with session.begin():
        session.add_all(users)
        session.add_all(tags)

    for user in users:
        for i in range(3, user.id - 1, -1):
            session.execute(user_tag.insert().values(
                user_id=user.id, tag_id=i))
    session.commit()

    for tag in users[0].tags:
        print(tag, end="" "")

main()

Gives me:
tag1 tag2 tag3 

",8,7537,"I've been stuck on this today too and I've narrowed it down to the fact that a lazyload is attempted, which GreenLet is not happy about. I wasn't sure whether this was just my lack of skill but I've found this article that details some of the common errors:
https://matt.sh/sqlalchemy-the-async-ening, where it's mentioned that this very issue will occur in this way. Furthermore, the docs go into detail about needing to avoid lazyloading: https://docs.sqlalchemy.org/en/14/orm/extensions/asyncio.html.
My solution at the moment is to effectively prefetch the Child relation upon the initial query of the Parent object and then manipulate it from there. Whether this is a true bug, in the sense that it should work in async when it already works in sync or simply a limitation of the async method, I've no idea.
Edit 06/08/21, here is how I am prefetching relationships:
import sqlalchemy as sa
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload
from . import models

async def get_parent_prefetch_children(db: AsyncSession, parent_id: int) -&gt; models.Parent:
    result = await db.execute(
        sa.select(models.Parent).where(models.Parent.id == parent_id).options(
            selectinload(models.Parent.children)
        )
    )
    return result.scalar()

In your case, you call users[0].tags, which makes a lazyload and fails. In order for you to avoid this, you must refetch users with their tags eagerly loaded.
","From their official doc for relationship, you can use lazy=""joined"" to ask the SQLAlchemy do prefetch for you on session.refresh(...) or session.execute(select(...)). This will do joined style eagerly load at the time you query for the object. Note that this may introduce performance issue to your application since sometime you are not going to access the foreign'd objects while the database will still do joined style query for you.
tags = relationship(""Tag"",
    secondary=user_tag,
    back_populates=""users"",
    lazy=""joined"")

The results and SQL stmt looks like:
&gt;&gt;&gt;tags = users[0].tags
&gt;&gt;&gt;print(tags)
[&lt;__main__.Tag object at 0x10bc68610&gt;, &lt;__main__.Tag object at 0x10bc41d00&gt;, &lt;__main__.Tag object at 0x10bc12b80&gt;]
&gt;&gt;&gt;from sqlalchemy.future import select
&gt;&gt;&gt;print(select(User))
FROM users LEFT OUTER JOIN (user_tag AS user_tag_1 JOIN tags AS tags_1 ON tags_1.id = user_tag_1.tag_id) ON users.id = user_tag_1.user_id

",
SQLAlchemy unexpected result,https://stackoverflow.com/questions/67897872,Exception has occurred: MissingGreenlet whe making connection query in sql alchemy 1.4,"I'm trying to do a simple query with SQLAlchemy 1.4.17 from within pytest
def test_first():
    engine = create_engine(settings.SQLALCHEMY_DATABASE_URI)
    result = engine.execute(text(""SELECT email FROM user""))

but am getting this error
Exception has occurred: MissingGreenlet
greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)
  File ""/Users/mattc/Development/inference/server/inference_server/app/tests/test_01_user.py"", line 27, in test_first
    result = engine.execute(text(""SELECT email FROM user""))

and do not know why? Any suggestions?
",8,20905,"You are trying to use an async connector package in the same way as a synchronous connector
&gt;&gt;&gt; import sqlalchemy as sa
&gt;&gt;&gt; engine = sa.create_engine('postgresql+asyncpg:///')
&gt;&gt;&gt; res = engine.execute(sa.text('SELECT 1'))
&lt;stdin&gt;:1: RemovedIn20Warning: The Engine.execute() method is considered legacy as of the 1.x series of SQLAlchemy and will be removed in 2.0. All statement execution in SQLAlchemy 2.0 is performed by the Connection.execute() method of Connection, or in the ORM by the Session.execute() method of Session. (Background on SQLAlchemy 2.0 at: http://sqlalche.me/e/b8d9)
Traceback (most recent call last):
...
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)

You need to either use a synchronous connector, for example psycopg2, pg8000, or write async code:
import sqlalchemy as sa

import asyncio

from sqlalchemy.ext.asyncio import create_async_engine


async def async_main():
    engine = create_async_engine(
        ""postgresql+asyncpg:///test"", echo=True,
    )

    async with engine.connect() as conn:

        # select a Result, which will be delivered with buffered
        # results
        result = await conn.execute(sa.text('select email from users'))

        print(result.fetchall())
    await engine.dispose()


asyncio.run(async_main())

",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/71116549,sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) with PostgreSQL,"I searched for this error a lot, but I only find some with more information behind that like ""FATAL: ..."". Mine has none. It only says
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) 

I have a postgres database inside a docker container that has set his port to the standard 5432.
I've created the container with the following command:
docker run --name some-postgres -e POSTGRES_PASSWORD=mysecretpassword -p 5432:5432 -d postgres

It is clean so no database created. The API should automatically create them.
I'm using Pycharm IDE, maybe it has something todo with that.
Traceback (most recent call last):
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\protocols\http\h11_impl.py"", line 373, in run_asgi
    result = await app(self.scope, self.receive, self.send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\proxy_headers.py"", line 75, in __call__
    return await self.app(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\debug.py"", line 96, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\debug.py"", line 93, in __call__
    await self.app(scope, receive, inner_send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\applications.py"", line 208, in __call__
    await super().__call__(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\applications.py"", line 112, in __call__
    await self.middleware_stack(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\middleware\errors.py"", line 181, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\middleware\errors.py"", line 159, in __call__
    await self.app(scope, receive, _send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\exceptions.py"", line 82, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\exceptions.py"", line 71, in __call__
    await self.app(scope, receive, sender)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 580, in __call__
    await route.handle(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 241, in handle
    await self.app(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 52, in app
    response = await func(request)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\routing.py"", line 226, in app
    raw_response = await run_endpoint_function(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\routing.py"", line 159, in run_endpoint_function
    return await dependant.call(**values)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\app\routers\v1\users.py"", line 31, in create_user
    session.commit()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1428, in commit
    self._transaction.commit(_to_root=self.future)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 829, in commit
    self._prepare_impl()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 808, in _prepare_impl
    self.session.flush()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3339, in flush
    self._flush(objects)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3479, in _flush
    transaction.rollback(_capture_exception=True)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3439, in _flush
    flush_context.execute()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 209, in save_obj
    for (
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 370, in _organize_states_for_save
    for state, dict_, mapper, connection in _connections_for_states(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 1709, in _connections_for_states
    connection = uowtransaction.transaction.connection(base_mapper)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 626, in connection
    return self._connection_for_bind(bind, execution_options)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 735, in _connection_for_bind
    conn = self._parent._connection_for_bind(bind, execution_options)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 747, in _connection_for_bind
    conn = bind.connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\future\engine.py"", line 419, in connect
    return super(Engine, self).connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3194, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 96, in __init__
    else engine.raw_connection()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3273, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3243, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 2097, in _handle_dbapi_exception_noconnection
    util.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3240, in _wrap_pool_connect
    return fn()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 476, in checkout
    rec = pool._do_get()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\impl.py"", line 146, in _do_get
    self._dec_overflow()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\impl.py"", line 143, in _do_get
    return self._create_connection()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 256, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 371, in __init__
    self.__connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 666, in __connect
    pool.logger.debug(""Error on connect(): %s"", e)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\create.py"", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\psycopg2\__init__.py"", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) 
(Background on this error at: https://sqlalche.me/e/14/e3q8)

This is my error that i get. My code looks like this:
main.py:
import os
import uvicorn
if __name__ == '__main__':
    port = int(os.getenv(""PORT""))
    uvicorn.run(""main:app"", host='0.0.0.0', port=port, reload=True, debug=True, workers=3)

app/main.py:
import os
from fastapi import FastAPI
from .database import engine
from .routers import v1
engine.init_db()
port = int(os.getenv(""PORT"")) #Port is 8000
app = FastAPI()

app.include_router(v1.router, prefix=""/v1"")

app/database/engine.py: (referenced in the file above)
import os

from fastapi.security import HTTPBearer
from sqlmodel import create_engine, SQLModel, Session
DATABASE_URL = ""postgresql+psycopg2://postgres:mysecretpassword@localhost:5432""
engine = create_engine(DATABASE_URL, echo=True)
token_auth_scheme = HTTPBearer()


async def init_db():
    async with engine.begin() as conn:
        # await conn.run_sync(SQLModel.metadata.drop_all)
        await conn.run_sync(SQLModel.metadata.create_all)


async def get_session():
    session = Session(engine)
    try:
        yield session
    finally:
        session.close()

The route ""users"" inside routers/v1/users.py:
from typing import Optional

from fastapi import APIRouter, Depends, HTTPException, Query, Path, Response, status
from pydantic import ValidationError
from sqlalchemy.exc import IntegrityError
from sqlalchemy.sql.functions import concat
from sqlalchemy import func
from sqlmodel import Session, select, col
from starlette import status

from app.database import models
from app.database.authentication import VerifyToken
from app.database.engine import get_session, token_auth_scheme

router = APIRouter()


@router.post("""", status_code=status.HTTP_201_CREATED, response_model=models.UserRead,
             response_model_exclude_none=True, name=""Create User"", tags=[""users""])
async def create_user(user_data: models.UserCreate,
                      session: Session = Depends(get_session)):

    try:
        new_user = models.User(**dict(user_data))
        session.add(new_user)
        session.commit()
        session.refresh(new_user)

        return new_user
    except IntegrityError:
        session.rollback()
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT, detail=""IntegrityError"")
    except ValidationError:
        session.rollback()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, detail=""ValidationError"")

The models.User:
class UserBase(SQLModel):
    id: str
    username: Optional[str]
    country_code: Optional[str]
    phone: Optional[str]

    class Config:
        allow_population_by_field_name = True

class User(UserBase, table=True):
    __tablename__ = 'users'
    id: str = Field(primary_key=True)
    username: Optional[str] = Field(sa_column=Column('username', VARCHAR(length=50), unique=True, default=None))
    phone: Optional[str] = Field(sa_column=Column('phone', VARCHAR(length=20), unique=True, default=None))

I hope that this is everything you guys need to find something. If you need something else contact me.
Best regards
Colin
EDIT:
After I changed the link from `postgresql+psycopg2` to `postgresql+asyncpg` I get a new error:

ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\protocols\http\h11_impl.py"", line 373, in run_asgi
    result = await app(self.scope, self.receive, self.send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\proxy_headers.py"", line 75, in __call__
    return await self.app(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\debug.py"", line 96, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\uvicorn\middleware\debug.py"", line 93, in __call__
    await self.app(scope, receive, inner_send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\applications.py"", line 208, in __call__
    await super().__call__(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\applications.py"", line 112, in __call__
    await self.middleware_stack(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\middleware\errors.py"", line 181, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\middleware\errors.py"", line 159, in __call__
    await self.app(scope, receive, _send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\exceptions.py"", line 82, in __call__
    raise exc from None
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\exceptions.py"", line 71, in __call__
    await self.app(scope, receive, sender)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 580, in __call__
    await route.handle(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 241, in handle
    await self.app(scope, receive, send)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\starlette\routing.py"", line 52, in app
    response = await func(request)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\routing.py"", line 226, in app
    raw_response = await run_endpoint_function(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\fastapi\routing.py"", line 159, in run_endpoint_function
    return await dependant.call(**values)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\app\routers\v1\users.py"", line 26, in create_user
    session.commit()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1428, in commit
    self._transaction.commit(_to_root=self.future)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 829, in commit
    self._prepare_impl()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 808, in _prepare_impl
    self.session.flush()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3339, in flush
    self._flush(objects)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3479, in _flush
    transaction.rollback(_capture_exception=True)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3439, in _flush
    flush_context.execute()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 209, in save_obj
    for (
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 370, in _organize_states_for_save
    for state, dict_, mapper, connection in _connections_for_states(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 1709, in _connections_for_states
    connection = uowtransaction.transaction.connection(base_mapper)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 626, in connection
    return self._connection_for_bind(bind, execution_options)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 735, in _connection_for_bind
    conn = self._parent._connection_for_bind(bind, execution_options)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 747, in _connection_for_bind
    conn = bind.connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\future\engine.py"", line 419, in connect
    return super(Engine, self).connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3194, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 96, in __init__
    else engine.raw_connection()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3273, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 3240, in _wrap_pool_connect
    return fn()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 476, in checkout
    rec = pool._do_get()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\impl.py"", line 146, in _do_get
    self._dec_overflow()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\impl.py"", line 143, in _do_get
    return self._create_connection()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 256, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 371, in __init__
    self.__connect()
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 666, in __connect
    pool.logger.debug(""Error on connect(): %s"", e)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\pool\base.py"", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\create.py"", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py"", line 748, in connect
    await_only(self.asyncpg.connect(*arg, **kw)),
  File ""C:\Users\Veril\PycharmProjects\partyapp-python\venv\lib\site-packages\sqlalchemy\util\_concurrency_py3k.py"", line 61, in await_only
    raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/14/xd2s)

EDIT 2:
If I use my local postgres database in docker i get this error. If I use my extern database from Heroku it works perfectly fine!
EDIT 3:
So apparently it won't work with my local docker postgres database. Now I am using my production database over heroku (so externally) and everything works with psycopg2. I didn't found the error why it won't let me use my local db but whatever.
",4,15895,"Even though I can see that you are using default port, maybe this can help to somebody with the same problem.
For me the problem was in explicit port definition - as I'm running two different postgres DB and (both from containers), one of them I set to listen to 5433 port - and this gives me exact same problem, so instead
SQLALCHEMY_DATABASE_URL = ""postgresql://postgres:password@localhost/fastapi""

I just put:
SQLALCHEMY_DATABASE_URL = ""postgresql://postgres:password@localhost:5433/fastapi""

Problem solved right away.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/46977257,SQLAlchemy returns unexpected results with limit/offset,,4,7032,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/54836672,"Using a Celery worker to interact with a SQLAlchemy DB, including knowing the user from the request",,3,4997,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/29222426,Comparing Dates in Flask-SQLAlchemy,"I've been trying to compare dates in a query given to SQLALchemy as follows:  

start = time.strptime(start, ""%d%m%y"")
end = time.strptime(end, ""%d%m%y"")
list_ideas = Idea.query.filter(time &gt;= start, time &lt;= end).all()


However, this does not return results regardless of dates given (where Idea.time = db.Column(db.DateTime, default=db.func.now())). I've searched through some other answers regarding this topic and from what I have gathered, I am not making the same mistakes.

In addition, changing the query to Idea.query.filter(time &gt;= start, time &lt;= end, deleted=False).all() gives the error:  


  TypeError: filter() got an unexpected keyword argument 'deleted'


Any pointers would be appreciated.

--
EDIT: I noticed that I was using import time, which may have caused the error. However, after changing it to from time import strptime, I now experience the error:  


  NameError: global name 'time' is not defined

",3,7372,"Try to use the func.DATE() function:
from sqlalchemy import func

start = time.strptime(start, ""%d%m%y"")
end = time.strptime(end, ""%d%m%y"")
list_ideas = Idea.query.filter(func.DATE(time) &gt;= start, func.DATE(time) &lt;= end).all()

refer to similar issue.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/68212540,How to use marshmallow-sqlalchemy with async code?,"I'm trying to use marshmallow-sqlalchemy with aiohttp and I have followed their docs with the basic example and I'm getting an error.
I have this schema:
from marshmallow_sqlalchemy import SQLAlchemyAutoSchema

from db.customer import Customer

class CustomerSchema(SQLAlchemyAutoSchema):
    class Meta:
        model = Customer
        include_relationships = True
        load_instance = True


And then the following code for the query:
from sqlalchemy import select
from db import db_conn
from db.customer import Customer
from queries.schema import CustomerSchema

customer_schema = CustomerSchema()

async def get_all_users():
    async with db_conn.get_async_sa_session() as session:
        statement = select(Customer)
        results = await session.execute(statement)
        _ = (results.scalars().all())
        print(_)
        response = customer_schema.dump(_, many=True)
        print(response)

For the first print statement I'm getting
[&lt;db.customer.Customer object at 0x10a183340&gt;, &lt;db.customer.Customer object at 0x10a183940&gt;, &lt;db.customer.Customer object at 0x10b0cd9d0&gt;]

But then it fails with
File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/util/_concurrency_py3k.py"", line 60, in await_only
    raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)

So how can I use marshmallow-sqlalchemy to serialize the SqlAlchemy reponse?
Another options (packages, etc) or a generic custom solutions are OK too.
For the time being I'm using this:
statement = select(Customer)
results = await session.execute(statement)
_ = (results.scalars().all())
response = {}
for result in _:
    value = {k: (v if not isinstance(v, sqlalchemy.orm.state.InstanceState) else '_') for k, v in result.__dict__.items()}
    response[f'customer {value[""id""]}'] = value
return response

Full traceback:

Traceback (most recent call last):
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/aiohttp/web_protocol.py"", line 422, in _handle_request
    resp = await self._request_handler(request)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/aiohttp/web_app.py"", line 499, in _handle
    resp = await handler(request)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/aiohttp/web_urldispatcher.py"", line 948, in _iter
    resp = await method()
  File ""/Users/ruslan/OneDrive/Home/Dev/projects/code/education/other/cft/views/user.py"", line 24, in get
    await get_all_users()
  File ""/Users/ruslan/OneDrive/Home/Dev/projects/code/education/other/cft/queries/user.py"", line 18, in get_all_users
    response = customer_schema.dump(_, many=True)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 547, in dump
    result = self._serialize(processed_obj, many=many)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 509, in _serialize
    return [
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 510, in &lt;listcomp&gt;
    self._serialize(d, many=False)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 515, in _serialize
    value = field_obj.serialize(attr_name, obj, accessor=self.get_attribute)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/fields.py"", line 310, in serialize
    value = self.get_value(obj, attr, accessor=accessor)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow_sqlalchemy/fields.py"", line 27, in get_value
    return super(fields.List, self).get_value(obj, attr, accessor=accessor)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/fields.py"", line 239, in get_value
    return accessor_func(obj, check_key, default)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/schema.py"", line 472, in get_attribute
    return get_value(obj, attr, default)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/utils.py"", line 239, in get_value
    return _get_value_for_key(obj, key, default)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/marshmallow/utils.py"", line 253, in _get_value_for_key
    return getattr(obj, key, default)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/attributes.py"", line 480, in __get__
    return self.impl.get(state, dict_)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/attributes.py"", line 931, in get
    value = self.callable_(state, passive)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/strategies.py"", line 879, in _load_for_state
    return self._emit_lazyload(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/strategies.py"", line 1036, in _emit_lazyload
    result = session.execute(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 1689, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1582, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/sql/lambdas.py"", line 481, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1451, in _execute_clauseelement
    ret = self._execute_context(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1813, in _execute_context
    self._handle_dbapi_exception(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1998, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
    raise exception
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1770, in _execute_context
    self.dialect.do_execute(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/engine/default.py"", line 717, in do_execute
    cursor.execute(statement, parameters)
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py"", line 449, in execute
    self._adapt_connection.await_(
  File ""/Users/ruslan/.local/share/virtualenvs/cft-RKlbQ9iX/lib/python3.9/site-packages/sqlalchemy/util/_concurrency_py3k.py"", line 60, in await_only
    raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: http://sqlalche.me/e/14/xd2s)

",2,485,"The problem in this case is that the Marshmallow schema is configured to load related models (include_relationships=True).  Since the initial query doesn't load them automatically, the schema triggers a query to fetch them, and this causes the error.
The simplest solution, demonstrated in the docs, is to eagerly load the related objects with their ""parent"":
async def get_all_users():
    async with db_conn.get_async_sa_session() as session:

        # Let's assume a Customer has a 1 to many relationship with an Order model
        statement = select(Customer).options(orm.selectinload(Customer.orders))

        results = await session.execute(statement)
        _ = (results.scalars().all())
        print(_)
        response = customer_schema.dump(_, many=True)
        print(response)

There is more discussion in the Preventing Implicit IO when Using AsyncSession section of the docs.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/78010668,Docker fastapi unable to connect to mysql,"I'm new to docker and fastapi and I'm trying to connect fastapi with mysql service but it always say that it cannot connect to mysql in the logs. What am I missing here?
.env file
API_PATH='api'
API_VERSION='v1'
FASTAPI_CONTAINER_NAME='app_fastapi'
FASTAPI_PORT=8000
REDIS_CONTAINER_NAME='app_redis'
REDIS_PORT=8001
MYSQL_CONTAINER_NAME='app_mysql'
MYSQL_DATABASE_NAME='fastapi_db'
MYSQL_ROOT_PASSWORD='p@ssw0rD'
MYSQL_USER='fastapi_mysql_admin'
MYSQL_USER_PASSWORD='p@ssw0rD'
MYSQL_HOST='mysql'
MYSQL_PORT=3306

docker-compose file
version: ""3.9""  # Specify a compatible Docker Compose version

services:
  fastapi:
    build:
      context: ./fastapi  # Build your FastAPI app
      dockerfile: Dockerfile  # Use your app's Dockerfile
    container_name: ${FASTAPI_CONTAINER_NAME}
    ports:
      - ""${FASTAPI_PORT}:${FASTAPI_PORT}""  # Expose port 8000 for the FastAPI app
    volumes:
      - .:/app
    env_file:
      - .env
    depends_on:
      mysql:  # Ensure mysql starts first
        condition: service_healthy
      redis: # Ensure redis starts first
        condition: service_healthy
    restart: on-failure
    tty: true
    networks:
      - backend

  mysql:
    build:
      context: ./mysql  # Build from the current directory
      dockerfile: Dockerfile  # Use the specific Dockerfile
    container_name: ${MYSQL_CONTAINER_NAME}
    ports:
      - ""${MYSQL_PORT}:${MYSQL_PORT}""  # Expose port 3306 for database access
    environment:
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_PASSWORD=${MYSQL_USER_PASSWORD}
      - MYSQL_DATABASE=${MYSQL_DATABASE_NAME}
    healthcheck:
      test: [""CMD"", ""mysqladmin"", ""ping"", ""-h"", ""localhost"", ""-p${MYSQL_USER_PASSWORD}""]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 7s
    tty: true
    restart: on-failure
    networks:
      - backend
  
  redis:
    build:
      context: ./redis  # Build from the current directory
      dockerfile: Dockerfile  # Use the specific Dockerfile
    container_name: ${REDIS_CONTAINER_NAME}
    ports:
      - ""${REDIS_PORT}:${REDIS_PORT}""  # Expose Redis port
    healthcheck:
      test: [""CMD"", ""redis-cli"", ""ping""]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 7s
    tty: true
    restart: on-failure
    networks:
      - backend

networks:
  backend:

database.py
import sqlalchemy
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.exc import OperationalError
from sqlalchemy.orm import sessionmaker
from os import environ as env

user = env['MYSQL_USER']
password = env['MYSQL_USER_PASSWORD']
host = env['MYSQL_HOST']
port = env['MYSQL_PORT']
database = env['MYSQL_DATABASE_NAME']

DATABASE_URL = ""mysql+pymysql://{}@{}:{}/{}"".format(
    user,
    host,
    port,
    database
)

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

main.py
from fastapi import FastAPI
from os import environ as env
from app.modules.todo import router as TodoRouter
app = FastAPI()
apiPath = '/' + env['API_PATH'] + '/' + env['API_VERSION']
# routers
app.include_router(TodoRouter.router, prefix=apiPath + '/todo', tags=['Todo'])

todo/router.py
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from app.database import get_db
from app.modules.todo.data_models import TodoRequestModel

router = APIRouter()

@router.post('/')
def create(todo: TodoRequestModel, db: Session = Depends(get_db)):
    return TodoController.create(todo, db)

On docker start


Docker container log when /post endpoint is hit
- INFO:     172.31.0.1:37760 - ""POST /api/v1/todo/ HTTP/1.1"" 500 Internal Server Error
- ERROR:    Exception in ASGI application
- Traceback (most recent call last):
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 796, in _connect
-     self._get_server_information()
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 994, in _get_server_information
-     self.server_charset = charset_by_id(lang).name
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/charset.py"", line 34, in by_id
-     return self._by_id[id]
- KeyError: 255
- 
- During handling of the above exception, another exception occurred:
- 
- Traceback (most recent call last):
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3250, in _wrap_pool_connect
-     return fn()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 310, in connect
-     return _ConnectionFairy._checkout(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 868, in _checkout
-     fairy = _ConnectionRecord.checkout(pool)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 476, in checkout
-     rec = pool._do_get()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 145, in _do_get
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 143, in _do_get
-     return self._create_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 256, in _create_connection
-     return _ConnectionRecord(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 371, in __init__
-     self.__connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 665, in __connect
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 661, in __connect
-     self.dbapi_connection = connection = pool._invoke_creator(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/create.py"", line 590, in connect
-     return dialect.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py"", line 597, in connect
-     return self.dbapi.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/__init__.py"", line 88, in Connect
-     return Connection(*args, **kwargs)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 634, in __init__
-     self._connect()
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 817, in _connect
-     raise OperationalError(
- pymysql.err.OperationalError: (2003, ""Can't connect to MySQL server on 'mysql' (255)"")
- 
- The above exception was the direct cause of the following exception:
- 
- Traceback (most recent call last):
-   File ""/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py"", line 404, in run_asgi
-     result = await app(  # type: ignore[func-returns-value]
-   File ""/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py"", line 84, in __call__
-     return await self.app(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/applications.py"", line 1054, in __call__
-     await super().__call__(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/applications.py"", line 123, in __call__
-     await self.middleware_stack(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 186, in __call__
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 164, in __call__
-     await self.app(scope, receive, _send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py"", line 62, in __call__
-     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 64, in wrapped_app
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
-     await app(scope, receive, sender)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 762, in __call__
-     await self.middleware_stack(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 782, in app
-     await route.handle(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 297, in handle
-     await self.app(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 77, in app
-     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 64, in wrapped_app
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
-     await app(scope, receive, sender)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 72, in app
-     response = await func(request)
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 299, in app
-     raise e
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 294, in app
-     raw_response = await run_endpoint_function(
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 193, in run_endpoint_function
-     return await run_in_threadpool(dependant.call, **values)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/concurrency.py"", line 40, in run_in_threadpool
-     return await anyio.to_thread.run_sync(func, *args)
-   File ""/usr/local/lib/python3.10/site-packages/anyio/to_thread.py"", line 56, in run_sync
-     return await get_async_backend().run_sync_in_worker_thread(
-   File ""/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 2134, in run_sync_in_worker_thread
-     return await future
-   File ""/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 851, in run
-     result = context.run(func, *args)
-   File ""/app/fastapi/app/modules/todo/router.py"", line 23, in create
-     return TodoController.create(todo, db)
-   File ""/app/fastapi/app/modules/todo/controller.py"", line 17, in create
-     return TodoService.create(student, db)
-   File ""/app/fastapi/app/modules/todo/repository.py"", line 62, in create
-     db.commit()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 1431, in commit
-     self._transaction.commit(_to_root=self.future)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 829, in commit
-     self._prepare_impl()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 808, in _prepare_impl
-     self.session.flush()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3363, in flush
-     self._flush(objects)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3502, in _flush
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3463, in _flush
-     flush_context.execute()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
-     rec.execute(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
-     util.preloaded.orm_persistence.save_obj(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 211, in save_obj
-     for (
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 372, in _organize_states_for_save
-     for state, dict_, mapper, connection in _connections_for_states(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 1711, in _connections_for_states
-     connection = uowtransaction.transaction.connection(base_mapper)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 626, in connection
-     return self._connection_for_bind(bind, execution_options)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 735, in _connection_for_bind
-     conn = self._parent._connection_for_bind(bind, execution_options)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 747, in _connection_for_bind
-     conn = bind.connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3204, in connect
-     return self._connection_cls(self, close_with_result=close_with_result)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 96, in __init__
-     else engine.raw_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3283, in raw_connection
-     return self._wrap_pool_connect(self.pool.connect, _connection)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3253, in _wrap_pool_connect
-     Connection._handle_dbapi_exception_noconnection(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 2100, in _handle_dbapi_exception_noconnection
-     util.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3250, in _wrap_pool_connect
-     return fn()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 310, in connect
-     return _ConnectionFairy._checkout(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 868, in _checkout
-     fairy = _ConnectionRecord.checkout(pool)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 476, in checkout
-     rec = pool._do_get()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 145, in _do_get
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 143, in _do_get
-     return self._create_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 256, in _create_connection
-     return _ConnectionRecord(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 371, in __init__
-     self.__connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 665, in __connect
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 661, in __connect
-     self.dbapi_connection = connection = pool._invoke_creator(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/create.py"", line 590, in connect
-     return dialect.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py"", line 597, in connect
-     return self.dbapi.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/__init__.py"", line 88, in Connect
-     return Connection(*args, **kwargs)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 634, in __init__
-     self._connect()
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/connections.py"", line 817, in _connect
-     raise OperationalError(
- sqlalchemy.exc.OperationalError: (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'mysql' (255)"")
- (Background on this error at: https://sqlalche.me/e/14/e3q8)

I've tried adding a password in the database connection string but gives me a different error like the following
DATABASE_URL = ""mysql+pymysql://{}:{}@{}:{}/{}"".format(
        user,
        password,
        host,
        port,
        database
    )

new log
- INFO:     172.31.0.1:52082 - ""POST /api/v1/todo/ HTTP/1.1"" 500 Internal Server Error
- ERROR:    Exception in ASGI application
- Traceback (most recent call last):
-   File ""/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py"", line 404, in run_asgi
-     result = await app(  # type: ignore[func-returns-value]
-   File ""/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py"", line 84, in __call__
-     return await self.app(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/applications.py"", line 1054, in __call__
-     await super().__call__(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/applications.py"", line 123, in __call__
-     await self.middleware_stack(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 186, in __call__
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 164, in __call__
-     await self.app(scope, receive, _send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py"", line 62, in __call__
-     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 64, in wrapped_app
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
-     await app(scope, receive, sender)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 762, in __call__
-     await self.middleware_stack(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 782, in app
-     await route.handle(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 297, in handle
-     await self.app(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 77, in app
-     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 64, in wrapped_app
-     raise exc
-   File ""/usr/local/lib/python3.10/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
-     await app(scope, receive, sender)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 72, in app
-     response = await func(request)
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 299, in app
-     raise e
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 294, in app
-     raw_response = await run_endpoint_function(
-   File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 193, in run_endpoint_function
-     return await run_in_threadpool(dependant.call, **values)
-   File ""/usr/local/lib/python3.10/site-packages/starlette/concurrency.py"", line 40, in run_in_threadpool
-     return await anyio.to_thread.run_sync(func, *args)
-   File ""/usr/local/lib/python3.10/site-packages/anyio/to_thread.py"", line 56, in run_sync
-     return await get_async_backend().run_sync_in_worker_thread(
-   File ""/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 2134, in run_sync_in_worker_thread
-     return await future
-   File ""/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 851, in run
-     result = context.run(func, *args)
-   File ""/app/fastapi/app/modules/todo/router.py"", line 23, in create
-     return TodoController.create(todo, db)
-   File ""/app/fastapi/app/modules/todo/controller.py"", line 17, in create
-     return TodoService.create(student, db)
-   File ""/app/fastapi/app/modules/todo/repository.py"", line 62, in create
-     db.commit()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 1431, in commit
-     self._transaction.commit(_to_root=self.future)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 829, in commit
-     self._prepare_impl()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 808, in _prepare_impl
-     self.session.flush()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3363, in flush
-     self._flush(objects)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3502, in _flush
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3463, in _flush
-     flush_context.execute()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
-     rec.execute(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
-     util.preloaded.orm_persistence.save_obj(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 211, in save_obj
-     for (
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 372, in _organize_states_for_save
-     for state, dict_, mapper, connection in _connections_for_states(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 1711, in _connections_for_states
-     connection = uowtransaction.transaction.connection(base_mapper)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 626, in connection
-     return self._connection_for_bind(bind, execution_options)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 735, in _connection_for_bind
-     conn = self._parent._connection_for_bind(bind, execution_options)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 747, in _connection_for_bind
-     conn = bind.connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3204, in connect
-     return self._connection_cls(self, close_with_result=close_with_result)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 96, in __init__
-     else engine.raw_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3283, in raw_connection
-     return self._wrap_pool_connect(self.pool.connect, _connection)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 3250, in _wrap_pool_connect
-     return fn()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 310, in connect
-     return _ConnectionFairy._checkout(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 868, in _checkout
-     fairy = _ConnectionRecord.checkout(pool)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 476, in checkout
-     rec = pool._do_get()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 145, in _do_get
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py"", line 143, in _do_get
-     return self._create_connection()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 256, in _create_connection
-     return _ConnectionRecord(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 371, in __init__
-     self.__connect()
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 665, in __connect
-     with util.safe_reraise():
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
-     compat.raise_(
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
-     raise exception
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/pool/base.py"", line 661, in __connect
-     self.dbapi_connection = connection = pool._invoke_creator(self)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/create.py"", line 590, in connect
-     return dialect.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py"", line 597, in connect
-     return self.dbapi.connect(*cargs, **cparams)
-   File ""/usr/local/lib/python3.10/site-packages/pymysql/__init__.py"", line 88, in Connect
-     return Connection(*args, **kwargs)
- TypeError: Connection.__init__() got an unexpected keyword argument 'password'

",1,47,"First, any container that wants to read data from an env file must have an env_file section and the path of the env file must be specified inside it. So set this for mysql and redis services as well.
version: ""3.9""
services:
  mysql:
    env_file:
      - .env
  redis:
    env_file:
      - .env

Second, in the mysql service inside docker compose, you read the name of the container from the env file, and inside the env file, you have set this value equal to app_mysql.
MYSQL_CONTAINER_NAME='app_mysql'

Since routing in the docker compose network is something like DNS, you must set the value of MYSQL_HOST equal to the name of the mysql container so that your fastapi service can connect to the mysql container.
MYSQL_HOST='app_mysql'

",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/77429344,How use SQL Expressions in @property decorator for async session in SQLAlchemy?,"I've got two models Media and Tweet.
Unfortunately, they don't have a one-to-many relationship. The specifics of the API are: one router sends images (Media), and the other router in the body receives a ids list of images that will be attached to the tweet.
I would like to set a attachments field in the Tweet model that would contain a list of paths of attached images, so that later I can simply validate using pydantic schemas.
class Media(Base):
    __tablename__ = 'medias'
    id: Mapped[int] = mapped_column(Sequence('media_id_seq'), primary_key=True, index=True)
    path: Mapped[str] = mapped_column(nullable=False)
    ....

class Tweet(Base):
    __tablename__ = 'tweets'
    id: Mapped[int] = mapped_column(Sequence('tweet_id_seq'), primary_key=True, index=True)
    content: Mapped[str] = mapped_column(TEXT, nullable=False)
 
    tweet_media_ids: Mapped[list[int]] = mapped_column(ARRAY(Integer), nullable=True)
  
    @property
    def attachments(self):
        if not self.tweet_media_ids:
            return []
        query = select(Media.path).where(Media.id.in_(self.tweet_media_ids))
        result = object_session(self).execute(query)
        return result.scalars().all()

I've tried to get a tweet and a list of appended media in attachments, but got  *sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/20/xd2s)* while reffering to item.attachments
`@router.get(""/tweets"", response_model=schemas.TweetsList)
async def get_tweets_list(db: Annotated[AsyncSession, Depends(get_db_session)]):
    tweets_orm = await get_tweets(db)
    for item in tweets_orm:
        print(""att="", item.attachments)
    tweets = [schemas.Tweet.model_validate(tweet) for tweet in tweets_orm]
    tweets_list = schemas.TweetsList(tweets=tweets)
    return tweets_list`

172.18.0.3:49338 - ""GET /api/tweets HTTP/1.0"" 500
 [2023-11-06 05:43:43 +0000] [7] [ERROR] Exception in ASGI application
 Traceback (most recent call last):
 File ""/usr/local/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py"", line 426, in run_asgi
 result = await app(  # type: ignore[func-returns-value]
 File ""/usr/local/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py"", line 84, in __call__
 return await self.app(scope, receive, send)
 172.18.0.1 - - [06/Nov/2023:05:43:43 +0000] ""GET /api/tweets HTTP/1.1"" 500 21 ""http://localhost/"" ""Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/119.0"" ""-""
 File ""/usr/local/lib/python3.10/site-packages/fastapi/applications.py"", line 292, in __call__
 await super().__call__(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/applications.py"", line 122, in __call__
 await self.middleware_stack(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 184, in __call__
 raise exc
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/errors.py"", line 162, in __call__
 await self.app(scope, receive, _send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/cors.py"", line 83, in __call__
 await self.app(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py"", line 79, in __call__
 raise exc
 File ""/usr/local/lib/python3.10/site-packages/starlette/middleware/exceptions.py"", line 68, in __call__
 await self.app(scope, receive, sender)
 File ""/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py"", line 20, in __call__
 raise e
 File ""/usr/local/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py"", line 17, in __call__
 await self.app(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 718, in __call__
 await route.handle(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 276, in handle
 await self.app(scope, receive, send)
 File ""/usr/local/lib/python3.10/site-packages/starlette/routing.py"", line 66, in app
 response = await func(request)
 File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 273, in app
 raw_response = await run_endpoint_function(
 File ""/usr/local/lib/python3.10/site-packages/fastapi/routing.py"", line 190, in run_endpoint_function
 return await dependant.call(**values)
 File ""/api/app/routes/tweets.py"", line 66, in get_tweets_list
 print(""att="", item.attachments)
 File ""/api/app/models.py"", line 79, in attachments
 result = object_session(self).execute(query)
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 2306, in execute
 return self._execute_internal(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 2188, in _execute_internal
 result: Result[Any] = compile_state_cls.orm_execute_statement(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/context.py"", line 293, in orm_execute_statement
 result = conn.execute(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1416, in execute
 return meth(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/sql/elements.py"", line 516, in _execute_on_connection
 return connection._execute_clauseelement(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1639, in _execute_clauseelement
 ret = self._execute_context(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1848, in _execute_context
 return self._exec_single_context(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1988, in _exec_single_context
 self._handle_dbapi_exception(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 2346, in _handle_dbapi_exception
 raise exc_info[1].with_traceback(exc_info[2])
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1969, in _exec_single_context
 self.dialect.do_execute(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py"", line 922, in do_execute
 cursor.execute(statement, parameters)
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py"", line 586, in execute
 self._adapt_connection.await_(
 File ""/usr/local/lib/python3.10/site-packages/sqlalchemy/util/_concurrency_py3k.py"", line 116, in await_only
 raise exc.MissingGreenlet(
 sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/20/xd2s)

Question:
how to fix the error or advise another implementation of the attachments field
",1,210,"Use hybrid_property instead of property decorator.
from sqlalchemy.ext.hybrid import hybrid_property  # add this line


class Tweet(Base):
    __tablename__ = 'tweets'
    id: Mapped[int] = mapped_column(Sequence('tweet_id_seq'), primary_key=True, index=True)
    content: Mapped[str] = mapped_column(TEXT, nullable=False)
 
    tweet_media_ids: Mapped[list[int]] = mapped_column(ARRAY(Integer), nullable=True)
  
    @hybrid_property
    def attachments(self):
        if not self.tweet_media_ids:
            return []
        query = select(Media.path).where(Media.id.in_(self.tweet_media_ids))
        result = object_session(self).execute(query)
        return result.scalars().all()

sqlalchemy.exc.MissingGreenlet raised by relations might be resolved using hybrid_property in your case. The error occurs when manipulating join commands in async functions with bad relationships, including lazy loading strategies.
In addition, SQLAlchemy doesn't have to have physical relationships. So you can use relationship in your code even if two tables do not have actual relations.
With that:
class Tweet(Base):
    __tablename__ = 'tweets'
    id: Mapped[int] = mapped_column(Sequence('tweet_id_seq'), primary_key=True, index=True)
    content: Mapped[str] = mapped_column(TEXT, nullable=False)
 
    medias: Mapped[list[Media]] = relationship(back_populates=""medias"")
  
    @hybrid_property
    def attachments(self) -&gt; list[Media]:
        return self.medias

SQLAlchemy will generate a query with the relationship automatically. In fact, when setting logical relationships, you don't need to write the attachments method.
Or use awaitable_attrs
You are using AsyncSession so that you can use awaitable_attrs
@router.get(""/tweets"", response_model=schemas.TweetsList)
async def get_tweets_list(db: Annotated[AsyncSession, Depends(get_db_session)]):
    tweets_orm = await get_tweets(db)
    for item in tweets_orm:
        print(""att="", item.awaitable_attrs.attachments)
    tweets = [schemas.Tweet.model_validate(tweet) for tweet in tweets_orm]
    tweets_list = schemas.TweetsList(tweets=tweets)
    return tweets_list

awaitable_attrs resolves deferred columns that are not loaded yet. It can await result = await object_session(self).execute(query) in method attachments.
","    async def get_tweets_list(
        db: Annotated[AsyncSession, Depends(get_db_session)]
):
    tweets_orm = await get_tweets(db)
    for item in tweets_orm:
        print(""att="", await item.awaitable_attrs.attachments)
    tweets = [schemas.Tweet.model_validate(tweet) for tweet in tweets_orm]

It seems item.attachments return coroutine.
await item.awaitable_attrs.attachments return desired list.
So I guess an addition hit to DB is needed. And those query in property decorator does not execute
",
SQLAlchemy unexpected result,https://stackoverflow.com/questions/69466354,MLflow S3UploadFailedError: Failed to upload,"I've created with docker a MinioS3 artifact storage and a MySQL backend storage using the next Docker Compose:
    version: '3.8'
    services:
        db:
           environment:
              - MYSQL_DATABASE=${MYSQL_DATABASE}
              - MYSQL_USER=${MYSQL_USER}
              - MYSQL_PASSWORD=${MYSQL_PASSWORD}
              - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
           expose:
              - '3306'        
           volumes:
              - '(path)/server_backend:/var/lib/mysql '
           image: 'mysql'
           container_name: db

        storage:
            environment:
                - MINIO_ACCESS_KEY=${MINIO_USR}
                - MINIO_SECRET_KEY=${MINIO_PASS}
            expose:
                - '9000'
            ports:
                - '9000:9000'        
            depends_on:
                - db
            command: server /data
            volumes:
                - '(path)/server_artifact:/data'
            image: minio/minio:RELEASE.2021-02-14T04-01-33Z
            container_name: MinIO

        mlflow:
            build: ./mlflow
            environment:
                - AWS_ACCESS_KEY_ID=${MINIO_USR}
                - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       
            expose:
                - '5000'
            ports:
                - '5000:5000'
            depends_on:
                - storage                       
            image: 'mlflow:Dockerfile'
            container_name: server

The Mlflow server docker was created using the next Dockerfile:
    FROM python:3.8-slim-buster
    WORKDIR /usr/src/app
    RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql
    ENV MLFLOW_S3_ENDPOINT_URL=http://storage:9000
    CMD mlflow server \
        --backend-store-uri mysql+pymysql://MLFLOW:temporal@db:3306/DBMLFLOW \
        --default-artifact-root s3://artifacts \
        --host 0.0.0.0

The credantials are defined in a .env file.
The results of the docker-compose up command:

    [+] Running 21/22
     - mlflow Error                                                                                                                              5.6s
     - storage Pulled                                                                                                                           36.9s
       - a6b97b4963f5 Pull complete                                                                                                             24.6s
       - 13948a011eec Pull complete                                                                                                             24.7s
       - 40cdef9976a6 Pull complete                                                                                                             24.7s
       - f47162848743 Pull complete                                                                                                             24.8s
       - 5f2758d8e94c Pull complete                                                                                                             24.9s
       - c2950439edb8 Pull complete                                                                                                             25.0s
       - 1b08f8a15998 Pull complete                                                                                                             30.7s
     - db Pulled                                                                                                                                45.8s
       - 07aded7c29c6 Already exists                                                                                                             0.0s
       - f68b8cbd22de Pull complete                                                                                                              0.7s
       - 30c1754a28c4 Pull complete                                                                                                              2.1s
       - 1b7cb4d6fe05 Pull complete                                                                                                              2.2s
       - 79a41dc56b9a Pull complete                                                                                                              2.3s
       - 00a75e3842fb Pull complete                                                                                                              6.7s
       - b36a6919c217 Pull complete                                                                                                              6.8s
       - 635b0b84d686 Pull complete                                                                                                              6.8s
       - 6d24c7242d02 Pull complete                                                                                                             39.4s
       - 5be6c5edf16f Pull complete                                                                                                             39.5s
       - cb35eac1242c Pull complete                                                                                                             39.5s
       - a573d4e1c407 Pull complete                                                                                                             39.6s
    [+] Building 1.4s (7/7) FINISHED
     =&gt; [internal] load build definition from Dockerfile                                                                                         0.0s
     =&gt; =&gt; transferring dockerfile: 32B                                                                                                          0.0s
     =&gt; [internal] load .dockerignore                                                                                                            0.0s
     =&gt; =&gt; transferring context: 2B                                                                                                              0.0s
     =&gt; [internal] load metadata for docker.io/library/python:3.8-slim-buster                                                                    1.3s
     =&gt; [1/3] FROM docker.io/library/python:3.8-slim-buster@sha256:13a3f2bffb4b18ff7eda2763a3b0ba316dd82e548f52ea8b4fd11c94b97afa7d              0.0s
     =&gt; CACHED [2/3] WORKDIR /usr/src/app                                                                                                        0.0s
     =&gt; CACHED [3/3] RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql                                                           0.0s
     =&gt; exporting to image                                                                                                                       0.0s
     =&gt; =&gt; exporting layers                                                                                                                      0.0s
     =&gt; =&gt; writing image sha256:76d4e4462b5c7c1826734e59a54488b56660de0dd5ecc188c308202608a8f20b                                                 0.0s
     =&gt; =&gt; naming to docker.io/library/mlflow:Dockerfile                                                                                         0.0s
    
    Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them
    [+] Running 3/3
     - Container db  Created                                                                                                       0.5s
     - Container MinIO      Created                                                                                                       0.1s
     - Container server     Created                                                                                                       0.1s
    Attaching to server, MinIO, db
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Initializing database files
    db  | 2021-10-06T12:12:57.679527Z 0 [System] [MY-013169] [Server] /usr/sbin/mysqld (mysqld 8.0.26) initializing of server in progress as process 44
    db  | 2021-10-06T12:12:57.687748Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
    db  | 2021-10-06T12:12:58.230036Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
    db  | 2021-10-06T12:12:59.888820Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main
    db  | 2021-10-06T12:12:59.889102Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main
    db  | 2021-10-06T12:12:59.997461Z 6 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.
    MinIO      | Attempting encryption of all config, IAM users and policies on MinIO backend
    MinIO      | Endpoint: http://172.18.0.3:9000  http://127.0.0.1:9000
    MinIO      |
    MinIO      | Browser Access:
    MinIO      |    http://172.18.0.3:9000  http://127.0.0.1:9000
    MinIO      |
    MinIO      | Object API (Amazon S3 compatible):
    MinIO      |    Go:         https://docs.min.io/docs/golang-client-quickstart-guide
    MinIO      |    Java:       https://docs.min.io/docs/java-client-quickstart-guide
    MinIO      |    Python:     https://docs.min.io/docs/python-client-quickstart-guide
    MinIO      |    JavaScript: https://docs.min.io/docs/javascript-client-quickstart-guide
    MinIO      |    .NET:       https://docs.min.io/docs/dotnet-client-quickstart-guide
    server     | 2021/10/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 0.1 seconds
    server     | 2021/10/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 0.3 seconds
    server     | 2021/10/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 0.7 seconds
    server     | 2021/10/06 12:13:03 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 1.5 seconds
    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Database files initialized
    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Starting temporary server
    db  | 2021-10-06T12:13:04.422603Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.26) starting as process 93
    db  | 2021-10-06T12:13:04.439806Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
    db  | 2021-10-06T12:13:04.575773Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
    db  | 2021-10-06T12:13:04.827307Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:04.827865Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:04.832827Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
    db  | 2021-10-06T12:13:04.834132Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
    db  | 2021-10-06T12:13:04.841629Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
    db  | 2021-10-06T12:13:04.855748Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: /var/run/mysqld/mysqlx.sock
    db  | 2021-10-06T12:13:04.855801Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.26'  socket: '/var/run/mysqld/mysqld.sock'  port: 0  MySQL Community Server - GPL.
    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Temporary server started.
    server     | 2021/10/06 12:13:05 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 3.1 seconds
    db  | Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
    db  | Warning: Unable to load '/usr/share/zoneinfo/leap-seconds.list' as time zone. Skipping it.
    db  | Warning: Unable to load '/usr/share/zoneinfo/zone.tab' as time zone. Skipping it.
    db  | Warning: Unable to load '/usr/share/zoneinfo/zone1970.tab' as time zone. Skipping it.
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating database DBMLFLOW
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating user MLFLOW
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Giving user MLFLOW access to schema DBMLFLOW
    db  |
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Stopping temporary server
    db  | 2021-10-06T12:13:06.948482Z 13 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.26).
    server     | 2021/10/06 12:13:08 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 6.3 seconds
    db  | 2021-10-06T12:13:08.716131Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.26)  MySQL Community Server - GPL.
    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: Temporary server stopped
    db  |
    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: MySQL init process done. Ready for start up.
    db  |
    db  | 2021-10-06T12:13:09.159115Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.26) starting as process 1
    db  | 2021-10-06T12:13:09.167405Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
    db  | 2021-10-06T12:13:09.298925Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
    db  | 2021-10-06T12:13:09.488958Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:09.489087Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:09.489934Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
    db  | 2021-10-06T12:13:09.490169Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
    db  | 2021-10-06T12:13:09.494728Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
    db  | 2021-10-06T12:13:09.509856Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: /var/run/mysqld/mysqlx.sock
    db  | 2021-10-06T12:13:09.509982Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.26'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server - GPL.
    db  | mbind: Operation not permitted
    server     | 2021/10/06 12:13:14 INFO mlflow.store.db.utils: Creating initial MLflow database tables...
    server     | 2021/10/06 12:13:14 INFO mlflow.store.db.utils: Updating database tables
    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.
    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
    server     | INFO  [alembic.runtime.migration] Running upgrade  -&gt; 451aebb31d03, add metric step
    server     | INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tags
    server     | INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric values
    server     | INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags Table
    server     | INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limit
    server     | INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -&gt; 89d4b8295536, create latest metrics table
    server     | INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!
    server     | INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -&gt; 2b4d017a5e9b, add model registry tables to db
    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.
    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!
    server     | INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -&gt; cfd24bdc0731, Update run status constraint with killed
    server     | INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -&gt; 0a8213491aaa, drop_duplicate_killed_constraint
    server     | INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -&gt; 728d730b5ebd, add registered model tags table
    server     | INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -&gt; 27a6a02d2cf1, add model version tags table
    server     | INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -&gt; 84291f40a231, add run_link to model_version
    server     | INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -&gt; a8c4a736bde6, allow nulls for run_id
    server     | INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -&gt; 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary
    server     | INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -&gt; c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql
    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.
    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
    db  | mbind: Operation not permitted
    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Starting gunicorn 20.1.0
    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Listening at: http://0.0.0.0:5000 (17)
    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Using worker: sync
    server     | [2021-10-06 12:13:16 +0000] [19] [INFO] Booting worker with pid: 19
    server     | [2021-10-06 12:13:16 +0000] [20] [INFO] Booting worker with pid: 20
    server     | [2021-10-06 12:13:16 +0000] [21] [INFO] Booting worker with pid: 21
    server     | [2021-10-06 12:13:16 +0000] [22] [INFO] Booting worker with pid: 22


It makes me suspect because on the second line appears - mlflow Error but I think that this is why the other builds haven't finished.
Then I've set my environment variables on the client to create the information flow between my script and the storages:

    os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://localhost:9000/'
    os.environ['AWS_ACCESS_KEY_ID'] = 'key'
    os.environ['AWS_SECRET_ACCESS_KEY'] = 'pw'
    
    remote_server_uri = ""http://localhost:5000/"" # server URI
    mlflow.set_tracking_uri(remote_server_uri)
    
    mlflow.set_experiment(""mnist_mLflow_demo"")


finally I trained a TensorFlow network and I didn't have problems storing parameters and metrics but gave me some warnings (referring to next error). But the model haven't been auto log, so I tried to do it manually:
    with mlflow.start_run(run_name = ""test0"") as run:
    
        mlflow.keras.log_model(model2, 'model2')

    mlflow.end_run()

It dosen't work and it gives me the next INFO (but essencialy an error):
    INFO:tensorflow:Assets written to: (path)\Temp\tmpgr5eaha2\model\data\model\assets
    INFO:tensorflow:Assets written to: (path)\Temp\tmpgr5eaha2\model\data\model\assets
    2021/10/06 14:16:00 ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: (path)\AppData\Local\Temp\tmpgr5eaha2\model, flavor: keras)
    Traceback (most recent call last):
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\environment.py"", line 212, in infer_pip_requirements
        return _infer_requirements(model_uri, flavor)
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\requirements_utils.py"", line 263, in _infer_requirements
        modules = _capture_imported_modules(model_uri, flavor)
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\requirements_utils.py"", line 221, in _capture_imported_modules
        _run_command(
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\requirements_utils.py"", line 163, in _run_command
        stderr = stderr.decode(""utf-8"")
    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 349: invalid continuation byte


And the next error:

    ClientError                               Traceback (most recent call last)
    ~\Python\Python39\lib\site-packages\boto3\s3\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)
        278         try:
    --&gt; 279             future.result()
        280         # If a client error was raised, add the backwards compatibility layer
    
    ~\Python\Python39\lib\site-packages\s3transfer\futures.py in result(self)
        105             # out of this and propogate the exception.
    --&gt; 106             return self._coordinator.result()
        107         except KeyboardInterrupt as e:
    
    ~\Python\Python39\lib\site-packages\s3transfer\futures.py in result(self)
        264         if self._exception:
    --&gt; 265             raise self._exception
        266         return self._result
    
    ~\Python\Python39\lib\site-packages\s3transfer\tasks.py in __call__(self)
        125             if not self._transfer_coordinator.done():
    --&gt; 126                 return self._execute_main(kwargs)
        127         except Exception as e:
    
    ~\Python\Python39\lib\site-packages\s3transfer\tasks.py in _execute_main(self, kwargs)
        149 
    --&gt; 150         return_value = self._main(**kwargs)
        151         # If the task is the final task, then set the TransferFuture's
    
    ~\Python\Python39\lib\site-packages\s3transfer\upload.py in _main(self, client, fileobj, bucket, key, extra_args)
        693         with fileobj as body:
    --&gt; 694             client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
        695 
    
    ~\Python\Python39\lib\site-packages\botocore\client.py in _api_call(self, *args, **kwargs)
        385             # The ""self"" in this scope is referring to the BaseClient.
    --&gt; 386             return self._make_api_call(operation_name, kwargs)
        387 
    
    ~\Python\Python39\lib\site-packages\botocore\client.py in _make_api_call(self, operation_name, api_params)
        704             error_class = self.exceptions.from_code(error_code)
    --&gt; 705             raise error_class(parsed_response, operation_name)
        706         else:
    
    ClientError: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.
    
    During handling of the above exception, another exception occurred:
    
    S3UploadFailedError                       Traceback (most recent call last)
    C:\Users\FCAIZA~1\AppData\Local\Temp/ipykernel_7164/2476247499.py in &lt;module&gt;
          1 with mlflow.start_run(run_name = ""test0"") as run:
          2 
    ----&gt; 3     mlflow.keras.log_model(model2, 'model2')
          4 
          5 mlflow.end_run()
    
    ~\Python\Python39\lib\site-packages\mlflow\keras.py in log_model(keras_model, artifact_path, conda_env, custom_objects, keras_module, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, **kwargs)
        402             mlflow.keras.log_model(keras_model, ""models"")
        403     """"""
    --&gt; 404     Model.log(
        405         artifact_path=artifact_path,
        406         flavor=mlflow.keras,
    
    ~\Python\Python39\lib\site-packages\mlflow\models\model.py in log(cls, artifact_path, flavor, registered_model_name, await_registration_for, **kwargs)
        186             mlflow_model = cls(artifact_path=artifact_path, run_id=run_id)
        187             flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)
    --&gt; 188             mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)
        189             try:
        190                 mlflow.tracking.fluent._record_logged_model(mlflow_model)
    
    ~\Python\Python39\lib\site-packages\mlflow\tracking\fluent.py in log_artifacts(local_dir, artifact_path)
        582     """"""
        583     run_id = _get_or_start_run().info.run_id
    --&gt; 584     MlflowClient().log_artifacts(run_id, local_dir, artifact_path)
        585 
        586 
    
    ~\Python\Python39\lib\site-packages\mlflow\tracking\client.py in log_artifacts(self, run_id, local_dir, artifact_path)
        975             is_dir: True
        976         """"""
    --&gt; 977         self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)
        978 
        979     @contextlib.contextmanager
    
    ~\Python\Python39\lib\site-packages\mlflow\tracking\_tracking_service\client.py in log_artifacts(self, run_id, local_dir, artifact_path)
        332         :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.
        333         """"""
    --&gt; 334         self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)
        335 
        336     def list_artifacts(self, run_id, path=None):
    
    ~\Python\Python39\lib\site-packages\mlflow\store\artifact\s3_artifact_repo.py in log_artifacts(self, local_dir, artifact_path)
        102                 upload_path = posixpath.join(dest_path, rel_path)
        103             for f in filenames:
    --&gt; 104                 self._upload_file(
        105                     s3_client=s3_client,
        106                     local_file=os.path.join(root, f),
    
    ~\Python\Python39\lib\site-packages\mlflow\store\artifact\s3_artifact_repo.py in _upload_file(self, s3_client, local_file, bucket, key)
         78         if environ_extra_args is not None:
         79             extra_args.update(environ_extra_args)
    ---&gt; 80         s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)
         81 
         82     def log_artifact(self, local_file, artifact_path=None):
    
    ~\Python\Python39\lib\site-packages\boto3\s3\inject.py in upload_file(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)
        128     """"""
        129     with S3Transfer(self, Config) as transfer:
    --&gt; 130         return transfer.upload_file(
        131             filename=Filename, bucket=Bucket, key=Key,
        132             extra_args=ExtraArgs, callback=Callback)
    
    ~\Python\Python39\lib\site-packages\boto3\s3\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)
        283         # client error.
        284         except ClientError as e:
    --&gt; 285             raise S3UploadFailedError(
        286                 ""Failed to upload %s to %s: %s"" % (
        287                     filename, '/'.join([bucket, key]), e))
    
    S3UploadFailedError: Failed to upload (path)\AppData\Local\Temp\tmpgr5eaha2\model\conda.yaml to artifacts/1/5ae5fcef2d07432d811c3d7eb534382c/artifacts/model2/conda.yaml: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.


",1,2123,"I found the solution of this issue. It is a tricky problem due to spanish characters, my system's user profile in ""C:/"" is ""fcaizares"" (Caizares is my first last name). I have created another user named ""fcanizares"" and all is working fine. Hope you find this solution helpfull.
PS: Moral of the issue, get rid of the extrange characters!
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/64952367,SQLAlchemy Core. Values method does not limit columns in insert statement,"SQLALchemy Core insert expressions documentation says:

Notice above that the INSERT statement names every column in the users table. This can be limited by using the values() method, which establishes the VALUES clause of the INSERT explicitly:

With that in mind, I wrote the following snippet, which returns unexpected results.
from datetime import datetime
import sqlalchemy
from sqlalchemy import types
from sqlalchemy.dialects import postgresql


metadata = sqlalchemy.MetaData()

users = sqlalchemy.Table(
    ""users"",
    metadata,
    sqlalchemy.Column(
        ""id"",
        postgresql.UUID(as_uuid=True),
        default=uuid.uuid4(),
        primary_key=True,
    ),
    sqlalchemy.Column(""email"", types.String, unique=True, index=True),
    sqlalchemy.Column(
        ""created_at"",
        types.TIMESTAMP(timezone=True),
        default=datetime.utcnow(),
    ),
    sqlalchemy.Column(
        ""updated_at"",
        types.TIMESTAMP(timezone=True),
        default=datetime.utcnow(),
        onupdate=datetime.utcnow(),
    ),
)


email = ""god@olympus.org""
query = users.insert().values(email=email)

# (Pdb) print(query)
# INSERT INTO users (id, email, created_at, updated_at) VALUES (:id, :email, :created_at, :updated_at)
#
# (Pdb) print(query.compile().params)
# {'id': None, 'email': 'god@olympus.org', 'created_at': None, 'updated_at': None}

I expected the query to be INSERT INTO users (email) VALUES (:email)
Is there anything I'm missing?
I'm using SQLAlchemy==1.3.20 by the way.
",1,182,"The issue does not relate to SQLAlchemy, but rather to encode/databases.
Support for ""default"" parameter in sqlalchemy.Column
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/64755423,Execution failed when using pandas to_sql and pyhive to replace table - DatabaseError: &quot;... not all arguments converted during string formatting&quot;,"I need to replace a table in Hive with a new pandas dataframe. I am using pyhive to create a connection engine and subsequently using pandas.to_sql with 'if_exists' as replace.
from pyhive import hive

my_data = pd.read_csv('my_data.csv')

conn = hive.Connection(host=""111.11.11.11"", port=10000, username=""abcd"")

my_data.to_sql(name='table_name', con=conn, if_exists='replace', schema='my_schema')

conn.close()

However, this results in an unexpected error as follows:
DatabaseError: Execution failed on sql: SELECT name FROM sqlite_master WHERE type='table' AND name=?;
not all arguments converted during string formatting
unable to rollback

Other answers seem to indicate that this is related to to_sql expecting a SqlAlchemy engine - I was under the impression that this is what pyhive uses to create a connection. Any guidance on how to appropriately execute this command would be appreciated.
It is worth noting that elsewhere, this same connection has no problem reading data from Hive.
Works as expected:
conn = hive.Connection(host=""111.11.11.11"", port=10000, username=""abcd"")

my_data = pd.read_sql('select * from my_table', conn)

conn.close()

",1,1703,"
Other answers seem to indicate that this is related to to_sql expecting a SqlAlchemy engine - I was under the impression that this is what pyhive uses to create a connection.

PyHive can create a SQLAlchemy Engine object, but not the way you're doing it. As illustrated in the PyHive docs, you need to do something like
engine = create_engine('hive://localhost:10000/default')

and then pass the engine object to to_sql.

[read_sql] Works as expected

read_sql will often work with just a DBAPI connection, but to_sql requires a SQLAlchemy Connectable (Engine or Connection) because it may need to generate DDL. See this answer for more information.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/59577085,Rest Api post definition failing | Python | Flask,"I am very new to python. I have basic idea with python and flask is totally new for me. Trying to learn rest api with python using flask. But, I am facing the issue while posting data. below are sample code and error. Please help me to fix this issue. Thanks in advance. I tried googling but did not get any solution.

Error while running

127.0.0.1 - - [03/Jan/2020 15:58:54] ""POST /api/User HTTP/1.1"" 500 -
  Traceback (most recent call last):
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1997, in __call__
      return self.wsgi_app(environ, start_response)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1985, in wsgi_app
      response = self.handle_exception(e)
    File ""/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py"", line 273, in error_router
      return original_handler(e)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1540, in handle_exception
      reraise(exc_type, exc_value, tb)
    File ""/usr/local/lib/python3.7/site-packages/flask/_compat.py"", line 32, in reraise
      raise value.with_traceback(tb)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1982, in wsgi_app
      response = self.full_dispatch_request()
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1614, in full_dispatch_request
      rv = self.handle_user_exception(e)
    File ""/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py"", line 273, in error_router
      return original_handler(e)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1517, in handle_user_exception
      reraise(exc_type, exc_value, tb)
    File ""/usr/local/lib/python3.7/site-packages/flask/_compat.py"", line 32, in reraise
      raise value.with_traceback(tb)
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1612, in full_dispatch_request
      rv = self.dispatch_request()
    File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1598, in dispatch_request
      return self.view_functions[rule.endpoint](**req.view_args)
    File ""/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py"", line 480, in wrapper
      resp = resource(*args, **kwargs)
    File ""/usr/local/lib/python3.7/site-packages/flask/views.py"", line 84, in view
      return self.dispatch_request(*args, **kwargs)
    File ""/usr/local/lib/python3.7/site-packages/flask_restful/__init__.py"", line 595, in dispatch_request
      resp = meth(*args, **kwargs)
    File ""/Users/z0034ff/Documents/music/resources/Users.py"", line 32, in post
      typeid=json_data['typeid'],
  TypeError: __init__() got an unexpected keyword argument 'fname'


File name: Model.py

from flask import Flask
  from marshmallow import Schema, fields, pre_load, validate
  from flask_marshmallow import Marshmallow
  from flask_sqlalchemy import SQLAlchemy


  ma = Marshmallow()
  db = SQLAlchemy()


  class Users(db.Model):
      __tablename__ = 'users'
      id = db.Column(db.Integer, primary_key=True)
      fname = db.Column(db.String(50), nullable=False)
      lname = db.Column(db.String(50), nullable=False)
      email = db.Column(db.String(100), nullable=False)
      phone = db.Column(db.String(50), nullable=False)
      typeid = db.Column(db.Integer, db.ForeignKey(
          'user_type.id', ondelete='CASCADE'), nullable=False)
      creation_date = db.Column(
          db.TIMESTAMP, server_default=db.func.current_timestamp(), nullable=False)

      def __init__(self, users, typeid):
          self.users = users
          self.typeid = typeid

  class UsersSchema(ma.Schema):
      id = fields.Integer(dump_only=True)
      fname = fields.String(required=True, validate=validate.Length(1))
      lname = fields.String(required=True, validate=validate.Length(1))
      email = fields.String(required=True, validate=validate.Length(1))
      phone = fields.String(required=True, validate=validate.Length(10))
      typeid = fields.Integer(required=True)
      creation_date = fields.DateTime()


File name: Users.py

from flask import jsonify, request
  from flask_restful import Resource
  from Model import db, Users, UsersSchema

  users_schema = UsersSchema(many=True)
  user_schema = UsersSchema()


  class UsersResource(Resource):
      def get(self):
          users = Users.query.all()
          users = users_schema.dump(users).data
          return {""status"": ""success"", ""data"": users}, 200

      def post(self):
          json_data = request.get_json(force=True)
          if not json_data:
              return {'message': 'No input data provided'}, 400
          # Validate and deserialize input
          data, errors = user_schema.load(json_data)
          if errors:
              return errors, 422
          user = Users.query.filter_by(email=data['email']).first()
          if user:
              return {'message': 'User already exists'}, 400
          print(json_data['fname'])
          user = Users(
              fname=json_data['fname'],
              lname=json_data['lname'],
              email=json_data['email'],
              phone=json_data['phone'],
              typeid=json_data['typeid'],
          )

          db.session.add(user)
          db.session.commit()

          result = user_schema.dump(user).data

          return {""status"": 'success', 'data': result}, 201

",1,508,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/49288255,SQLAlchemy batch insert with database functions,,1,1498,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/35693081,sqlalchemy: order of query result unexpected,,1,653,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/32786334,Python3 sqlalchemy pymysql gevent sqlalchemy.util.queue.Empty gevent.hub.LoopExit: This operation would block forever,,1,2022,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/29308836,Turbogears nostests results in OperationalError when using SQLAlchemy-FullText-Search,,1,59,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/10739967,Python subprocess calling bcp on .csv: &#39;unexpected eof&#39;,,1,3082,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/77644943,"asynch.errors.UnexpectedPacketFromServerError: Code: 102. Unexpected packet from server &lt;host:port&gt; (expected Hello or Exception, got Unknown packet)","I'm trying to connect to clickhouse with sqlalchemy. I'm using:

python3.11
clickhouse-driver == 0.2.6
sqlalchemy == 2.0.23
clickhouse-sqlalchemy == 0.3.0
asynch == 0.2.3
asyncio == 3.4.3

Here is my script I used:
import asyncio
import contextlib
import pydantic
import traceback
import typing
from sqlalchemy import text, TextClause, engine
from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession, create_async_engine
from sqlalchemy.pool import Pool, QueuePool 

class AsyncDatabase:
    def __init__(self):
        self.ch_uri: str = ""clickhouse+asynch://admin:Password123@host:31123/db""
        self.ch_engine: AsyncEngine = create_async_engine(
            url=self.ch_uri,
            echo=False,
            pool_size=100,
            max_overflow=20,
            poolclass=QueuePool,
        )
        self.ch_session: AsyncSession = AsyncSession(bind=self.ch_engine)
        self.ch_pool: Pool = self.ch_engine.pool

async_db: AsyncDatabase = AsyncDatabase()

@contextlib.asynccontextmanager
async def get_ch_session() -&gt; typing.AsyncGenerator[AsyncSession, None]:
    try:
        yield async_db.ch_session
    except Exception as e:
        print(traceback.print_exc())
        await async_db.ch_session.rollback()
    finally:
        await async_db.ch_session.close()


async def hello() -&gt; str:
    session: AsyncSession = None
    async with get_ch_session() as session:
        stmt: TextClause = text(""SELECT * FROM table_name LIMIT 1"")
        result: engine.Result = await session.execute(stmt)
        print(result.all())

    return ""ok""


if __name__ == ""__main__"":
    asyncio.run(hello())

I did try without async but still got the same error
asynch.errors.UnexpectedPacketFromServerError: Code: 102. Unexpected packet from server host:31123 (expected Hello or Exception, got Unknown packet)
when I use DataGrid to connect with above creds, it works fine. so I think it's not about the 31123 port
I stuck on this for 5 hrs, all answers from the web doesn't help me at all.
",0,54,"You tried to use TCP Native protocol, but connected to HTTP based port
look grep tcp_port /var/lib/clickhouse/preprocessed_configs/config.xml
and use this value
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/76663686,"aws python 3.8 lambda + async sqlalchemy 2.0.10, asyncpg 0.28.0","from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession

engine_async = create_async_engine(
    ""postgresql+asyncpg://postgres:password@host_name.rds.amazonaws.com/postgres"")

def get_data(event, context): # this is the lambda handler
    return asyncio.get_event_loop().run_until_complete(get_data_async(event, context))

async def get_datas_async(event, context):
    org_id = event['org_id']
    async_session = sessionmaker(bind=engine_async, future=True, class_=AsyncSession)
    async with async_session() as session1, async_session() as session2, async_session() as session3:
        users_count_query = select(func.count(User.user_id)).filter_by(org_id=org_id)
        org_name_query = select(Organization.name).filter_by(org_id=org_id)
        other_data_query = select(
            func.count(Transaction.transaction_id).label('transactions_count'),
            func.sum(Transaction.total_cost).label('total_cost'),
            func.sum(Transaction.total_time).label('total_time')).where(Transaction.org_id == org_id)

        tasks = [
            session1.execute(users_count_query),
            session2.execute(other_data_query),
            session3.execute(org_name_query)
        ]
        results = await asyncio.gather(*tasks)
        userss_count, data, org_name = results
        data = data.fetchone()
        uesrs_count = users_count.scalar_one()
        org_name = org_name.scalar_one()

        return {
            ""statusCode"": 200,
            ""users_count"": users_count,
            ""transactions_count"": data.transactions_count,
            ""total_cost"": 0 if not data.total_cost else data.total_cost,
            ""total_time"": data.total_time,
            ""org_name"": org_name,
        }

I get this error:
{
    ""errorMessage"": ""Method 'close()' can't be called here; method '_connection_for_bind()' is already in progress and this would cause an unexpected state change to &lt;SessionTransactionState.CLOSED: 5&gt;"",
    ""errorType"": ""IllegalStateChangeError"",
    ""stackTrace"": [
        ""  File \""/var/task/app.py\"", line 131, in get_data\n    return asyncio.get_event_loop().run_until_complete(get_data_async(event, context))\n"",
        ""  File \""/var/lang/lib/python3.8/asyncio/base_events.py\"", line 616, in run_until_complete\n    return future.result()\n"",
        ""  File \""/var/task/app.py\"", line 175, in get_data_async\n    return {\n"",
        ""  File \""/var/task/sqlalchemy/ext/asyncio/session.py\"", line 859, in __aexit__\n    await asyncio.shield(task)\n"",
        ""  File \""/var/task/sqlalchemy/ext/asyncio/session.py\"", line 840, in close\n    await greenlet_spawn(self.sync_session.close)\n"",
        ""  File \""/var/task/sqlalchemy/util/_concurrency_py3k.py\"", line 154, in greenlet_spawn\n    result = context.switch(*args, **kwargs)\n"",
        ""  File \""/var/task/sqlalchemy/orm/session.py\"", line 2382, in close\n    self._close_impl(invalidate=False)\n"",
        ""  File \""/var/task/sqlalchemy/orm/session.py\"", line 2424, in _close_impl\n    transaction.close(invalidate)\n"",
        ""  File \""&lt;string&gt;\"", line 2, in close\n"",
        ""  File \""/var/task/sqlalchemy/orm/state_changes.py\"", line 121, in _go\n    raise sa_exc.IllegalStateChangeError(\n""
    ]
}

",0,209,"solution and explanation are here: https://github.com/sqlalchemy/sqlalchemy/discussions/9312
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/75071043,Read from the server failed (20004) (SQLExecDirectW)Read from the server failed (20004) (SQLExecDirectW),"I am using the sqlalchemy to access sql server using pyodbc in Ubuntu OS
the sql server runs in docker container
the url for connecting the server in sqlalchemy, which I used is
'mssql+pyodbc:///?odbc_connects=""Driver=;SERVER=;DATABASE=;UID=;PWD=;port=;TDS_Version=8.0""
I run this statement - result = session.query(User).filter(User.username == username).first()
I got this error
(pyodbc.OperationalError) ('08S01', '[08S01] [FreeTDS][SQL Server]Read from the server failed (20004) (SQLExecDirectW)')
[SQL: SELECT TOP 1 [user].user_id AS user_user_id, [user].username AS user_username, [user].password AS user_password, [user].first_name AS user_first_name, [user].last_name AS user_last_name, [user].designation AS user_designation, [user].last_login AS user_last_login
FROM [user]
WHERE [user].username = ?]
[parameters: ('device1',)]
(Background on this error at: http://sqlalche.me/e/e3q8)
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1249, in _execute_context
    cursor, statement, parameters, context
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 552, in do_execute
    cursor.execute(statement, parameters)
pyodbc.OperationalError: ('08S01', '[08S01] [FreeTDS][SQL Server]Read from the server failed (20004) (SQLExecDirectW)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""./database1.py"", line 43, in get_user_details
    result = session.query(User).filter(User.username == username).first()
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3232, in first
    ret = list(self[0:1])
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3018, in __getitem__
    return list(res)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3334, in __iter__
    return self._execute_and_instances(context)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3359, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 988, in execute
    return meth(self, multiparams, params)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/sql/elements.py"", line 287, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1107, in _execute_clauseelement
    distilled_params,
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1253, in _execute_context
    e, statement, parameters, cursor, context
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1473, in _handle_dbapi_exception
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 152, in reraise
    raise value.with_traceback(tb)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1249, in _execute_context
    cursor, statement, parameters, context
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 552, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (pyodbc.OperationalError) ('08S01', '[08S01] [FreeTDS][SQL Server]Read from the server failed (20004) (SQLExecDirectW)')
[SQL: SELECT TOP 1 [user].user_id AS user_user_id, [user].username AS user_username, [user].password AS user_password, [user].first_name AS user_first_name, [user].last_name AS user_last_name, [user].designation AS user_designation, [user].last_login AS user_last_login
FROM [user]
WHERE [user].username = ?]
[parameters: ('device1',)]
(Background on this error at: http://sqlalche.me/e/e3q8)

Due to this error, all the following queries are triggering the different error each time-
sqlalchemy.exc.StatementError: (sqlalchemy.exc.InvalidRequestError) Can't reconnect until invalid transaction is rolled back

sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('01000', '[01000] [FreeTDS][SQL Server]Unexpected EOF from the server (20017) (SQLExecDirectW)')

pyodbc.OperationalError: ('08S01', '[08S01] [FreeTDS][SQL Server]Communication link failure (0) (SQLExecDirectW)')

sqlalchemy.exc.ResourceClosedError: This result object does not return rows. It has been closed automatically.

sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('HY000', '[HY000] [FreeTDS][SQL Server]Unknown error (0) (SQLExecDirectW)')

pyodbc.OperationalError: ('08S01', '[08S01] [FreeTDS][SQL Server]Bad token from the server: Datastream processing out of sync (20020) (SQLExecDirectW)')

sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('HY008', '[HY008] [FreeTDS][SQL Server]Operation was cancelled (0) (SQLExecDirectW)')

2023-01-10 16:38:20,010 - ERROR - sqlalchemy.pool.impl.QueuePool - Exception during reset or similar
dbapi_connection.rollback()
pyodbc.Error: ('HY000', 'The driver did not supply an error!')

What is the cause of this error?
UPDATE
the sql server version - select @@verion -

Microsoft SQL Server 2017 (RTM-CU22) (KB4577467) - 14.0.3356.20 (X64)
Aug 20 2020 22:33:27
Copyright (C) 2017 Microsoft Corporation
Express Edition (64-bit) on Linux (Ubuntu 16.04.7 LTS)

",0,439,"You are using FreeTDS instead of the supported Microsoft ODBC Driver for SQL Server.  Download instructions are here.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/74461385,Select specific columns with cast using SQLAlchemy,"I'm using SQLAlchemy (Version: 1.4.44) and I'm having some unexpected results when trying to select columns and using cast on those columns.
First, most of the examples and even current documentation suggests column selection should work by passing an array to the select function like this:
s = select([table.c.col1])

However, I get the following error if I try this:
s = my_table.select([my_table.columns.user_id])

sqlalchemy.exc.ArgumentError: SQL expression for WHERE/HAVING role expected, got [Column('user_id', String(), table=&lt;my_table&gt;)].

Some examples suggest just placing the field directly in the select query.
s = select(table.c.col1)


But this seems to do nothing more than create an idle where-clause out of the field.
I eventually was able to achieve column selection with this approach:
s = my_table.select().with_only_columns(my_table.columns.created_at)


But I am not able to use cast for some reason with this approach.
s = my_table.select().with_only_columns(cast(my_table.columns.created_at, Date))

ValueError: Couldn't parse date string '2022' - value is not a string.

All help appreciated!
",0,214,"I don't think table.select() is common usage.  SQLAlchemy is in a big transition right now on its way to 2.0.  In 1.4 (and in 2) the following syntax should work, use whatever session handling you already have working I just mean the select(...):
from sqlalchemy.sql import select, cast
from sqlalchemy.dialects.postgresql import INTEGER

class User(Base):
    __tablename__ = ""users""
    id = Column(
        Integer, nullable=False, primary_key=True
    )
    name = Column(Text)

with Session(engine) as session:
    u1 = User(name=""1"")
    session.add(u1)
    session.commit()

with Session(engine) as session:
    my_table = User.__table__
    # Cast user name into integer.
    print (session.execute(select(cast(my_table.c.name, INTEGER))).all())

",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/72381857,Unable to get an id from the database after doing insert using sqlalchemy,"Assuming this model:
class Pipeline(Base):
    __tablename__ = ""pipeline""
    pipeline_id = Column(String, primary_key=True)
    pipeline_version_id = Column(Integer, primary_key=True, autoincrement=True)
    is_active = Column(Boolean, nullable=False)
    update_ts = Column(DateTime, nullable=False)
    config = Column(TEXT(), nullable=True)
    parser_version = Column(String, nullable=True)

And this insert code:
        async with self.sessionFactory() as session:
            async with session.begin():
                query = select(Pipeline).filter_by(pipeline_id=pipeline_id).filter_by(is_active=True)
                result = await session.execute(query)
                active_pipelines = result.scalars().all()
                for entry in active_pipelines:
                    entry.is_active = False
                    self.logger.info(f""Marking pipeline/version {entry.pipeline_id}/{entry.pipeline_version_id} inactive"")

                self.logger.info(f""Creating update_pipeline:  {updated_pipeline}"")

                session.add(updated_pipeline)
                pipeline_version_id = updated_pipeline.pipeline_version_id
                self.logger.info('adding updated pipeline with version id {id}'.format(id=str(pipeline_version_id)))

When I run this, it is returning a null pipeline_version_id
[Edit]
Here are other alternatives I tried:
        async with self.sessionFactory() as session:
            async with session.begin():
                query = select(Pipeline).filter_by(pipeline_id=pipeline_id).filter_by(is_active=True)
                result = await session.execute(query)
                active_pipelines = result.scalars().all()
                for entry in active_pipelines:
                    entry.is_active = False
                    self.logger.info(f""Marking pipeline/version {entry.pipeline_id}/{entry.pipeline_version_id} inactive"")
                self.logger.info(f""Creating update_pipeline:  {updated_pipeline}"")
                session.add(updated_pipeline)

        pipeline_version_id = updated_pipeline.pipeline_version_id
        self.logger.info('adding updated pipeline with version id {id}'.format(id=str(pipeline_version_id)))

Result: Failed to update DB. Error: Instance &lt;Pipeline at 0x110c31da0&gt; is not bound to a Session; attribute refresh operation cannot proceed (Background on this error at: https://sqlalche.me/e/14/bhk3)
        async with self.sessionFactory() as session:
            async with session.begin():
                query = select(Pipeline).filter_by(pipeline_id=pipeline_id).filter_by(is_active=True)
                result = await session.execute(query)
                active_pipelines = result.scalars().all()
                for entry in active_pipelines:
                    entry.is_active = False
                    self.logger.info(f""Marking pipeline/version {entry.pipeline_id}/{entry.pipeline_version_id} inactive"")
                self.logger.info(f""Creating update_pipeline:  {updated_pipeline}"")
                session.add(updated_pipeline)

            pipeline_version_id = updated_pipeline.pipeline_version_id
            self.logger.info('adding updated pipeline with version id {id}'.format(id=str(pipeline_version_id)))

Result: Failed to update DB. Error: greenlet_spawn has not been called; can't call await_() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/14/xd2s)
How do i get the ID of the inserted record without doing another query?
These are my dependencies:
python = ""^3.7""
setuptools = ""&lt;58""
fastapi = ""^0.54.1""
gunicorn = ""&lt;20.0""
uvicorn = ""^0.11.5""
sqlalchemy = ""^1.4.25""
apache-airflow=""1.10.10""
psycopg2-binary = ""^2.9.1""

Here is my session creation:
def create_session_factory(db_conn=None):
    #
    # Using https://rogulski.it/blog/sqlalchemy-14-async-orm-with-fastapi/
    #
    if not db_conn:
        db_conn = get_default_parser_db_conn_str()
    global _engine
    if not _engine:
        _engine = create_async_engine(db_conn, **DB_ENGINE_OPTIONS)
    sessionFactory = sessionmaker(_engine, class_=AsyncSession)
    sessionFactory.configure()
    return sessionFactory

and the DB_ENGINE_OPTIONS
DB_ENGINE_OPTIONS = {
    ""poolclass"": QueuePool,
    ""pool_size"": 10,
    ""max_overflow"": 50,
    ""pool_recycle"": 3600,
    ""pool_timeout"": 30
}

Any help appreciated.
this is my first time at async DB calls with python, so I am sure i am missing something.
",0,130,"Thank you all, who answered.
Ultimately, just adding a session.flush() after the call to session.add(obj) gave me what i needed.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/71004077,"How to get average value from SQL(Superset, SQLAlchemy under PostgreSQL) timestamp","I tried to calculate the average timestamp and the query is successfully executed directly in postgresql, but when executed through superset(python's SQLAlchemy), an error occurs when converting an integer to a timestamp. How can I fix the error or do it in another way?
Query1:
select 
    to_timestamp(
        avg(
            cast(
                extract(epoch from last_checked_at) as integer)
                )::integer
        ) as datetime
from
    the_best_table

Query1 postgresql result:




datetime




2022-02-06 03:15:19+00




Query1 superset(SQLAlchemy) result:

PostgreSQL Error postgresql error: '&gt;=' not supported between
instances of 'datetime.timedelta' and 'int'
This may be triggered by: Issue 1002 - The database returned an
unexpected error. link



 

 
Also below you can see the results of the query to convert an integer to datetime. The problem is in this part of the code.
Query2:
select to_timestamp(1644117319) as datetime

Query2 postgresql result:




datetime




2022-02-06 03:15:19+00




Query2 superset(SQLAlchemy) result:

PostgreSQL Error postgresql error: '&gt;=' not supported between
instances of 'datetime.timedelta' and 'int'
This may be triggered by: Issue 1002 - The database returned an
unexpected error. link

",0,600,"I was not able to reproduce the error.
Works fine.
Setup in Postgresql Database:
postgres=&gt; create table the_best_table(last_checked_at timestamp);
CREATE TABLE
postgres=&gt; insert into the_best_table values(now());
INSERT 0 1
postgres=&gt; select
    to_timestamp(
        avg(
            cast(
                extract(epoch from last_checked_at) as integer)
                )::integer
        ) as datetime
from
    the_best_table;
        datetime
------------------------
 2022-02-07 04:54:06+09
(1 row)

The exact two queries run fine for me in SQLAlchemy:
&gt;&gt;&gt; from sqlalchemy import create_engine
&gt;&gt;&gt; engine = create_engine('postgresql+pg8000://&lt;username&gt;:&lt;passwd&gt;@192.xxx.xxx.123/postgres')
&gt;&gt;&gt;  metadata.create_all(engine)
&gt;&gt;&gt; with engine.connect() as con:
...     rs = con.execute('select to_timestamp(avg(cast(extract(epoch from last_checked_at) as integer))::integer ) as datetime from the_best_table;')
...     for row in rs:
...             print(row)
... 
(datetime.datetime(2022, 2, 7, 4, 54, 6, tzinfo=datetime.timezone(datetime.timedelta(seconds=32400))),)
&gt;&gt;&gt; with engine.connect() as con:
...     rs = con.execute('select to_timestamp(1644117319) as datetime;')
...     for row in rs:
...             print(row)
... 
(datetime.datetime(2022, 2, 6, 12, 15, 19, tzinfo=datetime.timezone(datetime.timedelta(seconds=32400))),)

",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/69447368,Connecting to MySQL using a token,"I've been trying to connect to an RDS instance using the sqlalchemy library using a token. According to the docs it should be possible via the cparams['token'] variable, but doing so I get an error of an unexpected argument.
Going deeper into the docs, there was this code where the connection could be established using the keyword argument attrs_before, but in this case such argument also does not exist.
Does anyone has any idea on how to connect to the DB using a token?
I've trying to do it via such code but with no effect:
import boto3
import struct
from sqlalchemy import create_engine, event
from sqlalchemy.engine.url import URL
from sqlalchemy.engine.url import URL
from sqlalchemy.orm import sessionmaker

SQL_COPT_SS_ACCESS_TOKEN = 1256
session = boto3.session.Session(profile_name='xxx')
rds = session.client('rds')

def get_authentication_token():
    return rds.generate_db_auth_token(DBHostname='xxxx', Port=3306, DBUsername='xxx')

engine_url = URL.create(drivername='mysql+pymysql', host='xxxx')
engine = create_engine(engine_url)

#@event.listens_for(engine, 'do_connect')
#def provide_token(dialect, conn_rec, cargs, cparams):
#    token_struct = struct.pack(f'&lt;I{len(token)}s', len(token), token)
#    cparams['attrs_before'] = {SQL_COPT_SS_ACCESS_TOKEN: token_struct}

@event.listens_for(engine, ""do_connect"")
def provide_token(dialect, conn_rec, cargs, cparams):
    cparams['token'] = get_authentication_token()

Session = sessionmaker(bind=engine)

with Session() as session:
    result = session.execute('select now()').first()


To be more precise I get the error:
TypeError: __init__() got an unexpected keyword argument 'token'
",0,758,"I had the same issue, what solved for me was replacing token with password:
@event.listens_for(engine, ""do_connect"")
def provide_token(dialect, conn_rec, cargs, cparams):
    cparams['password'] = get_authentication_token()

Another remark is that you must also pass the SSL certificate in the connection, I simply added another param:
@event.listens_for(engine, ""do_connect"")
def provide_token(dialect, conn_rec, cargs, cparams):
    cparams['password'] = get_authentication_token()
    caparam['ssl'] = {""ca"": ""rds-combined-ca-bundle.pem""}

Bear in mind that you might have to pass the full path to the rds-combined-ca-bundle.pem file. If you don't pass the SSL file and the connection param you'll still get an Access Denied even if everything else is setup correctly.
The SSL file can be downloaded from https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/66277687,Save an object using Celery and Gino,"I have the following pipeline
Event model (based on Gino 'db' object):
class Event(db.Model):
    __tablename__ = ""events""

    id = db.Column(db.BigInteger(), primary_key=True)
    request_timestamp = db.Column(db.DateTime(), nullable=False)
    service = db.Column(db.String(), nullable=False)
    url = db.Column(db.String(), nullable=False)
    status_code = db.Column(db.Integer(), nullable=False)
    response_time = db.Column(db.DateTime(), nullable=False)

FastApi app has a POST view with call to celery task to pass the data of the Event object:
@router.post(""/events/add"")
async def add_event(event: EventModel):
    event_data = {'request_timestamp': event.request_timestamp.replace(tzinfo=None),
                  'service': event.service,
                  'url': event.url,
                  'status_code': event.status_code,
                  'response_time': event.response_time.replace(tzinfo=None)
                  }

    task = celery_app.send_task(""monitoring_service.src.monitoring_service.worker.celery_worker.add_to_db"",
                                kwargs=event_data)

    return JSONResponse(content=""Event recorded successfully"", status_code=200)

And celery worker which must save an Event object to database:
@celery_app.task(acks_late=True)
async def add_to_db(request_timestamp, service, url, status_code, response_time):
    event = await Event.create(
        request_timestamp=datetime.strptime(request_timestamp, '%Y-%m-%dT%H:%M:%S.%f'),
        service=service,
        url=url,
        status_code=status_code,
        response_time=datetime.strptime(response_time, '%Y-%m-%dT%H:%M:%S.%f'),
    )

    return {""status"": True}

Now I'm receiving an error during object saving process:
Traceback (most recent call last):

File ""/usr/local/lib/python3.8/site-packages/celery/app/trace.py"", line 479, in trace_task

mark_as_done(

File ""/usr/local/lib/python3.8/site-packages/celery/backends/base.py"", line 158, in mark_as_done

self.store_result(task_id, result, state, request=request)

File ""/usr/local/lib/python3.8/site-packages/celery/backends/base.py"", line 442, in store_result

self._store_result(task_id, result, state, traceback,

File ""/usr/local/lib/python3.8/site-packages/celery/backends/database/__init__.py"", line 51, in _inner

return fun(*args, **kwargs)

File ""/usr/local/lib/python3.8/site-packages/celery/backends/database/__init__.py"", line 130, in _store_result

session.commit()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 1042, in commit

self.transaction.commit()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 504, in commit

self._prepare_impl()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 483, in _prepare_impl

self.session.flush()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2523, in flush

self._flush(objects)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2664, in _flush

transaction.rollback(_capture_exception=True)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py"", line 68, in __exit__

compat.raise_(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_

raise exception

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2624, in _flush

flush_context.execute()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 422, in execute

rec.execute(self)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 586, in execute

persistence.save_obj(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 230, in save_obj

_emit_update_statements(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 994, in _emit_update_statements

c = cached_connections[connection].execute(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1014, in execute

return meth(self, multiparams, params)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py"", line 298, in _execute_on_connection

return connection._execute_clauseelement(self, multiparams, params)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1127, in _execute_clauseelement

ret = self._execute_context(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1207, in _execute_context

self._handle_dbapi_exception(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1511, in _handle_dbapi_exception

util.raise_(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_

raise exception

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1205, in _execute_context

context = constructor(dialect, self, conn, *args)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 858, in _init_compiled

param = dict(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 861, in &lt;genexpr&gt;

processors[key](compiled_params[key])

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/sqltypes.py"", line 1689, in process

value = dumps(value, protocol)

sqlalchemy.exc.SQLAlchemyError: (builtins.TypeError) cannot pickle 'coroutine' object


warn(RuntimeWarning(

[2021-02-19 10:19:39,010: ERROR/ForkPoolWorker-1] Task monitoring_service.src.monitoring_service.worker.celery_worker.add_to_db[994ae754-f216-4fa9-b502-5fbb0221011c] raised unexpected: SQLAlchemyError(""(builtins.TypeError) cannot pickle 'coroutine' object"")

Traceback (most recent call last):

File ""/usr/local/lib/python3.8/site-packages/celery/app/trace.py"", line 479, in trace_task

mark_as_done(

File ""/usr/local/lib/python3.8/site-packages/celery/backends/base.py"", line 158, in mark_as_done

self.store_result(task_id, result, state, request=request)

File ""/usr/local/lib/python3.8/site-packages/celery/backends/base.py"", line 442, in store_result

self._store_result(task_id, result, state, traceback,

File ""/usr/local/lib/python3.8/site-packages/celery/backends/database/__init__.py"", line 51, in _inner

return fun(*args, **kwargs)

File ""/usr/local/lib/python3.8/site-packages/celery/backends/database/__init__.py"", line 130, in _store_result

session.commit()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 1042, in commit

self.transaction.commit()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 504, in commit

self._prepare_impl()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 483, in _prepare_impl

self.session.flush()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2523, in flush

self._flush(objects)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2664, in _flush

transaction.rollback(_capture_exception=True)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py"", line 68, in __exit__

compat.raise_(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_

raise exception

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2624, in _flush

flush_context.execute()

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 422, in execute

rec.execute(self)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 586, in execute

persistence.save_obj(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 230, in save_obj

_emit_update_statements(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 994, in _emit_update_statements

c = cached_connections[connection].execute(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1014, in execute

return meth(self, multiparams, params)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py"", line 298, in _execute_on_connection

return connection._execute_clauseelement(self, multiparams, params)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1127, in _execute_clauseelement

ret = self._execute_context(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1207, in _execute_context

self._handle_dbapi_exception(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1511, in _handle_dbapi_exception

util.raise_(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_

raise exception

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1205, in _execute_context

context = constructor(dialect, self, conn, *args)

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 858, in _init_compiled

param = dict(

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 861, in &lt;genexpr&gt;

processors[key](compiled_params[key])

File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/sqltypes.py"", line 1689, in process

value = dumps(value, protocol)

sqlalchemy.exc.SQLAlchemyError: (builtins.TypeError) cannot pickle 'coroutine' object

If I call event = await Event.create(...) inside a view it is successfull and saves data to db.
If I delete 'async/await' from Celery worker, this error doesn't show up, Celery task has 'SUCCESS' state  but database is empty.
Can't really understand what is not right.
",0,506,"Simple: currently, Celery is not able to handle asynchronous functions as tasks. You need to wrap it with asyncio.run (https://docs.python.org/3/library/asyncio-task.html#asyncio.run):
async def add_to_db(request_timestamp, service, url, status_code, response_time):
    event = await Event.create(
        request_timestamp=datetime.strptime(request_timestamp, '%Y-%m-%dT%H:%M:%S.%f'),
        service=service,
        url=url,
        status_code=status_code,
        response_time=datetime.strptime(response_time, '%Y-%m-%dT%H:%M:%S.%f'),
    )

@celery_app.task(acks_late=True)
def add_to_db_task(request_timestamp, service, url, status_code, response_time):
    asyncio.run(add_to_db(request_timestamp, service, url, status_code, response_time))
    return {""status"": True}

",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/66274054,Great Expectations custom expectation not ignoring nulls as requested,"Versions of the libraries we're using:
snowconn==3.7.1
snowflake-connector-python==2.3.10
snowflake-sqlalchemy==1.2.3
SQLAlchemy==1.3.23
great_expectations==0.13.10
pandas==1.1.5

Note we're grabbing data from Snowflake on our own and then feeding a dataframe of it into Great Expectations. I'm aware GE has a Snowflake data source and it's on my list to add it. But I think this setup should work even without using that data source.
We have the following Great Expectations data context config:
    data_context_config = DataContextConfig(
        datasources={
            datasource_name: DatasourceConfig(
                class_name='PandasDatasource',
                data_asset_type={
                    'module_name': 'dataqa.dataset',
                    'class_name': 'CustomPandasDataset'
                }
            )
        },
        store_backend_defaults=S3StoreBackendDefaults(
            default_bucket_name=METADATA_BUCKET,
            expectations_store_prefix=EXPECTATIONS_PATH,
            validations_store_prefix=VALIDATIONS_PATH,
            data_docs_prefix=DATA_DOCS_PATH,
        ),
        validation_operators={
            ""action_list_operator"": {
                ""class_name"": ""ActionListValidationOperator"",
                ""action_list"": [
                    {
                        ""name"": ""store_validation_result"",
                        ""action"": {""class_name"": ""StoreValidationResultAction""},
                    },
                    {
                        ""name"": ""store_evaluation_params"",
                        ""action"": {""class_name"": ""StoreEvaluationParametersAction""},
                    },
                    {
                        ""name"": ""update_data_docs"",
                        ""action"": {""class_name"": ""UpdateDataDocsAction""},
                    },
                ],
            }
        }
    )
    ge_context = BaseDataContext(project_config=data_context_config)

CustomPandasDataset is defined as:
class CustomPandasDataset(PandasDataset):
    _data_asset_type = ""CustomPandasDataset""

    @MetaPandasDataset.multicolumn_map_expectation
    def expect_column_A_equals_column_B_column_C_ratio(
        self,
        column_list,
        ignore_row_if='any_value_is_missing'
    ):
        column_a = column_list.iloc[:,0]
        column_b = column_list.iloc[:,1]
        column_c = column_list.iloc[:,2]

        return abs(column_a - (1.0 - (column_b/column_c))) &lt;= 0.001

and called like:
    cols = ['a', 'b', 'c']
    batch.expect_column_A_equals_column_B_column_C_ratio(
        cols,
        catch_exceptions=True
    )

Later on we validate the data context like so:
    return ge_context.run_validation_operator(
        ""action_list_operator"",
        assets_to_validate=batches,
        run_id=run_id)[""success""]

Often times, columns a and b are null in our data. Given I've set the ignore_row_if='any_value_is_missing' flag on the custom expectation, I'm expecting rows with null values in any of columns a, b, or c to be skipped. But Great Expectations doesn't skip them, instead adding them to the unexpected, or ""failed"" field of output:
result  
element_count   1000
missing_count   0
missing_percent 0
unexpected_count    849
unexpected_percent  84.89999999999999
unexpected_percent_total    84.89999999999999
unexpected_percent_nonmissing   84.89999999999999result 
element_count   1000
missing_count   0
missing_percent 0
unexpected_count    849
unexpected_percent  84.89999999999999
unexpected_percent_total    84.89999999999999
unexpected_percent_nonmissing   84.89999999999999


partial_unexpected_list 

0   
a   null
b   null
c   1.63

I'm unsure why this is happening. In the Great Expectations source, the multicolumn_map_expectation does:
...
            elif ignore_row_if == ""any_value_is_missing"":
                boolean_mapped_skip_values = test_df.isnull().any(axis=1)
...
            boolean_mapped_success_values = func(
                self, test_df[boolean_mapped_skip_values == False], *args, **kwargs
            )
            success_count = boolean_mapped_success_values.sum()
            nonnull_count = (~boolean_mapped_skip_values).sum()
            element_count = len(test_df)

            unexpected_list = test_df[
                (boolean_mapped_skip_values == False)
                &amp; (boolean_mapped_success_values == False)
            ]
            unexpected_index_list = list(unexpected_list.index)

            success, percent_success = self._calc_map_expectation_success(
                success_count, nonnull_count, mostly
            )

which I interpret as ignoring null-containing rows (not adding them to the unexpected list and not using them to determine percent_success). I've dropped a pdb in our code and verified that the dataframe we're calling the expectation on can be manipulated in the correct way to get ""sensible"" data (test_df.isnull().any(axis=1)), but for some reason Great Expectations is allowing those nulls to slip through. Anyone know why?
",0,2069,"I believe the poster filed a Github issue here: https://github.com/great-expectations/great_expectations/issues/2460. The progress can be tracked there.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/64046595,Unable to use amancevice/superset docker image to use Oracle DB as datasource,"I'm trying to test if superset would be adecuate for our data visualization needs. As I've no knowledge of python, I'm using a docker image of a superset installation (v0.37.1) published with all the libraries needed to connect to different databases (https://github.com/amancevice/docker-superset). When I try to set up a new database source to an Oracle database I get this error in the superset.log:
Unexpected error (cx_Oracle.DatabaseError) DPI-1047: Cannot locate a 64-bit Oracle Client library: ""libclntsh.so: cannot open shared object file: No such file or directory"". See https://oracle.github.io/odpi/doc/installation.html#linux for help
(Background on this error at: http://sqlalche.me/e/13/4xp6)

After googling I've added to the container (when I have this solved I'll investigate how to create a new image with all the changes I've performed directly to the container) the Oracle Instant Client library and modified the .bashrc file of the container (for the user superset) to add a new env variable LD_LIBRARY_PATH pointing to the directory with the Oracle Instant Client.
I've restarted the container.
To check that SQLAlchemy can connect to my OracleDB correctly I've created a test_script.py:
import sqlalchemy as sa
engine = sa.create_engine('oracle+cx_oracle://user:password@host:port/?service_name=service')
with engine.connect() as connection:
    result = connection.execute(""select * from dual"")
    for row in result:
        print(row)

And then if I connect to the container directly docker exec -it my_container bash and execute my script python test_script.py, I get the correct results and no error message, however, connecting to the superset url and trying to create a new datasource to that Oracle DB, I'm still getting the same error.
Does someone know if there's another configuration change needed in the container to make this work?
",0,952,"When you download Oracle Instant Client Basic or Basic Light ZIP packages from here, then run something like this in your Dockerfile:
RUN echo /opt/oracle/instantclient_19_8 &gt; /etc/ld.so.conf.d/oic.conf &amp;&amp; \
    ldconfig

Also see sample docker images for Linux developers and for basic Oracle Instant Client.  If you are not on an RPM based system, check out the sample Docker files in Docker for Oracle Database Applications in Node.js and Python.
The details vary with what base Docker image you are using, and whether you want to install Instant Client from ZIP files or RPMs.
Update: prebuilt containers are available from Oracle's GitHub Container Registry.
",,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/58691132,Flask SQLAlchemy - Default filter value if no results for provided filter,"Using Flask SQLAlchemy I'm querying a MySQL database with a table called bar and looking for rows that match a filter consisting of foo and country_code:

foo_filter = 'hello'
country_code_filter = 'ES'
result = Bar.filter_by(foo=foo_filter, country_code=country_code_filter).first()


The above code will return the first row in which foo = foo_filter and country_code = country_code_filter.

However, it's possible that we might not have rows for certain country codes that match foo. In these cases (i.e, cases where the above query returns 0 results), I'd like to use a default country filter of 'RoW' as our dataset should always have an RoW value for each possible value of foo. In the unexpected occurrence that this also doesn't return any results then an error should be thrown. This is the code I have for this:

foo_filter = 'hello'
country_code_filter = 'ES'
result = Bar.filter_by(foo=foo_filter, country_code=country_code_filter).first()
if not result:
    result = Bar.filter_by(foo=foo_filter, country_code='RoW').first()
if not result:
    raise RuntimeException(f""No data for combination {foo_filter}, {country_code_filter} or {foo_filter}, RoW"")


This approach of running similar queries multiple times and checking the result each time until I get a row feels very messy/wrong but I haven't been able to find any better approaches that allow you to set an 'alternative' filter when your initial query returns 0 rows in Flask SQLAlchemy

Is there a cleaner approach to this?
",0,1239,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/58534100,"TimeoutError QueuePool limit of size x overflow xy reached, connection timed out, timeout 30","This is the error that I get:
RemoteError: Remote error: TimeoutError QueuePool limit of size x overflow xy reached, connection timed out, timeout 30 (Background on this error at: http://sqlalche.me/e/3o7r)

We have an issue with metadata agent. One compute node hosts many vms and the vms can't get ip. I've tried to restart on the dhcp metadata server both service, tried to restart our rabbitmq server as well. Also restarted neutron server, but the issue still persist and the vms are down.

Errors like these:

2019-10-24 10:34:47.794 86154 ERROR neutron.agent.metadata.agent
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent [-] Unexpected error.: RemoteError: Remote error: TimeoutError QueuePool limit of size 5 overflow 50 reached, connection timed out, timeout 30 (Background on this error at: http://sqlalche.me/e/3o7r)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent Traceback (most recent call last):
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 89, in __call__
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     instance_id, tenant_id = self._get_instance_and_tenant_id(req)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 162, in _get_instance_and_tenant_id
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     ports = self._get_ports(remote_address, network_id, router_id)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 155, in _get_ports
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return self._get_ports_for_remote_address(remote_address, networks)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/common/cache_utils.py"", line 116, in __call__
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return self.func(target_self, *args, **kwargs)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 137, in _get_ports_for_remote_address
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     ip_address=remote_address)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 106, in _get_ports_from_server
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return self.plugin_rpc.get_ports(self.context, filters)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py"", line 72, in get_ports
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return cctxt.call(context, 'get_ports', filters=filters)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/neutron/common/rpc.py"", line 150, in call
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     return self._original_context.call(ctxt, method, **kwargs)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py"", line 179, in call
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     retry=self.retry)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/oslo_messaging/transport.py"", line 133, in _send
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     retry=retry)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py"", line 584, in send
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     call_monitor_timeout, retry=retry)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent   File ""/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py"", line 575, in _send
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent     raise result
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent RemoteError: Remote error: TimeoutError QueuePool limit of size 5 overflow 50 reached, connection timed out, timeout 30 (Background on this error at: http://sqlalche.me/e/3o7r)
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent [u'Traceback (most recent call last):\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_messaging/rpc/server.py"", line 163, in _process_incoming\n    res = self.dispatcher.dispatch(message)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py"", line 265, in dispatch\n    return self._do_dispatch(endpoint, method, ctxt, args)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py"", line 194, in _do_dispatch\n    result = func(ctxt, **new_args)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron/api/rpc/handlers/metadata_rpc.py"", line 43, in get_ports\n    return self.plugin.get_ports(context, filters=filters)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron/db/api.py"", line 123, in wrapped\n    return method(*args, **kwargs)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron_lib/db/api.py"", line 140, in wrapped\n    setattr(e, \'_RETRY_EXCEEDED\', True)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 220, in __exit__\n    self.force_reraise()\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 196, in force_reraise\n    six.reraise(self.type_, self.value, self.tb)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron_lib/db/api.py"", line 136, in wrapped\n    return f(*args, **kwargs)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_db/api.py"", line 154, in wrapper\n    ectxt.value = e.inner_exc\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 220, in __exit__\n    self.force_reraise()\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 196, in force_reraise\n    six.reraise(self.type_, self.value, self.tb)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_db/api.py"", line 142, in wrapper\n    return f(*args, **kwargs)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron_lib/db/api.py"", line 183, in wrapped\n    LOG.debug(""Retry wrapper got retriable exception: %s"", e)\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 220, in __exit__\n    self.force_reraise()\n', u'  File ""/usr/lib/python2.7/site-packages/oslo_utils/excutils.py"", line 196, in force_reraise\n    six.reraise(self.type_, self.value, self.tb)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron_lib/db/api.py"", line 179, in wrapped\n    return f(*dup_args, **dup_kwargs)\n', u'  File ""/usr/lib/python2.7/site-packages/neutron/db/db_base_plugin_v2.py"", line 1435, in get_ports\n    items = [self._make_port_dict(c, fields) for c in query]\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2925, in __iter__\n    return self._execute_and_instances(context)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2946, in _execute_and_instances\n    close_with_result=True)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2955, in _get_bind_args\n    **kw\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2937, in _connection_from_session\n    conn = self.session.connection(**kw)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 1035, in connection\n    execution_options=execution_options)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 1042, in _connection_for_bind\n    conn = engine.contextual_connect(**kw)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2123, in contextual_connect\n    self._wrap_pool_connect(self.pool.connect, None),\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2158, in _wrap_pool_connect\n    return fn()\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py"", line 403, in connect\n    return _ConnectionFairy._checkout(self)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py"", line 788, in _checkout\n    fairy = _ConnectionRecord.checkout(pool)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py"", line 532, in checkout\n    rec = pool._do_get()\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py"", line 1186, in _do_get\n    (self.size(), self.overflow(), self._timeout), code=""3o7r"")\n', u'TimeoutError: QueuePool limit of size 5 overflow 50 reached, connection timed out, timeout 30 (Background on this error at: http://sqlalche.me/e/3o7r)\n'].
2019-10-24 10:34:48.433 86149 ERROR neutron.agent.metadata.agent
2019-10-24 10:35:04.436 86151 WARNING oslo_messaging._drivers.amqpdriver [-] Number of call queues is 21, greater than warning threshold: 20. There could be a leak. Increasing threshold to: 40
2019-10-24 10:35:04.637 86156 WARNING oslo_messaging._drivers.amqpdriver [-] Number of call queues is 21, greater than warning threshold: 20. There could be a leak. Increasing threshold to: 40
2019-10-24 10:35:05.339 86149 WARNING oslo_messaging._drivers.amqpdriver [-] Number of call queues is 21, greater than warning threshold: 20. There could be a leak. Increasing threshold to: 40


What should I do to fix it :( ?

Or how can I finetune this sqlalchemy? I have no idea to be honest what is this.
",0,797,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/56877620,SqlAlchemy Query.All() Unexpectedly Returning OperationalError,"I'm implementing search functionality using Elasticsearch in a ""Reddit clone"" web application that I'm developing. I want to support searching for threads, users, and subreddits, but when I enter a search query and search for one of the 3 above mentioned categories that does not hold any matches, I'm getting an unexpected ""OperationalError"" instead of an empty set of results.

As shown in the code I included, I attempted to use the sqlalchemy.orm.query.Query.all() function which returned the following error:

OperationalError: (sqlite3.OperationalError) near ""END"": syntax error
[SQL: SELECT user.id AS user_id, user.username AS user_username, user.email AS user_email, user.password_hash AS user_password_hash, user.last_sign_in AS user_last_sign_in 
FROM user 
WHERE 1 != 1 ORDER BY CASE user.id END]
(Background on this error at: http://sqlalche.me/e/e3q8)


I researched other StackOverflow posts and found that the first() function internally processes the database result and returns None if no results are found, but when I switched to that function, I faced this error:

OperationalError: (sqlite3.OperationalError) near ""END"": syntax error
[SQL: SELECT user.id AS user_id, user.username AS user_username, user.email AS user_email, user.password_hash AS user_password_hash, user.last_sign_in AS user_last_sign_in 
FROM user 
WHERE 1 != 1 ORDER BY CASE user.id END
 LIMIT ? OFFSET ?]
[parameters: (1, 0)]
(Background on this error at: http://sqlalche.me/e/e3q8)


Checking the documentation for SqlAlchemy, I don't see any mention of this error in either function, and reading the meaning of OperationalError, I'm concerned that my database setup is possibly incorrect. 

app/routes.py: This is the route that handles search requests made to the following URL: http://localhost:5000/search?q=&amp;index=

@app.route('/search', methods=['GET'])
def search():
    print 'Hit the /search route!'
    if not g.search_form.validate():
        return redirect(url_for('index'))
    page = request.args.get('page', 1, type=int)
    target_index = request.args.get('index', 'thread')
    if target_index == 'thread':
        results, total = Thread.search(g.search_form.q.data, page, app.config['POSTS_PER_PAGE'])
        print 'Called Thread.search(), total results = {}'.format(total['value'])
    elif target_index == 'user':
        results, total = User.search(g.search_form.q.data, page, app.config['POSTS_PER_PAGE'])
        print 'Called User.search(), total results = {}'.format(total['value'])
    elif target_index == 'subreddit':
        results, total = Subreddit.search(g.search_form.q.data, page, app.config['POSTS_PER_PAGE'])
        print 'Called Subreddit.search(), total results = {}'.format(total['value'])
    else:
        return render_template('404.html')
    try:
        results = results.all()
    except OperationalError:
        results = [None]
    total = total['value']
    next_url = url_for('search', index=target_index, q=g.search_form.q.data, page=page + 1) if total &gt; page * app.config['POSTS_PER_PAGE'] else None
    prev_url = url_for('search', index=target_index, q=g.search_form.q.data, page=page - 1) if page &gt; 1 else None
    results_list = zip(results, [None] * len(results)) # Temporarily to match expected input for template
    return render_template('search.html', title=_('Search'), results_list=results_list, next_url=next_url, prev_url=prev_url, query=g.search_form.q.data, index=target_index)


app/models.py:

class SearchableMixin(object):
    @classmethod
    def search(cls, expression, page, per_page):
        ids, total = query_index(cls.__tablename__, expression, page, per_page)
        if total == 0:
            return cls.query.filter_by(id=0), 0
        when = []
        for i in range(len(ids)):
            when.append((ids[i], i))
        return cls.query.filter(cls.id.in_(ids)).order_by(
            db.case(when, value=cls.id)), total

    @classmethod
    def before_commit(cls, session):
        session._changes = {
            'add': list(session.new),
            'update': list(session.dirty),
            'delete': list(session.deleted)
        }

    @classmethod
    def after_commit(cls, session):
        for obj in session._changes['add']:
            if isinstance(obj, SearchableMixin):
                add_to_index(obj.__tablename__, obj)
        for obj in session._changes['update']:
            if isinstance(obj, SearchableMixin):
                add_to_index(obj.__tablename__, obj)
        for obj in session._changes['delete']:
            if isinstance(obj, SearchableMixin):
                remove_from_index(obj.__tablename__, obj)
        session._changes = None

    @classmethod
    def reindex(cls):
        for obj in cls.query:
            add_to_index(cls.__tablename__, obj)

db.event.listen(db.session, 'before_commit', SearchableMixin.before_commit)
db.event.listen(db.session, 'after_commit', SearchableMixin.after_commit)

# Below is one model that implements SearchableMixin to allow searching # for users. Thread and Subreddit models follow the same logic.
class User(db.Model, UserMixin, SearchableMixin):
    __searchable__ = ['username']
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(64), index=True, unique=True)
    # &lt;Remaining User model fields here...&gt;


app/search.py: (Holds the underlying search functions to query Elasticsearch indices)

def add_to_index(index, model):
    if not app.elasticsearch:
        return
    payload = {}
    for field in model.__searchable__:
        payload[field] = getattr(model, field)
    app.elasticsearch.index(index=index, doc_type=index, id=model.id,
                                    body=payload)

def remove_from_index(index, model):
    if not app.elasticsearch:
        return
    app.elasticsearch.delete(index=index, doc_type=index, id=model.id)

def query_index(index, query, page, per_page):
    if not app.elasticsearch:
        return [], 0
    search = app.elasticsearch.search(
        index=index,
        body={'query': {'multi_match': {'query': query, 'fields': ['*']}},
              'from': (page - 1) * per_page, 'size': per_page})
    ids = [int(hit['_id']) for hit in search['hits']['hits']]
    return ids, search['hits']['total']


As my included app/routes.py shows, I made a workaround by catching the OperationalError and treating it as an indicator that no results were found, but since the all() documentation makes no mention of it, I did not expect there to be this exception being raised.
",0,371,,,
SQLAlchemy unexpected result,https://stackoverflow.com/questions/28008124,How to find all columns with a particular number as an attribute?,,0,268,,,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/17198595,SQLAlchemy DELETE Error caused by having a both lazy-load AND a dynamic version of the same relationship,"Here is some example code:

users_groups = Table('users_groups', Model.metadata,
    Column('user_id', Integer, ForeignKey('users.id')),
    Column('group_id', Integer, ForeignKey('groups.id'))
)

class User(Model):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)


class Group(Model):
    __tablename__ = 'groups'
    id = Column(Integer, primary_key=True)

    users = relationship('User', secondary=users_groups, lazy='select', backref='groups')
    users_dynamic = relationship('User', secondary=users_groups, lazy='dynamic')


So what happens here is that if you add a bunch of users to a group like so:

g = Group()
g.users = [User(), User(), User()]
session.add(g)
session.commit()


and then try to delete the group

session.delete(g)
session.commit()


You will get some form of this error:

DELETE statement on table 'users_groups' expected to delete 3 row(s); Only 0 were matched.


Removing the 2nd version of the relationship (the dynamic one in my case) fixes this problem. I am not even sure where to begin in terms of understanding why this is happening. I have been using 2 versions of various relationships in many cases throughout my SQLAlchemy models in order to make it easy to use the most appropriate query-strategy given a situation. This is the first time it has caused an unexpected issue.

Any advice is welcome.
",12,6206,"both the Group.users and Group.users_dynamic relationships are attempting to reconcile the fact that the Group is being deleted along with being able to manage the User() objects they refer to; one relationship succeeds while the second one fails, as the rows in the association table were already deleted.  The most straightforward solution is to mark all but one of the identical relationships as viewonly:

class Group(Base):
    __tablename__ = 'groups'
    id = Column(Integer, primary_key=True)

    users = relationship('User', secondary=users_groups, lazy='select', backref='groups')
    users_dynamic = relationship('User', viewonly=True, secondary=users_groups, lazy='dynamic')


if you're still wanting to have both relationships handle some degree of mutations, you'd need to do this carefully as SQLAlchemy doesn't know how to coordinate among changes in two relationships at the same time, so conflicts like this can continue to happen (like double inserts, etc) if you make equivalent mutations on both relationships.   To just take care of the ""delete"" issue by itself, you can also try setting Group.users_dynamic to passive_deletes=True:

class Group(Base):
    __tablename__ = 'groups'
    id = Column(Integer, primary_key=True)

    users = relationship('User', secondary=users_groups, lazy='select', backref='groups')
    users_dynamic = relationship('User', passive_deletes=True, secondary=users_groups, lazy='dynamic')

","I just add another simple workaround.

You can delete the collections before deleting the item itself:

&gt;&gt;&gt; for user in group.users:
        group.users.remove(user)
&gt;&gt;&gt; db.session.delete(group)
&gt;&gt;&gt; db.session.commit()


Alternatively, you can also set it as an empty list:

&gt;&gt;&gt; group.users = []
&gt;&gt;&gt; db.session.commit()
&gt;&gt;&gt; db.session.delete(group)
&gt;&gt;&gt; db.session.commit()

",
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/21076105,Is this an acceptable way to make threaded SQLAlchemy queries from Twisted?,"I've been doing some reading on using SQLAlchemy's ORM in the context of a Twisted application.  It's a lot of information to digest, so I'm having a bit of trouble putting all the pieces together.  So far, I've gathered the following absolute truths:


One session implies one thread.  Always.
scoped_session, by default, provides us with a way of constraining sessions to a given thread.  In other words, I am sure that by using scoped_session, I will not pass sessions to other threads (unless I do so explicitly, which I won't).


I also gathered that there are some issues relating to lazy/eager-loading and that one possible approach is to dissociate ORM objects from a session and reattach them to another session when changing threads.  I'm quite fuzzy on the details, but I also concluded that scoped_session renders many of these points moot.

My first question is whether or not I am severely mistaken in my above conclusions.

Beyond that, I've crafted this approach, which I hope is satisfactory.

I begin by creating a scoped_session object...

Session = scoped_session(sessionmaker(bind=_my_engine))


... which I will then use from a context manager, in order to handle exceptions and clean-up gracefully:

@contextmanager
def transaction_context():
    session = Session()
    try:
        yield session
        session.commit()
    except:
        session.rollback()
        raise
    finally:
        session.remove()  # dispose of the session


Now all I need to do is to use the above context manager in a function that is deferred to a separate thread.  I've thrown together a decorator to make things a bit prettier:

def threaded(fn):
    @wraps(fn)  # functools.wraps
    def wrapper(*args, **kwargs):
        return deferToThread(fn, *args, **kwargs)  # t.i.threads.deferToThread
    return wrapper


Here is an example of how I intend to use the whole shebang.  Below is a function that performs a DB lookup using the SQLAlchemy ORM:

@threaded
def get_some_attributes(group):
    with transaction_context() as session:
        return session.query(Attribute).filter(Attribute.group == group)


My second question is whether or not this approach is viable.


Am I making any fundamentally flawed assumptions?
Are there any caveats?
Is there a better way?


Edit: Here is a related question concerning the unexpected error in my context manager.
",4,1723,"Right now I work on this exact problem, and I think I found a solution. 

Indeed, you must defer all database access functions to a thread. But in your solution, you remove the session after querying the database, so all your results ORM objects will be detached and you wont have access to their fields.

You can't use scoped_session because in Twisted we have only one MainThread (except with things that work in deferToThread). We can, however, use scoped_sesssion with scopefunc. 

In Twisted there is a great thing known as ContextTracker:


  provides a way to pass arbitrary key/value data up and down a call
  stack without passing them as parameters to the functions on that call
  stack.


In my twisted web app in method render_GET I set a uuid parameter:

call = context.call({""uuid"": str(uuid.uuid4())}, self._render, request)


and then I call the _render method to do the actual work (work with db, render html, etc).

I create the scoped_session like this: 

scopefunc = functools.partial(context.get, ""uuid"")
Session = scoped_session(session_factory, scopefunc=scopefunc)


Now within any function calls of _render I can get session with:

Session()


and at the end of _render I have to do Session.remove() to remove the session.

It worksa with my webapp and I think can work for other tasks.

This is completely standalone example, show how all it work together.

from twisted.internet import reactor, threads
from twisted.web.resource import Resource
from twisted.web.server import Site, NOT_DONE_YET
from twisted.python import context
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import sessionmaker, scoped_session
from sqlalchemy.ext.declarative import declarative_base
import uuid
import functools

engine = create_engine(
    'sqlite:///test.sql',
    connect_args={'check_same_thread': False},
    echo=False)

session_factory = sessionmaker(bind=engine)
scopefunc = functools.partial(context.get, ""uuid"")
Session = scoped_session(session_factory, scopefunc=scopefunc)
Base = declarative_base()


class User(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)
    name = Column(String)

Base.metadata.create_all(bind=engine)


class TestPage(Resource):
    isLeaf = True

    def render_GET(self, request):
        context.call({""uuid"": str(uuid.uuid4())}, self._render, request)
        return NOT_DONE_YET

    def render_POST(self, request):
        return self.render_GET(request)

    def work_with_db(self):
        user = User(name=""TestUser"")
        Session.add(user)
        Session.commit()
        return user

    def _render(self, request):
        print ""session: "", id(Session())
        d = threads.deferToThread(self.work_with_db)

        def success(result):
            html = ""added user with name - %s"" % result.name
            request.write(html.encode('UTF-8'))
            request.finish()
            Session.remove()
        call = functools.partial(context.call, {""uuid"": scopefunc()}, success)
        d.addBoth(call)
        return d

if __name__ == ""__main__"":
    reactor.listenTCP(8888, Site(TestPage()))
    reactor.run()


I print out id of session, and you can see that its different for each request. If you remove scopefunc from scoped_session constructor and do two simultaneous request(insert time.sleep to work_with_db), you will get one common session for this two requests.


  The scoped_session object by default uses threading.local() as storage, so that a single Session is maintained for all who call upon the scoped_session registry, but only within the scope of a single thread


a problem here that in twisted we have only one thread for all requests. Thats why we have to create own scopefunc, that will show the difference between requests.

An other problem, that twisted didnt pass context to callbacks and we have to wrap callback and send current context to it.

call = functools.partial(context.call, {""uuid"": scopefunc()}, success)


Still I dont know how to make it work with defer.inLineCallback, that I use everywhere in my code.
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/76107844,SqlAlchemy StaleDataError on simple update statement,"I'm trying to update a user in the database but keep running into a StaleDataError.
user = session.query(User).get(1)
user.first_name # John
user.first_name = 'Sally'
session.commit()

# &gt; sqlalchemy.orm.exc.StaleDataError: UPDATE statement on table 'user' expected to update 1 row(s);
#   -1 were matched.

From the SqlAlchemy docs on StaleDataError:

An operation encountered database state that is unaccounted for.
Conditions which cause this to happen include:
A flush may have attempted to update or delete rows and an unexpected
number of rows were matched during the UPDATE or DELETE statement.
Note that when version_id_col is used, rows in UPDATE or DELETE
statements are also matched against the current known version
identifier.
A mapped object with version_id_col was refreshed, and the version
number coming back from the database does not match that of the object
itself.
A object is detached from its parent object, however the object was
previously attached to a different parent identity which was garbage
collected, and a decision cannot be made if the new parent was really
the most recent parent.

The docs don't elaborate on version_id_col. Could this be the issue? What is this column and where can I find if it's active?
Why can't SqlAlchemy locate the row id (which is obviously there since it pulled the row just moments before) and what's wrong with my update command?
",2,578,"I found the solution in an answer by @tamersalama. I'm reposting it here because that question was hard to find in connection to the StaleDataError.
Add __mapper_args__ to the SqlAlchemy model:
class User(Base):

    __tablename__ = 'user'
    user_id = Column(Integer, primary_key=True, nullable=False)
    first_name = Column(String(20))
    last_name = Column(String(20))

    __mapper_args__ = {
        'version_id_col': user_id, # this is the id column of the model
        'version_id_generator': False
    }

",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/75758327,SQLAlchemy: method &#39;_connection_for_bind()&#39; is already in progress,"I recently updated SQLAlchemy (with [asyncio] package) to 1.4.46 and started to get the following exception when committing:

sqlalchemy.exc.IllegalStateChangeError: Method 'commit()' can't be called here; method '_connection_for_bind()' is already in progress and this would cause an unexpected state change to &lt;SessionTransactionState.CLOSED: 5&gt;

Before updating to the new version, it was working fine.
# -*- coding:utf-8 -*-

from sqlalchemy import exc, event, text
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession as SQLAlchemyAsyncSession
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.ext.asyncio import AsyncEngine
from sqlalchemy.pool import NullPool, Pool
from contextvars import ContextVar
from sanic import Sanic
import asyncio


class EngineNotInitialisedError(Exception):
    pass


class DBSessionContext:
    def __init__(self, session: Session, commit_on_exit: bool = True) -&gt; None:
        self.session = session
        self._query = None
        self.commit_on_exit = commit_on_exit
        self.token = None

    async def close(self, exc_type=None, exc_value=None, traceback=None):
        if self._query:
            if exc_value and getattr(exc_value, 'status_code', 500) &gt; 300:
                await self._query.rollback()
                self._post_processing.clear()
            else:
                await self._query.commit()
                await self.run_post_processing()

            await self._query.close()

        if self._post_processing:
            await self.run_post_processing()

    def set_token(self, token):
        self.token = token

    @property
    def query(self) -&gt; Session:
        if not self._query:
            self._query = self.session()

        return self._query


class AsyncSession(SQLAlchemyAsyncSession):
    async def execute(self, statement, **parameters):
        try:
            if isinstance(statement, str):
                # We wrap around the `text()` method automatically
                statement = text(statement)
            return await super().execute(statement, parameters)
        except exc.OperationalError as e:
            if e.orig.args[0] == 1205:
                # Lock wait timeout exceeded
                await self.rollback()
                return await super().execute(statement, parameters)

            raise e


class DBSession:
    def __init__(self):
        self.engine = None
        self.session = None
        self._session = None
        self.context = ContextVar(""context"", default=None)

    def init_app(self, app: Sanic, url: str, commit_on_exit: bool = True) -&gt; None:
        self.commit_on_exit = commit_on_exit

        engine_args = {
            'echo': app.config.get('DATABASE_ECHO', cast=bool, default=False),
            'echo_pool': app.config.get('DATABASE_ECHO_POOL', cast=bool, default=False),
            'poolclass': NullPool,  # will be used to create a connection pool instance using the connection parameters given in the URL
            # if pool_class is not NullPool:

            # the number of connections to allow in connection pool overflow
            # 'max_overflow': app.config.get('DATABASE_MAX_OVERFLOW', cast=int, default=10),
            # if True will enable the connection pool pre-ping feature that tests connections for liveness upon each checkout
            # 'pool_pre_ping': app.config.get('DATABASE_POOL_PRE_PING', cast=bool, default=True),
            # the number of connections to keep open inside the connection pool
            # 'pool_size': app.config.get('DATABASE_POOL_SIZE', cast=int, default=5),
            # this setting causes the pool to recycle connections after the given number of seconds has passed
            # 'pool_recycle': app.config.get('DATABASE_POOL_RECYCLE', cast=int, default=-1),
            # number of seconds to wait before giving up on getting a connection from the pool
            # 'pool_timeout': app.config.get('DATABASE_POOL_TIMEOUT', cast=int, default=3600),
        }

        self.engine = create_async_engine(
            url,
            **engine_args
        )

        self.session = sessionmaker(
            bind=self.engine,
            expire_on_commit=False,
            class_=AsyncSession,
            autoflush=False
        )

    async def __aenter__(self):
        if not isinstance(self.engine, AsyncEngine):
            raise EngineNotInitialisedError

        session_ctx = DBSessionContext(self.session, self.commit_on_exit)
        session_ctx.set_token(self.context.set(session_ctx))

        return session_ctx

    async def __aexit__(self, exc_type, exc_value, traceback):
        session_ctx = self.context.get()
        await asyncio.shield(session_ctx.close(exc_type, exc_value, traceback))

        self.context.reset(session_ctx.token)

    @property
    def query(self) -&gt; Session:
        return self.context.get().query


@event.listens_for(Pool, ""checkout"")
def check_connection(dbapi_con, con_record, con_proxy):
    '''Listener for Pool checkout events that pings every connection before using.
    Implements pessimistic disconnect handling strategy. See also:
    http://docs.sqlalchemy.org/en/rel_0_8/core/pooling.html#disconnect-handling-pessimistic'''

    cursor = dbapi_con.cursor()
    try:
        cursor.execute(""SELECT 1"")
    except exc.OperationalError as ex:
        if ex.args[0] in (2006,   # MySQL server has gone away
                          2013,   # Lost connection to MySQL server during query
                          2055):  # Lost connection to MySQL server at '%s', system error: %d
            raise exc.DisconnectionError()  # caught by pool, which will retry with a new connection
        else:
            raise

    cursor.close()


db = DBSession()

The code is called with the following :
async with db:
    await db.query.execute('INSERT INTO ...')

What is causing the InvalidStateChangeError I'm having? How can I avoid this issue?
",2,2652,"There is a discussion on the Github repository of SQLAlchemy, that gives a reason why the issue is occurring:
https://github.com/sqlalchemy/sqlalchemy/discussions/9312
The suggestion is that the code is calling something like
asyncio.gather(func(session), func2(session) with the two function sharing the same session, which causes the sqlalchemy.exc.IllegalStateChangeError
Removing the asyncio.gather call resolve the issue. (Or use two sessions, one for each functions).
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/77280663,What is the difference between utf8mb4_cs_0900_as_cs and utf8mb4_900_as_cs?,"I'm facing issues when building unit tests for my SQLALCHEMY api. I noticed that my MySQL DB returns an unexpected order when asking to order by a string column.
A little investigation showed that our DB is using utf8mb4_cs_0900_as_cs.
But when trying to order with utf8mb4_0900_as_cs, I get the expected order.
Can someone explain to me what are the differences between these 2 collations?
mysql&gt; SELECT
    -&gt;     column_0 AS name1
    -&gt; FROM
    -&gt;     (VALUES
    -&gt;         ROW('default_chanakah'),
    -&gt; ROW('default_aaa'),
    -&gt; ROW('default_zzz'),
    -&gt;         ROW('default_hsunan'),
    -&gt;         ROW('default_kourin1')
    -&gt;     ) AS hardcodedNames
    -&gt; ORDER BY
    -&gt; name1
    -&gt; COLLATE utf8mb4_cs_0900_as_cs;
+------------------+
| name1            |
+------------------+
| default_aaa      |
| default_hsunan   |
| default_chanakah |
| default_kourin1  |
| default_zzz      |
+------------------+
5 rows in set (0.00 sec)

mysql&gt; SELECT
    -&gt;     column_0 AS name1
    -&gt; FROM
    -&gt;     (VALUES
    -&gt;         ROW('default_chanakah'),
    -&gt; ROW('default_aaa'),
    -&gt; ROW('default_zzz'),
    -&gt;         ROW('default_hsunan'),
    -&gt;         ROW('default_kourin1')
    -&gt;     ) AS hardcodedNames
    -&gt; ORDER BY
    -&gt; name1
    -&gt; COLLATE utf8mb4_0900_as_cs;
+------------------+
| name1            |
+------------------+
| default_aaa      |
| default_chanakah |
| default_hsunan   |
| default_kourin1  |
| default_zzz      |
+------------------+
5 rows in set (0.00 sec)

",1,55,"utf8mb4_cs_0900_as_cs is Czech utf8 (first cs indicate this), this alphabet contains ""letter"" Ch and it goes after h
this is why hsusan is before chanakah
from https://en.wikipedia.org/wiki/Czech_orthography

",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/76038019,SQL Alchemy error: MissingGreenlet: greenlet_spawn has not been called,"I am building backend application with fastapi and sqlalchemy, and while trying to get a relationship between tables, I am being hit with this error.
full error:

raise exc.MissingGreenlet(
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place? (Background on this error at: https://sqlalche.me/e/20/xd2s)

I have stablished other relationships between other tables on the same project the exact same way and had no issues whatsoever up to this point. Since all of the other relationships worked with no issues I can't tell why this one isn't working.
This is what my models for both tables looks like:
class ServiceOrderModel(Settings.DB_BASE_MODEL):
    __tablename__ = 'service_order'
    __allow_unmapped__ = True

    id = Column(Integer, primary_key=True, autoincrement=True)
    identifier = Column(String(16))
    description = Column(String(256))
    execution_value = Column(Float, nullable=True)
    charged_value = Column(Float, nullable=True)
    status_id = Column(Integer, ForeignKey('service_status.id'))
    status = relationship('ServiceStatusModel', lazy='joined', back_populates='service_orders')


class ServiceStatusModel(Settings.DB_BASE_MODEL):
    __tablename__ = 'service_status'
    __allow_unmapped__ = True

    id=Column(Integer, primary_key=True, autoincrement=True)
    status=Column(String(32))
    service_orders=relationship('ServiceOrderModel', back_populates='status', lazy='joined', uselist=True)

Can anyone help me spot why is this error happening?
For more context, this is the function that I am trying to use prior to receiving the error:
@service_order_router.post('/', status_code=status.HTTP_201_CREATED, response_model=ServiceOrderReturnSchema)
async def post(data: ServiceOrderUpdateSchema,db: AsyncSession = Depends(get_session)) -&gt; Response:

    async with db as database:
        new_service_order = ServiceOrderModel(
            identifier=data.identifier,
            description=data.description,
            execution_value=None,
            charged_value=None,
            status_id=data.status_id,
        )
        database.add(new_service_order)
        await database.commit()

        return new_service_order

",1,2605,"Seems like status is probably referenced during rendering the response but isn't loaded during creation because just an id is passed.  Can you try adding this line before returning the service order?
    await database.commit()
    # Force status to load
    await database.refresh(new_service_order, [""status""])
    return new_service_order

",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/77811401,Correct way of retaining database information with fastapi and sqlalchemy,"we've recently changed tech stack for our python API so it now uses fastAPI and SQLAlchemy 2. Great combo, I'm sure, but I'm having issues creating something which I'd imagine would be pretty simple. I'm trying to build a simple post endpoint which takes a request body, processes it, adds it to the database, then return the inserted database entity. Something like this:
async def create_new_shipment_with_full_data(
    session: AsyncSession, data: FullShipmentCreationRequestBody
) -&gt; Shipment:
    async with session:
        shipment_data = data.model_dump(exclude_none=True)
        cursor = await session.execute(insert(Shipment).values(**shipment_data).returning(Shipment))
        inserted_shipment = cursor.scalar()
        await session.commit()
    
        return inserted_shipment

This of course doesn't work (wouldn't be here if it did) since the shipment data seems to be lost once the session commits, yielding me this error:
MissingGreenlet(""greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place?"")
Now, I've tried solving it by expunging the instance from the session, but that doesn't seem to help in this situation. I've also fixed a similar endpoint by converting the shipment to a dictionary, yet I don't know if this is the best solution.
I'm really asking about the correct way of implementing this type of endpoints using fastapi and sqlalchemy. It feels like a very standard use case and I've been trying to get help from chatgpt which has currently shown me around 467 ways which don't work. Why does this feel like rocket science?
Thanks in advance
",0,72,"this error occurs when you try to pull up attributes synchronously, working in an asynchronous context, specifically in your case (I assume) that you have a Shipment object that has relation with any-obj object, and when you inserted something and indicated .returning, it will returns the Shipment object without any loaded relations, so after insert you have to session.refresh(inserted_shipment, attribute_names=[""any-obj""]) and its should fix your problem :)
also if you retrieve Shipment by id, for example, you have to load again any-obj attribute, it means:
async def retrieve(pk: int):
    stmt = select(Shipment).where(Shipment.id == pk).options(joinedload(Shipment.any-obj))
    result = await session.execute(stmt)
    return result.scalar()

or you may use selectinload (depends on your relations), read about it
you may read here about it
Speaking about how the greenlet pops up, Ill note that when you dont load these attributes yourself, the pydantic tries to get them synchronously
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/77383467,SQLAlchemy MissingGreenlet Error When Accessing User Object,"I'm encountering an issue in my FastAPI application when trying to access the user.password attribute. Here's a summary of the problem:
I'm using SQLAlchemy with an asynchronous MySQL database.
I've defined a User model with a password field in my application.
I'm trying to verify a user's password during the login process.
However, when I try to access user.password, I encounter the following error:
sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called; can't call await_only() here. Was IO attempted in an unexpected place?
I've ensured that the user object is retrieved from the database, and I can print the user object successfully before the password verification step.
Here's the relevant code:
async def login(request: LoginSchema):
    user = await UserRepository.find_by_email(request.email)

    if user and pwd_context.verify(request.password, user.password):
        # ... (token generation and response)


My database session setup and UserRepository are as follows:
# UserRepository
class UserRepository(BaseRepo):
    model = User

    @staticmethod
    async def find_by_email(email: str):
        qry = select(User).where(User.email == email)
        return (await db.execute(qry)).scalar_one_or_none()
    
    ```

#db session
class DatabaseSession:
def init(self):
self.session = None
self.engine = None
def __getattr__(self, item):
    return getattr(self.session, item)

def init(self):
    self.engine = create_async_engine(DATABASE_URL, future=True, echo=True)
    self.session = async_sessionmaker(self.engine, expire_on_commit=False, class_=AsyncSession)()

async def create_all(self):
    async with self.engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

db = DatabaseSession()
`
I suspect that this issue may be related to SQLAlchemy's lazy loading mechanism. Could someone provide insights into what might be causing this error and how I can access the user.password attribute without triggering this exception?
Thank you in advance for your help.
",0,162,"I realized in my models, the password field used the ""deferred"" keyword, in SQLAlchemy it's used to delay the loading of certain columns, which can be useful for optimizing database queries, especially when dealing with large datasets. However, in my case, using ""deferred"" on the password column caused unexpected behavior.
When I tried to access the ""user.password"" attribute, it triggered a database query because the password column was not loaded into memory until that point.
To resolve this issue, I removed the ""deferred"" keyword from the password column definition in my User model to ensure that the password column is loaded immediately when the User object is queried from the database, eliminating the greenlet spawn error.
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/76947178,"Update: Issue connecting to PostgreSQL database using ipython-sql (psycopg2==2.9.3, sqlalchemy==2.0.2, ipython-sql==0.3.9, postgres==15.3)","I am trying to connect to an existing PostgreSQL database. I am able to connect using the psql command-line interface, as well as pgAdmin. However, when I try to connect a python environment (JupyterLab and DataSpell), I am unable to connect.
Version Information:

psycopg2 version: 2.9.3
sqlalchemy version: 2.0.2
ipython-sql version: 0.3.9
postgres version: 15.3


%load_ext sql

%env DATABASE_URL=postgresql://retail_user:retail@localhost/retail_db

%%sql
SELECT * FROM orders LIMIT 10

The above code outputs:
__init__() got an unexpected keyword argument 'bind'
Connection info needed in SQLAlchemy format, example: postgresql://username:password@hostname/dbname or an existing connection: dict_keys([])

After some investigation, I discovered that downgrading SQLAlchemy to version 1.4.22 resolved this issue.
",0,228,"The error was not due to incorrect connection details, but rather a compatibility issue between the versions of SQLAlchemy and ipython-sql I was using. I was initially using SQLAlchemy version 2.0.2, which resulted in this error.
To resolve the issue, I downgraded SQLAlchemy to version 1.4.22. After making this change, the connection worked as expected and I was able to execute SQL queries using ipython-sql without encountering the error.
For those who might face a similar problem:

Check the versions of SQLAlchemy and ipython-sql you are using.
If you are using a version of SQLAlchemy higher than 1.4.22, consider downgrading it to version 1.4.22.
Re-run your code after the downgrade to confirm that the issue is resolved.

Thank you to Adrian Klaver for suggesting the steps that ultimately led to the solution.
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/74809770,While upgrading Airflow version 2.0.0 to 2.2.5 got an error in initdb. the log is attached below,"
*While upgrading Airflow version 2.0.0 to 2.2.5 got an error in initdb. the log is attached below.
*





category=DeprecationWarning,
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361: DeprecationWarning: The logging_level option in [core] has been moved to the logging_level option in [logging] - the old setting has been used, but please update your config.
option = self._get_option_from_config_file(deprecated_key, deprecated_section, key, kwargs, section)
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The hide_sensitive_variable_fields option in [admin] has been moved to the hide_sensitive_var_conn_fields option in [core] - the old setting has been used, but please update your config.
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The base_log_folder option in [core] has been moved to the base_log_folder option in [logging] - the old setting has been used, but please update your config.
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The default_queue option in [celery] has been moved to the default_queue option in [operators] - the old setting has been used, but please update your config.
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The statsd_on option in [scheduler] has been moved to the statsd_on option in [metrics] - the old setting has been used, but please update your config.
/usr/local/lib/python3.6/site-packages/airflow/configuration.py:361 DeprecationWarning: The default_queue option in [celery] has been moved to the default_queue option in [operators] - the old setting has been used, but please update your config.
DB: postgresql+psycopg2://airflow-dev-ipc:***@7a2dcbf5-4b02-4462-9ab2-f52f835fd961.c7e06sed0lktba7pbqj0.databases.appdomain.cloud:31604/airflow_dev_ipc
[2022-12-15 09:36:38,662] {db.py:919} INFO - Creating tables
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.s3.S3Hook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.emr.EmrHook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.s3.S3Hook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
WARNI [airflow.providers_manager] Exception when importing 'airflow.providers.amazon.aws.hooks.emr.EmrHook' from 'apache-airflow-providers-amazon' package: deprecated() got an unexpected keyword argument 'name'
INFO  [alembic.runtime.migration] Running upgrade c8ffec048a3b -&gt; a56c9515abdc, Remove dag_stat table
Traceback (most recent call last):
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context
cursor, statement, parameters, context
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 608, in do_execute
cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: table ""dag_stats"" does not exist
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
File ""/usr/local/bin/airflow"", line 8, in 
sys.exit(main())
File ""/usr/local/lib/python3.6/site-packages/airflow/main.py"", line 48, in main
args.func(args)
File ""/usr/local/lib/python3.6/site-packages/airflow/cli/cli_parser.py"", line 48, in command
return func(*args, **kwargs)
File ""/usr/local/lib/python3.6/site-packages/airflow/cli/commands/db_command.py"", line 31, in initdb
db.initdb()
File ""/usr/local/lib/python3.6/site-packages/airflow/utils/session.py"", line 70, in wrapper
return func(*args, session=session, **kwargs)
File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 592, in initdb
upgradedb(session=session)
File ""/usr/local/lib/python3.6/site-packages/airflow/utils/session.py"", line 67, in wrapper
return func(*args, **kwargs)
File ""/usr/local/lib/python3.6/site-packages/airflow/utils/db.py"", line 920, in upgradedb
command.upgrade(config, 'heads')
File ""/usr/local/lib/python3.6/site-packages/alembic/command.py"", line 320, in upgrade
script.run_env()
File ""/usr/local/lib/python3.6/site-packages/alembic/script/base.py"", line 563, in run_env
util.load_python_file(self.dir, ""env.py"")
File ""/usr/local/lib/python3.6/site-packages/alembic/util/pyfiles.py"", line 92, in load_python_file
module = load_module_py(module_id, path)
File ""/usr/local/lib/python3.6/site-packages/alembic/util/pyfiles.py"", line 108, in load_module_py
spec.loader.exec_module(module)  # type: ignore
File """", line 678, in exec_module
File """", line 219, in _call_with_frames_removed
File ""/usr/local/lib/python3.6/site-packages/airflow/migrations/env.py"", line 107, in 
run_migrations_online()
File ""/usr/local/lib/python3.6/site-packages/airflow/migrations/env.py"", line 101, in run_migrations_online
context.run_migrations()
File """", line 8, in run_migrations
File ""/usr/local/lib/python3.6/site-packages/alembic/runtime/environment.py"", line 851, in run_migrations
self.get_context().run_migrations(**kw)
File ""/usr/local/lib/python3.6/site-packages/alembic/runtime/migration.py"", line 620, in run_migrations
step.migration_fn(**kw)
File ""/usr/local/lib/python3.6/site-packages/airflow/migrations/versions/a56c9515abdc_remove_dag_stat_table.py"", line 39, in upgrade
op.drop_table(""dag_stats"")
File """", line 8, in drop_table
File """", line 3, in drop_table
File ""/usr/local/lib/python3.6/site-packages/alembic/operations/ops.py"", line 1349, in drop_table
operations.invoke(op)
File ""/usr/local/lib/python3.6/site-packages/alembic/operations/base.py"", line 392, in invoke
return fn(self, operation)
File ""/usr/local/lib/python3.6/site-packages/alembic/operations/toimpl.py"", line 80, in drop_table
operation.to_table(operations.migration_context)
File ""/usr/local/lib/python3.6/site-packages/alembic/ddl/impl.py"", line 372, in drop_table
self._exec(schema.DropTable(table))
File ""/usr/local/lib/python3.6/site-packages/alembic/ddl/impl.py"", line 193, in _exec
return conn.execute(construct, multiparams)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1011, in execute
return meth(self, multiparams, params)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/sql/ddl.py"", line 72, in _execute_on_connection
return connection._execute_ddl(self, multiparams, params)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1073, in _execute_ddl
compiled,
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1317, in _execute_context
e, statement, parameters, cursor, context
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1511, in _handle_dbapi_exception
sqlalchemy_exception, with_traceback=exc_info[2], from_=e
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 182, in raise_
raise exception
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context
cursor, statement, parameters, context
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 608, in do_execute
cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) table ""dag_stats"" does not exist
[SQL:
DROP TABLE dag_stats]
(Background on this error at: http://sqlalche.me/e/13/f405)





I tried creating a table (dag_stats) in Postgres but still the issue continues.







",0,692,"You will have to upgrade the database, please refer this link for uprading Airflow version
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/67743572,Exporting data into CSV file from Flask-SQLAlchemy,"I'm looking to generate(export) a csv from a flask-sqlalchemy app i'm developing. But i'm getting some unexpected outcomes in my csv i.e. instead of the actual data from the MySQL DB table populated in the csv file, i get the declarative class model entries (placeholders??). The issue possibly could be the way i structured the query or even, the entire function.
Oddly enough - judging from the csv output (pic) - it would seem i'm on the right track since the row/column count is the same as the DB table but actual data is just not populated. I'm fairly new to SQLAlchemy ORM and Flask, so looking for some guidance here to pull through. Constructive feedback appreciated.
#class declaration with  DB object (divo)
class pearl(divo.Model):
    __tablename__ = 'users'                 
    work_id = divo.Column(divo.Integer, primary_key=True)
    user_fname = divo.Column(divo.String(length=255))
    user_lname = divo.Column(divo.String(length=255))
    user_category = divo.Column(divo.String(length=255))
    user_status = divo.Column(divo.String(length=1))
    login_id = divo.Column(divo.String(length=255))
    login_passwd = divo.Column(divo.String(length=255))

#user report function
@app.route(""/reports/users"")
def users_report():
    with open(r'C:\Users\Xxxxxxx\Projects\_repository\zzz.csv', 'w') as s_key:
        x15 = pearl.query.all()
        for i in x15:
#        x16 = tuple(x15)
            csv_out = csv.writer(s_key)
            csv_out.writerow(x15)
    flash(""Report generated. Please check designated repository."", ""green"")
    return redirect(url_for('reports_landing'))  # return redirect(url_for('other_tasks'))

#csv outcome (see attached pic)

",0,2406,"
instead of the actual data from the MySQL DB table populated in the csv file, i get the declarative class model entries (placeholders??)

Each object in the list
x15 = pearl.query.all()

represents a row in your users table.
What you're seeing in the spreadsheet are not placeholders, but string representations of each row object (See object.repr).
You could get the value of a column for a particular row object by the column name attribute, for example:
x15[0].work_id # Assumes there is at least one row object in x15

What you could do instead is something like this:
with open(r'C:\Users\Xxxxxxx\Projects\_repository\zzz.csv', 'w') as s_key:
    x15 = divo.session.query(pearl.work_id, pearl.user_fname) # Add columns to query as needed
    for i in x15:
        csv_out = csv.writer(s_key)
        csv_out.writerow(i)

i in the code above is a tuple of the form:
('work_id value', 'user_fname value')

",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/65815257,"SQLAclhemy, auroa-serverless invalid transaction issue on commit (aurora_data_api.exceptions.DatabaseError)","I'm using the sqlalchemy-aurora-data-api to connect to aurora-postgresql-serverless, with SQLalchemy as an ORM.
For the most part, this has been working fine, but I keep hitting unexpected errors from the aurora_data_api (which sqlalchemy-aurora-data-api is built upon) during commits.
I've tried to handle this in the application logic by catching the exception and re-trying, however, this is still failing:

from aurora_data_api.exceptions import DatabaseError
from botocore.exceptions import ClientError

def handle_invalid_transaction_id(func):
    retries = 3

    @wraps(func)
    def inner(*args, **kwargs):
        for i in range(retries):
            try:
                return func(*args, **kwargs)
            except (DatabaseError, ClientError):
                if i != retries:
                    # The aim here is to try and force a new transaction 
                    # If an error occurs and retry
                    db.session.close()
                else:
                    raise

    return inner


And then in my models doing something like this:
class MyModel(db.Model):
    @classmethod
    @handle_invalid_transaction_id
    def create(cls, **kwargs):
        instance = cls(**kwargs)
        db.session.add(instance)
        db.session.commit()
        db.session.close()
        return kwargs

However, I keep hitting unpredictable transaction failures:
DatabaseError: (aurora_data_api.exceptions.DatabaseError) An error occurred (BadRequestException) when calling the ExecuteStatement operation: Transaction AXwQlogMJsPZgyUXCYFg9gUq4/I9FBEUy1zjMTzdZriEuBCF44s+wMX7+aAnyyJH/6arYcHxbCLW73WE8oRYsPMN17MOrqWfUdxkZRBrM/vBUfrP8FKv6Phfr6kK6o7/0mirCtRJUxDQAQPotaeP+hHj6/IOGUCaOnodt4M3015c0dAycuqhsy4= is not found [+26ms]
It is worth noting that these are not particularly long-running transactions, so I do not think that I'm hitting the transaction expiry issue that can occur with aurora-serverless as documented here.
Is there something fundamentally wrong with my approach to this or is there a better way to handle transactions failures when they occur?
",0,534,"Just to close this off, and in case it helps anyone else, found the issue was in the transactions that were being created by in the cursor here
I can't answer the why, but we noticed that transactions were expiring despite the fact the data successfully committed. e.g:
request 1 - creates a bunch of transactions, write data, exits.
request 2 - creates a bunch of transactions, some transaction id for request 1 fails, exits.
So yeah, I don't think the issue is with the aurora-data-api, but somehow to do with transaction mgmt in general in aurora-serverless. In the end, we forked the repo and refactored so that everything is handled with ExecuteStatment calls rather than using transactions. It's been working fine so far (note we're using SQLalchemy so transactions are handled at the ORM level anyway).
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/64688620,How do I fix connection to db2 using SQLAlchemy in python?,"I'm having trouble connecting to my database BLUDB in IBM Db2 on Cloud using SQLAlchemy. Here is the code I've always used and it's always worked fine:
%sql ibm_db_sa://user:pswd@some-host.services.dal.bluemix.net:50000/BLUDB

But now I get this error:

(ibm_db_dbi.ProgrammingError) ibm_db_dbi::ProgrammingError:
Exception('[IBM][CLI Driver] SQL1042C  An unexpected system error
occurred.  SQLSTATE=58004\r SQLCODE=-1042') (Background on this error
at: http://sqlalche.me/e/13/f405) Connection info needed in SQLAlchemy
format, example: postgresql://username:password@hostname/dbname or an
existing connection: dict_keys([])

These packages are loaded as always:
import ibm_db
import ibm_db_sa
import sqlalchemy
from sqlalchemy.engine import create_engine
I looked at the python db2 documentation on ibm and the sqlalchemy error message but couldn't get anywhere.
I am working in Jupyterlab locally. I've recently reinstalled Python and Jupyterlab. That's the only thing locally that's changed.
I am able to successfully run the notebooks in the cloud at kaggle and cognitive class. I am also able to connect and query sqlite3 via python without an issue using my local notebook.
All the ibm modules and version numbers are the same before and after installation. I used requirements.txt for reinstallation.
In db2diag.log here are the last two entries:
2020-11-05-14.06.47.081000-300 I13371F372           LEVEL: Warning
PID     : 17500                TID : 7808           PROC : python.exe
INSTANCE:                      NODE : 000
HOSTNAME: DESKTOP-6FFFO2E
EDUID   : 7808
FUNCTION: DB2 UDB, bsu security, sqlexLogPluginMessage, probe:20
DATA #1 : String with size, 43 bytes
loadAuthidMapper: GetModuleHandle rc = 126
2020-11-05-14.13.49.282000-300 I13745F373           LEVEL: Warning
PID     : 3060                 TID : 12756          PROC : python.exe
INSTANCE:                      NODE : 000
HOSTNAME: DESKTOP-6FFFO2E
EDUID   : 12756
FUNCTION: DB2 UDB, bsu security, sqlexLogPluginMessage, probe:20
DATA #1 : String with size, 43 bytes
loadAuthidMapper: GetModuleHandle rc = 126
",0,1317,"I think the root of this will be down to the new version of Python and pip caching.
What version did you move from and what version are you now on. Is this a Python 2 to Python 3 change?  When changing versions, normally you would need to clean pip install all components, but pip does use a cache. Even for components that may need to be compiled, and there is a good chance that Db2 components are being compiled.
So what you will need to do is to re-install the dependancies with
pip install --no-cache-dir
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/59162110,Writing to SQL Server from Python,"This is the smallest example I can give that is a MRE

I am attempting to do the following:


Using pyodbc, read from a SQL Server instance (complete)
Then, print that data to verify it (complete)
Then, take that data, and insert it into a new table (or overwrite the table if it exists) &lt;- FAILURE


The code is below:

import pyodbc
import pandas as pd
import sqlalchemy as sa

sqlConn = pyodbc.connect(
    ""DRIVER={SQL Server};""
    ""SERVER=servername;""
    ""DATABASE=dbname;""
    ""Trusted_Connection=yes;""
)

sql = """"""
SELECT TOP (1000) [PART]
      ,[STEP]
      ,[COMPLETIONTIME]
  FROM [dbname].[dbo].[STEPS]
""""""

engine = sa.create_engine('mssql+pyodbc://servername/dbname')


df = pd.read_sql(sql, sqlConn)

df.to_sql(name = 'Test', con = engine, if_exists = 'replace', index = False)
sqlConn.commit()
sqlConn.close()


I get the following for an error:

  File ""WiP.py"", line 26, in &lt;module&gt;
    df.to_sql(name = 'Test', con = engine, if_exists = 'replace', index = False)
  File ""C:\Python367-64\lib\site-packages\pandas\core\generic.py"", line 2712, in to_sql
    method=method,
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 518, in to_sql
    method=method,
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 1319, in to_sql
    table.create()
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 641, in create
    if self.exists():
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 628, in exists
    return self.pd_sql.has_table(self.name, self.schema)
  File ""C:\Python367-64\lib\site-packages\pandas\io\sql.py"", line 1344, in has_table
    self.connectable.dialect.has_table, name, schema or self.meta.schema
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2162, in run_callable
    with self._contextual_connect() as conn:
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2242, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2280, in _wrap_pool_connect
    e, dialect, self
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 152, in reraise
    raise value.with_traceback(tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2276, in _wrap_pool_connect
    return fn()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 492, in checkout
    rec = pool._do_get()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 139, in _do_get
    self._dec_overflow()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 153, in reraise
    raise value
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 136, in _do_get
    return self._create_connection()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 308, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 437, in __init__
    self.__connect(first_connect_check=True)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 639, in __connect
    connection = pool._invoke_creator(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\strategies.py"", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\default.py"", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
sqlalchemy.exc.InterfaceError: (pyodbc.InterfaceError) ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')
(Background on this error at: http://sqlalche.me/e/rvf5)


I looked that up on this site, and got the following:


  Exception raised for errors that are related to the databases
  operation and not necessarily under the control of the programmer,
  e.g. an unexpected disconnect occurs, the data source name is not
  found, a transaction could not be processed, a memory allocation error
  occurred during processing, etc.


I have also consulted:


Connecting to Microsoft SQL server using Python
List sql tables in pandas.read_sql
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql_table.html
https://github.com/mkleehammer/pyodbc/issues/300
Connecting to SQL Server 2012 using sqlalchemy and pyodbc


I also tried switching df.to_sql(name = 'Test', con = engine, if_exists = 'replace', index = False) to df.to_sql(name = 'Test', con = sqlConn, if_exists = 'replace', index = False) but got the exact same error.

What am I doing incorrectly? How can I wrote to a new table (or overwrite an existing one) from a Pandas dataframe?

UPDATE

It appears the connection is failing. The following:

import pyodbc
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

sqlConn = pyodbc.connect(
    ""DRIVER={SQL Server};""
    ""SERVER=servername;""
    ""DATABASE=dbname;""
    ""Trusted_Connection=yes;""
)

sql = """"""
SELECT TOP (1000) [ORDR_PART_NO]
      ,[OROP_ID]
      ,[COMPLETIONTIME]
  FROM [dbname].[dbo].[OpsLookup]
""""""
engine = sa.create_engine('mssql+pyodbc://servername/dbname')

cnxn = engine.connect()
result = cnxn.execute(""SELECT TOP (1000) * FROM [dbname].[dbo].[STEPS]"")
for row in result:
    print(row)
cnxn.close()


Yields:

C:\Python367-64\lib\site-packages\sqlalchemy\connectors\pyodbc.py:79: SAWarning: No driver name specified; this is expected by PyODBC when using DSN-less connections
  ""No driver name specified; ""
Traceback (most recent call last):
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2276, in _wrap_pool_connect
    return fn()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 303, in unique_connection
    return _ConnectionFairy._checkout(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 492, in checkout
    rec = pool._do_get()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 139, in _do_get
    self._dec_overflow()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 153, in reraise
    raise value
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 136, in _do_get
    return self._create_connection()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 308, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 437, in __init__
    self.__connect(first_connect_check=True)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 639, in __connect
    connection = pool._invoke_creator(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\strategies.py"", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\default.py"", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
pyodbc.InterfaceError: ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""WiP.py"", line 21, in &lt;module&gt;
    cnxn = engine.connect()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2209, in connect
    return self._connection_cls(self, **kwargs)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 103, in __init__
    else engine.raw_connection()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2307, in raw_connection
    self.pool.unique_connection, _connection
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2280, in _wrap_pool_connect
    e, dialect, self
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 152, in reraise
    raise value.with_traceback(tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\base.py"", line 2276, in _wrap_pool_connect
    return fn()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 303, in unique_connection
    return _ConnectionFairy._checkout(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 492, in checkout
    rec = pool._do_get()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 139, in _do_get
    self._dec_overflow()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\util\compat.py"", line 153, in reraise
    raise value
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\impl.py"", line 136, in _do_get
    return self._create_connection()
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 308, in _create_connection
    return _ConnectionRecord(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 437, in __init__
    self.__connect(first_connect_check=True)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\pool\base.py"", line 639, in __connect
    connection = pool._invoke_creator(self)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\strategies.py"", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File ""C:\Python367-64\lib\site-packages\sqlalchemy\engine\default.py"", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
sqlalchemy.exc.InterfaceError: (pyodbc.InterfaceError) ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')
(Background on this error at: http://sqlalche.me/e/rvf5)

",0,418,"engine = sa.create_engine('mssql+pyodbc://servername/dbname')

needed to be changed to

engine = sa.create_engine('mssql+pyodbc://servername/dbname?trusted_connection=yes&amp;driver=ODBC Driver 13 for SQL Server')

Per: https://docs.sqlalchemy.org/en/13/dialects/mssql.html#hostname-connections
",,
SQLAlchemy unexpected issue,https://stackoverflow.com/questions/31325860,Dynamic Datasets and SQLAlchemy,"I am refactoring some old SQLite3 SQL statements in Python into SQLAlchemy.  In our framework, we have the following SQL statements that takes in a dict with certain known keys and potentially any number of unexpected keys and values (depending what information was provided).  

import sqlite3
import sys

def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d


def Create_DB(db):
    #    Delete the database
    from os import remove
    remove(db)

#   Recreate it and format it as needed
    with sqlite3.connect(db) as conn:
        conn.row_factory = dict_factory
        conn.text_factory = str

        cursor = conn.cursor()

        cursor.execute(""CREATE TABLE [Listings] ([ID] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL UNIQUE, [timestamp] REAL NOT NULL DEFAULT(( datetime ( 'now' , 'localtime' ) )), [make] VARCHAR, [model] VARCHAR, [year] INTEGER);"")


def Add_Record(db, data):
    with sqlite3.connect(db) as conn:
        conn.row_factory = dict_factory
        conn.text_factory = str

        cursor = conn.cursor()

        #get column names already in table
        cursor.execute(""SELECT * FROM 'Listings'"")
        col_names = list(map(lambda x: x[0], cursor.description))

        #check if column doesn't exist in table, then add it
        for i in data.keys():
            if i not in col_names:
                cursor.execute(""ALTER TABLE 'Listings' ADD COLUMN '{col}' {type}"".format(col=i, type='INT' if type(data[i]) is int else 'VARCHAR'))

        #Insert record into table
        cursor.execute(""INSERT INTO Listings({cols}) VALUES({vals});"".format(cols = str(data.keys()).strip('[]'), 
                    vals=str([data[i] for i in data]).strip('[]')
                    ))

#Database filename
db = 'test.db'

Create_DB(db)

data = {'make': 'Chevy',
    'model' : 'Corvette',
    'year' : 1964,
    'price' : 50000,
    'color' : 'blue',
    'doors' : 2}
Add_Record(db, data)

data = {'make': 'Chevy',
    'model' : 'Camaro',
    'year' : 1967,
    'price' : 62500,
    'condition' : 'excellent'}
Add_Record(db, data)


This level of dynamicism is necessary because there's no way we can know what additional information will be provided, but, regardless, it's important that we store all information provided to us.  This has never been a problem because in our framework, as we've never expected an unwieldy number of columns in our tables.

While the above code works, it's obvious that it's not a clean implementation and thus why I'm trying to refactor it into SQLAlchemy's cleaner, more robust ORM paradigm.  I started going through SQLAlchemy's official tutorials and various examples and have arrived at the following code:

from sqlalchemy import Column, String, Integer
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

Base = declarative_base()

class Listing(Base):
    __tablename__ = 'Listings'
    id = Column(Integer, primary_key=True)
    make = Column(String)
    model = Column(String)
    year = Column(Integer)

engine = create_engine('sqlite:///')

session = sessionmaker()
session.configure(bind=engine)
Base.metadata.create_all(engine)

data = {'make':'Chevy',
    'model' : 'Corvette',
    'year' : 1964}

record = Listing(**data)

s = session()
s.add(record)
s.commit()
s.close()


and it works beautifully with that data dict.  Now, when I add a new keyword, such as

data = {'make':'Chevy',
'model' : 'Corvette',
'year' : 1964,
'price' : 50000}


I get a TypeError: 'price' is an invalid keyword argument for Listing error.  To try and solve the issue, I modified the class to be dynamic, too:

class Listing(Base):
    __tablename__ = 'Listings'
    id = Column(Integer, primary_key=True)
    make = Column(String)
    model = Column(String)
    year = Column(Integer)

    def __checker__(self, data):
        for i in data.keys():
            if i not in [a for a in dir(self) if not a.startswith('__')]:
                if type(i) is int:
                    setattr(self, i, Column(Integer))
                else:
                    setattr(self, i, Column(String))
            else:
                self[i] = data[i]


But I quickly realized this would not work at all for several reasons, e.g. the class was already initialized, the data dict cannot be fed into the class without reinitializing it, it's a hack more than anything, et al.).  The more I think about it, the less obvious the solution using SQLAlchemy seems to me.  So, my main question is, how do I implement this level of dynamicism using SQLAlchemy?

I've researched a bit to see if anyone has a similar issue.  The closest I've found was Dynamic Class Creation in SQLAlchemy but it only talks about the constant attributes (""tablename"" et al.).  I believe the unanswered https://stackoverflow.com/questions/29105206/sqlalchemy-dynamic-attribute-change may be asking the same question.  While Python is not my forte, I consider myself a highly skilled programmer (C++ and JavaScript are my strongest languages) in the context scientific/engineering applications, so I may not hitting the correct Python-specific keywords in my searches.

I welcome any and all help.
",0,572,"class Listing(Base):
    __tablename__ = 'Listings'
    id = Column(Integer, primary_key=True)
    make = Column(String)
    model = Column(String)
    year = Column(Integer)
    def __init__(self,**kwargs):
       for k,v in kwargs.items():
           if hasattr(self,k):
              setattr(self,k,v)
           else:
              engine.execute(""ALTER TABLE %s AD COLUMN %s""%(self.__tablename__,k)
              setattr(self.__class__,Column(k, String))
              setattr(self,k,v)


might work ... maybe ... I am not entirely sure I did not test it

a better solution would be to use a relational table

class Attribs(Base):
    listing_id = Column(Integer,ForeignKey(""Listing""))
    name = Column(String)
    val = Column(String)

class Listing(Base):
    id = Column(Integer,primary_key = True)
    attributes = relationship(""Attribs"",backref=""listing"")
    def __init__(self,**kwargs):
        for k,v in kwargs.items():
            Attribs(listing_id=self.id,name=k,value=v)
    def __str__(self):
        return ""\n"".join([""A LISTING"",] + [""%s:%s""%(a.name,a.val) for a in self.attribs])


another solution would be to store json

class Listing(Base):
    __tablename__ = 'Listings'
    id = Column(Integer, primary_key=True)
    data = Column(String)
    def __init__(self,**kwargs):
       self.data = json.dumps(kwargs)
       self.data_dict = kwargs


the best solution would be to use a no-sql key,value store (maybe even just a simple json file? or perhaps shelve? or even pickle I guess)
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/4253176,Issue with SqlAlchemy - &quot;Parent instance &lt;SomeClass&gt; is not bound to a Session; lazy load operation...&quot;,"I have a small thrift server in python that I use do some fast lookups. The server queries mysql via SqlAlchemy on the first request and shoves all returned objects into a dictionary so on subsequent requests no DB call is needed. I just get the object from the dict and then call some of the object methods needed to give the proper response.

Initially, everything is fine. However, after the server runs a while, I am getting this exception when accessing the sqlalchemy object methods:


  
    Parent instance  is not bound to a Session; lazy load operation of attribute 'rate' cannot proceed.
  


Strange, because I set eagerload('rate').

I cannot really see a pattern to this behavior, it only affects some objects. However, once it does affect an object it will continue to do so on each request until I restart my python server.

Any ideas?
",9,11256,"You probably cache objects between the requests, and when the commit happens, session object is getting cleared, invalidating your objects. If you start your server via some multithreaded web server that starts workers as needed, that explains why there's no pattern.
If you dont want to get the bottom of this and just need a quick fix, this will always work:  

if obj not in session:
    obj = session.query(ObjClass).get(obj.id)


The proper solution would be to make sure you don't cache objects between requests.
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/38357352,Convert datetime to unix timestamp in SQLAlchemy model before executing query?,"I am using SQLAlchemy to work with a remote database that uses a strange timestamp format--it stores timestamps as double-precision milliseconds since epoch. I'd like to work with python datetime objects, so I wrote getter/setter methods in my model, following this gist:

from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import synonym
from sqlalchemy.dialects.mysql import DOUBLE
import datetime

Base = declarative_base()
class Table(Base):
    __tablename__ = ""table""

    id = Column(Integer, primary_key=True)
    _timestamp = Column(""timestamp"", DOUBLE(asdecimal=False))

    @property
    def timestamp(self):
        return datetime.datetime.utcfromtimestamp(float(self._timestamp)/1000.)

    @timestamp.setter
    def timestamp(self, dt):
        self._timestamp = float(dt.strftime(""%s""))*1000.

    timestamp = synonym('_timestamp', descriptor=timestamp)


This works great for inserting new rows into the table and working with objects from the table:

&gt;&gt;&gt; table = session.query(Table).first()
&lt;Table id=1&gt;
&gt;&gt;&gt; table.timestamp
datetime.datetime(2016, 6, 27, 16, 9, 3, 320000)
&gt;&gt;&gt; table._timestamp
1467043743320.0


However, it breaks down when I try to use a datetime in a filter expression:

&gt;&gt;&gt; july = datetime.datetime(2016, 7, 1)
&gt;&gt;&gt; old = session.query(Table).filter(Table.timestamp &lt; july).first()
/lib/python2.7/site-packages/sqlalchemy/engine/default.py:450: Warning: Truncated incorrect DOUBLE value: '2016-07-01 00:00:00'
&gt;&gt;&gt; july_flt = float(july.strftime(""%s""))*1000.
&gt;&gt;&gt; old = session.query(Table).filter(Table.timestamp &lt; july_flt).first()
&lt;Table id=1&gt;


I assume this is because my getter/setter methods apply to instances of the table class, but don't change the behavior of the class itself. I've tried rewriting using a hybrid property instead of a synonym:

from sqlalchemy.ext.hybrid import hybrid_property

class Table(Base):
    __tablename__ = ""table""

    id = Column(Integer, primary_key=True)
    _timestamp = Column(""timestamp"", DOUBLE(asdecimal=False))

    @hybrid_property
    def timestamp(self):
        return datetime.datetime.utcfromtimestamp(float(self._timestamp)/1000.)

    @timestamp.setter
    def timestamp(self, dt):
        self._timestamp = float(dt.strftime(""%s""))*1000.


Again, this works with Table instances, but fails on a query--now it's hitting my getter method when I run the query:

&gt;&gt;&gt; july = datetime.datetime(2016, 7, 1)
&gt;&gt;&gt; old = session.query(Table).filter(Table.timestamp &lt; july).first()
Traceback:
  File ""models.py"", line 42, in timestamp
    return datetime.datetime.utcfromtimestamp(float(self._timestamp)/1000.)
TypeError: float() argument must be a string or a number


With the debugger, I can see that the getter is receiving the Table._timestamp class (not a specific Table._timestamp, and not 'july').

I see that I could use the hybrid_property.expression decorator to define a SQL expression for converting timestamps into datetime, but what I'd really like is to convert the datetime into a timestamp on the python side, then run the query using timestamps. In other words, I'd like to use datetimes everywhere (including in queries), but have everything done with the microsecond timestamps on the SQL side. How can I do this?
",9,15018,"You have to use a custom type, which isn't as scary as it sounds.

from sqlalchemy.types import TypeDecorator


class DoubleTimestamp(TypeDecorator):
    impl = DOUBLE

    def __init__(self):
        TypeDecorator.__init__(self, as_decimal=False)

    def process_bind_param(self, value, dialect):
        return value.replace(tzinfo=datetime.timezone.utc).timestamp() * 1000

    def process_result_value(self, value, dialect):
        return datetime.datetime.utcfromtimestamp(value / 1000)


Then Table becomes:

class Table(Base):
    __tablename__ = ""table""

    id = Column(Integer, primary_key=True)
    timestamp = Column(DoubleTimestamp)


And then everything you mention works. You insert, select and compare with datetimes but it's stored as a DOUBLE.

Here I've used different logic for converting between timestamps since strftime('%s') isn't the correct solution. It's a different question which has been answered correctly here. Oh and I noticed you said microseconds but only convert to milliseconds in the code you posted, unless it was a slip of the tongue .
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/5675654,How to correctly achieve test isolation with a stateful Python module?,"The project I'm working on is a business logic software wrapped up as a Python package. The idea is that various script or application will import it, initialize it, then use it.

It currently has a top level init() method that does the initialization and sets up various things, a good example is that it sets up SQLAlchemy with a db connection and stores the SA session for later access. It is being stored in a subpackage of my project (namely myproj.model.Session, so other code could get a working SA session after import'ing the model).

Long story short, this makes my package a stateful one. I'm writing unit tests for the project and this stafeful behaviour poses some problems:


tests should be isolated, but the internal state of my package breaks this isolation
I cannot test the main init() method since its behavior depends on the state
future tests will need to be run against the (not yet written) controller part with a well known model state (eg. a pre-populated sqlite in-memory db)


Should I somehow refactor my package because the current structure is not the Best (possible) Practice(tm)? :)

Should I leave it at that and setup/teardown the whole thing every time? If I'm going to achieve complete isolation that'd mean fully erasing and re-populating the db at every single test, isn't that overkill?

This question is really on the overall code &amp; tests structure, but for what it's worth I'm using nose-1.0 for my tests. I know the Isolate plugin could probably help me but I'd like to get the code right before doing strange things in the test suite.
",8,1778,"You have a few options:

Mock the database

There are a few trade offs to be aware of.

Your tests will become more complex as you will have to do the setup, teardown and mocking of the connection.  You may also want to do verification of the SQL/commands sent.  It also tends to create an odd sort of tight coupling which may cause you to spend additonal time maintaining/updating tests when the schema or SQL changes.

This is usually the purest for of test isolation because it reduces a potentially large dependency from testing.  It also tends to make tests faster and reduces the overhead to automating the test suite in say a continuous integration environment.

Recreate the DB with each Test

Trade offs to be aware of.

This can make your test very slow depending on how much time it actually takes to recreate your database.  If the dev database server is a shared resource there will have to be additional initial investment in making sure each dev has their own db on the server.  The server may become impacted depending on how often tests get runs.  There is additional overhead to running your test suite in a continuous integration environment because it will need at least, possibly more dbs (depending on how many branches are being built simultaneously).

The benefit has to do with actually running through the same code paths and similar resources that will be used in production.  This usually helps to reveal bugs earlier which is always a very good thing.

ORM DB swap

If your using an ORM like SQLAlchemy their is a possibility that you can swap the underlying database with a potentially faster in-memory database.  This allows you to mitigate some of the negatives of both the previous options.

It's not quite the same database as will be used in production, but the ORM should help mitigate the risk that obscures a bug.  Typically the time to setup an in-memory database is much shorter that one which is file-backed.  It also has the benefit of being isolated to the current test run so you don't have to worry about shared resource management or final teardown/cleanup.
","Working on a project with a relatively expensive setup (IPython), I've seen an approach used where we call a get_ipython function, which sets up and returns an instance, while replacing itself with a function which returns a reference to the existing instance. Then every test can call the same function, but it only does the setup for the first one.

That saves doing a long setup procedure for every test, but occasionally it creates odd cases where a test fails or passes depending on what tests were run before. We have ways of dealing with that - a lot of the tests should do the same thing regardless of the state, and we can try to reset the object's state before certain tests. You might find a similar trade-off works for you.
","Mock is a simple and powerfull tool to achieve some isolation. There is a nice video from Pycon2011 which shows how to use it. I recommend to use it together with py.test which reduces the amount of code required to define tests and is still very, very powerfull.
"
SQLAlchemy strange behavior,https://stackoverflow.com/questions/7987981,strange python regex behavior - maybe connected to unicode or sqlalchemy,"I'm trying to search for a pattern in sqlalchemy results (actually filter by a 'like' or 'op'('regexp')(pattern) which I believe is implanted with regex somewhere) - the string and the search string are both in hebrew, and presumably (maybe I'm wrong-)-unicode
where r = u'' and c = u', ,    '
when I do re.search(r,c) I get the SRE.match object
but when I query the db like:

f = session.query(classname)
c = f[0].color


and c gives me:

'\xd7\x9c\xd7\x91\xd7\x9f,\xd7\x95\xd7\xa8\xd7\x95\xd7\x93,'


or print (c):

,,


practicaly the same but running re.search(r,c) gives me no match object.

Since I suspected a unicode issue I tried to transform to unicode with unicode(c)
and I get an 'UnicodeDecodeError: 'ascii' codec can't decode byte 0xd7 in position 0: ordinal' which I guess means this is already unicode string - so where's the catch here?
I would prefer using the sqlalchemy 'like' but I get the same problem there = where I know for sure (as I showed in my example that the data contains the string)

Should I transform the search string,pattern somehow? is this related to unicode? something else?

The db table (which I'm quering) collation is utf8_unicode_ci
",3,411,"c = f[0].color


is not returning a Unicode string (or its repr() would show a u'...' kind of string), but a UTF-8 encoded string.

Try 

c = f[0].color.decode(""utf-8"")


which results in

u'\u05dc\u05d1\u05df,\u05d5\u05e8\u05d5\u05d3,'


or

u',,'


if your console can display Hebrew characters.
","'\xd7\x9c\xd7\x91\xd7\x9f,\xd7\x95\xd7\xa8\xd7\x95\xd7\x93, is encoded representation of string u', , '. So in the second example you should write re.search(r,c.decode('utf-8'))
You're trying to do almost the same except setting encoding parameter. It makes python try ascii encoding
",
SQLAlchemy strange behavior,https://stackoverflow.com/questions/20933018,Random errors with SQLAlchemy,"I'm using a setup with nginx, uwsgi and SQLAlchemy. I recently switched from SQLObject and I'm now seeing strange random errors with SQLAlchemy. For instance:

sqlalchemy.exc.ResourceClosedError: This result object does not return rows. It has been closed automatically.


or:

sqlalchemy.exc.NoSuchColumnError: ""Could not locate column in row for column 'module.id'""


Is this some kind of behavior in SQLAlchemy which I'm not aware of? Can it be related to multiple processes/threads in uwsgi?

My uwsgi config file looks like this:

[uwsgi]
plugins=python
socket = 127.0.0.1:9002
wsgi-file = /thesystem/code/api.py
master = True
processes  = 4
threads = 2
daemonize = /thesystem/logs/uwsgi.log
pidfile = /thesystem/uwsgi.pid

",3,847,"Very probably you are opening connections in /thesystem/code/api.py entry point.

That means your file descriptors will be inherited in workers and this does not work with sqlalchemy.

Add --lazy-apps (lazy-apps = true in your ini config) to load /thesystem/code/api.py in each worker instead of loading it in the master and then calling fork()
","In addition to the accepted answer, if you do not want to (or cannot) change preforking for lazy-apps, because of the increase in memory usage, for instance, or the changes in your uwsgi reload strategy, you can simply reconnect to the database after forking:

import uwsgi
def setup_db():
    """""" routine that sets up the connection to your database """"""
    ...

uwsgi.post_fork_hook = setup_db

",
SQLAlchemy strange behavior,https://stackoverflow.com/questions/39298822,SQLAlchemy secondary join model fails under strange conditions,"I have a strange issue that I simply cannot resolve. Essentially, I have a model and system which works perfectly - except under a very specific (and seemingly arbitrary) set of circumstances.

I'll paste the model in a second but here's the idea. I want certain tables to be versioned. That means for a given table, I break it into two tables, the Master part which has the natural keys for the object, and the Version table which has all the associated data which may change. Then some of my models of course have a relationship, so I create a join table that links versions.

Here are the models:

class Versioned(object):

    def __init__(self, **kwargs):

        super(Versioned, self).__init__(**kwargs)

        self.active = True
        self.created_on = datetime.datetime.now()

    active = Column(BOOLEAN)
    created_on = Column(TIMESTAMP, server_default=func.now())

    def __eq__(self, other):

        return self.__class__ == other.__class__ and \
            all([getattr(self, key) == getattr(other, key)
                for key in self.comparison_keys
                ])

    def __ne__(self, other):

        return not self.__eq__(other)

    comparison_keys = []

class Parent(Base):

    __tablename__ = 'parent'

    id = Column(INTEGER, primary_key=True)

    name = Column(TEXT)

    versions = relationship(""ParentVersion"", back_populates=""master"")

    children = relationship(""Child"", back_populates=""parent"")

    @property
    def current_version(self):
        active_versions = [v for v in self.versions if v.active==True]

        return active_versions[0] if active_versions else None

class ParentVersion(Versioned, Base):

    __tablename__ = 'parent_version'

    id = Column(INTEGER, primary_key=True)

    master_id = Column(INTEGER, ForeignKey(Parent.id))

    address = Column(TEXT)

    master = relationship(""Parent"", back_populates=""versions"")

    children = relationship(""ChildVersion"",
        secondary=lambda : Parent_Child.__table__
    )

class Child(Base):

    __tablename__ = 'child'

    id = Column(INTEGER, primary_key=True)

    parent_id = Column(INTEGER, ForeignKey(Parent.id))

    name = Column(TEXT)

    versions = relationship(""ChildVersion"", back_populates=""master"")

    parent = relationship(""Parent"", back_populates=""children"")

    @property
    def current_version(self):
        active_versions = [v for v in self.versions if v.active==True]

        return active_versions[0] if active_versions else None


class ChildVersion(Versioned, Base):

    __tablename__ = 'child_version'

    id = Column(INTEGER, primary_key=True)

    master_id = Column(INTEGER, ForeignKey(Child.id))

    age = Column(INTEGER)

    fav_toy = Column(TEXT)

    master = relationship(""Child"", back_populates=""versions"")

    parents = relationship(""ParentVersion"",
        secondary=lambda: Parent_Child.__table__,
    )

    comparison_keys = [
        'age',
        'fav_toy',
    ]

class Parent_Child(Base):

    __tablename__ = 'parent_child'

    id = Column(INTEGER, primary_key=True)

    parent_id = Column(INTEGER, ForeignKey(ParentVersion.id))
    child_id = Column(INTEGER, ForeignKey(ChildVersion.id))


Okay, so I know the more recent SQLAlchemy models have some idea of versioning, it's possible that I'm doing this the wrong way. But this fits my use case very well. So humor me and let's assume the model is okay (in the general sense - if there's a minor detail causing the bug that would be good to fix)

Now suppose I want to insert data. I have data from some source, I take it in and build models. Ie, split things into Master/Version, assign the child relationships, assign the version relationships. Now I want to compare it against the data already in my database. For each master object, if I find it, I compare the versions. If the versions are different, you create a new version. The tricky part becomes, if a Child version is different, I want to insert a new Parent version, and update all its relationships. Maybe the code makes more sense to explain this part. search_parent is the object I have created in my pre-parsing stage. It has a version, and children objects, which also have versions.

parent_conds = [
    getattr(search_parent.__class__, name) == getattr(search_parent, name)
    for name, column in search_parent.__class__.__mapper__.columns.items()
    if not column.primary_key
]

parent_match = session.query(Parent).filter(*parent_conds).first()

# We are going to make a new version
parent_match.current_version.active=False
parent_match.versions.append(search_parent.current_version)

for search_child in search_parent.children[:]:

    search_child.parent_id = parent_match.id

    search_conds = [
        getattr(search_child.__class__, name) == getattr(search_child, name)
        for name, column in search_child.__class__.__mapper__.columns.items()
        if not column.primary_key
    ]

    child_match = session.query(Child).filter(*search_conds).first()

    if child_match.current_version != search_child.current_version:
        # create a new version: deactivate the old one, insert the new
        child_match.current_version.active=False
        child_match.versions.append(search_child.current_version)

    else:
        # copy the old version to point to the new parent version
        children = parent_match.current_version.children

        children.append(child_match.current_version)
        children.remove(search_child.current_version)
        session.expunge(search_child.current_version)

    session.expunge(search_child)

session.expunge(search_parent)

session.add(parent_match)

session.commit()


Okay, so once again, this might not be the perfect or even best approach. But it does work. EXCEPT, and this is what I can't figure out. It doesn't work if I'm updating the child's age attribute to the integer value zero. If the child objects start with age 0, and I change it to something else, this works beautifully. If I start with some non-zero integer, and update the age to 0, I get this warning:

SAWarning: Object of type &lt;ChildVersion&gt; not in session, add operation   along 'ParentVersion.children' won't proceed (mapperutil.state_class_str(child), operation, self.prop))


The updated version is inserted, however the insert into the parent_child join table doesn't happen. And it's not that it fails, it's that SQLAlchemy has determined the child object doesn't exist and can't create the join. But it does exist, I know it gets inserted.

Again, this only happens if I'm inserting a new version with age=0. If I'm inserting a new version with any other age, this works exactly as I want it to.

There are other odd things about the bug - it doesn't happen if you don't insert enough children (seems to be around 12 triggers the bug), it doesn't happen depending on other attributes sometimes. I don't think I fully understand the surface area of what causes it.

Thanks for taking the time to read this far. I have a fully working demo complete with source data I'd be happy to share, it just requires some setup so I didn't know if it was appropriate in this post. I hope someone has ideas for what to look at because at this point I'm totally out.

edit: Here is the full stack trace leading to the warning.

  File ""repro.py"", line 313, in &lt;module&gt;
  load_data(session, second_run)
File ""repro.py"", line 293, in load_data
  session.commit()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 801, in commit
  self.transaction.commit()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 392, in commit
  self._prepare_impl()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 372, in _prepare_impl
  self.session.flush()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 2019, in flush
  self._flush(objects)
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 2101, in _flush
  flush_context.execute()
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py"", line 373, in execute
  rec.execute(self)
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/unitofwork.py"", line 487, in execute
  self.dependency_processor.process_saves(uow, states)
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/dependency.py"", line 1053, in process_saves
  False, uowcommit, ""add""):
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/dependency.py"", line 1154, in _synchronize
  (mapperutil.state_class_str(child), operation, self.prop))
File ""/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 1297, in warn
  warnings.warn(msg, exc.SAWarning, stacklevel=2)
File ""repro.py"", line 10, in warn_with_traceback
  traceback.print_stack()
/Users/me/virtualenvs/dev/lib/python2.7/site-packages/sqlalchemy/orm/dependency.py:1154: SAWarning: Object of type &lt;ChildVersion&gt; not in session, add operation along 'ParentVersion.children' won't proceed
(mapperutil.state_class_str(child), operation, self.prop))


edit2:
Here is a gist with a python file you can run to see the strange behavior.
https://gist.github.com/jbouricius/2ede420fb1f7a2deec9f557c76ced7f9
",3,505,"The reason you get this error is that you've inadvertently added objects into the session.

Here is the MVCE:

engine = create_engine(""sqlite://"", echo=False)


def get_data():
    children = [
        Child(name=""Carol"", versions=[ChildVersion(age=0, fav_toy=""med"")]),
        Child(name=""Timmy"", versions=[ChildVersion(age=0, fav_toy=""med"")]),
    ]
    return Parent(
        name=""Zane"", children=children,
        versions=[
            ParentVersion(
                address=""123 Fake St"",
                children=[v for child in children for v in child.versions]
            )
        ]
    )


def main():
    Base.metadata.create_all(engine)

    session = Session(engine)
    parent_match = get_data()
    session.add(parent_match)
    session.commit()

    with session.no_autoflush:
        search_parent = get_data()

        parent_match.versions.append(search_parent.current_version)
        for search_child in search_parent.children[:]:
            child_match = next(c for c in parent_match.children if c.name == search_child.name)

            if child_match.current_version != search_child.current_version:
                child_match.versions.append(search_child.current_version)
            else:
                session.expunge(search_child.current_version)

            session.expunge(search_child)

        session.expunge(search_parent)
        session.commit()


Aside: this is what you needed to provide in the question itself. Providing a tarball with instructions is not the best way to get answers.

The line

parent_match.versions.append(search_parent.current_version)


not only adds search_parent.current_version, it also adds search_parent, which in turn adds all related objects, including the child versions of other children. Judging by the fact that you later expunge other related objects to prevent them from being added to the session, I conclude that you only want to add search_parent.current_version without adding other related objects. Due to the circular nature of your relationships you need to take care to lift only the objects you want out of search_parent before you add them. Here is the fixed MVCE:

with session.no_autoflush:
    search_parent = get_data()

    current_parent_version = search_parent.current_version
    search_parent.versions.remove(current_parent_version)
    current_parent_version.children = []  # &lt;--- this is key
    for search_child in search_parent.children[:]:
        child_match = next(c for c in parent_match.children if c.name == search_child.name)

        if child_match.current_version != search_child.current_version:
            current_child_version = search_child.current_version
            search_child.versions.remove(current_child_version)
            child_match.versions.append(current_child_version)
            current_parent_version.children.append(current_child_version)

    parent_match.versions.append(current_parent_version)

    session.commit()

",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/66896139,Flask-Executor and Flask-SQLAlchemy: Can&#39;t get updated data from DB inside of executor function,"I'm adding a Flask-based API to my web application to control the start and stop of some network automation functions. I've ran into a strange behavior where functions called by a Flask-Executor .submit() method are seemingly unable to get new or updated data from the database.
I know this question is very involved, so thank you to anyone who shares their time and input. See the end of this question for an overview of my project structure.
The flask-executor documentation says:

When calling submit() or map() Flask-Executor will wrap ThreadPoolExecutor callables with a copy of both the current application context and current request context

I don't quite fully understand what it means by context, but I feel that it might be a good hint about why this should or shouldn't work. (I am using the ThreadPoolExecutor, by the way). I assume that the db SQLAlchemy object is part of the application context, and as such a copy of db should be made available in the executor function. This didn't seem to be the case because I still had to import db in the file containing the function called by the executor, as you'll see later on in this post.
My front end has simple start and stop buttons which send a POST to the following API route:

file: app/api.py

from flask import request
from flask_login import login_required
from app import app, db, executor
from app.models import Project
from datetime import datetime
from automation.Staging import control

@app.route('/api/staging/control', methods=['POST'])
@login_required
def staging_control():
    data = request.json
    project_id = data['project-id']
    action = data['staging-control']

    project = Project.query.get(project_id)
    sp = project.staging_profile
    current_status = sp.status

    if action == 'start':
        if current_status == 'STARTED':
            return {'response': 200, 'message': 'Job already running!'}
        else:
            sp.status = 'STARTED'
            db.session.commit()
            # The executor only spawns the thread if the task status was not already started.
            executor.submit(control.start_staging, project_id)
        
    
    elif action == 'stop':
        if current_status == 'STARTED':
            sp.status = 'STOPPED'
            db.session.commit()

    return {'response' : 200, 'message': 'OK'}

Background
The status of the job is stored in a DB model.  If a start action is POSTed, the DB model's status column is updated. Likewise, if a stop action is POSTed, the DB model's status is updated.
The executor's function call to control.start_staging spawns a thread that begins an infinite loop which does some work and then sleeps for X seconds.  At the start of each time through the loop, I am trying to check the DB model's status column to determine whether or not to break from the loop and close the thread.
Starting the thread works just fine. The database model gets updated, the executor spawns the thread, and my while loop begins.
Sending the stop action from my frontend works just fine too. The status in the DB is set to STOPPED, and I can see this with manual queries to in my DB shell.
However, the control.start_staging function originally started by the executor still thinks the status is set to STARTED, even though it will actually be updated to STOPPED at some time during the thread's operation. I have attempted to get the updated value as many ways as I can think of from inside the thread. I've seen this similar question.
Here is the control.start_staging function. I've shared a few of the different ways that I've tried to get the updated status in the excerpt below as comments:

file: automation/Staging/control.py

from app import db
from app.models import Project, Staging_Profile
from app.config import STAGING_DURATION_MINS
from datetime import datetime, timedelta
from time import sleep


def start_staging(project_id):    
    project = Project.query.get(project_id)
    print(f""Received START for project {project.project_name}"")
    sp = project.staging_profile
    sp.last_activity = datetime.utcnow()
    db.session.commit()
    status = sp.status

    # Staging Loop Start
    while True:

        # This just serves as a force-stop if the job runs for more than STAGING_DURATION_MINUTES minutes.
        if sp.last_activity + timedelta(minutes=STAGING_DURATION_MINS) &gt; datetime.utcnow():
            print(f""Status is: {sp.status}"")

            # ATTEMPT 1: does not get updated data
            # status = sp.status

            # ATTEMPT 2: does not get updated data
            # status = Staging_Profile.query.get(project.staging_profile_id).status

            # ATTEMPT 3: does not get updated data
            all_profiles = db.session.query(Staging_Profile).all()
            this_profile = [profile for profile in all_profiles if profile.id == sp.id][0]

            if this_profile.status == 'STOPPED':
                print(""Status is STOPPED. Returning"")
                break
            
            else:
                print(f""Status is {this_profile.status}"")

            # Do work
            do_some_stuff()

        else:
            break
        sleep(5)

    return

Now, what's really puzzling is that I can write data to the database from inside the executor function. The line sp.last_activity = datetime.utcnow() followed by db.session.commit() successfully writes the current time when the thread is started.
My Suspicions
I have built this application in a very modular style approach, and I feel that perhaps this is the source of the issue.
Here is an overview of the relevant parts of my application structure:
app/
 __init__.py   # This is where my db &amp; executor are instantiated
 api.py        # This is where the /api/staging/control route lives
 models.py     # This holds my SQLAlchemy DB classes
 routes.py     # This holds my regular front-end routes
 config.py     # General config parameters

automation/
 Staging/
   control.py    # This is where the function passed to the executor is defined
   __init__.py   # Empty
 __init__.py      # Empty

Thanks again. I will post a resolution or a workaround to this issue when I find one.
",3,437,"use update
Project.query.filter_by(id=project_id).update({
 'last_activity': datetime.utcnow()
})

",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/46440663,Strange filter behavior in flask-sqlalchemy,"I have a query in flask-sqlalchemy and filter is behaving strange:

q.filter(Transaction.transaction_id == ReconciledTransaction.safe_withdraw_id).all()


It works fine, but:

q.filter(Transaction.transaction_id != ReconciledTransaction.safe_withdraw_id).all()


Doesn't work correctly! What seems to be the problem?

UPD
My models:
Reconciled transaction model:

class ReconciledTransactionModel(db.Model):
    """"""Reconciled Transaction model""""""

    __tablename__ = 'ReconciledTransaction'

    id = db.Column('id', db.Integer, primary_key=True, nullable=False)
    balance_entry_id = db.Column('BalanceEntry_id', db.Integer, db.ForeignKey(""BalanceEntry.id""), nullable=False)
    safe_withdraw_id = db.Column('Transaction_id', db.String, nullable=False)
    datetime = db.Column('datetime', db.Date(), nullable=False)
    balance_entry_amount = db.Column('BalanceEntry_amount', db.Float)
    reconciled_amount = db.Column('ReconciledAmount', db.Float)
    currency = db.Column('currency', db.String)
    reconciliation_status = db.Column('reconciliation_status', db.String, nullable=False)
    status_code = db.Column('status_code', db.Integer, nullable=False)


Transaction Model:

class TransactionModel(db.Model):
    """"""Transaction SA model.""""""

    __tablename__ = 'Transaction'

    id = db.Column('id', db.Integer, primary_key=True)
    till_id = db.Column('Till_id', db.Integer, db.ForeignKey(""Till.id""),
                        nullable=False)
    till = relationship(""Till"", foreign_keys=[till_id], backref=""transactions"", enable_typechecks=False)
    establishment_id = db.Column('Establishment_id', db.Integer,
                                 db.ForeignKey(""Establishment.id""),
                                 nullable=False)
    establishment = relationship(""Establishment"",
                                 foreign_keys=[establishment_id],
                                 backref=""transactions"",
                                 enable_typechecks=False)
    employee_id = db.Column('Employee_id', db.Integer,
                            db.ForeignKey(""Employee.id""),
                            nullable=False)
    employee = relationship(""Employee"",
                            foreign_keys=[employee_id],
                            backref=""transactions"",
                            enable_typechecks=False)
    local_time = db.Column('local_time', db.DateTime, nullable=False)
    create_time = db.Column('create_time', db.TIMESTAMP(timezone=True),
                            nullable=False)
    send_time = db.Column('send_time', db.TIMESTAMP(timezone=True),
                          nullable=False)
    receive_time = db.Column('receive_time', db.TIMESTAMP(timezone=True),
                             nullable=False)
    total_value = db.Column('total_value', db.Integer, nullable=False)
    amount = db.Column('amount', db.Float, nullable=False)
    discrepancy = db.Column('discrepancy', db.Float, nullable=False)
    type = db.Column('type', db.Enum('shift',
                                     'payment',
                                     'skimming',
                                     'withdraw',
                                     'refund',
                                     'till',
                                     'till_deposit',
                                     'safe_deposit',
                                     'safe_withdraw',
                                     'till_reset',
                                     name='transaction_type'),
                     nullable=False)
    status = db.Column('status',
                       db.Enum('start', 'end', name='transaction_status'),
                       nullable=False)
    receipt_id = db.Column('receipt_id', db.String(32), server_default=None)
    transaction_id = db.Column('transaction_id', db.String(32),
                               server_default=None)
    parent_transaction = db.Column('parent_transaction', db.String(32),
                                   server_default=None)
    discrepancy_reason = db.Column('discrepancy_reason', db.String(1024))
    resolve_discrepancy_reason = db.Column('resolve_discrepancy_reason',
                                           db.String(1024))
    accounted = db.Column('accounted', db.Boolean, default=False)


And here is my query:

_transactions = db.session.query(Transaction,
                                 status_sq.c.count,
                                 end_transaction_sq.c.discrepancy,
                                 end_transaction_sq.c.discrepancy_reason,
                                 end_transaction_sq.c.resolve_discrepancy_reason,
                                 end_transaction_sq.c.amount,
                                 ). \
    filter(Transaction.establishment_id.in_(store_ids)). \
    filter(Transaction.amount != 0). \
    filter_by(status='start')

transactions = _transactions. \
    filter(Transaction.type.in_(transaction_types)). \
    outerjoin(status_sq,
              Transaction.transaction_id == status_sq.c.transaction_id). \
    outerjoin(end_transaction_sq,
              Transaction.transaction_id == end_transaction_sq.c.transaction_id)

# check possible values for sorting and pages
if sort_field not in allowed_sort_fields:
    sort_field = Transaction.default_sort_field
if sort_dir not in (ASCENDING, DESCENDING):
    sort_dir = Transaction.default_sort_dir
if per_page &gt; 100:  # hard limit
    per_page = Transaction.default_per_page

if sort_dir == ASCENDING:
    order = allowed_sort_fields[sort_field].desc()
else:
    order = allowed_sort_fields[sort_field].desc()

q = transactions.\
    join(Establishment).\
    join(Employee, Transaction.employee_id == Employee.id). \
    outerjoin(Currency). \
    group_by(Transaction,
             status_sq.c.count,
             end_transaction_sq.c.discrepancy,
             end_transaction_sq.c.discrepancy_reason,
             end_transaction_sq.c.resolve_discrepancy_reason,
             end_transaction_sq.c.amount,
             allowed_sort_fields[sort_field]).\
    order_by(order)
items = q.filter(Transaction.transaction_id == ReconciledTransaction.safe_withdraw_id).limit(per_page).offset((page - 1) * per_page).all()


'Doesn't work correctly' means that in second case(when I place !=, and wanna take transactions only, which are not in ReconciledTransaction table) filter gets ignored, but when filter contains ==, all works correctly(I have only matched transactions).
",2,211,"When you use query like this:

q = db.session.query(Transaction). \
    filter(Transaction.transaction_id != ReconciledTransaction.safe_withdraw_id)


it transforms into SQL query:

SELECT Transaction.* FROM Transaction, ReconciledTransaction
WHERE Transaction.transaction_id != ReconciledTransaction.safe_withdraw_id


which means you will get all Transaction rows with all ReconciledTransaction rows except those with matching ids.

If you need to get all Transaction objects which are not in ReconciledTransaction table you can first get all ReconciledTransaction ids:

r_query = db.session.query(ReconciledTransaction.safe_withdraw_id). \
    group_by(ReconciledTransaction.safe_withdraw_id)
r_ids = [x[0] for x in r_query]


and then use NOT IN filter in your Transaction query:

q = q.filter(Transaction.transaction_id.notin_(r_ids))


Or your can use subquery:

q = q.filter(Transaction.transaction_id.notin_(
    db.session.query(ReconciledTransaction.safe_withdraw_id)
))


Edit: as Ilja Everil stated NOT EXISTS operator performance might be better than NOT IN. SQLAlchemy query will look like this:

q = q.filter(~session.query(ReconciledTransaction). \
    filter(ReconciledTransaction.safe_withdraw_id == Transaction.id).exists())

",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/56317578,SQLAlchemy does not update/expire model instances with external changes,"Recently I came across strange behavior of SQLAlchemy regarding refreshing/populating model instances with the the changes that were made outside of the current session. I created the following minimal working example and was able to reproduce problem with it.


from time import sleep

from sqlalchemy import orm, create_engine, Column, BigInteger, Integer
from sqlalchemy.ext.declarative import declarative_base

DATABASE_URI = ""postgresql://{user}:{password}@{host}:{port}/{name}"".format(
    user=""postgres"",
    password=""postgres"",
    host=""127.0.0.1"",
    name=""so_sqlalchemy"",
    port=""5432"",
)


class SQLAlchemy:
    def __init__(self, db_url, autocommit=False, autoflush=True):
        self.engine = create_engine(db_url)
        self.session = None

        self.autocommit = autocommit
        self.autoflush = autoflush

    def connect(self):
        session_maker = orm.sessionmaker(
            bind=self.engine,
            autocommit=self.autocommit,
            autoflush=self.autoflush,
            expire_on_commit=True
        )
        self.session = orm.scoped_session(session_maker)

    def disconnect(self):
        self.session.flush()
        self.session.close()
        self.session.remove()
        self.session = None


BaseModel = declarative_base()


class TestModel(BaseModel):
    __tablename__ = ""test_models""

    id = Column(BigInteger, primary_key=True, nullable=False)
    field = Column(Integer, nullable=False)


def loop(db):
    while True:
        with db.session.begin():
            t = db.session.query(TestModel).with_for_update().get(1)
            if t is None:
                print(""No entry in db, creating..."")
                t = TestModel(id=1, field=0)
                db.session.add(t)
                db.session.flush()

            print(f""t.field value is {t.field}"")
            t.field += 1
            print(f""t.field value before flush is {t.field}"")
            db.session.flush()
            print(f""t.field value after flush is {t.field}"")

        print(f""t.field value after transaction is {t.field}"")
        print(""Sleeping for 2 seconds."")
        sleep(2.0)


def main():
    db = SQLAlchemy(DATABASE_URI, autocommit=True, autoflush=True)
    db.connect()
    try:
        loop(db)
    except KeyboardInterrupt:
        print(""Canceled"")


if __name__ == '__main__':
    main()



My requirements.txt file looks like this:

alembic==1.0.10
psycopg2-binary==2.8.2
sqlalchemy==1.3.3


If I run the script (I use Python 3.7.3 on my laptop running Ubuntu 16.04), it will nicely increment a value every two seconds as expected:

t.field value is 0
t.field value before flush is 1
t.field value after flush is 1
t.field value after transaction is 1
Sleeping for 2 seconds.
t.field value is 1
t.field value before flush is 2
t.field value after flush is 2
t.field value after transaction is 2
Sleeping for 2 seconds.
...


Now at some point I open postgres database shell and begin another transaction:

so_sqlalchemy=# BEGIN;
BEGIN
so_sqlalchemy=# UPDATE test_models SET field=100 WHERE id=1;
UPDATE 1
so_sqlalchemy=# COMMIT;
COMMIT


As soon as I press Enter after the UPDATE query, the script blocks as expected, as I'm issuing SELECT ... FOR UPDATE query there. However, when I commit the transaction in the database shell, script continues from the previous value (say, 27) and does not detect that external transaction has changed the value of field in database to 100.

My question is, why does this happen at all? There are several factors that seem to contradict the current behavior:


I'm using expire_on_commit setting set to True, which seems to imply that every model instance that has been used in transaction will be marked as expired after the transaction has been committed. (Quoting documentation, ""When True, all instances will be fully expired after each commit(), so that all attribute/object access subsequent to a completed transaction will load from the most recent database state."").
I'm not accessing some old model instance but rather issue completely new query every time. As far as I understand, this should lead to direct query to the database and not access cached instance. I can confirm that this is indeed the case if I turn sqlalchemy debug log on.


The quick and dirty fix for this problem is to call db.session.expire_all() right after the transaction has begun, but this seems very inelegant and counter-intuitive. I would be very glad to understand what's wrong with the way I'm working with sqlalchemy here.
",2,1378,"I ran into a very similar situation with MySQL. I needed to ""see"" changes to the table that were coming from external sources in the middle of my code's database operations. I ended up having to set autocommit=True in my session call and use the begin() / commit() methods of the session to ""see"" data that was updated externally.

The SQLAlchemy docs say this is a legacy configuration:


  Warning
  
  autocommit mode is a legacy mode of use and should not be considered for new projects.


but also say in the next paragraph:


  Modern usage of autocommit mode tends to be for framework integrations that wish to control specifically when the begin state occurs


So it doesn't seem to be clear which statement is correct.
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/17315422,Unable to store datetime.datetime.max using SQLAlchemy==0.8.1 with the mysql-python==1.2.4 driver,"I've noticed a change in behavior for storing datetime.datetime.max via SQLAlchemy==SQLAlchemy==0.8.1 and going from mysql-python==1.2.3 to mysql-python==1.2.4. By only changing the driver from 1.2.3 to 1.2.4 I go from being able to store to being unable to store it.

Where do I turn to for help in this matter? SQLAlchemy or mysql-python? Is this expected behaviour or a bug or do I have a bad setup? I fear that a change like this will break a lot of systems out there.

This is my SQLAlchemy setup:

from sqlalchemy import create_engine, Integer, DateTime, Column
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base
from datetime import datetime

engine = create_engine('mysql://root@localhost/test_database', echo=True)
Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)
    age = Column(DateTime, default=datetime.max)

Base.metadata.create_all(engine)
session = sessionmaker(bind=engine)()
u = User()
session.add(u)
session.commit()


I also have a virtualenv called test. This is what happens when I run the code above.

(test)  ~  pip install MySQL-python==1.2.3
(test)  ~  python test.py
2013-06-26 10:29:18,885 INFO sqlalchemy.engine.base.Engine SELECT DATABASE()
2013-06-26 10:29:18,885 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:29:18,887 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'character_set%%'
2013-06-26 10:29:18,887 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:29:18,891 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'sql_mode'
2013-06-26 10:29:18,891 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:29:18,896 INFO sqlalchemy.engine.base.Engine DESCRIBE `users`
2013-06-26 10:29:18,896 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:29:18,904 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
2013-06-26 10:29:18,905 INFO sqlalchemy.engine.base.Engine INSERT INTO users (age) VALUES (%s)
2013-06-26 10:29:18,905 INFO sqlalchemy.engine.base.Engine (datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),)
2013-06-26 10:29:18,908 INFO sqlalchemy.engine.base.Engine COMMIT


And the database (test_database) looks like this:

mysql&gt; select * from users;
+----+---------------------+
| id | age                 |
+----+---------------------+
|  1 | 9999-12-31 23:59:59 |
+----+---------------------+
1 row in set (0.00 sec)


This is my expected result so nothing strange here.

However, by simply switching the driver to mysql-python==1.2.4 I get this result.

(test)  ~  pip install MySQL-python==1.2.4
(test)  ~  python test.py
2013-06-26 10:33:39,544 INFO sqlalchemy.engine.base.Engine SELECT DATABASE()
2013-06-26 10:33:39,544 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:33:39,546 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'character_set%%'
2013-06-26 10:33:39,546 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:33:39,546 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'sql_mode'
2013-06-26 10:33:39,546 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:33:39,547 INFO sqlalchemy.engine.base.Engine DESCRIBE `users`
2013-06-26 10:33:39,547 INFO sqlalchemy.engine.base.Engine ()
2013-06-26 10:33:39,551 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)
2013-06-26 10:33:39,552 INFO sqlalchemy.engine.base.Engine INSERT INTO users (age) VALUES (%s)
2013-06-26 10:33:39,552 INFO sqlalchemy.engine.base.Engine (datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),)
/Users/pelle/.virtualenvs/test/lib/python2.7/site-packages/sqlalchemy/engine/default.py:324: Warning: Datetime function: datetime field overflow
  cursor.execute(statement, parameters)
/Users/pelle/.virtualenvs/test/lib/python2.7/site-packages/sqlalchemy/engine/default.py:324: Warning: Out of range value for column 'age' at row 1
  cursor.execute(statement, parameters)
2013-06-26 10:33:39,553 INFO sqlalchemy.engine.base.Engine COMMIT


And the database looks like this.

mysql&gt; select * from users;
+----+---------------------+
| id | age                 |
+----+---------------------+
|  1 | 0000-00-00 00:00:00 |
+----+---------------------+
1 row in set (0.00 sec)


So now all of the sudden I receive a warning Warning: Datetime function: datetime field overflow and I end up with a nullable value in my database.
",1,832,"This is a reported bug in the new version of the MySQL server (5.6.X) to do with the rounding of fractional seconds.

See this link for more information:
http://bugs.mysql.com/bug.php?id=68760

The way round this is to round out the milliseconds.
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/20025324,Why does SQLAlchemy insert expression values() method function differently when executing in one statement or two?,"I'm working through the SQLAlchemy core tutorial (http://docs.sqlalchemy.org/en/rel_0_8/core/tutorial.html) and found a strange behavior. In the Insert Expressions section they first create a basic insert expression and print it.

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; str(ins)
'INSERT INTO users (id, name, fullname) VALUES (:id, :name, :fullname)'


They then perform the same operation, adding values to specific columns, which limits the number of columns in the expression to those listed in the values() call.

&gt;&gt;&gt; ins = users.insert().values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; str(ins)
'INSERT INTO users (name, fullname) VALUES (:name, :fullname)'


Why is it that if I take the second version and perform it in 2 lines instead of 1, the values() call doesn't take?

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; ins.values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; str(ins)
'INSERT INTO users (id, name, fullname) VALUES (:id, :name, :fullname)'


I know the values() call is truly not doing anything because the I tested the params value from a ins.compile().params call and they are all None in the version in 2 lines.

&gt;&gt;&gt; ins = users.insert().values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; ins.compile().params
{'fullname': 'Jack Jones', 'name': 'jack'}

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; ins.values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; ins.compile().params
{'fullname': None, 'password': None, 'id': None, 'name': None}

",1,197,"The difference between these two:

&gt;&gt;&gt; ins = users.insert().values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; ins.compile().params
{'fullname': 'Jack Jones', 'name': 'jack'}

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; ins.values(name='jack', fullname='Jack Jones')
&gt;&gt;&gt; ins.compile().params
{'fullname': None, 'password': None, 'id': None, 'name': None}


is that you are not saving the returned value of ins in the second case.
I haven't tested, but to make them equivalent, it should be something like:

&gt;&gt;&gt; ins = users.insert()
&gt;&gt;&gt; ins = ins.values(name='jack', fullname='Jack Jones')


Otherwise, what you do with ins.values just vanishes
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/32149305,MySQL data not updating in web app - Python/Flask,"I'm building a Python/Flask Application using a MySQL backend. Many of the views have forms that submit data to a route, and then the route adds the data via db.session.commit(). I can confirm that the MySQL db updates with every commit, but then if I subsequently refresh the page multiple times, the data changes every time (i.e. most recently added items will disappear, reappear).

I've done a couple things to try to fix:

HTML Headers
I know it's not best to add a ton of headers and hope for the best, but I added these headers to fight caching:

&lt;meta http-equiv=""cache-control"" content=""max-age=0"" /&gt;
&lt;meta http-equiv=""cache-control"" content=""private, proxy-revalidate, s-maxage=0, no-cache, no-store, must-revalidate"" /&gt;
&lt;meta http-equiv=""expires"" content=""0"" /&gt;
&lt;meta http-equiv=""expires"" content=""Tue, 01 Jan 1980 1:00:00 GMT"" /&gt;
&lt;meta http-equiv=""pragma"" content=""no-cache"" /&gt;


Flask Headers
I added headers to the Flask app itself:

response.headers[""Cache-Control""] = ""no-cache, no-store, must-revalidate"" # HTTP 1.1.
response.headers[""Pragma""] = ""no-cache"" # HTTP 1.0.
response.headers[""Expires""] = ""0"" # Proxies.


Disable MySQL Caching

SET SESSION query_cache_type=OFF;


Apache2 Disable Caching

&lt;filesMatch ""\.(html|htm|js|css)$""&gt;
    FileETag None
    &lt;ifModule mod_headers.c&gt;
            Header unset ETag
            Header set Cache-Control ""max-age=0, no-cache, no-store, must-revalidate""
            Header set Pragma ""no-cache""
            Header set Expires ""Wed, 11 Jan 1984 05:00:00 GMT""
    &lt;/ifModule&gt;
&lt;/filesMatch&gt;


Some behaviors based on just prying around and facts:


Whenever I execute sudo service apache2 restart, following it everything works fine. Some other posts mentioned that there may be processes that are started that are causing this?
I'm using SQLAlchemy as my ORM, so at one point I thought that maybe it was SQLAlchemy caching information, so following every commit I tried db.session.close() so that the connection would be fresh every time I queried.


Can anyone give me a hand? Like many folks learning Flask, I'm a beginner at web application development.

Many thanks!

EDIT: 


When I run the application by just doing python init.py, it works perfectly fine.
I receive intermittent HTTP 500 errors when refreshing the page at a quick pace.


EDIT 2:
Here's a sample of a route that I'm POSTing to, following which I'm redirecting to itself and displaying the GET option to the user. I would expect that after committing data, the subsequent query should include it, but many times the web application doesn't show the most recently added data. On multiple refreshes, it'll sometimes show and sometimes disappear. Very strange:

@app.route('/projects', methods=['GET','POST'])
@login_required
def projects():
    user = return_current_user()
    if request.method == 'POST':
            if request.form['title'] and request.form['description']:
                    project = Projects(request.form['title'], request.form['description'])
                    if project.title != None and project.description != None:
                            user.projects.append(project)
                            db.session.commit()

            return redirect('/projects')
    elif request.method == 'GET':
            if 'clear-page' in request.args:
                    return redirect('/projects')
            elif 'query' in request.args:
                    projects = Projects.query.filter(Projects.title.like('%'+request.args['query']+'%')).order_by(Projects.timestamp.desc()).all()

            else:
                    projects = Projects.query.order_by(Projects.timestamp.desc()).all()

            return render_template('projects.html', title=""Projects"", user=user, projects=projects)

",1,2368,"I actually figured this out - a solve was to add db.session.commit() before the query. Not sure if it's the ""correct"" way to approach but it does the trick for my small-scale application!
","You might try switching off all the special changes you have made on the apparent assumption that some caching mechanism is responsible for the observed results, and show some of the code of your flask site. Then start to gather documented evidence of what is actually happening.
",
SQLAlchemy strange behavior,https://stackoverflow.com/questions/53164153,why should we set the local_infile=1 in sqlalchemy to load local file? Load file not allowed issue in sqlalchemy,"I am using sqlalchemy to connect to MySQL database and found a strange behavior.
If I query 

LOAD DATA LOCAL INFILE 
'C:\\\\Temp\\\\JaydenW\\\\iata_processing\\\\icer\\\\rename\\\\ICER_2017-10- 
12T09033
7Z023870.csv    


It pops an error:

sqlalchemy.exc.InternalError: (pymysql.err.InternalError) (1148, u'The used 
command is not allowed with this MySQL versi
on') [SQL: u""LOAD DATA LOCAL INFILE 
'C:\\\\Temp\\\\JaydenW\\\\iata_processing\\\\icer\\\\rename\\\\ICER_2017-10- 
12T090337Z023870.csv' INTO TABLE genie_etl.iata_icer_etl LINES TERMINATED BY 
'\\n' 
IGNORE 1 Lines   (rtxt);""] (Background on this error at: 
http://sqlalche.me/e/2j85)


And I find the reason is that:
I need to set the parameter as

args = ""mysql+pymysql://""+username+"":""+password+""@""+hostname+""/""+database+""? 
local_infile=1""


If I use MySQL official connection library. I do not need to do so.

myConnection = MySQLdb.connect(host=hostname, user=username, passwd=password, db=database)


Can anyone help me to understand the difference between the two mechanisms?
",1,1080,"The reason is that the mechanisms use different drivers.
In SQLAlchemy you appear to be using the pymysql engine, which uses the PyMySQL Connection class to create the DB connection. That one requires the user to explicitly pass the local_infile parameter if they want to use the LOAD DATA LOCAL command.

The other example uses MySQLdb, which is basically a wrapper around the MySQL C API (and to my knowledge not the official connection library; that would be MySQL Connector Python, which is also available on SQLAlchemy as mysqlconnector). This one apparently creates the connection in a way that the LOAD DATA LOCAL is enabled by default.
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/64855545,SQLalchemy rowcount always -1 for statements,"I was playing around with SQLalchemy and Microsoft SQL Server to get a hang of the functions when I came across a strange behavior. I was taught that the attribute rowcount on the result proxy object will tell how many rows were effected by executing a statement. However, when I select or insert single or multiple rows in my test database, I always get -1. How could this be and how can I fix this to reflect the reality?
connection = engine.connect()
metadata = MetaData()

# Ex1: select statement for all values
student = Table('student', metadata, autoload=True, autoload_with=engine)
stmt = select([student])
result_proxy = connection.execute(stmt)
results = result_proxy.fetchall()
print(result_proxy.rowcount)

# Ex2: inserting single values
stmt = insert(student).values(firstname='Severus', lastname='Snape')
result_proxy = connection.execute(stmt)
print(result_proxy.rowcout)
 
# Ex3: inserting multiple values 
stmt = insert(student)
values_list = [{'firstname': 'Rubius', 'lastname': 'Hagrid'},
               {'firstname': 'Minerva', 'lastname': 'McGonogall'}]
result_proxy = connection.execute(stmt, values_list)
print(result_proxy.rowcount)

The print function for each block seperately run example code prints -1. The Ex1 successfully fetches all rows and both insert statements successfully write the data to the database.
According to the following issue, the rowcount attribute isn't always to be trusted. Is that true here as well? And when, how can I compensate with a Count statement in a SQLalcehmy transaction?
PDO::rowCount() returning -1
",1,1388,"The single-row INSERT  VALUES (  ) is trivial: If the statement succeeds then one row was affected, and if it fails (throws an error) then zero rows were affected.
For a multi-row INSERT simply perform it inside a transaction and rollback if an error occurs. Then the number of rows affected will either be zero or len(values_list).
To get the number of rows that a SELECT will return, wrap the select query in a SELECT count(*) query and run that first, for example:
select_stmt = sa.select([Parent])
count_stmt = sa.select([sa.func.count(sa.text(""*""))]).select_from(
    select_stmt.alias(""s"")
)
with engine.connect() as conn:
    conn.execution_options(isolation_level=""SERIALIZABLE"")
    rows_found = conn.execute(count_stmt).scalar()
    print(f""{rows_found} row(s) found"")
    results = conn.execute(select_stmt).fetchall()
for item in results:
    print(item.id)

",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/65100066,SQLAlchemy loads unrelated cached objects when using contains_eager,"I'm using the contains_eager functionality of SQLAlchemy and I'm seeing strange behavior when objects are already loaded in an existing session. Specifically, it seems that those objects are not filtered out of the relationship collection as they would be when loading data fresh.
Here is a minimal example. The steps are

Create a parent-child relationship.
Add a parent and two children with different values.
Perform a joined, filtered query, using contains_eager to load matching children for the parent. Note that the filter should exclude one of the two children.
Observe that both children have been populated on the children property of the resulting object.
The correct results can be obtained by using a new session, or even by calling session.expire_all(), which indicates that the issue is that the children already exist in the current session.

Is this the expected behavior? And if so, is calling expire_all the right thing to do to avoid this?
More generally, should contains_eager be avoided because of this? It seems like a break in the abstraction if one has to keep track of whether or not a child object already exists before issuing a query. But maybe I am missing something.
from sqlalchemy import and_, Column, create_engine, DateTime, ForeignKey, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import contains_eager, relationship, sessionmaker


create_statements = [""""""
    DROP TABLE IF EXISTS child;
    """""", """"""
    DROP TABLE IF EXISTS parent;
    """""", """"""
    CREATE TABLE parent
    (
        id INTEGER NOT NULL PRIMARY KEY,
        name VARCHAR
    );
    """""", """"""
    CREATE TABLE child
    (
        id INTEGER NOT NULL PRIMARY KEY,
        parent_id INTEGER REFERENCES parent(id),
        value INTEGER
    );
    """"""
]

Base = declarative_base()


class Parent(Base):
    __tablename__ = ""parent""
    __table_args__ = {'implicit_returning': False}
    id = Column(Integer, primary_key=True)
    name = Column(String)

    children = relationship(""Child"", back_populates=""parent"")


class Child(Base):
    __tablename__ = ""child""
    __table_args__ = {'implicit_returning': False}
    id = Column(Integer, primary_key=True)
    parent_id = Column(Integer, ForeignKey(Parent.id))
    value = Column(Integer)

    parent = relationship(Parent, back_populates=""children"")


if __name__ == ""__main__"":
    engine = create_engine(f""sqlite:///"")
    session = sessionmaker(bind=engine)()

    for statement in create_statements:
        session.execute(statement)

    p1 = Parent(id=1, name=""A"")
    c1 = Child(id=1, parent=p1, value=10)
    c2 = Child(id=2, parent=p1, value=20)
    session.add_all([p1, c1, c2])
    session.flush()
    # session.expire_all()  # Uncommenting this makes the below work as expected.

    results = session \
        .query(Parent) \
        .join(Child, Parent.id == Child.parent_id) \
        .options(
            contains_eager(Parent.children)
        ).filter(Child.value &lt; 15) \
        .order_by(Parent.id) \
        .all()

    print(len(results[0].children))  # We should only have 1 child.
    print(all(c.value &lt; 15 for c in results[0].children))  # All children should match the above filter condition.

",1,551,"I asked this question on the SQLAlchemy GitHub page. The solution is to use populate_existing on any query that uses contains_eager and filter. In my specific example, this query does the right thing
session \
        .query(Parent) \
        .join(Child, Parent.id == Child.parent_id) \
        .options(
            contains_eager(Parent.children)
        ).filter(Child.value &lt; 15) \
        .order_by(Parent.id) \
        .populate_existing() \
        .all()

",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/71445447,"How can I access the joined results of lazy=joined, without executing a second SQL statement, or changing the parent query","Sqlalchemy lazy=joined performs a join for a simple parent query, but does not seem to remember what was joined.
I have this simple one-to-one relationship defined:
class User(Base):
    __tablename__ = 'user'

    email = db.Column(db.Unicode(255), nullable=False, server_default=u'', unique=True)
    password = db.Column(db.String(255), nullable=False, server_default='')
    ...
    userprofile = db.relationship(""Userprofile"",  
                                    uselist=False,
                                    backref=db.backref('userprofile', lazy='joined', innerjoin=True), 
                                    passive_deletes=True)   


class Userprofile(Base):
    __tablename__ = 'userprofile'
    user_id = db.Column(db.Integer, db.ForeignKey('user.id', ondelete='CASCADE'))
    first_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')
    last_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')
    ...
    user = db.relationship(""User"", uselist=False, backref=db.backref('user', lazy='joined', innerjoin=True))

I know the userprofile relationship may have too many options specified but I have tried everything I can think of.
As part of a 3rd party user management package, this query is executed for each web page, in order to get the user making the request, resulting in this SQL:
user = User.query.filter(User.id == user_id).one()

sqlalchemy.engine.Engine - INFO - SELECT  user.id AS user_id, user.email AS user_email, user.password AS user_password, userprofile_1.id AS userprofile_1_id, userprofile_1.user_id AS userprofile_1_user_id, userprofile_1.first_name AS userprofile_1_first_name, userprofile_1.last_name AS userprofile_1_last_name ...
FROM user JOIN userprofile AS userprofile_1 ON user.id = userprofile_1.user_id 
WHERE user.id = ?


Then, in the same view, when I want to access a field from user profile, this SQL is executed:
user.userprofile.first_name
  
sqlalchemy.engine.Engine - INFO - SELECT userprofile.id AS userprofile_id,  userprofile.user_id AS userprofile_user_id, userprofile.first_name AS userprofile_first_name, userprofile.last_name AS userprofile_last_name  ...
FROM userprofile 
WHERE ? = userprofile.user_id


Which to me is very strange.  The first query has the userprofile fields already, so why the second SQL statement?
I cant really change the user query to add something like contains_eager to the query, so that approach is not an option.  Also, sqlalchemy complains if I try to use lazy='dynamic' for a one-to-one relationship.
I have 2 questions then:

what can I do to the table definitions, if anything, to eliminate the second SQL statement?  Again, changing the User query is not an option.

Any idea why contains_eager is not the default behavior for lazy=joined?  It seems like that should be the default.  Or is there an option in the db.relationsip function to request eager loading?


",1,1362,"Gord Thompson solved it for me.
Since I posted the original code, I thought it might be helpful to post the modified code that worked.
class User(Base, UserMixin):
    __tablename__ = 'user'

    email = db.Column(db.Unicode(255), nullable=False, server_default=u'', unique=True)

    userprofile = db.relationship(
        ""Userprofile"",
        back_populates=""user"",
        uselist=False,
        lazy=""joined"",
        innerjoin=True,
        passive_deletes=True)   

class Userprofile(Base):
    __tablename__ = 'userprofile'
    user_id = db.Column(db.Integer, db.ForeignKey('user.id', ondelete='CASCADE'))
    first_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')
    last_name = db.Column(db.Unicode(100), nullable=False, server_default=u'')

user = db.relationship(""User"", back_populates=""userprofile"", uselist=False)



",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/19970809,SQLAlchemy: Unable to get the first item of query,"I'm currently learning SQLAlchemy, and i found this strange thing. I was experimenting with a table which stores a person's name and address, and to get them i use this:

session.query(User)


And to get the first item, i tried:

session.query(User).first()


Which throws a DatabaseError:

Traceback (most recent call last):
  File ""&lt;pyshell#4&gt;"", line 1, in &lt;module&gt;
    session.query(User).first()
  File ""build\bdist.win32\egg\sqlalchemy\orm\query.py"", line 2275, in first
    ret = list(self[0:1])
  File ""build\bdist.win32\egg\sqlalchemy\orm\query.py"", line 2142, in __getitem__
    return list(res)
  File ""build\bdist.win32\egg\sqlalchemy\orm\query.py"", line 2346, in __iter__
    return self._execute_and_instances(context)
  File ""build\bdist.win32\egg\sqlalchemy\orm\query.py"", line 2361, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 664, in execute
    return meth(self, multiparams, params)
  File ""build\bdist.win32\egg\sqlalchemy\sql\elements.py"", line 272, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 761, in _execute_clauseelement
    compiled_sql, distilled_params
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 874, in _execute_context
    context)
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 1023, in _handle_dbapi_exception
    exc_info
  File ""build\bdist.win32\egg\sqlalchemy\util\compat.py"", line 185, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb)
  File ""build\bdist.win32\egg\sqlalchemy\engine\base.py"", line 867, in _execute_context
    context)
  File ""build\bdist.win32\egg\sqlalchemy\engine\default.py"", line 376, in do_execute
    cursor.execute(statement, parameters)
DatabaseError: (DatabaseError) ORA-01036: illegal variable name/number
 'SELECT test_user_uid, test_user_name, test_user_address \nFROM (SELECT test_user.""uid"" AS test_user_uid, test_user.name AS test_user_name, test_user.address AS test_user_address \nFROM test_user) \nWHERE ROWNUM &lt;= :ROWNUM_1' {'ROWNUM_1': 1}


However, i was able to retrieve what i wanted if i select all the rows, and loop through the query object:

users = [user for user in session.query(User)]
user1 = users[0]


That's all, i thought it's strange. Here's my mapping class:

class User(Base):
    __tablename__ = 'test_user'

    uid = Column(Integer, primary_key = True)
    name = Column(String(50))
    address = Column(String(100))

    def __repr__(self):
        return ""&lt;User (%s, %s)""%(self.name, self.address)


My best guess is that Session.query().first() is looking for the first row, with the generated query. However, the working method retrieves all the rows, and select the first one in Python. The problem is clearly from the generated query (invalid query). The main question is, what caused SQLAlchemy to create an invalid query?

Also, i noticed that SQLAlchemy makes things more difficult by making a query with sub-query. Is that behavior intended?

I hope i can get a satisfying answer, thanks!
",0,708,"Well, it didn't take me long to realize this. It turns out that it's a version problem, i was previously using cx_Oracle version 5.0.2 10g, i tried to upgrade it to version 5.1.2 10g, and things works fine. 

This is probably an undocumented bug in SQLAlchemy, i can't find a place where they mention it. 

Conclusion: If you want to use the latest version of SQLAlchemy (0.9.0b1) with Oracle 10g, you shouldn't use cx_Oracle older than version 5.1.2 10g.

Hope this helps, and thanks for reading the question!
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/51361689,FLASK After commit causes odd IntegrityError violates unique constraint,"UPDATE / CLARIFICATION

I confirmed that this strange behavior only occurs on the macOS machine, moving everything to a windows machine (using sqlite and doing a fresh init and migrate) doesn't cause the error... doing the same on my High Sierra box does cause the odd error. 

Is anyone familiar with some known difference between sqlalchemy on Windows and macOS that might help?



Short version... I'm getting an integrity error (unique constraint) after I try to commit ANY entry to the DB, even if there are NO EXISTING entries at all in the table... why?

DETAILS

I've built a FLASK project (roughly based on the Miguel Grinberg Flask Maga Tutorial) using postgresql and sqlalchemy, the front-end has a page to register a user with a confirmation email (which works fine)...  to save time I've written a route (see below) which pre-loads a confirmed user to the Users database, this user is the ONLY user in the Users table and I only visit the route ONE TIME. 

After a successful commit I get an IntegrityError ""duplicate key value violates unique constraint"". This route only adds ONE user to an existing EMPTY Users table. The data IS successfully saved to the DB, the user can log in, but an error gets thrown. I get a similar error (see below) but am focusing on this route as an example because it is shorter than other views I've written.

EXAMPLE OF ROUTE CAUSING UNIQUE CONSTRAINT ERROR 

@main.route('/popme')
#@login_required
def popme():
    ## add user
    u1 = User()
    u1.email = 'user@domain.com'
    u1.username = 'someuser'
    u1.password_hash = 'REMOVED'
    u1.confirmed = '1'
    u1.role_id = 3
    u1.name = 'Some User'
    db.session.add(u1)
    db.session.commit()
    flash('User someuser can now login!')
    return redirect(url_for('main.index'))


I only started getting this error after moving the entire project from a Windows machine to a MacOS machine. I'm running Python 3.6 in a virtual environment, this error occurs if I'm using sqlite3 or postgresql.

I've written a much longer route which pre-fills in about 20 other tables successfully (does on commit() at the end, all data IS stored in the DB), however I get an IntegrityError ""duplicate key value violates unique constraint"" every time for a seemingly random entry. I've destroyed the DB, done an init, migrated... each time when the commit() is called a IntegrityError is thrown, each time on a different table, there is no apparent reasoning. 

BELOW IS USER MODEL

class User(UserMixin, db.Model):
    __tablename__ = 'users'
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(64), unique=True, index=True)
    username = db.Column(db.String(64), unique=True, index=True)
    password_hash = db.Column(db.String(128))
    confirmed = db.Column(db.Boolean, default=False)
    role_id = db.Column(db.Integer, db.ForeignKey('roles.id'))
    name = db.Column(db.String(64))
    last_seen = db.Column(db.DateTime(), default=datetime.utcnow)

    def ping(self):
        self.last_seen = datetime.utcnow()
        db.session.add(self)

    @property
    def password(self):
        raise AttributeError('password is not a readable attribute')

    @password.setter
    def password(self, password):
        self.password_hash = generate_password_hash(password)

    def verify_password(self, password):
        return check_password_hash(self.password_hash, password)

    def generate_confirmation_token(self, expiration=3600):
        s = Serializer(current_app.config['SECRET_KEY'], expiration)
        return s.dumps({'confirm': self.id})

    def confirm(self, token):
        s = Serializer(current_app.config['SECRET_KEY'])
        try:
            data = s.loads(token)
        except:
            return False
        if data.get('confirm') != self.id:
            return False
        self.confirmed = True
        db.session.add(self)
        return True

    def generate_reset_token(self, expiration=3600):
        s = Serializer(current_app.config['SECRET_KEY'], expiration)
        return s.dumps({'reset': self.id})

    def reset_password(self, token, new_password):
        s = Serializer(current_app.config['SECRET_KEY'])
        try:
            data = s.loads(token)
        except:
            return False
        if data.get('reset') != self.id:
            return False
        self.password = new_password
        db.session.add(self)
        return True

    def generate_email_change_token(self, new_email, expiration=3600):
        s = Serializer(current_app.config['SECRET_KEY'], expiration)
        return s.dumps({'change_email': self.id, 'new_email': new_email})

    def change_email(self, token):
        s = Serializer(current_app.config['SECRET_KEY'])
        try:
            data = s.loads(token)
        except:
            return False
        if data.get('change_email') != self.id:
            return False
        new_email = data.get('new_email')
        if new_email is None:
            return False
        if self.query.filter_by(email=new_email).first() is not None:
            return False
        self.email = new_email
        db.session.add(self)
        return True

    def can(self, permissions):
        return self.role is not None and (self.role.permissions &amp; permissions) == permissions

    def is_administrator(self):
        return self.can(Permission.ADMINISTRATOR)

    def __init__(self, **kwargs):
        super(User, self).__init__(**kwargs)
        if self.role is None:
            if self.email == current_app.config['FLASKY_ADMIN']:
                self.role = Role.query.filter_by(permissions=0xff).first()
            if self.role is None:
                self.role = Role.query.filter_by(default=True).first()

    def __repr__(self):
        return '&lt;User %r&gt;' % self.username


I've tried Sql-alchemy Integrity error but its my understanding that sqlalchemy does auto-increment primary keys. 

UPDATED INTEGRITY ERROR

sqlalchemy.exc.IntegrityError: (psycopg2.IntegrityError) duplicate key value violates unique constraint ""ix_users_email""
DETAIL:  Key (email)=(worldbmd@gmail.com) already exists.
 [SQL: 'INSERT INTO users (email, username, password_hash, confirmed, role_id, name, last_seen) VALUES (%(email)s, %(username)s, %(password_hash)s, %(confirmed)s, %(role_id)s, %(name)s, %(last_seen)s) RETURNING users.id'] [parameters: {'email': 'user@domain.com', 'username': 'someuser', 'password_hash': 'REMOVED', 'confirmed': '1', 'role_id': 1, 'name': 'Some User', 'last_seen': datetime.datetime(2018, 7, 16, 17, 27, 13, 451593)}]

",0,867,"I also got into the same problem..as Joost said flask app runs twice. So we need to set it to run only once. We can achieve it by adding use_reloader=False like:
if __name__ = ""__main__"":
    app.run(debug=True, use_reloader=False)

or
we can directly set debug=False,
if __name__ = ""__main__"":
    app.run(debug=False)

","The integrity error is caused by trying to add a blister to the blisters table with a non unique property. I think your model looks something like this:

class Blister(db.Model):
    __tablename__ = 'blisters'
    id = db.Column(db.Integer, primary_key=True)
    name= db.Column(db.String(64), unique=True)
    notes= db.Column(db.String(64))
    cost= db.Column(db.Float)


And you're trying to add a blister with a name Small round dome which is already in the blisters table in the database, therefore causing the Integrity Error.
",
SQLAlchemy strange behavior,https://stackoverflow.com/questions/61943730,Fields of SQLAlchemy models filled after creating new object,"I am creating a Python application using the Flask framework with SQLAlchemy and now struggling with a strange behavior on creating new model objects.

Lets say I have a model Offer which has some fields, a private property __validation_errors and a public method validate:

from app import db

class Offer(db.Model):
    currency = db.Column(db.String(255))
    price = db.Column(db.Numeric(20, 5))
    __validation_errors = []

    def validate(self, errors: List[Exception]) -&gt; bool:
        self.validate_not_empty()
        self.validate_price()
        for err in self.__validation_errors:
            errors.append(err)
        if len(self.__validation_errors) &gt; 0:
            return False
        return True


the object itself is created in a service class by calling

offer = Offer(currency, price)


and providing data received with a POST request in a router.
If the validation fails, an error message is returned to the user and the object is not written into DB.

And here begins the strange part. If a request with invalid data is received and another request with correct data is sent, the values in the __validation_errors list don't disappear in the offer object, like if the object is reused and not reset. If another invalid request is made, the errors are just appended to the already existing in the list. Of course, it can be fixed by setting __validation_errors = [] every time the validate() method is called, but I would like to understand what is happening here. Do I miss some SQLAlchemy specific features? The app.db object and the Session are created on app and exist all the time.
",0,495,"So the trouble here is that __validation_errors is bound to the class Offer, rather than an instance of Offer, which means that unless each instance sets its own self.__validation_errors, self.__validation_errors just maps to Offer.__validation_errors for every instance (kind of like accessing an attribute/function of a subclass that only exists on the parent class).

Take for example:

&gt;&gt;&gt; class SomeClass:
...     some_list = []
... 
&gt;&gt;&gt; a = SomeClass()
&gt;&gt;&gt; b = SomeClass()
&gt;&gt;&gt; a.some_list.append(5)
&gt;&gt;&gt; a.some_list
[5]
&gt;&gt;&gt; b.some_list
[5]
&gt;&gt;&gt; a.some_list is b.some_list
True


This is because anything in the class body is an attribute of the class itself (this makes a little more sense considering the fact that methods are also class attributes - unlike in prototype-based languages), rather than a value each instance is assigned after it is created

&gt;&gt;&gt; class SomeClass:
...     x = [1, 2, 3, 4, 5]
...
&gt;&gt;&gt; SomeClass.x
[1, 2, 3, 4, 5]
&gt;&gt;&gt; SomeClass().x
[1, 2, 3, 4, 5]
&gt;&gt;&gt; SomeClass.x is SomeClass().x
True


If you want to bind the attribute to the instance rather than the class, you attach it to self, preferably in __init__

class SomeClass:
    def __init__(self):
        self.x = 5




PS, it's worth noting that this is only really noticeable when you're working with mutable attributes. With immutable attributes, any ""change"" will create a new value on the object, and since &lt;instance&gt;.&lt;attribute&gt; is retrieved instead of &lt;instance&gt;.__class__.&lt;attribute&gt; whenever it exists, it will appear that the instance was separate from the class to begin with.



So, back to your example, I'd suggest initializing __validation_errors at the same time you __init__ialize the rest of the object:

class Offer(db.Model):
    ...
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.__validation_errors = []

    def validate(self, errors: List[Exception]) -&gt; bool:
        ...

",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/64770459,"In Python/Flask, why would &quot;from models import Result&quot; cause a circular import error, while &quot;from models import *&quot; and &quot;import models&quot; both work?","I'm working through the Flask By Example tutorial, and I'm running into a circular import error when setting up the database through SQLAlchemy. When other people run into this problem, it seems to be because of a misplaced import statement in app.py. In my case, the error seems to depend on the way I'm importing the database models instead, which I don't understand. import models and from models import * both work, but from models import Result fails with the following error message:
Traceback (most recent call last):
  File ""app.py"", line 13, in &lt;module&gt;
    from models import Result
  File ""/mnt/c/users/power/desktop/projects/fbe/models.py"", line 1, in &lt;module&gt;
    from app import db
  File ""/mnt/c/users/power/desktop/projects/fbe/app.py"", line 13, in &lt;module&gt;
    from models import Result
ImportError: cannot import name 'Result' from partially initialized module 'models' (most likely due to a circular import) (/mnt/c/users/power/desktop/projects/fbe/models.py)

Any ideas why this might be happening? It seems like there may be some nuances to how Python does imports that I'm unaware of. It's also strange that my code is pretty much identical to the provided code, and and yet I still get an error. Could the behavior with newer versions of Python have changed? I highly doubt it, but for what it's worth I'm using 3.8.5.
My code is below. All files live in the main app directory.
app.py
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate

app = Flask(__name__)
app.config.from_pyfile('config.py')
app.config.from_pyfile('instance/prod-config.py', silent=True)
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

db = SQLAlchemy(app)
migrate = Migrate(app, db)

### this works ###
import models

### this works too ###
# from models import *

### this fails with the above error ###
# from models import Result

@app.route('/')
def hello():
    return 'Hello world!'

@app.route('/&lt;name&gt;')
def hello_personalized(name):
    return ('Hello ' + name)

if __name__ == '__main__':
    app.run()

models.py
from app import db
from sqlalchemy.dialects.postgresql import JSON

class Result(db.Model):
    __tablename__ = 'results'

    id = db.Column(db.Integer, primary_key=True)
    url = db.Column(db.String())
    result_all = db.Column(JSON)
    result_no_stop_words = db.Column(JSON)

    def __init__(self, url, result_all, result_no_stop_words):
        self.url = url
        self.result_all = result_all
        self.result_no_stop_words = result_no_stop_words

    def __repr__(self):
        return '&lt;id {}&gt;'.format(self.id)

config.py
DEBUG = True
TESTING = True
CSRF_ENABLED = True
SECRET_KEY = 'needs-to-be-changed'
SQLALCHEMY_DATABASE_URI = 'postgresql://username:password@localhost/fbe'

",0,626,"Tbh, neither of those examples should work. You clearly have a circular import between those files: from app import db and from models import Result.
My guess here is that when you're using import models or even from models import *, you're importing from another package called models that exists on your python environment.
My solution would be to use separation of concerns and remove db initialization from your Flask view declarations. You can take a look on one of the many possible solutions here.
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/65742509,How to get list of objects from multi-value field with SqlAlchemy using ORM?,"I have MS Access DB file (.accdb) from my client and need to describe tables and columns with declarative_base class. As I can see in table constructor - one of column has Integer value and has relationship ""one-to-many"" with another column in some another table (foreign key).
But actually in this foreign key stored not single Integer value, but string with number values separated with semicolons. This technique called as ""multi-value fields"". In fact this is ""many-to-many"" relationship without associative tables.
Very simplified scheme:
Persons
-------------
id - Integer
name - String
vacancy_id - Integer (multi-value, Foreign key of Vacancies.id)

Vacancies
-------------
id - Integer
vacancy_name - String

I tried to map classes to tables using declarative_base parent class. But can't find how to declare ""many-to-many"" relationship without associative table. Now I have such code.
Base = declarative_base()


class Vacancy(Base):
    __tablename__ = 'Vacancies'
    id = sa.Column(sa.Integer, name='id', primary_key=True, autoincrement=True)
    vacancy_name = sa.Column(sa.String(255), name='vacancy_name')


class Person(Base):
    __tablename__ = 'Persons'
    id = sa.Column(sa.Integer, name='id', primary_key=True, autoincrement=True)
    name = sa.Column(sa.String(255), name='name')
    vacancy_id = sa.Column(sa.Integer, ForeignKey(Vacancy.id), name='vacancy_id')
    vacancies = relationship(Vacancy)

During request Person I have strange behavior:

If vacancy_id not specified, I get Person.vacancies as None.
If vacancy_id specified as single value (i.e. ""1""), in Person.vacancies I get single object of Vacancy class.
If vacancy_id specified as multiple value (i.e. ""1;2;3""), in Person.vacancies I also get None.

Of course I can request raw Person.vacancy_id, split it with semicolon, and make request to get Vacancies with list of ID's.
But I wonder - if SqlAlchemy can process ""multi-value fields""? And what the best way to work with such fileds?
UPDATE
At present I made following workaround to automatically parse multi-value columns. This should be added to Persons class:
@orm.reconstructor
def load_on_init(self):
    if self.vacancy_id:
        ids = self.vacancy_id.split(';')
        self.vacancies = [x for x in Vacancy.query.filter(Vacancy.id.in_(ids)).all()]
    else:
        self.vacancies = []

Vacancies class should have fllowing attribute:
query = DBSession.query_property()

Finally we have to prepare session for in-class usage:
engine = create_engine(CONNECTION_URI)
DBSession = scoped_session(sessionmaker(bind=engine))
Base = declarative_base()

",0,519,"Access ODBC provides very limited support for multi-value lookup fields. Such fields are actually implemented using a hidden association table (with a name like f_1BC9E55B5578456EB5ACABC99BB2FF0B_vacancies) but those tables are not accessible from SQL statements:
SELECT * from f_1BC9E55B5578456EB5ACABC99BB2FF0B_vacancies

results in the error

The Microsoft Access database engine cannot find the input table or query ''. Make sure it exists and that its name is spelled correctly.

As you have discovered, Access ODBC will read the key values of the multiple entries and present them as a semicolon-separated list that we can parse, but we cannot update those values
UPDATE Persons SET vacancies = '1;2' WHERE id = 1

fails with

An UPDATE or DELETE query cannot contain a multi-valued field. (-3209)

So, TL;DR, if you only need to read from the database then your workaround may be sufficient, but if you need to modify those multi-valued fields then Access ODBC is not going to get the job done for you.
",,
SQLAlchemy strange behavior,https://stackoverflow.com/questions/71630879,Strange bevaiour of session.add in sqhalchemy (duplicate key value violates unique constraint in session.add)),"I encountered strange behaviour of SQLAlchemy
I tried to add a new object to the DB. This is done in the function add_tag()
class News(Base):
    __tablename__ = ""news""

    id = Column(Integer, primary_key=True)

    title = Column(String)
    description = Column(String)
    
    def add_tag(self, tag_name, session):
        logging.debug(""adding news tags   "" + tag_name)
        tag = session.query(Tag).filter_by(name=tag_name).first()
        logging.debug(tag)
        if tag:
            nt = NewsTags()
            nt.tag_id = tag.id
           self.tags.append(nt)
        else:
            new_tag = Tag()
            logging.debug('Creating a new tag:  ' + tag_name)
            new_tag.name = tag_name
            session.add(new_tag)
            session.commit()
            nt = NewsTags()
            nt.tag_id = new_tag.id
            self.tags.append(nt)


class NewsTags(Base):
    __tablename__ = ""news_tags""

    news_id = Column(ForeignKey(""news.id""), primary_key=True)
    tag_id = Column(ForeignKey(""tag.id""), primary_key=True)
    news = relationship(""News"", backref=""tags"")



class Tag(Base):

    __tablename__ = ""tag""
    id = Column(Integer, primary_key=True)
    name = Column(String(30))

    news = relationship(NewsTags, backref=""tag"", post_update=True)
 



I first check if the object already exists. If it does not, I try to create a new one and add it to the session (add it to the DB).
I already had some objects in the table (create by other means).
See a copy paste of values in table tag:
id, Name
1   Sports
2   Business
3   Finance
4   World
5   US
6   UK
7   Technology
8   Science
9   Health
10  Video Games
11  IT
12  Startups
13  Europe
14  Apps
15  Space

I think I might have added some the data to the DB manually (via SQL insert), but some of it was created through SQLAlchemy.
I came across very strange behavior when trying to add a new object.
First it tried to create an object with id=1 and failed as it was a duplicate. I did not save the stack trace, but I ran it again, it tried to add the object with id=2, here's the stack trace.
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint ""tag_pkey""
DETAIL:  Key (id)=(2) already exists.

Full stack trace:
DEBUG:root:adding news tags   AI
2022-03-28 06:52:33,188 INFO sqlalchemy.engine.Engine SELECT tag.id AS tag_id, tag.name AS tag_name 
FROM tag 
WHERE tag.name = %(name_1)s 
 LIMIT %(param_1)s
INFO:sqlalchemy.engine.Engine:SELECT tag.id AS tag_id, tag.name AS tag_name 
FROM tag 
WHERE tag.name = %(name_1)s 
 LIMIT %(param_1)s
2022-03-28 06:52:33,188 INFO sqlalchemy.engine.Engine [cached since 0.005467s ago] {'name_1': 'AI', 'param_1': 1}
INFO:sqlalchemy.engine.Engine:[cached since 0.005467s ago] {'name_1': 'AI', 'param_1': 1}
DEBUG:root:None
DEBUG:root:Creating a new tag:  AI
2022-03-28 06:52:33,191 INFO sqlalchemy.engine.Engine INSERT INTO tag (name) VALUES (%(name)s) RETURNING tag.id
INFO:sqlalchemy.engine.Engine:INSERT INTO tag (name) VALUES (%(name)s) RETURNING tag.id
2022-03-28 06:52:33,191 INFO sqlalchemy.engine.Engine [generated in 0.00015s] {'name': 'AI'}
INFO:sqlalchemy.engine.Engine:[generated in 0.00015s] {'name': 'AI'}
2022-03-28 06:52:33,192 INFO sqlalchemy.engine.Engine ROLLBACK
INFO:sqlalchemy.engine.Engine:ROLLBACK
Traceback (most recent call last):
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1808, in _execute_context
    self.dialect.do_execute(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/default.py"", line 732, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint ""tag_pkey""
DETAIL:  Key (id)=(2) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/app/assign.py"", line 59, in &lt;module&gt;
    main()
  File ""/app/assign.py"", line 46, in main
    parse_feeds(session)
  File ""/app/models/VitalNewsFeed.py"", line 77, in parse_feeds
    feed.parse_feed(session)
  File ""/app/models/VitalNewsFeed.py"", line 63, in parse_feed
    new_news = News(entry=e, source=self.source, tags=self.tags, session=session)
  File ""&lt;string&gt;"", line 4, in __init__
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/state.py"", line 480, in _initialize_instance
    manager.dispatch.init_failure(self, args, kwargs)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
    raise exception
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/state.py"", line 477, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File ""/app/models/VitalNews.py"", line 178, in __init__
    self.add_tag(tag, session)
  File ""/app/models/VitalNews.py"", line 121, in add_tag
    session.commit()
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 1431, in commit
    self._transaction.commit(_to_root=self.future)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 829, in commit
    self._prepare_impl()
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 808, in _prepare_impl
    self.session.flush()
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 3363, in flush
    self._flush(objects)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 3503, in _flush
    transaction.rollback(_capture_exception=True)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
    raise exception
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 3463, in _flush
    flush_context.execute()
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/persistence.py"", line 244, in save_obj
    _emit_insert_statements(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/orm/persistence.py"", line 1237, in _emit_insert_statements
    result = connection._execute_20(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1620, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/sql/elements.py"", line 325, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1487, in _execute_clauseelement
    ret = self._execute_context(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1851, in _execute_context
    self._handle_dbapi_exception(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 2032, in _handle_dbapi_exception
    util.raise_(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 207, in raise_
    raise exception
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1808, in _execute_context
    self.dialect.do_execute(
  File ""/app/.heroku/python/lib/python3.9/site-packages/sqlalchemy/engine/default.py"", line 732, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""tag_pkey""
DETAIL:  Key (id)=(2) already exists.

[SQL: INSERT INTO tag (name) VALUES (%(name)s) RETURNING tag.id]
[parameters: {'name': 'AI'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)

I tried running the script again. Now it tired to create with id=3
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""tag_pkey""
DETAIL:  Key (id)=(3) already exists.

[SQL: INSERT INTO tag (name) VALUES (%(name)s) RETURNING tag.id]
[parameters: {'name': 'AI'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)

id=4
and so on. Every time it failed and I had to run the script again. As if it did not know the last value of id in the table of Tags. After a few fails it created with id=9, which was not a duplicate and the script continued working fine. But the error was strange. Shouldn't it check if primary key is available before creating a new object?
Now the error does not reproduce on dev. But when I pushed my code to prod the same thing happens again.
I am using PostgreSQL with SQLAlchemy.
",0,1051,"This was not SQLAlchemy's fault. It was PostgresSQL and me inserting into the table with a PostgreSQL client Postico without auto-increment which broke the sequence.
Running SQL command of ""SELECT setval('tag_id_seq', (SELECT MAX(id) FROM tag), true);"" fixed it.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/25768428,SQLAlchemy connection errors,"I'm experiencing some strange bugs which seem to be caused by connections used by Sqlalchemy, which i can't pin down exactly.. i was hoping someone has a clue whats going on here.

We're working on a Pyramid (version 1.5b1) and use Sqlalchemy (version 0.9.6) for all our database connectivity. Sometimes we get errors related to the db connection or session, most of the time this would be a cursor already closed or This Connection is closed error, but we get other related exceptions too:

(OperationalError) connection pointer is NULL
(InterfaceError) cursor already closed
Parent instance &lt;...&gt; is not bound to a Session, and no contextual session is established; lazy load operation of attribute '...' cannot proceed

A conflicting state is already present in the identity map for key (&lt;class '...'&gt;, (1001L,))
This Connection is closed (original cause: ResourceClosedError: This Connection is closed)
(InterfaceError) cursor already closed
Parent instance &lt;...&gt; is not bound to a Session; lazy load operation of attribute '...' cannot proceed
Parent instance &lt;...&gt; is not bound to a Session, and no contextual session is established; lazy load operation of attribute '...' cannot proceed
'NoneType' object has no attribute 'twophase'
(OperationalError) connection pointer is NULL
This session is in 'prepared' state; no further


There is no silver bullet to reproduce them, only by refreshing many times they are bound to happen one at some point. So i made a script using multi-mechanize to spam different urls concurrently and see where and when it happens.

It appears the url triggered doesn't really matter, the errors happen when there are concurrent requests that span a longer time (and other requests get served in between). This seems to indicate there is some kind of threading problem; that either the session or connection is shared among different threads.

After googling for these issues I found a lot of topics, most of them tell to use scoped sessions, but the thing is we do use them already:

db_session = scoped_session(sessionmaker(extension=ZopeTransactionExtension(), autocommit=False, autoflush=False))
db_meta = MetaData()



We have a BaseModel for all our orm objects:

BaseModel = declarative_base(cls=BaseModelObj, metaclass=BaseMeta, metadata=db_meta)
We use the pyramid_tm tween to handle transactions during the request
We hook db_session.remove() to the pyramid NewResponse event (which is fired after everything has run). I also tried putting it in a seperate tween running after pyramid_tm or even not doing it at all, none of these seem to have effect, so the response event seemed like the most clean place to put it.
We create the engine in our main entrypoint of our pyramid project and use a NullPool and leave connection pooling to pgbouncer. We also configure the session and the bind for our BaseModel here:

engine = engine_from_config(config.registry.settings, 'sqlalchemy.', poolclass=NullPool)
db_session.configure(bind=engine, query_cls=FilterQuery)
BaseModel.metadata.bind = engine
config.add_subscriber(cleanup_db_session, NewResponse)
return config.make_wsgi_app()
In our app we access all db operation using:

from project.db import db_session
...
db_session.query(MyModel).filter(...)
db_session.execute(...)
We use psycopg2==2.5.2 to handle the connection to postgres with pgbouncer in between
I made sure no references to db_session or connections are saved anywhere (which could result in other threads reusing them)


I also tried the spamming test using different webservers, using waitress and cogen i got the errors very easily, using wsgiref we unsurprisingly have no errors (which is singlethreaded). Using uwsgi and gunicorn (4 workers, gevent) i didn't get any errors.

Given the differences in the webserver used, I thought it either has to do with some webservers handling requests in threads and some using new processes (maybe a forking problem)? To complicate matters even more, when time went on and i did some new tests, the problem had gone away in waitress but now happened with gunicorn (when using gevent)! I have no clue on how to go debugging this...

Finally, to test what happens to the connection, i attached an attribute to the connection at the start of the cursor execute and tried to read the attribute out at the end of the execute:

@event.listens_for(Engine, ""before_cursor_execute"")
def _before_cursor_execute(conn, cursor, stmt, params, context, execmany):
  conn.pdtb_start_timer = time.time()

@event.listens_for(Engine, ""after_cursor_execute"")
def _after_cursor_execute(conn, cursor, stmt, params, context, execmany):
  print conn.pdtb_start_timer


Surprisingly this sometimes raised an exception: 'Connection' object has no attribute 'pdtb_start_timer'

Which struck me as very strange.. I found one discussion about something similar: https://groups.google.com/d/msg/sqlalchemy/GQZSjHAGkWM/rDflJvuyWnEJ
And tried adding strategy='threadlocal' to the engine, which from what i understand should force 1 connection for the tread. But it didn't have any effect on the errors im seeing.. (besides some unittests failing because i need two different sessions/connections for some tests and this forces 1 connection to be associated)

Does anyone have any idea what might go on here or have some more pointers on how to attack this problem?

Thanks in advance!

Matthijs Blaas
",9,5967,"Update: The errors where caused by multiple commands that where send in one prepared sql statement. Psycopg2 seems to allow this, but apparently it can cause strange issues. The PG8000 connector is more strict and bailed out on the multiple commands, sending one command fixed the issue!
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/14470688,SQLAlchemy Bidirectional Relationship association proxy,"Update:

For anyone having this issue, with the very latest SQLAlchemy this behaviour has been fixed.

Original issue:

I am having a problem with getting association proxies to update correctly.

Using the example models here: http://docs.sqlalchemy.org/en/rel_0_7/orm/extensions/associationproxy.html#simplifying-association-objects

But changing UserKeyword with this line:

keyword = relationship(""Keyword"", backref=backref(""user_keywords"", cascade=""all, delete-orphan""))


and adding this to Keyword:

users = association_proxy('user_keywords', 'user')


So a keyword instance has a list of users.

The following works as expected:

&gt;&gt;&gt; rory = User(""rory"")
&gt;&gt;&gt; session.add(rory)
&gt;&gt;&gt; chicken = Keyword('chicken')
&gt;&gt;&gt; session.add(chicken)
&gt;&gt;&gt; rory.keywords.append(chicken)
&gt;&gt;&gt; chicken.users
[&lt;__main__.User object at 0x1f1c0d0&gt;]
&gt;&gt;&gt; chicken.user_keywords
[&lt;__main__.UserKeyword object at 0x1f1c450&gt;]


But removals do strange things. Removing from the association proxy lists like so:

&gt;&gt;&gt; rory.keywords.remove(chicken)


Causes an integrity error as SA tries to set one of the foreign key columns to null.

Doing this:

&gt;&gt;&gt; rory.user_keywords.remove(rory.user_keywords[0])


Results in this:

&gt;&gt;&gt; chicken.users
[None]


I have missed something obvious haven't I?
",8,2495,"UserKeyword requires that it be associated with both a Keyword and User at the same time in order to be persisted.   When you associate it with a User and Keyword, but then remove it from the User.user_keywords collection, it's still associated with the Keyword.   

&gt;&gt;&gt; rory.keywords.remove(chicken)

# empty as we expect
&gt;&gt;&gt; rory.user_keywords
[]   

# but the other side, still populated.  UserKeyword 
# has no User, but still has Keyword
&gt;&gt;&gt; chicken.user_keywords
[&lt;__main__.UserKeyword object at 0x101748d10&gt;]

# but the User on that UserKeyword is None
&gt;&gt;&gt; chicken.user_keywords[0].user is None
True

# hence accessing the ""association"" gives us None
# as well
&gt;&gt;&gt; chicken.users
[None]


So if we were to flush() this right now, you've got a UserKeyword object ready to go but it has no User on it, so you get that NULL error.    At INSERT time, the object is not considered to be an ""orphan"" unless it is not associated with any Keyword.user_keywords or User.user_keywords collections.   Only if you were to say, del chicken.user_keywords[0] or equivalent, would you see that no INSERT is generated and the UserKeyword object is forgotten.

If you were to flush the object to the database before removing it from ""rory"", then things change.  The UserKeyword is now persistent, and when you remove ""chicken"" from ""rory.keywords"", a ""delete-orphan"" event fires off which does delete the UserKeyword, even though it still is associated with the Keyword object:

rory.keywords.append(chicken)

session.flush()

rory.keywords.remove(chicken)

session.flush()


you see the SQL:

INSERT INTO ""user"" (name) VALUES (%(name)s) RETURNING ""user"".id
{'name': 'rory'}

INSERT INTO keyword (keyword) VALUES (%(keyword)s) RETURNING keyword.id
{'keyword': 'chicken'}

INSERT INTO user_keyword (user_id, keyword_id, special_key) VALUES (%(user_id)s, %(keyword_id)s, %(special_key)s)
{'keyword_id': 1, 'special_key': None, 'user_id': 1}

DELETE FROM user_keyword WHERE user_keyword.user_id = %(user_id)s AND user_keyword.keyword_id = %(keyword_id)s
{'keyword_id': 1, 'user_id': 1}


Now a reasonable person would ask, ""isn't that inconsistent?""   And at the moment I'd say, ""absolutely"".   I need to look into the test cases to see what the rationale is for this difference in behavior, I've identified in the code why it occurs in this way and I'm pretty sure this difference in how an ""orphan"" is considered for ""pending"" versus ""persistent"" objects is intentional, but in this particular permutation obviously produces a weird result.   I might make a change in 0.8 for this if I can find one that is feasible.

edit: http://www.sqlalchemy.org/trac/ticket/2655 summarizes the issue which I'm going to have to think about.   There is a test for this behavior specifically, need to trace that back to its origin.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/24231535,Flask-SQLAlchemy Can&#39;t Create Relationship - SAWarning,"I used fbone to start this project off and I'm using its implementation of a Flask-SQLAlchemy column extension named DenormalizedText. I get the concept of this and how it works (not my issue) but my implementation of the add method is having odd results.

DenormalizedText

class DenormalizedText(Mutable, types.TypeDecorator):
    """"""
    Stores denormalized primary keys that can be
    accessed as a set.

    :param coerce: coercion function that ensures correct
               type is returned

    :param separator: separator character
    """"""

    impl = types.Text

    def __init__(self, coerce=int, separator="" "", **kwargs):

        self.coerce = coerce
        self.separator = separator

    super(DenormalizedText, self).__init__(**kwargs)

    def process_bind_param(self, value, dialect):
        if value is not None:
            items = [str(item).strip() for item in value]
            value = self.separator.join(item for item in items if item)
        return value

    def process_result_value(self, value, dialect):
        if not value:
            return set()
        return set(self.coerce(item) for item in value.split(self.separator))

    def copy_value(self, value):
        return set(value)


My class Person has a DenormalizedText parameter named family

family = Column(DenormalizedText)

and here's the add method

# just adds each object to the other's family relationship
def add_family(self, person):
    self.family.add(person.id)
    person.family.add(self.id)


So here's the strange bits:


I have another relationship for Person done the exact same way for a different class called Residence. This works fine. So I think for a minute maybe there's an issue with a self-referencing implementation. But fbone does this with their provided User class and that works?!
So I wrote a test... and it passed!
Everything leading up to the use of this method works fine. Both Person objects are in session and committed, I double check the ""family member"" before trying to add (make sure they were saved to the db and have an id).


Occasionally I get this rare error:
SAWarning: The IN-predicate on ""persons.id"" was invoked with an empty sequence. This results in a contradiction, which nonetheless can be expensive to evaluate.  Consider alternative strategies for improved performance.



Like I said this works fine for the residences relationship with Person but here's the relevant code from the view and form handler in case:

def create_step2_family():

    client = Person()
    spouse = Person()
    spouse_form = PersonalForm()

    # default action
    try:
        client = Person.get_by_id(session['working_client'])
    except KeyError:
        flash('Please complete this section first.', 'warning')
        return redirect(url_for('client.create_step1_personal'))
    else:

        # spouse form action
        if request.method == 'POST' and spouse_form.validate_on_submit():
            spouse_form.populate_obj(spouse)
            spouse.type_code = SPOUSE

            db.session.add(spouse)
            db.session.commit()

            if Person.get_by_id(spouse.id):
                client.add_family(spouse)
                db.session.add(client)
                db.session.commit()

    return render_template('clients/wizard/step2/family.html', active=""Create"",
    client=client, spouse=spouse, spouse_form=spouse_form, active_tab=""family"")

",3,650,"I need a vacation

The error ""SAWarning: The IN-predicate on ""persons.id"" was invoked with an empty sequence. This results in a contradiction, which nonetheless can be expensive to evaluate.  Consider alternative strategies for improved performance."" was because I was calling a query on the column that returned nothing, or the list of persons.ids was empty because, well, nothing was being added.

And nothing was being added because I forgot these little lines at the bottom of my add method:

def add_family(self, person):
    self.family.add(person.id)
    person.family.add(self.id)
    self.family=set(self.family)
    person.family=set(person.family)


I'm a mess. Anyway, I won't accept my answer for this if someone can provide a better explanation or improvement to the code.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/7017058,SQLAlchemy intricacies with relationships and uncommitted objects,"I am using SQLAlchemy via Flask, and I want to add simple personal messaging to my webapp.  The model has a User class, a PersonalMessage class and a PersonalMessageUser association class, with the latter setting up relationships to the former twonothing fancy.  Here is a stripped down version:

import collections
import datetime

from flaskext.sqlalchemy import SQLAlchemy
from . import app

db = SQLAlchemy(app)

def current_ts():
    return datetime.datetime.utcnow()

class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(127), nullable=False, unique=True)

    def __repr__(self):
        return '&lt;User {0.username!r} (#{0.id})&gt;'.format(self)

class PersonalMessage(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    subject = db.Column(db.String(127), nullable=False)
    body = db.Column(db.Text, nullable=False)
    date = db.Column(db.DateTime, nullable=False, default=current_ts)

    def __repr__(self):
        return '&lt;PersonalMessage {0.subject!r} (#{0.id})&gt;'.format(self)

    def __init__(self, subject, body, from_, to=None, cc=None, bcc=None):
        self.subject = subject
        self.body = body
        if not to and not cc and not bcc:
            raise ValueError, 'No recipients defined'
        self._pm_users.append(PersonalMessageUser(
            message=self, user_type='From', user=from_,
        ))
        for type, values in {'To': to, 'CC': cc, 'BCC': bcc}.items():
            if values is None:
                continue
            if not isinstance(values, collections.Iterable):
                values = [values]
            for value in values:
                self._pm_users.append(PersonalMessageUser(
                    message=self, user=value, user_type=type,
                ))

class PersonalMessageUser(db.Model):
    pm_id = db.Column(db.ForeignKey(PersonalMessage.id), nullable=False,
                      primary_key=True)
    message = db.relationship(PersonalMessage, backref='_pm_users',
                              lazy='subquery')
    user_id = db.Column(db.ForeignKey(User.id), nullable=False,
                        primary_key=True)
    user = db.relationship(User, backref='_personal_messages')
    user_type = db.Column(
        db.Enum('From', 'To', 'CC', 'BCC', name='user_type'),
        nullable=False, default='To', primary_key=True,
    )

    def __repr__(self):
        return (
            '&lt;PersonalMessageUser '
            '{0.user_type}: {0.user.username!r} '
            '(PM #{0.pm_id}: {0.message.subject!r})&gt;'
        ).format(self)


Everything works fine basically, but I noticed something strange when I played around with it in the Python interpreter: when I create a new PersonalMessage with one sender and one recipient, the _pm_users backref actually lists each user twice.  Once the object has been committed to the database, it looks okay, though.  See the following session as an example:

&gt;&gt;&gt; al = User(username='al')
&gt;&gt;&gt; db.session.add(al)
&gt;&gt;&gt; steve = User(username='steve')
&gt;&gt;&gt; db.session.add(steve)
&gt;&gt;&gt; db.session.commit()
BEGIN (implicit)
INSERT INTO user (username) VALUES (?)
('al',)
INSERT INTO user (username) VALUES (?)
('steve',)
COMMIT
&gt;&gt;&gt; pm = PersonalMessage('subject', 'body', from_=al, to=steve)
&gt;&gt;&gt; pm._pm_users
BEGIN (implicit)
SELECT user.id AS user_id, user.username AS user_username 
FROM user 
WHERE user.id = ?
(1,)
SELECT user.id AS user_id, user.username AS user_username 
FROM user 
WHERE user.id = ?
(2,)
[&lt;PersonalMessageUser From: u'al' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser From: u'al' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #None: 'subject')&gt;]
&gt;&gt;&gt; len(pm._pm_users)
4
&gt;&gt;&gt; db.session.add(pm)
&gt;&gt;&gt; pm._pm_users
[&lt;PersonalMessageUser From: u'al' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser From: u'al' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #None: 'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #None: 'subject')&gt;]
&gt;&gt;&gt; db.session.commit()
INSERT INTO personal_message (subject, body, date) VALUES (?, ?, ?)
('subject', 'body', '2011-08-10 19:48:15.641249')
INSERT INTO personal_message_user (pm_id, user_id, user_type) VALUES (?, ?, ?)
((1, 1, 'From'), (1, 2, 'To'))
COMMIT
&gt;&gt;&gt; pm._pm_users
BEGIN (implicit)
SELECT personal_message.id AS personal_message_id,
    personal_message.subject AS personal_message_subject,
    personal_message.body AS personal_message_body,
    personal_message.date AS personal_message_date 
FROM personal_message 
WHERE personal_message.id = ?
(1,)
SELECT personal_message_user.pm_id AS personal_message_user_pm_id,
    personal_message_user.user_id AS personal_message_user_user_id,
    personal_message_user.user_type AS personal_message_user_user_type 
FROM personal_message_user 
WHERE ? = personal_message_user.pm_id
(1,)
SELECT user.id AS user_id, user.username AS user_username 
FROM user 
WHERE user.id = ?
(1,)
SELECT user.id AS user_id, user.username AS user_username 
FROM user 
WHERE user.id = ?
(2,)
[&lt;PersonalMessageUser From: u'al' (PM #1: u'subject')&gt;,
 &lt;PersonalMessageUser To: u'steve' (PM #1: u'subject')&gt;]


At least the final result is what I expect it to be, but each user showing up twice before committing makes me feel uncomfortable; Id like to understand whats going on there.  Do I miss something in my relationship/backref setup, or shall I just ignore this?
",3,1390,"When you call

self._pm_users.append(PersonalMessageUser(
    message=self, user_type='From', user=from_,
))


you have twice append object to _pm_users list

This should work for you:

PersonalMessageUser(
    message=self, user_type='From', user=from_,
)


or

self._pm_users.append(
    PersonalMessageUser(user_type='From', user=from_,)
)       


When set relationship property, sqlalchemy associate objects for you
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/63081680,Error &#39;str&#39; object has no attribute &#39;toordinal&#39; in asyncpg,"My queries were giving strange results so i debugged a little bit, i change my String, date object to sqlalchemy Date so it raised this error
asyncpg.exceptions.DataError: invalid input for query argument $2: '2020-03-11'
('str' object has no attribute 'toordinal')

Here is my sqlalchemyTable
db = sqlalchemy.Table(
""db"",
metadata,
sqlalchemy.Column(""date"", Date),
sqlalchemy.Column(""data"", JSONB),
)

how i insert values:
query = db.insert().values(
    date=datetime.strptime(key,""%d/%m/%Y"").strftime(""%Y-%m-%d""),
    data=value,
)
try:
    await database.execute(query)
except UniqueViolationError:
    pass

Why did i change the type String to Date,because when i ran a query like
query = f""""""SELECT * FROM db WHERE date BETWEEN SYMMETRIC '{start_at}' AND '{end_at}'""""""
return await database.execute(query)

It was only returning one row and one column like 2020-03-11
",2,2635,"I would say your issue is here:
date=datetime.strptime(key,""%d/%m/%Y"").strftime(""%Y-%m-%d""),
You are passing a string to a date field.  SQLAlchemy is looking for a date object to be passed in, hence the ('str' object has no attribute 'toordinal') error. toordinal being an attribute of a date object. Remove the .strftime(""%Y-%m-%d"") and it should work.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/47276146,How to add a CASE column to SQLAlchemy output?,"So far I've got basically the following:

MyTable.query
    .join()
    .filter()


The filter has a complicated case insensitive prefix check:

or_(*[MyTable.name.ilike(""{}%"".format(prefix)) for prefix in prefixes])


Now I have to retrieve the matching prefix in the result. In pseudo-SQL:

SELECT CASE
           WHEN strpos(lower(my_table.foo), 'prefix1') = 0 THEN 'prefix1'
           WHEN 
       END AS prefix
 WHERE prefix IS NOT NULL;


The SQLAlchemy documentation demonstrates how to use CASE within WHERE, which seems like a strange edge case, and not how to use it to get a new column based on an existing one.

The goal is to avoid duplicating the logic and prefix names anywhere.
",2,1710,"You know that case is an expression. You can use it in any place where SQL allows it, and link to docs you have provided show how to construct the case expression.

from sqlalchemy import case, literal
from sqlalchemy.orm import Query

q = Query(case([
    (literal(1) == literal(1), True),
], else_ = False))

print(q)

SELECT CASE WHEN (:param_1 = :param_2) THEN :param_3 ELSE :param_4 END AS anon_1

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/67975069,Phantom Queries in PyMSQL + SQLAlchemy on Lambda,"I'm trying to debug an issue that one of our developers recently revealed.
We're using AWS Lambda functions to periodically query our MySQL DB.
These functions are written in Python 3.7, with a Lambda layer added to support using PyMSQL, SQLAlchemy, and Pandas.
The (summarized and redacted) code looks like this:
import pymysql
from sqlalchemy import create_engine
import pandas as pd

mysql_connector = create_engine('mysql+pymysql://' + oltp_username + ':' + oltp_password + '@' + host + '/' + db_name)
query = """"""
        SELECT sum(field) + """"""
        FROM schema.table;
        """"""
df = pd.read_sql(query, mysql_connector)

The works just fine, and returns the expected results, however there is a strange side-effect. Our query-logging software indicates that two queries are being received from this function: the expected query in its proper format, along with a mysterious query that looks like this:
DESCRIBE `
        SELECT sum(field)
        FROM schema.table;
        `

In MySQL this is obviously a malformed query, since DESCRIBE operates on tables. Our query logging software indicates that this error appears for every single query that is run from this Lambda function.
Does anybody know where these phantom queries might be coming from? I presume it's some option in either PyMYSQL or SQLAlchemy, but I can't find anything in the documentation. Also, why are these being sent? Including a DESCRIBE makes no sense. I suspect it's just sending the raw query string to the DESCRIBE function, due to an expression like DATE_FORMAT(datefield,""%%Y-%%m-%%d %%H:%%i:00"") appearing with the consecutive %, instead of being properly escaped, as it is in the correct query.
",1,161,"snakecharmerb and Ilja Everil provided a few answers in the comments, all of which work. Thanks!

Upgrade SQLAlchemy to version 1.4
Don't use pandas to execute the SQL.
Use pandas read_sql_query function instead of read_sql.

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/59093282,Why is Oracle Pivot producing non-existent results?,"I manage a database holding a large amount of climate data collected from various stations. It's an Oracle 12.2 DB, and here's a synopsis of the relevant tables:

FACT = individual measurements at a particular time


UTC_START = time in UTC at which the measurement began
LST_START = time in local standard time (to the particular station) at which the measurement began
SERIES_ID = ID of the series to which the measurement belongs (FK to SERIES)
STATION_ID = ID of the station at which the measurement occurred (FK to STATION)
VALUE = value of the measurement


Note that UTC_START and LST_START always have a constant difference per station (the LST offset from UTC). I have confirmed that there are no instances where the difference between UTC_START and LST_START is anything other than what is expected.

SERIES = descriptive data for series of data


SERIES_ID = ID of the series (PK)
NAME = text name of the series (e.g. Temperature)


STATION = descriptive data for stations


STATION_ID = ID of the station (PK)
SITE_ID = ID of the site at which a station is located (most sites have one station, but a handful have 2)
SITE_RANK = rank of the station within the site if there are more than 1 stations.
EXT_ID = external ID for a site (provided to us)


The EXT_ID of a site applies to all stations at that site (but may not be populated unless SITE_RANK == 1, not ideal, I know, but not the issue here), and data from lower ranked stations is preferred. To organize this data into a consumable format, we're using a PIVOT to collect measurements occurring at the same site/time into rows.

Here's the query:

WITH
    primaries AS (
        SELECT site_id, ext_id
        FROM station
        WHERE site_rank = 1
    ),

    data as (
        SELECT d.site_id, d.utc_start, d.lst_start, s.name, d.value FROM (
            SELECT s.site_id, f.utc_start, f.lst_start, f.series_id, f.value,
                 ROW_NUMBER() over (PARTITION BY s.site_id, f.utc_start, f.series_id ORDER BY s.site_rank) as ORDINAL
                 FROM fact f
                      JOIN station s on f.station_id = s.station_id
        ) d
            JOIN series s ON d.series_id = s.series_id
            WHERE d.ordinal = 1
                AND d.site_id = ?
                AND d.utc_start &gt;= ?
                AND d.utc_start &lt; ?
    )

    records as (

        SELECT * FROM data
        PIVOT (
               MAX(VALUE) AS VALUE
               FOR NAME IN (
                   -- these are a few series that we would want to collect by UTC_START
                   't5' as t5,
                   'p5' as p5,
                   'solrad' as solrad,
                   'str' as str,
                   'stc_05' as stc_05,
                   'rh' as rh,
                   'smv005_05' as smv005_05,
                   'st005_05' as st005_05,
                   'wind' as wind,
                   'wet1' as wet1
                   )
                )
    )

SELECT r.*, p.ext_id
FROM records r JOIN primaries p on r.site_id = p.site_id


Here's where things get odd. This query works perfectly in SQLAlchemy, IntelliJ (using OJDBC thin), and Orcale SQL Developer. But when it's run from within our Java program (same JDBC urls, and credentials, using plain old JDBC statments and result sets), it gives results that don't make sense. Specifically for the same station, it will return 2 rows with the same UTC_START, but different LST_START (recall that I have verified that this 100% does not occur anywhere in the FACT table). Just to ensure there was no weird parameter handling going on, we tested hard-coding values in for the placeholders, and copy-and-pasted the exact same query between various clients, and the only one that returns these strange results is the Java program (which is using the exact same OJDBC jar as IntelliJ).

If anyone has any insight or possible causes, it would be greatly appreciated. We're at a bit of a loss right now.
",1,72,"It turns out that Nathan's comment was correct. Though it seems counter-intuitive (to me, at least), it appears that calling ResultSet.getString on a DATE column will in fact convert to Timestamp first. Timestamp has the unfortunate default behavior of using the system default timezone unless you specify otherwise explicitly.

This default behavior meant that daylight saving's time was taken into account when we didn't intend it to be, leading to the odd behavior described.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/49360525,SqlAlchemy like filter case insensitive but it should be case sensitive,"I am using SqlAlchemy for a database, but I have a problem when using like function in queries.

My database is SQLite.

My request is like this: 

self.session.query(Value.scan).filter(Value.current_value.like(""%"" + search + ""%""), Value.tag == Tag.tag, Tag.visible == True).distinct().all()


The column Value.current_value is a String, you can see the declaration here:

class Value(Base):
    current_value = Column(String, nullable=False)


search variable is a str coming from a rapid search bar, and is case sensitive (I never call lower or upper on it).

I want to do a case sensitive search, but the results are case insensitive.

I did some research and like should be case sensitive, and ilike case insensitive, so I don't understand why it's case insensitive.

Should I choose another type for my column that has to be case sensitive?

Another strange thing is that I have the same problem when using the function contains on the same column (case insensitive result), but not when using operators like ==, !=, &lt;, or &gt; (case sensitive result)

Does semeone knows why it's case sensitive with operators, but not with like and contains functions?

Best regards

Lucie
",1,4581,"In SQLite, LIKE is by default case insensitive.

What I had to do is activating the case_sensitive_like pragma.

I created a class to activate the pragma like this:

class ForeignKeysListener(PoolListener):
    """"""
    Class to activate the pragma case_sensitive_like, that makes the
    like and contains functions case sensitive
    """"""
    def connect(self, dbapi_con, con_record):
        db_cursor = dbapi_con.execute('pragma case_sensitive_like=ON')


When you create the engine, you just have to add the listener like this:

engine = create_engine(
    'sqlite:///' + os.path.join(self.folder, 'database', 'mia2.db'),
    listeners=[ForeignKeysListener()])

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/75913528,AWS lambda does not update rds postgres table but the execution is successful,"I have a postgres DB in RDS. When I connect it from my local and insert into it via sqlalchemy, I can see the updated results but when I run the same code from Lambda, the execution is shown as completed (i.e code runs correctly) but the results are not updated in the DB. In the same code when I print out result of `SELECT * table_name*, I can see the new row addition but when I check from local (using both pgadmin and through code) whether the row was really added or not, it it is missing.
One strange thing I observed is that I have a column 'ID' which is a primary key. When I add the row with ID 25 from lambda followed by adding another row from local, the row added from local is assigned row 27 (instead of 26 which was assigned to row added from lambda, however I can't see the row with id 26).
The following is my code:
from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, text
engine = create_engine(""postgresql://%s:%s@%s:%s/%s"" % (username, password, host, port, database))
conn = engine.connect()
print(""CONNECTED TO RDS"")

query = 'select * from predictions'
result = conn.execute(text(query)).fetchall()
print(""THE QUERY RESULT IS"", '\n', result)
upload_query = ""INSERT INTO predictions (image_url, image_class) VALUES ('LOCAL_ADD', 'LOCAL_ADD');""
conn.execute(text(upload_query))
query = 'select * from predictions'
result = conn.execute(text(query)).fetchall()
print(""THE QUERY RESULT IS"", '\n', result)

conn.close()

My lambda has no VPC to it. However my RDS DB has 4 security groups with two of them being inbound/outbound rules set to 'All Traffic' with '0.0.0.0'
",0,563,"You've executed the changes with conn.execute, but if autocommit is not set within the lambda session then the changes won't be issued to the database until you commit them with conn.commit.
So add the line conn.commit() like so:
conn.execute(text(upload_query))
conn.commit()

","Enable autocommit or explicitly COMMIT the transacion in the Lambda function.
Sequence values are updated whenever the next value is requested. This occurs outside of the transaction. It can be viewed as occurring in an independent transaction. This is necessary so that sequences can be used by concurrent transactions without introducing waits or deadlocks.
",
SQLAlchemy strange result,https://stackoverflow.com/questions/73440112,MySql: Insert record doesn&#39;t exists or actually never being created?,"Recently I've encountered a strange problem which I couldn't see any ideas based on my current knowledge.
Backend:  Python3, Sqlalchemy,
MySQL Config: read-committed, id auto-increment, cluster with 3 nodes.
Query: insert into xxx values(xxx...) and then db.session.commit()
Expect Result: New record id returned and mysql successfully create one record.
Actually Result: New record id returned and no mysql record created and no binlog found.
I wonder: if something panic, the transcation should've rollback and no id should returned. What I missed?
",0,27,"Possibly one is overwriting the session with child processes,
Connection problems with SQLAlchemy and multiple processes
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/72293864,Getting sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) NOT NULL constraint failed: user.image_file,"I am new to learning Flask. following youtube tutorial, I did the same as the trainer did. But I am getting the Integrity error.
Flask_Blog.Py:
from datetime import datetime
from flask import Flask, render_template, url_for, flash, redirect
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy.com import dynamic

from forms import RegistrationForm, LoginForm

app = Flask(__name__)
app.config['SECRET_KEY'] = '5791628bb0b13ce0c676dfde280ba245'
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///site.db'
db = SQLAlchemy(app)


class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(20), unique=True, nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)
    image_file = db.Column(db.String(20), nullable=False)
    password = db.Column(db.String(60), nullable=False)
    posts = db.relationship('Post', backref='author', lazy=True)

    def __repr__(self):
        return f""User('{self.username}', '{self.email}', '{self.image_file}')""


class Post(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    title = db.Column(db.String(100), nullable=False)
    date_posted = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)
    content = db.Column(db.Text, nullable=False)
    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)

    def __repr__(self):
        return f""Post('{self.title}', '{self.date_posted}')""


posts = [
    {
        'author': 'Corey Schafer',
        'title': 'Blog Post 1',
        'content': 'First post content',
        'date_posted': 'April 20, 2018'
    },
    {
        'author': 'Jane Doe',
        'title': 'Blog Post 2',
        'content': 'Second post content',
        'date_posted': 'April 21, 2018'
    }
]


@app.route(""/"")
@app.route(""/home"")
def home():
    return render_template('home.html', posts=posts)


@app.route(""/about"")
def about():
    return render_template('about.html', title='About')


@app.route(""/register"", methods=['GET', 'POST'])
def register():
    form = RegistrationForm()
    if form.validate_on_submit():
        flash(f'Account created for {form.username.data}!', 'success')
        return redirect(url_for('home'))
    return render_template('register.html', title='Register', form=form)


@app.route(""/login"", methods=['GET', 'POST'])
def login():
    form = LoginForm()
    if form.validate_on_submit():
        if form.email.data == 'admin@blog.com' and form.password.data == 'password':
            flash('You have been logged in!', 'success')
            return redirect(url_for('home'))
        else:
            flash('Login Unsuccessful. Please check username and password', 'danger')
    return render_template('login.html', title='Login', form=form)

 if __name__ == '__main__':
        app.run(debug=True) 

This the commands I am trying to execute.
Commands:
**from Flask_Blog import db
db.create_all()
from Flask_Blog import User, Post
user_1 = User(username='Strange', email='doctor@strange.com', password='password')
db.session.add(user_1)
db.session.commit()**

After executing commit I am getting below error.

Traceback (most recent call last):
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1819, in _execute_context
self.dialect.do_execute(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 732, in do_execute
cursor.execute(statement, parameters)
sqlite3.IntegrityError: NOT NULL constraint failed: user.image_file
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
File ""C:\Users\darjunku\AppData\Local\Programs\Python\Python310\lib\code.py"", line 90, in runcode
exec(code, self.locals)
File """", line 1, in 
File """", line 2, in commit
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1435, in commit
self._transaction.commit(_to_root=self.future)
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 829, in commit
self._prepare_impl()
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 808, in _prepare_impl
self.session.flush()
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3367, in flush
self.flush(objects)
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3506, in flush
with util.safe_reraise():
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in exit
compat.raise(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise
raise exception
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3467, in _flush
flush_context.execute()
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 456, in execute
rec.execute(self)
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 630, in execute
util.preloaded.orm_persistence.save_obj(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 245, in save_obj
_emit_insert_statements(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 1238, in _emit_insert_statements
result = connection._execute_20(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1631, in _execute_20
return meth(self, args_10style, kwargs_10style, execution_options)
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\sql\elements.py"", line 325, in _execute_on_connection
return connection._execute_clauseelement(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1498, in _execute_clauseelement
ret = self._execute_context(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1862, in _execute_context
self.handle_dbapi_exception(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 2043, in handle_dbapi_exception
util.raise(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise
raise exception
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1819, in _execute_context
self.dialect.do_execute(
File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 732, in do_execute
cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) NOT NULL constraint failed: user.image_file
[SQL: INSERT INTO user (username, email, image_file, password) VALUES (?, ?, ?, ?)]
[parameters: ('Strange', 'doctor@strange.com', None, 'password')]
(Background on this error at: https://sqlalche.me/e/14/gkpj)

Traceback (most recent call last):
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1819, in _execute_context
    self.dialect.do_execute(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 732, in do_execute
    cursor.execute(statement, parameters)
sqlite3.IntegrityError: NOT NULL constraint failed: user.image_file
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""C:\Users\darjunku\AppData\Local\Programs\Python\Python310\lib\code.py"", line 90, in runcode
    exec(code, self.locals)
  File ""&lt;input&gt;"", line 1, in &lt;module&gt;
  File ""&lt;string&gt;"", line 2, in commit
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 1435, in commit
    self._transaction.commit(_to_root=self.future)
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 829, in commit
    self._prepare_impl()
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 808, in _prepare_impl
    self.session.flush()
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3367, in flush
    self._flush(objects)
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3506, in _flush
    with util.safe_reraise():
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\session.py"", line 3467, in _flush
    flush_context.execute()
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 245, in save_obj
    _emit_insert_statements(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\orm\persistence.py"", line 1238, in _emit_insert_statements
    result = connection._execute_20(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1631, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\sql\elements.py"", line 325, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1498, in _execute_clauseelement
    ret = self._execute_context(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1862, in _execute_context
    self._handle_dbapi_exception(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 2043, in _handle_dbapi_exception
    util.raise_(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\util\compat.py"", line 207, in raise_
    raise exception
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 1819, in _execute_context
    self.dialect.do_execute(
  File ""C:\Users\darjunku\PycharmProjects\Working_with_Flask\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 732, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) NOT NULL constraint failed: user.image_file
[SQL: INSERT INTO user (username, email, image_file, password) VALUES (?, ?, ?, ?)]
[parameters: ('Corey', 'C@demo.com', None, 'password')]
(Background on this error at: https://sqlalche.me/e/14/gkpj)*

",0,853,"Try:
image_file = db.Column(db.String(20), nullable=True)

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/71451982,How to make alembic or flask migrate name foreign keys when autogenerating migrations?,"I've been struggling with this issue on and off for quite some time, and strangely could not find a straightforward question/answer combo on this on SO. Related questions here and here. I finally found a solution so I will ask and answer my own question.
In Flask SQLAlchemy (and regular SQLAlchemy), you can have a column like this:
class Character(db.model):
  background_id = db.Column(db.Integer, db.ForeignKey('backgrounds.id'))

When you run flask db migrate, or alembic revision --autogenerate, this will result in an operation that looks like this:
def upgrade():
  op.create_foreign_key(None, 'characters', 'backgrounds', ['background_id'], ['id'])

def downgrade():
  op.drop_constraint(None, 'characters', type_='foreignkey')

The None here is bad. In fact, if you try to downgrade later, this will always fail, because drop_constraint needs the name of the constraint.
You can change this every time you generate a migration, like this:
def upgrade():
  op.create_foreign_key('fk_characters_backgrounds', 'characters', 'backgrounds', ['background_id'], ['id'])

def downgrade():
  op.drop_constraint('fk_characters_backgrounds', 'characters', type_='foreignkey')

Which works!
But if you're like me, you don't want to have to remember to do this every time you autogenerate a revision with a foreign key.
So the question is, how can we make this automatic?
",0,1258,"There is an answer to this question in the best practices suggested here, at the end of the section on The Importance of Naming Conventions. The solution is to add a naming_convention to your sqlalchemy metadata, like this:
convention = {
  ""ix"": ""ix_%(column_0_label)s"",
  ""uq"": ""uq_%(table_name)s_%(column_0_name)s"",
  ""ck"": ""ck_%(table_name)s_%(constraint_name)s"",
  ""fk"": ""fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s"",
  ""pk"": ""pk_%(table_name)s""
}

metadata = MetaData(naming_convention=convention)

More specifically, with Flask-SQLAlchemy, do this when initializing your db:
from sqlalchemy import MetaData

convention = {
  ""ix"": ""ix_%(column_0_label)s"",
  ""uq"": ""uq_%(table_name)s_%(column_0_name)s"",
  ""ck"": ""ck_%(table_name)s_%(constraint_name)s"",
  ""fk"": ""fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s"",
  ""pk"": ""pk_%(table_name)s""
}

db = SQLAlchemy(metadata=MetaData(naming_convention=convention))

And voila! If you run autogenerate, you'll get this:
def upgrade():
  op.create_foreign_key(op.f('fk_characters_background_id_backgrounds'), 'characters', 'backgrounds', ['background_id'], ['id'])

def downgrade():
  op.drop_constraint(op.f('fk_characters_background_id_backgrounds'), 'characters', type_='foreignkey')

Thanks (unsurprisingly) to  Miguel Grinberg, creator of Flask Migrate, for having linked to the correct page in the Alembic docs that finally allowed me to solve this problem! Someone had asked about this in an issue on Flask Migrate GitHub, and Miguel correctly pointed out that this was an Alembic issue, not a Flask Migrate issue.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/69566226,&quot;MySQLInterfaceError: Python type list cannot be converted&quot; on all sqlalchemy.orm.Query methods,"I am running a Python 3.9.0 server through uvicorn with sqlalchemy connecting to a mysql server.
I'm trying to make queries with a db (session) object that I construct as such, based on examples I've pulled from the docs and web:
database.py
from sqlalchemy.orm import sessionmaker
...
engine = create_engine(
    SQLALCHEMY_DATABASE_URL,
    connect_args={'auth_plugin': 'mysql_native_password'}
)
...
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

main.py
from sqlalchemy.orm import Session
from .database import SessionLocal, engine, Base

def get_db():
    try:
        db = SessionLocal()
        yield db
    finally:
        db.close()

Per the docs, I'm trying to invoke queries as follows:
main.py
db.query(models.OriginalSong).filter_by(original_song_title=original_song_title).first()

However, methods such as first() and all() are throwing the same error:
_mysql_connector.MySQLInterfaceError: Python type list cannot be converted

pointing to the line where the all() or first() method was invoked.
I checked the type of the invocation and they appear to be correct:
print(type(db.query(models.OriginalSong).filter_by(original_song_title=original_song_title)))

&lt;class 'sqlalchemy.orm.query.Query'&gt;

I don't understand what I'm doing differently from what the docs on the query type or the tutorial. There was one stackoverflow question I found with the same error, but trying to join the ""list"" just resulted in the same error. I'm not even sure what the return type of first() or one() are as it errors before I can try to print the type.
How can I access the result of my query without an error?
(I tried to include only the necessary code, but the full repo can be seen on github)
EDIT: Strangely, a prior query works:
query = db.query(models.RemixArtist).filter_by(remix_artist_name=remix_artist_name).first()

I don't see any difference between the two.
",0,2078,"The issue wasn't with the return of the query, the issue was with the parameter passed to the filter_by. Without realizing it, I was passing it a list, when it expected a string. I had to change the argument's value for parameter original_song_title to be original_song_title[0].
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/67858384,SQLAlchemy: How do I avoid this inconsistency between ORM cache and DB,"Here's a simplified version of my problem. I have a program query.py:
import time
from models import Ball, session

time.sleep(1)
r = session.query(Ball).filter(Ball.color=='red').first()
print(f'Red ball color is {r.color}')
time.sleep(2)
b = session.query(Ball).filter(Ball.color=='blue').first()
print(f'Blue ball color is {b.color}')
print(f'Red ball id is {r.id}, blue ball id is {b.id}')

When I run query.py at the same time as modify.py (included below), I get the following output:
$ python modify.py &amp;! python query.py
Red ball color is red                                                                                                                                                   
Blue ball color is red                                                                                                                                                  
Red ball id is 1, blue ball id is 1                                                                                                                  

The problem is that the blue ball is red!
Here is the content of models.py:
import sqlalchemy as sa
import sqlalchemy.orm as sao
import sqlalchemy.ext.declarative as saed

Base = saed.declarative_base()

class Ball(Base):
    __tablename__ = 'ball'
    id = sa.Column(sa.Integer, primary_key=True)
    color = sa.Column(sa.String)

engine = sa.create_engine('sqlite:///test.db')
Base.metadata.create_all(engine)
session = sao.Session(engine)

And here is modify.py:
import time
from models import Ball, session

session.query(Ball).delete()
b = Ball(color='red')
session.add(b)
session.commit()
time.sleep(2)
b.color = 'blue'
session.add(b)
session.commit()

I find it very strange that I get an inconsistency here between my DB query (that sees the latest DB state) and the object returned via the SQLAlchemy identiy map for my DB query (which is stale, reflecting the DB state the first time the row in question was read). I know that restarting my transaction in the query.py process before each query will invalidate the cached objects in the identity map and result in the blue ball being blue here, but that's a non-starter.
I'd be happy if the blue ball were blue -- i.e. if the DB query and object it returned agreed -- or if the blue ball query returned None -- i.e. if the concurrent DB modification was not visible in the query transaction. But I seem to be stuck in the middle.
",0,178,"It seems the underlying issue is that SQLite support is buggy by default in Python, and SQLAlchemy knowingly inherits this buggy behavior. I eventually figured out how to get both possible correct behaviors, i.e. either make the blue ball blue, by invalidating the identity map/cache without cancelling the current transaction, or make the query for blue balls return None, by running the query.py in a properly isolated transaction.
Achieving isolation / making the blue ball query return None
I found that setting the isolation level, by passing isolation_level=&lt;level&gt; to create_engine had no effect, i.e. this didn't provide a way to make the query.py run in a transaction that is isolated from the writes by modify.py, where the query for blue balls would return None. After reading about isolation levels in SQLite it seems that setting the isolation level to SERIALIZABLE should accomplish this, but it does not. However, the SQLAlchemy documentation warns that by default SQLite transactions are broken:

In the section Database Locking Behavior / Concurrency, we refer to the pysqlite drivers assortment of issues that prevent several features of SQLite from working correctly. The pysqlite DBAPI driver has several long-standing bugs which impact the correctness of its transactional behavior. In its default mode of operation, SQLite features such as SERIALIZABLE isolation, transactional DDL, and SAVEPOINT support are non-functional, and in order to use these features, workarounds must be taken.

That page goes on to suggest workarounds to get functioning transactions, and those workarounds work for me. Namely, adding the following to the bottom of models.py achieves the isolation behavior where the blue balls query returns None:
@sa.event.listens_for(engine, ""connect"")
def do_connect(dbapi_connection, connection_record):
    # disable pysqlite's emitting of the BEGIN statement entirely.
    # also stops it from emitting COMMIT before any DDL.
    dbapi_connection.isolation_level = None

@sa.event.listens_for(engine, ""begin"")
def do_begin(conn):
    # emit our own BEGIN
    conn.exec_driver_sql(""BEGIN"")

With this change, the output becomes
$ python modify.py &amp;! python query.py
Red ball color is red                                                                                                                                                   
Traceback (most recent call last):                                                                                                                                      
  File ""query.py"", line 10, in &lt;module&gt;
    print(f'Blue ball color is {b.color}')
AttributeError: 'NoneType' object has no attribute 'color'

I.e. the DB write in modify.py that makes the ball blue is not visible in the (implicit) transaction in query.py.
Making the query and returned object agree / making the blue ball blue
On the other hand, to get the behavior where the the blue ball is blue, it's enough to invalidate the cache/identity map before each query, using session.expire_all(). I.e., changing query.py to the following works:
import time
from models import Ball, session

time.sleep(1)
r = session.query(Ball).filter(Ball.color=='red').first()
print(f'Red ball color is {r.color}')
time.sleep(2)
# Added this line:
session.expire_all()
b = session.query(Ball).filter(Ball.color=='blue').first()
print(f'Blue ball color is {b.color}')
print(f'Red ball id is {r.id}, blue ball id is {b.id}')

With this change, the output becomes
$ python modify.py &amp;! python query.py                                                                                                                                
Red ball color is red                                                                                                                                                   
Blue ball color is blue                                                                                                                                                 
Red ball id is 1, blue ball id is 1

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/66004467,MySQL BIGINT inconsistent for inserts?,"On ubuntu.. running MySQL v 5.6.
created a python program that performs all my operations.
my app creates tables dynamically. there are many. a few are very similar.. for example, here are two:
create table tst.intgn_party_test_load (
  party_id bigint unsigned NOT NULL,
  party_supertype varchar(15) NOT NULL,
  carrier_party_id bigint unsigned NOT NULL,
  full_name varchar(500),
  lda_actv_ind integer,
  lda_file_id integer,
  lda_created_by varchar(100),
  lda_created_on datetime,
  lda_updated_by varchar(100),
  lda_updated_on datetime, 
  PRIMARY KEY(party_id,party_supertype,carrier_party_id)
) 

and
create table tst.intgn_party_relationship (
  parent_party_id bigint unsigned NOT NULL,
  child_party_id bigint unsigned NOT NULL,
  relationship_type varchar(10),
  lda_actv_ind integer,
  lda_file_id integer,
  lda_created_by varchar(100),
  lda_created_on datetime,
  lda_updated_by varchar(100),
  lda_updated_on datetime, 
  PRIMARY KEY(parent_party_id,child_party_id,relationship_type)
) 

My program also dynamically populates the tables. I construct the party id fields using source data converted to an BIGINT.
For example, the insert it constructs for the first table is:
INSERT INTO intgn_party_test_load (
  party_supertype, 
  carrier_party_id, 
  party_id, 
  full_name, 
  lda_actv_ind, 
  lda_file_id) 
SELECT  
  'Agency' as s0,
  0 as s1,
  CONV(SUBSTRING(CAST(SHA(CONCAT(full_name,ga)) AS CHAR), 1, 16), 16, 10) as s2,
  CONCAT(full_name,'-',ga) as s3, 
  lda_actv_ind, 
  lda_file_id 
FROM tst.raw_listing_20210118175114 
ON DUPLICATE KEY 
UPDATE  
  full_name = VALUES(full_name), 
  lda_actv_ind = VALUES(lda_actv_ind), 
  lda_file_id = VALUES(lda_file_id) ;

and for the second table the insert constructed looks very similar, and is based on the exact same source data:
INSERT INTO tst.intgn_party_relationship (
  parent_party_id,
  relationship_type,
  child_party_id, 
  lda_actv_ind, 
  lda_file_id) 
SELECT (Select party_id 
        from intgn_party 
        where full_name = 'xxx') as s0,
       'Location' as s1,
       CONV(SUBSTRING(CAST(SHA(CONCAT(full_name,ga)) AS CHAR), 1, 16), 16, 10) as s2, 
       lda_actv_ind, 
       lda_file_id 
FROM tst.raw_listing_20210118175114 
ON DUPLICATE KEY 
UPDATE  
  lda_actv_ind = VALUES(lda_actv_ind), 
  lda_file_id = VALUES(lda_file_id) 

Now... the first table (intgn_party_test_load) is the issue. I can drop it, recreate it manually even.. no matter what i do, the data inserted into it via python has the BIGINT party_id truncated to just 16 digits.
EVERY OTHER TABLE that uses the exact same formula to populate the party_id, creates BIGINT numbers that are between 18 and 20 digits long. I can see all the same source records loaded in the tables, and i see the truncated values in the first table (intgn_party_test_load). for example, the first table has a record with party id = 7129232523783260.  the second table (and many others) has the same record loaded with [child]party id  = 7129232523783260081.
The exact same formula, executed the exact same way from python.. but this table gets shorter BIGINTs.
Interestingly, I tried manually running the insert into this table (not using the python program), and it inserts the full BIGINT values.
So I'm confused why the python program has 'chosen' this table to not work correctly, while it works fine on all other tables.
Is there some strange scenario where values get truncated?
BTW, my python program utilizes sqlalchemy to run the creations/inserts. Since it works manually, I have to assume its related to sqlalchemy.. but no idea why it works on all but this table..
[edit]
to add, the sql commands through sqlalchemy are executed using db_connection.execute(sql)
[edit - adding more code detail]
from sqlalchemy import create_engine, exc

engine = create_engine(
            connection_string,
            pool_size=6, max_overflow=10, encoding='latin1', isolation_level='AUTOCOMMIT'
        )
        connection = engine.connect()

sql = ""INSERT INTO intgn_party_test_load (
  party_supertype, 
  carrier_party_id, 
  party_id, 
  full_name, 
  lda_actv_ind, 
  lda_file_id) 
SELECT  
  'Agency' as s0,
  0 as s1,
  CONV(SUBSTRING(CAST(SHA(CONCAT(full_name,ga)) AS CHAR), 1, 16), 16, 10) as s2,
  CONCAT(full_name,'-',ga) as s3, 
  lda_actv_ind, 
  lda_file_id 
FROM tst.raw_listing_20210118175114 
ON DUPLICATE KEY 
UPDATE  
  full_name = VALUES(full_name), 
  lda_actv_ind = VALUES(lda_actv_ind), 
  lda_file_id = VALUES(lda_file_id) ;""

        result = db_connection.execute(sql)

Thats as best i can reduce it too (the code is much more complicated as it dynamically creates the statement amoungst other things).. but from my logging, i see the exact statement it is executing (As above), and i see the result in the BIGINT columns after.  all tables but this one. And only when through the app.
so it doesn't happen to the other tables even through the app..
very confusing.. was hoping someone just knew a bug in mySQL 5.6 around BIGINTs as it pertains to maybe the destination table's key construct or total length of records.. or some other crazy reason.  I do see that interestingly, if i do a distinct on BIGINT column that has &gt;18 digit lengths, it comes back as 16 digits - guess the distinct function doesn't support BIGINT..
was kinda hoping this hints at an issue, but i don't get why the other tables would work fine...
[EDIT - adding some of the things i see sqlalchemy running apparently, around the actual run of my query.. just in the crazy case they impact anything - for the one table?? ]
SET AUTOCOMMIT = 0
SET AUTOCOMMIT = 1
SET NAMES utf8mb4
SHOW VARIABLES LIKE 'sql_mode'
SHOW VARIABLES LIKE 'lower_case_table_names'
SELECT VERSION()
SELECT DATABASE()
SELECT @@tx_isolation
show collation where `Charset` = 'utf8mb4' and `Collation` = 'utf8mb4_bin'
SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1
SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1
SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8mb4) COLLATE utf8mb4_bin AS anon_1
ROLLBACK
SET NAMES utf8mb4

hard to say the order or anything like that.. there are a ton that get run at the same microsecond.
",0,326,"after racking my brains for days.. coming at it from all angles, i could not figure out why 1 table out of many, had issues with truncating the SHA'd value.
In the end, i have redesigned how i hold my Ids, and i no longer bother converting to BIGINT. it all works fine when i leave it as CHAR.
CAST(SHA(CONCAT(full_name,ga)) AS CHAR)

So changed all my Id columns to varchar(40) and use the above style.  All good now. Joins will use varchar instead of bigint - which i'm ok with.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/61657802,Flask SQLAlchemy Sessions commit() not working &quot;sometimes&quot;,"I've been working in a web app for a while and this is the first time I realize this problem, I think it could be related with how SQLAlchemy sessions are handled, so some clarification in simple term would be helpful.
My configuration for work with flask sqlAlchemy is:

from flask_sqlalchemy import SQLAlchemy
db = SQLAlchemy(app)


My problem: db.session.commit() sometimes doesn't save changes. I wrote some flask endpoints which are reached via the front end requests in the user browser.
In this particular case, I'm editing a hotel ""Booking"" object altering the ""Rooms"" columns which is a Text field. 

the function does the following:

1-Query the Booking object from the dates in the request

2- Edit the Rooms column of this Booking object

3- Commit the changes ""db.session.commit()""

4- If a user has X functionality active, I make some checks calling a second function:

4.1- This functions make some checks and query and edit another object in the database different from the ""Booking"" object I edited previously.

4.2- At the end of this secondary function I call db.session.commit() ""Note this changes always got saved correctly in the database""

4.3- Return the results to the previous function

5- Return results to the front end (""just before this return, I print the Booking.Rooms to make sure it looks as it should, and it does... I even tried to make a second commit after the print but before the return... But after this, sometimes Booking.Rooms are updated as expected but some other times it doesn't... I noted if repeat the action many times it finally works, but given the intermediate function ""described in point 4"" saves all his changes correctly, this causes an inconsistency in the data and drives me mad because if I repeat the action and procedure in the function of point 4 worked correctly, I can't repeat the mod Rooms action...

So, I'm now really confused if this is something I don't understand from flask sessions, for what I understand, whenever I make a new request to flask, it's an isolated session, right?
I mean, if 2 concurrent users are storing some changes in the database, a db.session.commit() from one of the users won't commit the changes from the other one, right?

Same way, if I call db.session.commit() in one request, that changes are stored in the database, and if after that ""in the same request"", I keep modding things, it's like another session, right? And the committed changes are there already safely stored? And I can still use previous objects for further modifications 

Anyway, all of this shouldn't be a problem because after the commit() I print the Booking.Rooms and looks as expected... And some times it works getting stored correctly and some times it doesn't...

Also note: When I return this result to the client, the client makes instantly a second request to the server to request updated Booking data, and then the data is returned without this expected changes committed... I suppose flask handled all the commit() before it gets the second request ""other way it wouldn't have returned the result previously..."" 

Can this be a limitation of the flask development server which can't handle correctly many requests and that when deployed with gunicorn it doesn't happen?

Any hint or clarification about Sessions would be nice, because this is pretty strange behaviour, especially that sometimes works and others don't...

And as requested here is the code, I know is not possible to reproduce, have a lot of setup behind and would need a lot of data to works as intended under same circumstances as in my case, but this should provide an overview of how the functions looks like and where are the commits I mention above. Any ideas of where can be the problem is very helpful.

#Main function hit by the frontend
@users.route('/url_endpoint1', methods=['POST'], strict_slashes=False)
@login_required
def add_room_to_booking_Api():
    try:
        bookingData = request.get_json()
        roomURL=bookingData[""roomSafeURL""]
        targetBooking = bookingData[""bookingURL""]
        startDate = bookingData[""checkInDate""]
        endDate = bookingData[""checkOutDate""]
        roomPrices=bookingData[""roomPrices""]

        booking = Bookings.query.filter_by(SafeURL=targetBooking).first() 
        alojamiento = Alojamientos.query.filter_by(id=reserva.CodigoAlojamiento).first() #owner of the booking
        room=Rooms.query.filter_by(SafeURL=roomURL).first()
        roomsInBooking=ast.literal_eval(reserva.Habitaciones) #I know, should be json.loads() and json.dumps() for better performance probably...

        #if room is available for given days add it to the booking
        if CheckIfRoomIsAvailableForBooking(alojamiento.id, room, startDate, endDate, booking) == ""OK"":

            roomsInBooking.append({""id"": room.id, ""Prices"": roomPrices, ""Guests"":[]}) #add the new room the Rooms column of the booking
            booking.Habitaciones = str(roomsInBooking)#save the new rooms data
            print(booking.Habitaciones) # check changes applied
            room.ReservaAsociada = booking.id  # associate booking and room
            for ocupante in room.Ocupantes: #associate people in the room with the booking
                ocupante.Reserva = reserva.id

            #db.session.refresh(reserva) # test I made to check if something changes but didn't worked
            if some_X_function() == True: #if user have some functionality enabled
                #db.session.begin() #another test which didn't worked
                RType = WuBook_Rooms.query.filter_by(ParentType=room.Tipo).first()
                RType=[RType] #convert to list because I resuse the function in cases with multiple types
                resultAdd = function4(RType, booking.Entrada.replace(hour=0, minute=0, second=0), booking.Salida.replace(hour=0, minute=0, second=0))
                if resultAdd[""resultado""] == True:  # ""resultado"":error, ""casos"":casos
                    return (jsonify({""resultado"": ""Error"", ""mensaje"": resultAdd[""casos""]}))

            print(booking.Habitaciones) #here I still get expected result
            db.session.commit()
            #I get this return of averything correct in my frontend but not really stored in the database
            return jsonify({""resultado"": ""Ok"", ""mensaje"": ""Room "" + str(room.Identificador) + "" added to the booking""})

        else:
            return (jsonify({""resultado"": ""Error"", ""mensaje"": ""Room "" + str(room.Identificador) + "" not available to book in target dates""}))

    except Exception as e:
        #some error handling which is not getting hit now
        db.session.rollback()
        print(e, "": en linea"", lineno())
        excepcion = str((''.join(traceback.TracebackException.from_exception(e).format()).replace(""\n"",""&lt;/br&gt;""), ""&lt;/br&gt;Excepcion emitida ne la lnea: "", lineno()))
        sendExceptionEmail(excepcion, current_user)
        return (jsonify({""resultado"":""Error"",""mensaje"":""Error""}))

#function from point 4
def function4(RType, startDate, endDate): 
    delta = endDate - startDate
    print(startDate, endDate)
    print(delta)
    for ind_type in RType: 
        calendarUpdated=json.loads(ind_type.updated_availability_by_date)
        calendarUpdatedBackup=calendarUpdated
        casos={}
        actualizar=False
        error=False
        for i in range(delta.days):
            day = (startDate + timedelta(days=i))
            print(day, i)
            diaString=day.strftime(""%d-%m-%Y"")
            if day&gt;=datetime.now() or diaString==datetime.now().strftime(""%d-%m-%Y""): #only care about present and future dates
                disponibilidadLocal=calendarUpdated[diaString][""local""]
                yaReservadas=calendarUpdated[diaString][""local_booked""]
                disponiblesChannel=calendarUpdated[diaString][""avail""]
                #adjust availability data
                if somecondition==True:
                    actualizar=True
                    casos.update({diaString:""Happened X""})
                else:
                    actualizar=False
                    casos.update({diaString:""Happened Y""})
                    error=""Error""
        if actualizar==True: #this part of the code is hit normally and changes stored correctly
            ind_type.updated_availability_by_date=json.dumps(calendarUpdated)
            wubookproperty=WuBook_Properties.query.filter_by(id=ind_type.PropertyCode).first()
            wubookproperty.SyncPending=True
            ind_type.AvailUpdatePending=True
        elif actualizar==False: #some error occured, revert changes
            ind_type.updated_availability_by_date = json.dumps(calendarUpdatedBackup)

    db.session.commit()#this commit persists 
    return({""resultado"":error, ""casos"":casos}) #return to main function with all this chnages stored

",0,1658,"Realized nothing was wrong at session level, it was my fault in another function client side which sends a request to update same data which is just being updated but with the old data... so in fact, I was getting the data saved correctly in the database but overwrote few milliseconds later. It was just a return statement missing in a javascript file to avoid this outcome...
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/54888558,Syntax for late-binding many-to-many self-referential relationship,"I have found many explanations for how to create a self-referential many-to-many relationship (for user followers or friends) using a separate table or class:
Below are three examples, one from Mike Bayer himself:

Many-to-many self-referential relationship in sqlalchemy
How can I achieve a self-referencing many-to-many relationship on the SQLAlchemy ORM back referencing to the same attribute?
Miguel Grinberg's Flask Megatutorial on followers

But in every example I've found, the syntax for defining the primaryjoin and secondaryjoin in the relationship is an early-binding one:
# this relationship is used for persistence
friends = relationship(""User"", secondary=friendship, 
                       primaryjoin=id==friendship.c.friend_a_id,
                       secondaryjoin=id==friendship.c.friend_b_id,
)

This works great, except for one circumstance: when one uses a Base class to define the id column for all of your objects as shown in Mixins: Augmenting the base from the docs
My Base class and followers table are defined thusly:
from flask_sqlchalchemy import SQLAlchemy
db = SQLAlchemy()

class Base(db.Model):
    __abstract__ = True
    id = db.Column(db.Integer, primary_key=True)

user_flrs = db.Table(
    'user_flrs',
    db.Column('follower_id', db.Integer, db.ForeignKey('user.id')),
    db.Column('followed_id', db.Integer, db.ForeignKey('user.id')))

But now I have trouble with the followers relationship that has served me loyally for a while before I moved the id's to the mixin:
class User(Base):
    __table_name__ = 'user'
    followed_users = db.relationship(
        'User', secondary=user_flrs, primaryjoin=(user_flrs.c.follower_id==id),
        secondaryjoin=(user_flrs.c.followed_id==id),
        backref=db.backref('followers', lazy='dynamic'), lazy='dynamic')

db.class_mapper(User)  # trigger class mapper configuration

Presumably because the id is not present in the local scope, though it seems to throw a strange error for that:

ArgumentError: Could not locate any simple equality expressions involving locally mapped foreign key columns for primary join condition 'user_flrs.follower_id = :follower_id_1' on relationship User.followed_users.  Ensure that referencing columns are associated with a ForeignKey or ForeignKeyConstraint, or are annotated in the join condition with the foreign() annotation. To allow comparison operators other than '==', the relationship can be marked as viewonly=True.

And it throws the same error if I change the parentheses to quotes to take advantage of late-binding. I have no idea how to annotate this thing with foreign() and remote() because I simply don't know what sqlalchemy would like me to describe as foreign and remote on a self-referential relationship that crosses a secondary table! I've tried many combinations of this, but it hasn't worked thus far.
I had a very similar (though not identical) problem with a self-referential relationship that did not span a separate table and the key was simply to convert the remote_side argument to a late-binding one. This makes sense to me, as the id column isn't present during an early-binding process.
If it is not late-binding that I am having trouble with, please advise. In the current scope, though, my understanding is that id is mapped to the Python builtin id() and thus will not work as an early-binding relationship.
Converting id to Base.id in the joins results in the following error:

ArgumentError: Could not locate any simple equality expressions involving locally mapped foreign key columns for primary join condition 'user_flrs.follower_id = ""&lt;name unknown&gt;""' on relationship User.followed_users.  Ensure that referencing columns are associated with a ForeignKey or ForeignKeyConstraint, or are annotated in the join condition with the foreign() annotation. To allow comparison operators other than '==', the relationship can be marked as viewonly=True.

",0,387,"You can't use id in your join filters, no, because that's the built-in id() function, not the User.id column.

You have three options:


Define the relationship after creating your User model, assigning it to a new User attribute; you can then reference User.id as it has been pulled in from the base:

class User(Base):
    # ...

User.followed_users = db.relationship(
    User,
    secondary=user_flrs,
    primaryjoin=user_flrs.c.follower_id == User.id,
    secondaryjoin=user_flrs.c.followed_id == User.id,
    backref=db.backref('followers', lazy='dynamic'),
    lazy='dynamic'
)

Use strings for the join expressions. Any argument to relationship() that is a string is evaluated as a Python expression when configuring the mapper, not just the first argument:

class User(Base):
    # ...

    followed_users = db.relationship(
        'User',
        secondary=user_flrs,
        primaryjoin=""user_flrs.c.follower_id == User.id"",
        secondaryjoin=""user_flrs.c.followed_id == User.id"",
        backref=db.backref('followers', lazy='dynamic'),
        lazy='dynamic'
    )

Define the relationships as callables; these are called at mapper configuration time to produce the final object:

class User(Base):
    # ...

    followed_users = db.relationship(
        'User',
        secondary=user_flrs,
        primaryjoin=lambda: user_flrs.c.follower_id == User.id,
        secondaryjoin=lambda: user_flrs.c.followed_id == User.id,
        backref=db.backref('followers', lazy='dynamic'),
        lazy='dynamic'
    )



For the latter two options, see the sqlalchemy.orgm.relationship() documentation:


  Some arguments accepted by relationship() optionally accept a callable function, which when called produces the desired value. The callable is invoked by the parent Mapper at mapper initialization time, which happens only when mappers are first used, and is assumed to be after all mappings have been constructed. This can be used to resolve order-of-declaration and other dependency issues, such as if Child is declared below Parent in the same file*[.]*
  
  [...]
  
  When using the Declarative extension, the Declarative initializer allows string arguments to be passed to relationship(). These string arguments are converted into callables that evaluate the string as Python code, using the Declarative class-registry as a namespace. This allows the lookup of related classes to be automatic via their string name, and removes the need to import related classes at all into the local module space*[.]*
  
  [...]
  
  
  primaryjoin   
  
  [...]
  
  primaryjoin may also be passed as a callable function which is evaluated at mapper initialization time, and may be passed as a Python-evaluable string when using Declarative.
  
  
  [...]
  
  
  secondaryjoin 
  
  [...]
  
  secondaryjoin may also be passed as a callable function which is evaluated at mapper initialization time, and may be passed as a Python-evaluable string when using Declarative.
  


Both the string and the lambda define the same user_flrs.c.followed_id == User.id / user_flrs.c.follower_id == User.id expressions as used in the first option, but because they are given as a string and callable function, respectively, you postpone evaluation until SQLAlchemy needs to have those declarations finalised.
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/49154668,SQLAlchemy plus FormAlchemy displays @hybrid_property name and not the returned value,"I have recently updated my SQLAlchemy and FormAlchemy to the newest ones and a strange thing has occurred. 

I do use Mako Templates to display data from my model. 

Python:

class Asset(object):
        id = Column(Integer, primary_key=True, nullable=False)
        ...

        @hybrid_property
        def underscore_name(self):
             return ""x_underscore_name_x""


Mako template:

  &lt;li&gt;Item: ${Asset.underscore_name}&lt;/li&gt;


Before the upgrade the web page rendered text was:

Item: x_underscore_name_x


After the upgrade it shows:

Item: Asset.underscore_name


Important! The method is being executed but the returned result is not being rendered on the webpage. Any ideas?

EDIT:

The library responsible for this behaviour is SQL Alchemy &gt;=1.1.0. Version 1.0.19 does not have this issue. Let's see what's the response from the developers.
",0,254,"By the courtesy of Michael Bayer who commented on reported issue #4214:
hybrid_property name being displayed rather than the returned value

The value of Asset.underscore_name is a SQL expression, which since the changes described in http://docs.sqlalchemy.org/en/latest/changelog/migration_11.html#hybrid-properties-and-methods-now-propagate-the-docstring-as-well-as-info is derived from the clause_element() method of the returned object that Core understands. The hybrid you illustrate above is not correctly written because it is not returning a Core SQL expression object:

    s = Session()
    print(s.query(Asset.underscore_name))

sqlalchemy.exc.InvalidRequestError: SQL expression, column, or mapped entity expected - got ''x_underscore_name_x''


if you are looking for a class-bound attribute to return a string, just assign it:

class Asset(...):
    underscore_name = ""x_underscore_name_x""


or if you want that to be a method that is callable at the classlevel, use a @classproperty, there's a recipe for that here: https://stackoverflow.com/a/5191224/34549 and SQLAlchemy has one you can copy: 
https://github.com/zzzeek/sqlalchemy/blob/master/lib/sqlalchemy/util/langhelpers.py#L1140

IMO there's no bug here because this is not the correct usage of @hybrid_property

Thank you very much Michael!
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/47710463,SQLite: the database is locked,"I'm using flask with sqlalchemy and sqlite db. I have 2 ajax that send some data from html to my .py file.

The problem is every time when i do any of these 2 operations, the second one become unavailable because of lock of db. Also, if first chosen action will be deleting, then exception firing no matter what operation will be chosen after. with first choice of adding, we can add without limitations that's strange too, because functions seem similar.

I've tried timeouts, closing sessions in a different ways, the result is always the same.

Here are two functions-handlers:

app = Flask(__name__)
csrf = CSRFProtect(app)

app.config.from_object('config')
db = SQLAlchemy(app)

import forms
import models


@app.route('/delete', methods = ['GET', 'POST'])
def delete():
    if request.method == ""POST"":
        if request.form['type'] == ""delete"":
            print(""delete"")
            engine = create_engine(SQLALCHEMY_DATABASE_URI)
            Session = sessionmaker(bind=engine)
            session = Session()

            try:
                print(""try"")
                requested = request.form['id']
                print(requested)
                models.Income.query.filter(models.Income.id == requested).delete()
                session.commit()
            except:
                print(""rollback"")
                session.rollback()
            finally:
                print(""fin"")
                session.close()

            ellist = models.Income.query.all()
            return render_template(""incomeSection.html"", list=ellist)


@app.route('/add', methods=['GET', 'POST'])
def add():
    if request.method == ""POST"":
        if request.form['type'] == ""add"":
            print('add')
            engine = create_engine(SQLALCHEMY_DATABASE_URI)
            Session = sessionmaker(bind=engine)
            session = Session()

            try:
                print(""try"")
                newItem = models.Income(name=request.form['name'], tag=request.form['tag'],
                                        account=request.form['account'],
                                        date=date(*(int(i) for i in request.form['date'].split(""-""))))
                session.add(newItem)
                session.commit()
            except:
                print('rollback')
                session.rollback()
            finally:
                print(""fin"")
                session.close()

            ellist = models.Income.query.all()
            print(ellist)
            return render_template(""incomeSection.html"", list=ellist)


I've read that this exception caused by non-closed connections, but I have .close() in every finally block. I think the problem might be because of the db = SQLAlchemy(app) but I don't know how to fix if that is the case. Because I use this variable to connect with db in forms.py where I have the form template and in models.py where I defined my tables within db.
",0,611,"So, aperrently, thete was an issue with number of connections.
The thing that solved my problem was the context manager for sqlalchemy, i used this one:

class SQLAlchemyDBConnection(object):

def __init__(self, connection_string):
    self.connection_string = connection_string
    self.session = None

def __enter__(self):
    engine = create_engine(self.connection_string)
    Session = sessionmaker()
    self.session = Session(bind=engine)
    return self

def __exit__(self, exc_type, exc_val, exc_tb):
    self.session.commit()
    self.session.close()


and in the handler just 

with SQLAlchemyDBConnection(SQLALCHEMY_DATABASE_URI) as db:
            newItem = models.Income(*your params*)
            db.session.add(newItem)


And now it works fine, but i still don't know what was the issue in version that was earlier. They are seem to be same just with or without context manager
",,
SQLAlchemy strange result,https://stackoverflow.com/questions/35247731,more than one row returned by a subquery used as an expression,"All my SQLAlchemy models query just fine, but there is one that gives me:

more than one row returned by a subquery used as an expression


This is the (shortened) SQL generated by SQLAlchemy:

SELECT poi.key
,poi.pok
,poi.noc
,coalesce((
    SELECT CASE 
        WHEN (sum(item.noc) IS NULL)
            THEN NULL
        ELSE :param_1
        END AS anon_1
    FROM item,poi
    WHERE poi.key = item.poi_key
    GROUP BY item.key
    ), poi.noc) AS coalesce_1
,coalesce((
    SELECT sum(soi.noc) AS sum_1
    FROM soi
    WHERE soi.poi_key = poi.key
        AND soi.is_shipped = 0
    ), :param_2) AS coalesce_2
,coalesce((
    SELECT CASE 
        WHEN (sum(item.noc) IS NULL)
            THEN NULL
        ELSE :param_1
        END AS anon_1
    FROM item,poi
    WHERE poi.key = item.poi_key
    GROUP BY item.key
    ), poi.noc) - ee((
    SELECT sum(soi.noc) AS sum_1
    FROM soi
    WHERE soi.poi_key = poi.key
        AND soi.is_shipped = 0
    ), :param_2) AS anon_2
FROM poi


The model is:

class POI(Base):
    key = Column(Integer, primary_key=True)
    pok = Column(Integer, nullable=False)
    noc = Column(Integer, nullable=False)
    __table_args__ = (UniqueConstraint(pok,),
        ForeignKeyConstraint([pok],),{})


I'm using scoped_session since my application is multi-threaded and I thought that could be the problem. But it wasn't. I've tried every variation of using Session, but the problem persist. The strange thing is that upon application initialization, when multiple threads are started that fetch data from the db, only this query throws the error. Invoking only this query (manually) works fine somehow. So the problem is apparently in interaction with other queries.

The error is a bit vague to me, but I think the problem is that a subquery should return one result, not many. I'm at a loss where to start looking for the answer. Is it a threading issue? A Session issue? Something else?
",0,556,"I believe you don't need FROM item,poi in 2 subqueries and I also remove the group by in those too. Regading the case expression involving :param_1 I am not sure if what I suggest below is funcionally correct or not, but you need to create a SUM() value before you can test if that SUM() IS NULL.

SELECT
      poi.key
    , poi.pok
    , poi.noc
    , COALESCE((
            SELECT SUM(COALESCE(item.noc,:param_1))
            FROM item
            WHERE poi.key = item.poi_key
      )
      , poi.noc) AS coalesce_1
    , COALESCE((
            SELECT
                  SUM(soi.noc) AS sum_1
            FROM soi
            WHERE soi.poi_key = poi.key
                  AND soi.is_shipped = 0
      )
      , @param_2) AS coalesce_2
    , COALESCE((
            SELECT SUM(COALESCE(item.noc,:param_1))
            FROM item
            WHERE poi.key = item.poi_key
      )
      , poi.noc) - ee((
            SELECT
                  SUM(soi.noc) AS sum_1
            FROM soi
            WHERE soi.poi_key = poi.key
                  AND soi.is_shipped = 0
      )
      , :param_2) AS anon_2
FROM poi

",,
SQLAlchemy strange result,https://stackoverflow.com/questions/24375722,sqlalchemy: dividing timedelta column by float giving strange results,"I'm trying to perform some manipulation on a column inside the query. I want to divide a timedelta column by 60. Unfortunately it is giving strange results.

In my query I return two datetimes, the timedelta between these two datetimes, and the timedelta between these two datetimes divided by 2. An example of the output:

(datetime.datetime(2013, 1, 1, 0, 0, 8), 
datetime.datetime(2013, 1, 1, 0, 7, 32), 
datetime.timedelta(0, 444), 
Decimal('362.0000'))


However, the timedelta 444 divided by 2 does not equal 362.0000. But this is what sqlalchemy is giving me. Is there anyway to get the correct result?

Its important to do this manipulation in the query because I am also trying to use the above as a subquery and average over the results but I'm getting strange averages back as well.

Here is my query:

    sub = session.query( 
                        merge.c.time_received,
                        merge.c.time_sent,
                        func.timediff(
                            merge.c.time_sent,
                            merge.c.time_received
                        ),
                        func.timediff(
                            merge.c.time_sent,
                            merge.c.time_received
                        ) / 2.0
                    ). \
                    filter(
                        merge.c.time_received != None,
                        merge.c.time_sent != None,
                    ). \
                    limit(1)

",0,589,"I am not sure what happens when you divide TIME by a number. But maybe converting it to seconds first using TIME_TO_SEC(time) will do the trick.

I assumed mysql` is used.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/17787042,SqlAlchemy connection string,"I faced very strange issue - my solution using sqlalchemy cannot connects to database. It depends on password I am using. for example, following records are working perfect:

PWD='123123123'
USR='test_user';
SQLALCHEMY_DATABASE_URI = 'mysql://{}:{}@localhost:3306/test_db'.format(USR, PWD)

#final result is 'mysql://test_user:123123123@localhost:3306/test_db'.format(USR, PWD)


But when I trying to put something serious to password (like '8yq+aB&amp;k$V') connection failed. How to 'escape' or encode password somehow that sqlalchemy passed it to mysql succesfully?
",12,36592,,,
SQLAlchemy strange issue,https://stackoverflow.com/questions/41514960,"Use DB data model to generate SQLAlchemy models, schemas, and JSON response","Using Flask and SQLAlchemy for a Python webapp, my goal is to create a system in which I can:


Import data models from an existing PostgreSQL DB, and map these to fields in corresponding SQLAlchemy models
Use these SQLAlchemy models to automatically generate a schema. This schema will then be used to perform data validation on user-submitted data.(I'm currently trying to use Marshmallow, but am open to other suggestions).
Perform JSON response formatting using the schema generated in step 2. (I'm currently trying to format my responses according to JsonAPI's schema - this could change if need be, but I would prefer it).


All of this will be packaged together in a single layer that should allow for simple data access, validation, and response formatting when writing an API, hopefully without ever having to manually define data models or schemas past the definitions that already exist in the DB. This leads me to my question:

Can I utilize existing frameworks to do everything that I'm trying to accomplish? I wouldn't expect that there's one single library that could do everything, but I'm hoping to be able to leverage several existing frameworks. However, I run into direct conflicts, particularly in steps 2 and 3. The stack I've been attempting to use so far is as follows:


Flask (My web framework)
SQLAlchemy (Used to access my data through reflection, see below)
Flask-Marshmallow (To generate a schema for data validation)
Marshmallow-JsonAPI (To format the JSON response according to the JsonAPI spec)


I have a solution for step 1: SQLAlchemy's reflection. This allows me to read the tables in an existing DB and map them to SQLAlchemy models. This works beautifully.

The combination of steps 2 and 3 is where it gets hazy - I'm trying to use Flask-Marshmallow and Marshmallow-JsonAPI. Flask-Marshmallow has the ModelSchema class, which allows you to generate a schema by passing it an existing SQLAlchemy Model. I can pass it my SQLAlchemy model that was generated in step 1, fulfilling my criteria for step 2. As for Marshmallow-JsonAPI, it has functionality such that you can define a schema for your model, and it will automatically create JSONAPI-compliant responses, fulfilling my criteria for step 3. I can individually get each of these frameworks to do what I need.

Unfortunately, since my schema has to inherit from Schema classes in both the Flask-Marshmallow and Marshmallow-JSONAPI, I run into issues. My Model for a ""Dummy"" class looks like this:

import flask
from marshmallow_jsonapi import Schema, fields
from sqlalchemy.ext.declarative import DeferredReflection
from flask_marshmallow import Marshmallow
from flask_sqlalchemy import SQLAlchemy

app = flask.Flask(__name__)
DB = SQLAlchemy()
DB.init_app(app)
MA = Marshmallow(app)
DeferredReflection.prepare(DB.get_engine(app))

class Dummy(DeferredReflection, DB.Model):
    __tablename__ = ""dummy""  # This model will be reflected from the Dummy table


class DummySchema(MA.ModelSchema, Schema):  # The problem line
    class Meta:
        model = Dummy  # Create schema for the Dummy model


Upon trying to start up my API, I get strange errors like this:

api_1        | [2017-01-05 23:16:13,379] ERROR in app: Exception on /api/dummy [POST]
api_1        | Traceback (most recent call last):
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1612, in full_dispatch_request
api_1        |     rv = self.dispatch_request()
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 1598, in dispatch_request
api_1        |     return self.view_functions[rule.endpoint](**req.view_args)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask_restful/__init__.py"", line 477, in wrapper
api_1        |     resp = resource(*args, **kwargs)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask/views.py"", line 84, in view
api_1        |     return self.dispatch_request(*args, **kwargs)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/flask_restful/__init__.py"", line 587, in dispatch_request
api_1        |     resp = meth(*args, **kwargs)
api_1        |   File ""/dummyproj/api/src/dummyproj/api/v1/dummy_resource.py"", line 34, in post
api_1        |     dummy_schema = dummy.DummySchema()
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow_sqlalchemy/schema.py"", line 143, in __init__
api_1        |     super(ModelSchema, self).__init__(*args, **kwargs)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow_jsonapi/schema.py"", line 81, in __init__
api_1        |     super(Schema, self).__init__(*args, **kwargs)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow/schema.py"", line 358, in __init__
api_1        |     self._update_fields(many=many)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow/schema.py"", line 750, in _update_fields
api_1        |     self.__set_field_attrs(ret)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow/schema.py"", line 772, in __set_field_attrs
api_1        |     self.on_bind_field(field_name, field_obj)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow_jsonapi/schema.py"", line 164, in on_bind_field
api_1        |     field_obj.load_from = self.inflect(field_name)
api_1        |   File ""/usr/local/lib/python3.5/site-packages/marshmallow_jsonapi/schema.py"", line 190, in inflect
api_1        |     return self.opts.inflect(text) if self.opts.inflect else text
api_1        | AttributeError: 'SchemaOpts' object has no attribute 'inflect'


It seems that since my Schema class inherits from two different superclasses, it causes issues. I would expect the two marshmallow libraries to be pretty compatible, but there does seem to be an issue with this.

The upshot of all of this is that I'm not quite sure what to try next. I'm not really expecting any specific advice with the Marshmallow frameworks, more just trying to show my train of thought. Is there any sort of best practice surrounding this sort of design, or am I trying to solve too much at once?

(New to both Python and SO - apologies if any of this is unclear).
",7,2392,"I'm successfully combining marshmallow-jsonapi + marshmallow-sqlalchemy. Here's the magic:

import marshmallow
import marshmallow_jsonapi
import marshmallow_jsonapi.flask
import marshmallow_sqlalchemy

import myapp.database as db


def make_jsonapi_schema_class(model_class):
    class SchemaOpts(marshmallow_sqlalchemy.SQLAlchemyAutoSchemaOpts, marshmallow_jsonapi.SchemaOpts):
        pass

    class Schema(marshmallow_sqlalchemy.SQLAlchemyAutoSchema, marshmallow_jsonapi.flask.Schema):
        OPTIONS_CLASS = SchemaOpts

        @marshmallow.post_load
        def make_instance(self, data, **kwargs):
            # Return deserialized data as a dict, not a model instance
            return data

        # You can add default behavior here, for example
        # id = fields.Str(dump_only=True)

    # https://marshmallow-sqlalchemy.readthedocs.io/en/latest/recipes.html#automatically-generating-schemas-for-sqlalchemy-models
    class Meta:
        # Marshmallow-SQLAlchemy
        model = model_class
        sqla_session = db.session

        # Marshmallow-JSONAPI
        type_ = model_class.__name__.lower()
        self_view = type_ + '_detail'
        self_view_kwargs = {'id': '&lt;id&gt;'}
        self_view_many = type_ + '_list'

    schema_class = type(model_class.__name__ + 'Schema', (Schema,), {'Meta': Meta})
    return schema_class


You then call

FooSchema = make_jsonapi_schema_class(Foo)


to generate the Marshmallow schema class from your SQLAlchemy declarative model. You can then in turn subclass that class if you want to customize it.

(To implement the REST API, I'm using Flask-REST-JSONAPI, which is built on top of Flask + SQLAlchemy + marshmallow-jsonapi.)
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/63435264,&quot;Maximum number of parameters&quot; error with filter .in_(list) using pyodbc,"One of our queries that was working in Python 2 + mxODBC is not working in Python 3 + pyodbc; it raises an error like this: Maximum number of parameters in the sql query is 2100. while connecting to SQL Server. Since both the printed queries have 3000 params, I thought it should fail in both environments, but clearly that doesn't seem to be the case here. In the Python 2 environment, both MSODBC 11 or MSODBC 17 works, so I immediately ruled out a driver related issue.
So my question is:

Is it correct to send a list as multiple params in SQLAlchemy because the param list will be proportional to the length of list? I think it looks a bit strange; I would have preferred concatenating the list into a single string because the DB doesn't understand the list datatype.
Are there any hints on why it would be working in mxODBC but not pyodbc? Does mxODBC optimize something that pyodbc does not? Please let me know if there are any pointers - I can try and paste more info here. (I am still new to debugging SQLAlchemy.)

Footnote: I have seen lot of answers that suggest to chunk the data, but because of 1 and 2, I wonder if I am doing the correct thing in the first place.
(Since it seems to be related to pyodbc, I have raised an internal issue in the official repository.)
import sqlalchemy
import sqlalchemy.orm

from sqlalchemy import MetaData, Table
from sqlalchemy.ext.declarative import declarative_base

from  sqlalchemy.orm.session import Session

Base = declarative_base()

create_tables = """"""
CREATE TABLE products(
    idn NUMERIC(8) PRIMARY KEY
);
""""""

check_tables = """"""   
SELECT * FROM products;
""""""

insert_values = """"""
INSERT INTO products
(idn)
values
(1),
(2);
""""""

delete_tables = """"""
DROP TABLE products;
""""""

engine = sqlalchemy.create_engine('mssql+pyodbc://user:password@dsn')
connection = engine.connect()
cursor = engine.raw_connection().cursor()
Session = sqlalchemy.orm.sessionmaker(bind=connection)
session = Session()

session.execute(create_tables)

metadata = MetaData(connection)

class Products(Base):
   __table__ = Table('products', metadata, autoload=True)

try:
    session.execute(check_tables)
    session.execute(insert_values)
    session.commit()
    query = session.query(Products).filter(
        Products.idn.in_(list(range(0, 3000)))
    )
    query.all()
    f = open(""query.sql"", ""w"")
    f.write(str(query))
    f.close()
finally:
    session.execute(delete_tables)
    session.commit()

",5,1352,"When you do a straightforward .in_(list_of_values) SQLAlchemy renders the following SQL ...
SELECT team.prov AS team_prov, team.city AS team_city 
FROM team 
WHERE team.prov IN (?, ?)

... where each value in the IN clause is specified as a separate parameter value. pyodbc sends this to SQL Server as ...
exec sp_prepexec @p1 output,N'@P1 nvarchar(4),@P2 nvarchar(4)',N'SELECT team.prov AS team_prov, team.city AS team_city, team.team_name AS team_team_name 
FROM team 
WHERE team.prov IN (@P1, @P2)',N'AB',N'ON'

... so you hit the limit of 2100 parameters if your list is very long. Presumably, mxODBC inserted the parameter values inline before sending it to SQL Server, e.g.,
SELECT team.prov AS team_prov, team.city AS team_city 
FROM team 
WHERE team.prov IN ('AB', 'ON')

You can get SQLAlchemy to do that for you with
provinces = [""AB"", ""ON""]
stmt = (
    session.query(Team)
    .filter(
        Team.prov.in_(sa.bindparam(""p1"", expanding=True, literal_execute=True))
    )
    .statement
)
result = list(session.query(Team).params(p1=provinces).from_statement(stmt))

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/41240754,Why can I import certain modules in Python only with administrator rights?,"I'm struggling with some strange issues in Python 2.7. I wrote a very long tool where I import different modules, which I had to install first using pip. The tool is to be shared within the company, where different users have different rights on their specific machines.
The problem occurred when another user logged into my machine (I'm having administrator rights there) and tried to use the tool. He was unable to run it, because specific modules could not be imported because of his status as a ""non-admin"". 

The error message is simply ""No module named XY"".
When we looked into the file system, we found that we were not able to look into the folder where the module had been installed, simply because the access was denied by the system. 
We also got this error message when trying to run pip from the cmd; it prints ""Access denied"" and won't do anything.

How is it possible, that some modules can be accessed by anyone, while others can't? And how can I get around this problem?

Specifically, I'm talking about sqlalchemy and pyodbc.

Thanks a lot in advance.

EDIT 1: Oh, and we're talking about Windows here, not Linux...

EDIT 2: Due to company policy it is not possible to set administrator permissions to all users. I tried, as suggested, but it didn't work and I learned that it's not possible within the company.
",3,4816,"Got it...

Following the advice of Nabeel Ahmed, I first uninstalled the packages which caused the issues from my admin account. Then I changed the script to

pip install --user {module_name}


and voila... it works for all users now.

Thanks a lot for you help, guys!
","One simply solution is set permissions for site-package directory (where the packages gt installed) as per usable by all i.e. read and execute permission for all on the directory:

sudo chmod -Rv ugo+rX /usr/lib/python2.7/site-packages/


Also for the lib64 packages - the path to site-packages may vary for various Linux distros.



Edit 1: For windows look into this 'File and Folder Permissions' for setting Read &amp; Execute permissions for all, for a file or folder (i.e. site-packges) 

The path 'd be - C:\Python27\Lib\site-packages



Edit 2: in apropos of:


  EDIT 2: Due to company policy it is not possible to set administrator
  permissions to all users. I tried, as suggested, but it didn't work
  and I learned that it's not possible within the company.


if so, simply install sqlalchemy (or any other package) for specific user using pip:

pip install --user {module_name} 


Source: Per user site-packages directory.
","You should either use virtualenv as stated before or set the proper permissions to the site-packages folder. I should be in C:\Python27\Lib.
"
SQLAlchemy strange issue,https://stackoverflow.com/questions/19963083,SQLAlchemy + SQLite Locking in IPython Notebook,"I'm getting an OperationalError: (OperationalError) database is locked error when connection via SQLAlchemy in an IPython notebook instance and I'm not sure why.

I've written a Python interface to a SQLite database using SQLAlchemy and the Declarative Base syntax. I import the database models into an IPython notebook to explore the data. This worked just fine this morning. Here is the code:

from psf_database_interface import session, PSFTable
query = session.query(PSFTable).first()


But this afternoon after I closed my laptop with IPython running (it restarts the server just fine) I started getting this error. It's strange because I can still open the database from the SQLite3 command line tool and query data. I don't expect any other processes to be connecting to this database and running fuser on the database confirms this. My application is not using any concurrent processes (in the code I've written, IDK if something is buried in SQLAlchemy or IPython), and even if it were I'm just doing a read operation, which SQLite does support concurrently.

I've tried restarting the IPython kernel as well as killing and restarting the IPython notebook server. I've tried creating a backup of the database and replacing the database with the backup as suggested here: https://stackoverflow.com/a/2741015/1216837. Lastly, out of desperation, I tried adding the following to see if I could clean out something stuck in the session somehow:

print session.is_active
session.flush()
session.close()
session.close_all()
print session.is_active


Which returns True and True. Any ideas?

Update: I can run the code snippet that is causing errors from a python file without any issues, the issue only occurs in IPython.
",3,2057,"I faced the same problem. I can run python scripts but the IPython raise the below exception.

You need to check with fuser there is no process which is using this. But if you cannot find anything and your history of commands are not important to you, you can use the following workaround.

When I deleted the /home/my_user/.ipython/profile_default/history.sqlite file, I can start the IPython. The history is empty as I mentioned above.

    $ ipython                                                             
    [TerminalIPythonApp] ERROR | Failed to create history session in /home/my_user/.ipython/profile_default/history.sqlite. History will not be saved.
    Traceback (most recent call last):
    File ""/home/esadrfa/libs/anaconda3/lib/python3.6/site-packages/IPython/core/history.py"", line 543, in __init__
        self.new_session()
    File ""&lt;decorator-gen-22&gt;"", line 2, in new_session
    File ""/home/esadrfa/libs/anaconda3/lib/python3.6/site-packages/IPython/core/history.py"", line 58, in needs_sqlite
        return f(self, *a, **kw)
    File ""/home/esadrfa/libs/anaconda3/lib/python3.6/site-packages/IPython/core/history.py"", line 570, in new_session
        self.session_number = cur.lastrowid
    sqlite3.OperationalError: database is locked
    [TerminalIPythonApp] ERROR | Failed to open SQLite history :memory: (database is locked).

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/39542204,Intermittent &quot;ORA-01458: invalid length inside variable character string&quot; error when inserting data into Oracle DB using SQLAlchemy,"I am working on a Python 3.5 server project and using SQLAlchemy 1.0.12 with cx_Oracle 5.2.1 to insert data into Oracle 11g. I noticed that many of my multi-row table insertions are failing intermittently with ""ORA-01458: invalid length inside variable character string"" error. 

I generally insert a few thousand to a few tens of thousands of rows at a time, and the data is mostly composed of strings, Pandas timestamps, and floating point numbers. I have made the following observations:


The error occurs on both Windows and Linux host OS for the Python server 
The error always occurs intermittently, even when the data doesn't change
If I don't insert floating point numbers, or if I round them, the error happens less often but still happens
If I insert the rows one at a time I don't encounter the error (but this is unacceptable for me performance-wise)


Additionally, I have tried to insert again if I encountered the error. The first thing I tried was to was put a try-except block around where I call execute on the sqlalchemy.engine.base.Connection object like the following:

try:
    connection.execute(my_table.insert(), records)
except DatabaseError as e:
    connection.execute(my_table.insert(), records)


I noticed that using this method the second insertion still often fails. The second thing I tried was to try the same in the implementation of do_executemany of OracleDialect_cx_oracle in the sqlalchemy package (sqlalchemy\dialects\oracle\cx_oracle.py):

def do_executemany(self, cursor, statement, parameters, context=None):
    if isinstance(parameters, tuple):
        parameters = list(parameters)

    # original code
    # cursor.executemany(statement, parameters)

    # new code
    try:
        cursor.executemany(statement, parameters)
    except Exception as e:
        print('trying again')
        cursor.executemany(statement, parameters)


Strangely, when I do it this way the second executemany call will always work if the first one fails. I'm not certain what this means but I believe this points to the cx_Oracle driver being the cause of the issue instead of sqlalchemy. 

I have searched everywhere online and have not seen any reports of the same problem. Any help would be greatly appreciated!
",2,3213,"I had the same problem, the code would some times fail and some times go through with no error. Apparently my chunk size was to big for buffer, the error did not occur anymore once I reduced the chunk size from 10K to 500 rows.
","We found out that if we replace all the float.nan objects with None before we do the insertion the error disappears completely. Very weird indeed!
",
SQLAlchemy strange issue,https://stackoverflow.com/questions/33161280,SQLAlchemy instantiate object from ORM fails with AttributeError: mapper,"I've been trying to get a decent-sized project going with SQLAlchemy on the backend. I have table models across multiple files, a declarative base in its own file, and a helper file to wrap common SQLAlchemy functions, and driver file.

I was uploading data, then decided to add a column. Since this is just test data I thought it'd be simplest to just drop all of the tables and start fresh... then when I tried to recreate the schema and tables, the common declarative base class suddenly had empty metadata. I worked around this by importing the class declaration files -- strange, since I didn't need those imports before -- and it was able to recreate the schema successfully.

But now when I try to create objects again, I get an error:

AttributeError: mapper


Now I'm totally confused! Can someone explain what's happening here? It was working fine before I dropped the schema and now I can't get it working.

Here's the skeleton of my setup:

base.py

from sqlalchemy.ext.declarative import declarative_base
Base = declarative_base()


models1.py

from base import Base
class Business(Base):
    __tablename__ = 'business'
    id = Column(Integer, primary_key=True)


models2.py:

from base import Base
class Category(Base):
    __tablename__ = 'category'
    id = Column(Integer, primary_key=True)


helper.py:

from base import Base

# I didn't need these two imports the first time I made the schema
# I added them after I was just getting an empty schema from base.Base
# but have no idea why they're needed now?
import models1
import models2

def setupDB():
    engine = getDBEngine(echo=True) # also a wrapped func (omitted for space)
    #instantiate the schema
    try:
        Base.metadata.create_all(engine, checkfirst=True)
        logger.info(""Successfully instantiated Database with model schema"")
    except:
        logger.error(""Failed to instantieate Database with model schema"")
        traceback.print_exc()

def dropAllTables():
    engine = getDBEngine(echo=True)
    # drop the schema
    try:
        Base.metadata.reflect(engine, extend_existing=True)
        Base.metadata.drop_all(engine)
        logger.info(""Successfully dropped all the database tables in the schema"")
    except:
        logger.error(""Failed to drop all tables"")
        traceback.print_exc()


driver.py:

import models1
import models2

# ^ some code to get to this point
categories []
categories.append(
                models2.Category(alias=category['alias'],
                                 title=category['title']) # error occurs here
                )


stack trace: (for completeness)

File ""./main.py"", line 16, in &lt;module&gt;
yelp.updateDBFromYelpFeed(fname)
  File ""/Users/thomaseffland/Development/projects/health/pyhealth/pyhealth/data/sources/yelp.py"", line 188, in updateDBFromYelpFeed
    title=category['title'])
  File ""&lt;string&gt;"", line 2, in __init__
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/instrumentation.py"", line 347, in _new_state_if_none
    state = self._state_constructor(instance, self)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 747, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/instrumentation.py"", line 177, in _state_constructor
    self.dispatch.first_init(self, self.class_)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/event/attr.py"", line 256, in __call__
    fn(*args, **kw)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/mapper.py"", line 2825, in _event_on_first_init
    configure_mappers()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/mapper.py"", line 2721, in configure_mappers
    mapper._post_configure_properties()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/mapper.py"", line 1710, in _post_configure_properties
    prop.init()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/interfaces.py"", line 183, in init
    self.do_init()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/relationships.py"", line 1616, in do_init
    self._process_dependent_arguments()
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/orm/relationships.py"", line 1673, in     _process_dependent_arguments
    self.target = self.mapper.mapped_table
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 833, in __getattr__
    return self._fallback_getattr(key)
  File ""/Users/thomaseffland/.virtualenvs/health/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 811, in _fallback_getattr
    raise AttributeError(key)
AttributeError: mapper


I know this post is long, but I wanted to give the complete picture. First I am confused why the base.Base schema was empty in the first place. Now I am confused why the Categories object is missing a mapper!

Any help/insight/advice is greatly appreciated, thanks!

Edit:

So the model files and helper.py are in a supackage and the driver.py is actually a file in a sibling subpackage and its code is wrapped in a function.  This driver function is called by a package-level main file.  So I don't think it can be because SQLAlchemy hasn't had time to initialize? (If I understand the answer correctly)  here is what the (relevant part of) main file looks like:

main.py:

import models.helper as helper
helper.setupDB(echo=true) # SQLAlchemy echos the correct statements

import driverpackage.driver as driver
driver.updateDBFromFile(fname) # error occurs in here


and driver.py actually looks like:

import ..models.models1
import ..models.models2

def updateDBFromFile(fname):
    # ^ some code to get to this point
    categories []
    categories.append(
                    models2.Category(alias=category['alias'],
                                     title=category['title']) # error occurs here
                    )
    # a bunch more code


Edit 2:
I'm beginning to suspect the underlying issue is the same reason I suddenly need to import all of the models to set up the schema in helper.py.  If I print the tables of the imported model objects, they have no bound MetaData or schema:

print YelpCategory.__dict__['__table__'].__dict__
####
{'schema': None, '_columns': &lt;sqlalchemy.sql.base.ColumnCollection object at 0x102312ef0&gt;, 
'name': 'yelp_category', 'description': 'yelp_category', 
'dispatch': &lt;sqlalchemy.event.base.DDLEventsDispatch object at 0x10230caf0&gt;, 
'indexes': set([]), 'foreign_keys': set([]), 
'columns': &lt;sqlalchemy.sql.base.ImmutableColumnCollection object at 0x10230fc58&gt;, 
'_prefixes': [], 
'_extra_dependencies': set([]), 
'fullname': 'yelp_category', 'metadata': MetaData(bind=None), 
'implicit_returning': True, 
'constraints': set([PrimaryKeyConstraint(Column('id', Integer(), table=&lt;yelp_category&gt;, primary_key=True, nullable=False))]), 'primary_key': PrimaryKeyConstraint(Column('id', Integer(), table=&lt;yelp_category&gt;, primary_key=True, nullable=False))}


I wonder why the metadata from the base that created the database is not gettng bound?
",2,2340,"The question seems to have died off, so I'll post my work around.  It's simple.  I refactored the code to explicitly supply the classes and the classic mappers, instead of using the declarative base and everything worked fine again...
","I guess the error happens because you are executing your code on Python module-level. This code is executed when Python imports the module.


Move your code to a function.
Call the function after SQLAlchemy has been properly initialized.
create_all() needs to call only once when the application is installed, because created tables persistent in the database
You need DBSession.configure(bind=engine) or related which will tell models to which database connection they are related. This is missing from the question.

",
SQLAlchemy strange issue,https://stackoverflow.com/questions/57014580,Can&#39;t access sqlalchemy database from heroku. Strange error about some &#39;user&#39; model that I don&#39;t have not existing,"I have an SQLAlchemy database running on a flask app that works fine locally, but as soon as I run it on Heroku I get a strange issue (See stack trace below).

I don't have a model called 'user' so I don't know what's going on. I've made sure that I've run db.create_all().

Creating a row in my table works fine, but it's only once I try selecting some rows do I get an issue (__init__.py):

@app.route('/blog')
def blog():
    title = 'Blog'
    name = get_name()

    blog_text = []

    for post in sorted(Post.query.filter_by(category=Post.Category.blog),
                       key=lambda p: p.posted_at, reverse=True):
        content = post.content

        blog_text.append(content)

    content = '\n'.join(blog_text)

    return render_template('blog.html', **locals())


And my 'Post' logic in my models.py:


class Post(db.Model):
    __tablename__ = 'user'

    id = db.Column(db.Integer, primary_key=True)

    name = db.Column(db.String(200), nullable=False)
    posted_at = db.Column(db.DateTime, nullable=False, default=datetime.now())
    content = db.Column(db.String(200), nullable=False)
    content_type = db.Column(db.String(200), default=""md"", nullable=False)  # md / html
    category = db.Column(db.String(200), nullable=False, default=""blog"")  # poetry / blog

    class Type:
        md = ""md""
        html = ""html""

    class Category:
        poetry = ""poetry""
        blog = ""blog""

   ...



Here's the error:

2019-07-12T21:59:09.120100+00:00 app[web.1]: 10.63.22.102 - - [12/Jul/2019:21:59:09 +0000] ""GET /blog HTTP/1.1"" 500 290 ""https://secure-savannah-20745.herokuapp.com/"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""
2019-07-12T21:59:09.120443+00:00 heroku[router]: at=info method=GET path=""/blog"" host=secure-savannah-20745.herokuapp.com request_id=c32b2484-c8ff-4910-a4e2-5089eca7c6d3 fwd=""98.0.144.26"" dyno=web.1 connect=2ms service=66ms status=500 bytes=455 protocol=https
2019-07-12T21:59:13.632588+00:00 app[web.1]: 10.63.22.102 - - [12/Jul/2019:21:59:13 +0000] ""GET /projects HTTP/1.1"" 200 2342 ""https://secure-savannah-20745.herokuapp.com/"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""
2019-07-12T21:59:13.837771+00:00 app[web.1]: 10.63.22.102 - - [12/Jul/2019:21:59:13 +0000] ""GET /static/circles.js HTTP/1.1"" 200 0 ""https://secure-savannah-20745.herokuapp.com/projects"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""
2019-07-12T21:59:13.838434+00:00 heroku[router]: at=info method=GET path=""/static/circles.js"" host=secure-savannah-20745.herokuapp.com request_id=8476c020-fc32-47b3-a4e1-7b6faeb115ca fwd=""98.0.144.26"" dyno=web.1 connect=1ms service=6ms status=200 bytes=5449 protocol=https
2019-07-12T21:59:13.633394+00:00 heroku[router]: at=info method=GET path=""/projects"" host=secure-savannah-20745.herokuapp.com request_id=faa3d473-7bda-4e39-810b-e3994eac7390 fwd=""98.0.144.26"" dyno=web.1 connect=1ms service=1494ms status=200 bytes=2504 protocol=https
2019-07-12T21:59:18.074035+00:00 heroku[router]: at=info method=GET path=""/blog"" host=secure-savannah-20745.herokuapp.com request_id=e9748e89-dcce-43db-8db8-a16c7fe657ab fwd=""98.0.144.26"" dyno=web.1 connect=3ms service=12ms status=500 bytes=455 protocol=https
2019-07-12T21:59:18.070372+00:00 app[web.1]: [2019-07-12 21:59:18,069] ERROR in app: Exception on /blog [GET]
2019-07-12T21:59:18.070385+00:00 app[web.1]: Traceback (most recent call last):
2019-07-12T21:59:18.070396+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1244, in _execute_context
2019-07-12T21:59:18.070398+00:00 app[web.1]: cursor, statement, parameters, context
2019-07-12T21:59:18.070403+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 550, in do_execute
2019-07-12T21:59:18.070405+00:00 app[web.1]: cursor.execute(statement, parameters)
2019-07-12T21:59:18.070407+00:00 app[web.1]: sqlite3.OperationalError: no such table: user
2019-07-12T21:59:18.070410+00:00 app[web.1]: 
2019-07-12T21:59:18.070412+00:00 app[web.1]: The above exception was the direct cause of the following exception:
2019-07-12T21:59:18.070414+00:00 app[web.1]: 
2019-07-12T21:59:18.070416+00:00 app[web.1]: Traceback (most recent call last):
2019-07-12T21:59:18.070418+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app
2019-07-12T21:59:18.070421+00:00 app[web.1]: response = self.full_dispatch_request()
2019-07-12T21:59:18.070423+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request
2019-07-12T21:59:18.070425+00:00 app[web.1]: rv = self.handle_user_exception(e)
2019-07-12T21:59:18.070427+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception
2019-07-12T21:59:18.070430+00:00 app[web.1]: reraise(exc_type, exc_value, tb)
2019-07-12T21:59:18.070432+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/_compat.py"", line 35, in reraise
2019-07-12T21:59:18.070434+00:00 app[web.1]: raise value
2019-07-12T21:59:18.070436+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request
2019-07-12T21:59:18.070438+00:00 app[web.1]: rv = self.dispatch_request()
2019-07-12T21:59:18.070446+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/flask/app.py"", line 1799, in dispatch_request
2019-07-12T21:59:18.070450+00:00 app[web.1]: return self.view_functions[rule.endpoint](**req.view_args)
2019-07-12T21:59:18.070452+00:00 app[web.1]: File ""/app/main.py"", line 65, in blog
2019-07-12T21:59:18.070455+00:00 app[web.1]: key=lambda p: p.posted_at, reverse=True):
2019-07-12T21:59:18.070457+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3324, in __iter__
2019-07-12T21:59:18.070459+00:00 app[web.1]: return self._execute_and_instances(context)
2019-07-12T21:59:18.070461+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3349, in _execute_and_instances
2019-07-12T21:59:18.070463+00:00 app[web.1]: result = conn.execute(querycontext.statement, self._params)
2019-07-12T21:59:18.070466+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 988, in execute
2019-07-12T21:59:18.070468+00:00 app[web.1]: return meth(self, multiparams, params)
2019-07-12T21:59:18.070470+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/sql/elements.py"", line 287, in _execute_on_connection
2019-07-12T21:59:18.070472+00:00 app[web.1]: return connection._execute_clauseelement(self, multiparams, params)
2019-07-12T21:59:18.070474+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1107, in _execute_clauseelement
2019-07-12T21:59:18.070477+00:00 app[web.1]: distilled_params,
2019-07-12T21:59:18.070479+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1248, in _execute_context
2019-07-12T21:59:18.070481+00:00 app[web.1]: e, statement, parameters, cursor, context
2019-07-12T21:59:18.070484+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1466, in _handle_dbapi_exception
2019-07-12T21:59:18.070486+00:00 app[web.1]: util.raise_from_cause(sqlalchemy_exception, exc_info)
2019-07-12T21:59:18.070488+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 399, in raise_from_cause
2019-07-12T21:59:18.070491+00:00 app[web.1]: reraise(type(exception), exception, tb=exc_tb, cause=cause)
2019-07-12T21:59:18.070493+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 153, in reraise
2019-07-12T21:59:18.070495+00:00 app[web.1]: raise value.with_traceback(tb)
2019-07-12T21:59:18.070498+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1244, in _execute_context
2019-07-12T21:59:18.070500+00:00 app[web.1]: cursor, statement, parameters, context
2019-07-12T21:59:18.070502+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 550, in do_execute
2019-07-12T21:59:18.070504+00:00 app[web.1]: cursor.execute(statement, parameters)
2019-07-12T21:59:18.070507+00:00 app[web.1]: sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: user
2019-07-12T21:59:18.070513+00:00 app[web.1]: [SQL: SELECT user.id AS user_id, user.name AS user_name, user.posted_at AS user_posted_at, user.content AS user_content, user.content_type AS user_content_type, user.category AS user_category
2019-07-12T21:59:18.070515+00:00 app[web.1]: FROM user
2019-07-12T21:59:18.070517+00:00 app[web.1]: WHERE user.category = ?]
2019-07-12T21:59:18.070520+00:00 app[web.1]: [parameters: ('blog',)]
2019-07-12T21:59:18.070661+00:00 app[web.1]: (Background on this error at: http://sqlalche.me/e/e3q8)
2019-07-12T21:59:18.072919+00:00 app[web.1]: 10.63.22.102 - - [12/Jul/2019:21:59:18 +0000] ""GET /blog HTTP/1.1"" 500 290 ""https://secure-savannah-20745.herokuapp.com/projects"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36""


",1,757,"You missed that your table is named user in your model:
class Post(db.Model):
    __tablename__ = 'user'
    #               ^^^^^^

The reason it doesn't exist is because Heroku doesn't keep your filesystem changes. You are using a SQLite database:
... app[web.1]: sqlite3.OperationalError: no such table: user
#               ^^^^^^^

but a sqlite database is stored on the filesystem of your Heroku dyno. Heroku cleans up (cycles) dynos all the time and spins up a new dyno for you for new requests. From their documentation on how Heroku works:

Changes to the filesystem on one dyno are not propagated to other dynos and are not persisted across deploys and dyno restarts. A better and more scalable approach is to use a shared resource such as a database or queue.

Each time a new dyno is spun up to handle your requests, you'll have a new, clean filesystem without your sqlite database, and so an empty database is created again without any tables in it.
Bottom line: You can't use sqlite on Heroku.
Use a Postgres database instead, see the Heroku Postgres documentation. Don't worry, there is a free tier you can use. When you provision the database as an add-on, the DATABASE_URL config var is set for you, which you can read from your app environment.
I usually configure Flask apps that are designed to run on Heroku to read the environment variable but to fall back to a SQLite database when the environment variable is not set:
import os
from pathlib import Path

_project_root = Path(__file__).resolve().parent.parent
_default_sqlite_db = _project_root / ""database.db""

SQLALCHEMY_DATABASE_URI = os.environ.get(
    ""DATABASE_URL"", f""sqlite:///{_default_sqlite_db}""
)

This makes it easy to develop the app locally on SQLite, or set up a local Postgres database for integration testing, and then deploy to Heroku for staging or production.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/53829617,SQLAlchemy Bakery -Function expects a certain array size,"I have a strange issue in my Python project. It uses SQLAlchemy and Bakery to have prepared queries. I have a function that takes the connection (db), bakery, and an array of objects.

This function is called several times by an other function in a for loop and here is my issue (at least what I understand):


Let's assume that the first time it receives an array with two elements.
The next time it is called the function will also expect an array with two elements




import sqlalchemy as sa
def cpe_filter(db, bakery, iterable):
    cpes = []

    try:
        query  = bakery(lambda s: s.query(Cpe))
        query += lambda y: y.filter(
            sa.or_(*[
                Cpe.cpe.like(sa.bindparam('cpe_{}'.format(i)))
                for i, _ in enumerate(iterable)
            ])
        )
        query += lambda y: y.filter_by(active=sa.bindparam('active'))

        cpes = query(db).params(active=True,
                                **{'cpe_{}'.format(i): e for i, e in enumerate(iterable)}) \
                        .all()
    except NoResultFound:
        log.info(""Found no CPE matching list {}."".format(iterable))


If the next array is smaller than the previous, I get this kind of error (Pastebin):

[2018-12-17 16:35:16 - INFO/sqlalchemy.engine.base.Engine:1151] SELECT cpe.id AS cpe_id, cpe.active AS cpe_active, cpe.date_created AS cpe_date_created, cpe.timestamp AS cpe_timestamp, cpe.cpe_part_id AS cpe_cpe_part_id, cpe.device_id AS cpe_device_id, cpe.cpe AS cpe_cpe, cpe.match_nvd AS cpe_match_nvd
FROM cpe
WHERE (cpe.cpe LIKE %(cpe_0)s OR cpe.cpe LIKE %(cpe_1)s OR cpe.cpe LIKE %(cpe_2)s) AND cpe.active = %(active)s
[2018-12-17 16:35:16 - INFO/sqlalchemy.engine.base.Engine:1154] {'cpe_0': 'cpe:/o:sun:solaris', 'cpe_1': 'cpe:/a:tritreal:ted_cde', 'cpe_2': 'cpe:/o:hp:hp-ux', 'active': 1}
[2018-12-17 16:35:16 - INFO/sqlalchemy.engine.base.Engine:1151] SELECT cpe.id AS cpe_id, cpe.active AS cpe_active, cpe.date_created AS cpe_date_created, cpe.timestamp AS cpe_timestamp, cpe.cpe_part_id AS cpe_cpe_part_id, cpe.device_id AS cpe_device_id, cpe.cpe AS cpe_cpe, cpe.match_nvd AS cpe_match_nvd
FROM cpe
WHERE (cpe.cpe LIKE %(cpe_0)s OR cpe.cpe LIKE %(cpe_1)s OR cpe.cpe LIKE %(cpe_2)s) AND cpe.active = %(active)s
[2018-12-17 16:35:16 - INFO/sqlalchemy.engine.base.Engine:1154] {'cpe_0': 'cpe:/a:hp:dtmail', 'cpe_1': 'cpe:/a:university_of_washington:pine', 'cpe_2': 'cpe:/o:sco:unixware', 'active': 1}
[2018-12-17 16:35:16 - ERROR/scap.abc:66] An error has occurred during task execution.
Traceback (most recent call last):
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1127, in _execute_context
    context = constructor(dialect, self, conn, *args)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 635, in _init_compiled
    grp, m in enumerate(parameters)]
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 635, in &lt;listcomp&gt;
    grp, m in enumerate(parameters)]
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/sql/compiler.py"", line 547, in construct_params
    % bindparam.key, code=""cd3x"")
sqlalchemy.exc.InvalidRequestError: A value is required for bind parameter 'cpe_2' (Background on this error at: http://sqlalche.me/e/cd3x)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/root/scap/project/scap/abc.py"", line 64, in run
    self(*args, **kwargs)
  File ""/root/scap/project/scap/tasks.py"", line 362, in __call__
    q.cve_insert_or_update(self.db, self.bakery, self.parse(name))
  File ""/root/scap/project/scap/queries.py"", line 148, in cve_insert_or_update
    cpes = list(cpe_filter(db, bakery, cpes))
  File ""/root/scap/project/scap/queries.py"", line 68, in cpe_filter
    **{'cpe_{}'.format(i): e for i, e in enumerate(products)}) \
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/ext/baked.py"", line 457, in all
    return list(self)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/ext/baked.py"", line 364, in __iter__
    return q._execute_and_instances(context)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3018, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 948, in execute
    return meth(self, multiparams, params)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/sql/elements.py"", line 269, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1060, in _execute_clauseelement
    compiled_sql, distilled_params
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1132, in _execute_context
    None, None)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1413, in _handle_dbapi_exception
    exc_info
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 265, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 248, in reraise
    raise value.with_traceback(tb)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1127, in _execute_context
    context = constructor(dialect, self, conn, *args)
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 635, in _init_compiled
    grp, m in enumerate(parameters)]
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 635, in &lt;listcomp&gt;
    grp, m in enumerate(parameters)]
  File ""/root/.local/share/virtualenvs/scap-TS2Ah8Sl/lib/python3.6/site-packages/sqlalchemy/sql/compiler.py"", line 547, in construct_params
    % bindparam.key, code=""cd3x"")
sqlalchemy.exc.StatementError: (sqlalchemy.exc.InvalidRequestError) A value is required for bind parameter 'cpe_2' [SQL: 'SELECT cpe.id AS cpe_id, cpe.active AS cpe_active, cpe.date_created AS cpe_date_created, cpe.timestamp AS cpe_timestamp, cpe.cpe_part_id AS cpe_cpe_part_id, cpe.device_id AS cpe_device_id, cpe.cpe AS cpe_cpe, cpe.match_nvd AS cpe_match_nvd \nFROM cpe \nWHERE (cpe.cpe LIKE %(cpe_0)s OR cpe.cpe LIKE %(cpe_1)s OR cpe.cpe LIKE %(cpe_2)s) AND cpe.active = %(active)s'] [parameters: [{'active': True, 'cpe_0': 'cpe:/a:university_of_washington:imap', 'cpe_1': 'cpe:/a:netscape:messaging_server'}]] (Background on this error at: http://sqlalche.me/e/cd3x)


As you can see, the function is called three times, the first two times it works without any issue (3 elements each time), and the third time it has only two elements and it expect a third element according to the error.

NB: The iterable can reach about 50 elements most of the time.
",1,424,"The problem stems from the observations 4. and 5. under ""Synopsis"" in the baked queries documentation:


  
  In the above code, even though our application may call upon search_for_user() many times, and even though within each invocation we build up an entirely new BakedQuery object, all of the lambdas are only called once. Each lambda is never called a second time for as long as this query is cached in the bakery.
  The caching is achieved by storing references to the lambda objects themselves in order to formulate a cache key; that is, the fact that the Python interpreter assigns an in-Python identity to these functions is what determines how to identify the query on successive runs. For those invocations of search_for_user() where the email parameter is specified, the callable lambda q: q.filter(User.email == bindparam('email')) will be part of the cache key thats retrieved; when email is None, this callable is not part of the cache key.
  


If you inspect your cpe_filter() function using dis you'll note that the lambda-functions are constants and so keep their identity between calls. As explained in the referenced documentation, SQLAlchemy caches queries based on those identities and calls

query += lambda y: y.filter(
    sa.or_(*[
        Cpe.cpe.like(sa.bindparam('cpe_{}'.format(i)))
        for i, _ in enumerate(iterable)
    ])
)


only once. In other words the placeholders will be set the first time you call cpe_filter(), based on iterable. They will be ""reset"" only when this query has been evicted from the cache.

The solution depends on your DBMS in use. For example Postgresql has the ANY array comparison that could be used:

query += lambda y: y.filter(Cpe.cpe.like(sa.any_(sa.bindparam('cpe'))))


and the parameter would be passed as

# This relies on Psycopg2's lists adaptation:
# http://initd.org/psycopg/docs/usage.html#lists-adaptation
cpes = query(db).params(active=True, cpe=list(iterable)).all()


On MS SQL Server you could perhaps create a full-text index and use CONTAINS:

query += lambda y: y.filter(func.contains(Cpe.cpe, sa.bindparam('cpe')))


The bind param cpe should pass the search condition, which must be formed from iterable:

search_cond = "" OR "".join(iterable)
cpes = query(db).params(active=True, cpe=search_cond).all()


This of course requires that the items in iterable are valid full-text search terms.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/43475159,Strange issue in SQLAlchemy,"Consider the following code which creates a very simple table (without using SQLAlchemy), then adds an entry to it using SQLAlchemy ORM and retrieves it:

import sqlite3
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

DB_PATH = '/tmp/tst.db'

#create a DB
sqlite_conn = sqlite3.connect(DB_PATH)
sqlite_conn.execute('''CREATE TABLE tst (
    id INTEGER PRIMARY KEY ASC AUTOINCREMENT,
    c0 INTEGER,
    c1 INTEGER
);''')
sqlite_conn.commit()

#intialize an SA engine/session/mapped class
engine = create_engine('sqlite:///{}'.format(DB_PATH))
Base = declarative_base()
Base.metadata.reflect(bind=engine)
Session = sessionmaker(bind=engine)

class Tst(Base):
    __table_name__ = 'tst'
    __table__ = Base.metadata.tables[__table_name__]
    columns = list(__table__.columns)
    field_names = [c.name for c in columns]

#add an entry to the table
session = Session()
inst = Tst()
session.add(inst)
session.commit()

#retrieve an entry from the table
session = Session()
inst = session.query(Tst).first()
print inst.c1


One may expect that the code above will just print 'None', as 'c1' was not assigned a value. Instead of it, I'm getting the following error message:

Traceback (most recent call last):
  File ""..."", line 39, in &lt;module&gt;
    print inst.c1
AttributeError: 'Tst' object has no attribute 'c1'


But if the following line will be removed/commented:

    field_names = [c.name for c in columns]


the output will be as expected.

In general, it looks like the iteration over Table.columns inside the class definition will cause the last column to be omitted from the class instances.

Following this answer, I actually changed the code to use Inspector, and it worked fine. However, AFAIK, accessing Table.columns is completely legitimate, so I wanted to understand whether it's buggy behavior or something wrong on my side.

P.S. tested with SQLAlchemy 1.1.9

P.P.S. the issue doesn't appear to be related to a specific DB dialect - reproduced with MySQL, sqlite.
",1,468,"This is more of a Python version issue than an SQLAlchemy issue. The root cause is the leaking of the name c from the list-comprehension in Python 2. It becomes part of the namespace of the constructed class, and so SQLAlchemy sees it as if you were explicitly naming the last column in the list columns in your class definition. Your class definition is equivalent to:

class Tst(Base):
    __table_name__ = 'tst'
    __table__ = Base.metadata.tables[__table_name__]
    columns = list(__table__.columns)
    ...
    c = columns[-1]  # The last column of __table__


If you change your print statement to:

print inst.c


you'll get None as you expected. If you must have your field_names, you could for example remove the name from the namespace:

class Tst(Base):
    __table_name__ = 'tst'
    __table__ = Base.metadata.tables[__table_name__]
    columns = list(__table__.columns)
    field_names = [c.name for c in columns]
    del c


but this is unportable (and ugly) between Python 2 and 3, since the name would not actually exist in 3. You could also work around the issue with attrgetter():

from operator import attrgetter

class Tst(Base):
    __table_name__ = 'tst'
    __table__ = Base.metadata.tables[__table_name__]
    columns = list(__table__.columns)
    field_names = list(map(attrgetter('name'), columns))


or use a generator expression:

    field_names = list(c.name for c in columns)

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/30942439,DataError: (DataError) invalid input syntax for integer: &quot;None&quot;,"Background

We're using Flask to develop a fairly simple application to ""scratch an itch"" we realized we had while working on a previous project. Session management is handled through the Flask-Login extension, coupled with the Github-Flask extension which we use for user authentication. The app uses Flask-SQLAlchemy and Psycopg2 (2.6.3) to connect to a PostgreSQL 9.1 database where user data is stored.

Issue

When the app is run locally (or deployed to a remote server) for testing we're seeing a strange condition during the first login attempt. The first time I log in I successfully get through the Github authentication sets, but immediately see this error.

DataError: (DataError) invalid input syntax for integer: ""None""
LINE 3: WHERE ""user"".id = 'None'
                      ^
'SELECT ""user"".id AS user_id, ""user"".date_created AS user_date_created,
""user"".date_modified AS user_date_modified, ""user"".nickname AS user_nickname,
""user"".email AS user_email, ""user"".about_me AS user_about_me,
""user"".github_access_token AS user_github_access_token, ""user"".github_id AS
user_github_id \nFROM ""user"" \nWHERE ""user"".id = %(param_1)s' {'param_1': u'None'}


I can close the browser tab and then revisit the application URL and see the same message. However, if I close the browser window, completely clear my browser's cache, and then attempt to log back in, it works as expected and I'm able to use the application. From that point on I can log out, switch browsers, clear browser cache again, etc., and there don't appear to be any problems -- it's only that first login until browser cache is cleared that this happens.

My testing shows that this happens on any user and two different users can be in two different states at the same time (one has cleared cache and now isn't having problems, the other who hasn't taken those steps is still stuck in the error state.

Originally we used a local SQLite database instead of Postgres. The issue does not exist there. It only has occurred since the switch to Postgres.

I'm at a loss for what steps I can take to remedy this situation. So far the questions I know to ask haven't been able to lead to the right answers when I search online.

For reference, I'll include the full stack trace as well.

Full Stack Trace

Traceback (most recent call last):
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1836, in __call__
    return self.wsgi_app(environ, start_response)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1820, in wsgi_app
    response = self.make_response(self.handle_exception(e))
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1403, in handle_exception
    reraise(exc_type, exc_value, tb)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1817, in wsgi_app
    response = self.full_dispatch_request()
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1477, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1381, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 1475, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask_debugtoolbar/__init__.py"", line 124, in dispatch_request
    return view_func(**req.view_args)
  File ""/Users/dev/PRODUCT/app/modules/mod_profile/controllers.py"", line 26, in profile
    title='PRODUCT')
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/templating.py"", line 126, in render_template
    ctx.app.update_template_context(context)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask/app.py"", line 716, in update_template_context
    context.update(func())
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask_login.py"", line 825, in _user_context_processor
    return dict(current_user=_get_user())
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask_login.py"", line 794, in _get_user
    current_app.login_manager._load_user()
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask_login.py"", line 363, in _load_user
    return self.reload_user()
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/flask_login.py"", line 325, in reload_user
    user = self.user_callback(user_id)
  File ""/Users/dev/PRODUCT/app/modules/mod_auth/controllers.py"", line 31, in load_user
    return User.query.get(id)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/orm/query.py"", line 840, in get
    return loading.load_on_ident(self, key)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/orm/loading.py"", line 231, in load_on_ident
    return q.one()
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2395, in one
    ret = list(self)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2438, in __iter__
    return self._execute_and_instances(context)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2453, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 729, in execute
    return meth(self, multiparams, params)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/sql/elements.py"", line 322, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 826, in _execute_clauseelement
    compiled_sql, distilled_params
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 958, in _execute_context
    context)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1159, in _handle_dbapi_exception
    exc_info
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/util/compat.py"", line 199, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 951, in _execute_context
    context)
  File ""/Users/dev/PRODUCT/venv/lib/python2.7/site-packages/sqlalchemy/engine/default.py"", line 436, in do_execute
    cursor.execute(statement, parameters)
DataError: (DataError) invalid input syntax for integer: ""None""
LINE 3: WHERE ""user"".id = 'None'
                          ^
 'SELECT ""user"".id AS user_id, ""user"".date_created AS user_date_created, ""user"".date_modified AS user_date_modified, ""user"".nickname AS user_nickname, ""user"".email AS user_email, ""user"".about_me AS user_about_me, ""user"".github_access_token AS user_github_access_token, ""user"".github_id AS user_github_id \nFROM ""user"" \nWHERE ""user"".id = %(param_1)s' {'param_1': u'None'}

",1,5121,"Resolved!

This was a race condition. During the login process our application redirects an authenticated user to their profile page. This was getting triggered correctly, but for new users it was getting called before the user was actually committed to the database. 

Visually it happened at the same time, so you could confirm that the callback was supplied with the ""right"" information, but it wasn't until I deconstructed and then reconstructed the login steps that I realized that it was hitting the route just before the record was actually committed to the database. Since the function that added the record wasn't interrupted by the redirect it did what it was supposed to do, but just a hair late.

Perhaps someone else will find this extract of my code helpful when troubleshooting a similar issue:

## from mod_auth/controllers.py
@mod_auth.before_app_request
def before_request():
    g.user = current_user

@mod_auth.route('/login', methods=['GET'])
def login():
    if g.user.is_authenticated():
        redirect(url_for('mod_home.index'))
    return github.authorize()

@login_manager.user_loader
def load_user(id):
    return User.query.get(id)

@github.access_token_getter
def token_getter():
    user = g.user
    if user is not None:
        return user.github_access_token

@mod_auth.route('/github')
@github.authorized_handler
def authorized(oauth_token):

    next_url = request.args.get('next') or url_for('mod_home.index')
    if oauth_token is None:
        flash(""Authorization failed."")
        return redirect(next_url)

    user = User.query.filter_by(github_access_token=oauth_token).first()
    if user is None:
        user = User(github_access_token=oauth_token)
        db.session.add(user)
        db.session.commit() ## &lt;-- adding a commit here fixed the issue

    user.github_access_token = oauth_token

    login_user(user) 
    ## login_user called, but because the oauth_token exist and had been
    ## added to the session, the user was flagged as ""is_authenticated""
    ## and the redirect was triggered

    ghinfo = github.get('user')

    if 'login' in ghinfo:
        user.nickname = ghinfo['login'].lower()
    else:
        user.nickname = 'nameless'

    db.session.commit() ## this was the only commit, originally

    return redirect(url_for('mod_profile.profile', username=user.nickname))

",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/28610563,SQLAlchemy: different output on queries &quot;field IS NULL&quot; and &quot;IS NOT NULL&quot; on aliased model,"I've noticed a very strange behaviour when querying against field from aliased model. Seems that SQLAlchemy breaks the aliasing while filtering relation against NOT NULL condition. Here is an example:

Base = declarative_base()

class Parent(Base):
    __tablename__ = 'parents'
    id = Column(Integer, primary_key=True)
    children = relation('Child', back_populates='parent')

class Child(Base):
    __tablename__ = 'children'
    id = Column(Integer, primary_key=True)
    parent_id = Column(Integer, ForeignKey('parents.id'))
    parent = relation('Parent', back_populates='children', uselist=False)

aChild = aliased(Child)

print Session.query(aChild.id).filter(aChild.parent == None)
"""""" SELECT children_1.id AS children_1_id
    FROM children AS children_1
    WHERE children_1.parent_id IS NULL
""""""

print Session.query(aChild.id).filter(aChild.parent != None)
"""""" SELECT children_1.id AS children_1_id
    FROM children AS children_1, children
    WHERE children.parent_id IS NOT NULL
""""""

print Session.query(aChild.id).filter(~(aChild.parent == None))
"""""" SELECT children_1.id AS children_1_id
    FROM children AS children_1
    WHERE children_1.parent_id IS NOT NULL
""""""


You can see there is a cross join in the second query, while 1st and 3rd are working as expected. Is there any explanation or solution for this issue except the 3rd example?
",1,345,,,
SQLAlchemy strange issue,https://stackoverflow.com/questions/69130661,"Flask SQL Alchemy try perform SET operation, when it is not needed","I am facing a very strange issue using SQL Alchemy. I have a table in SQL server, to store Sales, with this format :
CREATE TABLE Resale (
  [address] VARCHAR(100) NOT NULL,
  [id] BIGINT NOT NULL,
  [price] INT NOT NULL,
  [start_date] DATETIME2(7) NOT NULL  DEFAULT GETUTCDATE(),
  [sys_start_time] DATETIME2 (7) GENERATED ALWAYS AS ROW START HIDDEN NOT NULL,
  [sys_end_time] DATETIME2 (7) GENERATED ALWAYS AS ROW END HIDDEN NOT NULL,
  PERIOD FOR SYSTEM_TIME ([sys_start_time], [sys_end_time]),
  CONSTRAINT [PK_ReSale] PRIMARY KEY CLUSTERED ([id] ASC))
  WITH (SYSTEM_VERSIONING = ON (HISTORY_TABLE=[dbo].[ResaleHistory], DATA_CONSISTENCY_CHECK=ON));

in python I used flask-sqlalchemy and build this model :
class Resale(db.Model):
    __tablename__ = 'Resale'
    address = db.Column(db.String(100), nullable=False)
    id = db.Column(db.BigInteger, primary_key=True)
    price = db.Column(db.Integer, nullable=False)
    start_date = db.Column(db.DateTime, nullable=False, default=datetime.utcnow())

now I am just trying to insert a record in this table using :
resale = Resale(address=address, id=id, price=price)
db.session.add(resale)
db.session.commit()

and flask return :
pyodbc.ProgrammingError: ('42000', ""[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Table 'Resale' does not have the identity property. Cannot perform SET operation. (8106) (SQLExecDirectW)"")
[SQL: SET IDENTITY_INSERT [Resale] ON]

I really don't understand why I have that, I am not using an autoincrement in my table, so my flask wants me to do ""SET IDENTITY_INSERT Resale ON ?
honnestly I am completly lost with this message, I am just trying to add a record in database
",0,281,"As stated in this answer for a previous similiar question you should consider put the flag ""autoincrement "" to False on your ""id"".
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/66781354,Heroku Postgres throwing int out of range error for ints that should be in range,"I'm attempting to collect options data using a heroku dyno (hobby $7 version) and a heroku Postgres database (standard $50 version). I've got a small script set up that uses yfinance to collect options data for a set of ticker symbols and SQLAlchemy + psycopg2 to insert this data into the postgres database.
I seem to be encountering a very strange error when inserting into the database. For many of the records, I'm seeing a (psycopg2.errors.NumericValueOutOfRange) integer out of range Exception thrown for records that should not have any integers out of range... I've included an example of the error below as well as my model definition. At first I thought maybe it was the id / pk column, but according to SQLAlchemy's docs they automatically treat these fields as a serial data type. So I'm not sure why this error is being thrown with all integer columns clearly in the range of the INT data type.
Is this a heroku thing? Has anyone seen / dealt with this before? I'm not sure how to debug the underlying issue. Thanks in advance!
2021-03-24 12:25:16 ERROR    Unable to commit Option(id=None, ticker=EDIT, option_type=PUT, contractSymbol=EDIT230120P00017500, lastTradeDate=2020-11-05 14:38:12, strike=17.5, lastPrice=4.7, bid=0.0, ask=0.0, change=0.0, percentChange=0.0, volume=1.0, openInterest=0, impliedVolatility=0.12500875, inTheMoney=False, contractSize=REGULAR, currency=USD)
2021-03-24 12:25:16 ERROR    This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.NumericValueOutOfRange) integer out of range

(Background on this error at: http://sqlalche.me/e/14/9h9h) (Background on this error at: http://sqlalche.me/e/14/7s2a)
Continuing.

Model definition:
class Option(config.Base):

    __tablename__ = ""options""

    id = Column(Integer, primary_key=True)
    ticker = Column(String, nullable=False)
    dt_created_db = Column(
        DateTime, nullable=False, default=datetime.datetime.utcnow()
    )
    dt_updated_db = Column(
        DateTime, nullable=False, default=datetime.datetime.utcnow()
    )
    dt_created_market_time_db = Column(
        DateTime, nullable=False,
        default=datetime.datetime.now().astimezone(
            pytz.timezone(
                ""America/New_York""
            )
        )
    )
    option_type = Column(String, nullable=False) # CALL or PUT
    contractSymbol = Column(String, nullable=False)
    lastTradeDate = Column(DateTime, nullable=False)
    strike = Column(Float, nullable=False)
    lastPrice = Column(Float, nullable=False)
    bid = Column(Float)
    ask = Column(Float)
    change = Column(Float)
    percentChange = Column(Float)
    volume = Column(Float)
    openInterest = Column(Integer)
    impliedVolatility = Column(Numeric)
    inTheMoney = Column(Boolean)
    contractSize = Column(String)
    currency = Column(String)

    @classmethod
    def create_option_record(cls, ticker, record, opt_type):
        """"""Creates an Option model instance.

        Args:
            ticker: The ticker symbol.
            record: A pandas dataframe row with ""."" access to columns.
            opt_type: A string, either CALL or PUT.
        Returns:
            Option object.
        """"""
        return cls(
            ticker=ticker,
            option_type=opt_type,
            contractSymbol=record.contractSymbol,
            lastTradeDate=record.lastTradeDate,
            strike=record.strike,
            lastPrice=record.lastPrice,
            bid=record.bid,
            ask=record.ask,
            change=record.change,
            percentChange=record.percentChange,
            volume=record.volume,
            openInterest=record.openInterest,
            impliedVolatility=round(record.impliedVolatility, 20),
            inTheMoney=record.inTheMoney,
            contractSize=record.contractSize,
            currency=record.currency
        )

    def __str__(self):
        return (
            f""Option(""
            f""id={self.id}, ""
            f""ticker={self.ticker}, ""
            f""option_type={self.option_type}, ""
            f""contractSymbol={self.contractSymbol}, ""
            f""lastTradeDate={self.lastTradeDate}, ""
            f""strike={self.strike}, ""
            f""lastPrice={self.lastPrice}, ""
            f""bid={self.bid}, ""
            f""ask={self.ask}, ""
            f""change={self.change}, ""
            f""percentChange={self.percentChange}, ""
            f""volume={self.volume}, ""
            f""openInterest={self.openInterest}, ""
            f""impliedVolatility={self.impliedVolatility}, ""
            f""inTheMoney={self.inTheMoney}, ""
            f""contractSize={self.contractSize}, ""
            f""currency={self.currency}""
            f"")""
        )

",0,366,"I've solved this. It turns out the database logs were behind the compute logs and so the record I thought was causing the error was not actually causing the error.
The root cause was that an np.nan value was being inserted into an INTEGER column. I had to replace np.nan's with None and this solved the problem.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/60812701,Installed with pip3 modules no longer recognizable. Need to reinstall with pip,"I faced a strange issue: I'm working on a Python/Flask project on my local computer, don't use the virtual env. I installed different modules with pip3, like Flask-SQLAlchemy, Flask-Migrate, etc. Everything worked perfectly.
Suddenly, after a few weeks, when running the program, it started to appear the errors, like ModuleNotFoundError: No module named 'flask_sqlalchemy' - and the same for Flask-Migrate, Flask-WTF, etc.

The solution is to install all these modules again but with pip (not pip3). Uninstallation/Installation with pip3 does not help. Why did it happen? Is it possible to turn it back to pip3?

I didn't change the environment, I have the only one actually. Using Python 3.8
",0,37,"I would say that the reason why it is like this, it's because maybe there is some libraries for other your projects that clash with flask libraries and that's why you have issues. E.g. I had the same issue when I didn't use virtual environments when I was learning Python, and when I had multiple projects with different python libraries, some of them clashed and produced unexpected errors, so I started using virtual environments - different virtual environment for each project and it solved all the problems with clashes between different libraries.

Here is a good official tutorial on how to use virtual environments
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/37174982,how to dump data to the database at startup,"I have a flask application, and when it spins up, it needs to dump data to the database. The datafiles are each about 100MB, and currently there are three of them, but soon there will be ~15. 

@app.before_first_request
def populateDB():
        reader = csv.DictReader(file, delimiter='\t', fieldnames=[""not relevant""])
        OCRs = [dict(row.items()) for row in reader]
        db.engine.execute(OpenChromatinRegion.__table__.insert(), OCRs)


I have a function called populateDB which works, so long as the file is very small (300 rows). When I go to run it on any of the large files:

[2016-05-11 23:22:09 +0000] [7] [INFO] working on datafile MCF7-all.fdr0.01TF_anno.txt
[2016-05-11 23:22:24 +0000] [7] [INFO] Datafile MCF7-all.fdr0.01TF_anno.txt properly configured in memory
[2016-05-11 23:22:39 +0000] [1] [CRITICAL] WORKER TIMEOUT (pid:7)
[2016-05-11 23:22:41 +0000] [11] [INFO] Booting worker with pid: 11


And then it just hangs. 

Is there a better way of going about this, or some way of not failing? Perhaps doing it incrementally rather than trying to dump 100MB at once? 

-- UPDATE --

I tried incrementally, in 1000-row chunks, and although it got through a couple such chunks, the worker still timed out. 

So my current questions are: 
 - if I were to use celery what would that look like?
 - would alembic's op.bulk_insert do this better?

-- UPDATE 2 --

Tried using Flask-Script, which didn't work either. 

from flask.ext.sqlalchemy import SQLAlchemy
from flask.ext.script import Manager
from app import app, db
from models import *
manager = Manager(app)

@manager.command
def populateDB():
    (same function as above)


The worked still timed out. Might this have anything to do with Docker? I'm using this project as my base.

-- UPDATE 3 --

Strangely, I got this to work locally for one file by db.session.add(), db.session.commit() for each example. But that seems to be the slowest way of going about it, per the documentation. That solution also doesn't work remotely, on digitalocean. I think this might have to do with the memory-pressure. I get through about 1,750,000 rows before it crashes.

-- UPDATE 4 -- 

After the machine dies, I run dmesg and the output reveals:

[ 2323.138921] Out of memory: Kill process 6578 (python) score 843 or sacrifice child


So this is a memory issue after all. But I believe the database isn't in memory, it's on disk, so what is taking up all the memory? What part of this eats through all the gigabytes of memory, or does docker limit it to less than that? (note, instead of the function above, I'm putting things in the database incrementally, in 10k row chunks. 

-- UPDATE 5 --

I moved to a ludicrously big VM and the error went away. But now it still fails when I go to query the database with 1000 keys at once. All it says is ""worker killed"". The logs don't seem to have any relevant information, making this debugging difficult and frustrating. 

I looked at docker stats &lt;container&gt; and it seems the memory pressure is at 10%, which should be low enough. 

What don't I understand about docker? Why is it arbitrarily killing my containers?

-- UPDATE 6 --

I think the scope of this question has moved too much. It seems like there may have been two separate issues: not enough memory to insert everything into the database previously, and now the queries are timing out. WORKER TIMEOUT I think is actually a message from gunicorn. I'm going to open a new question about gunicorn timeouts and sqlalchemy queries. 

If someone finds similar issues in the future, the way I got through not being able to add everything is just increasing the size of the VM. Seems like it should be able to run on a smaller VM, but I guess you do what you can. 
",0,1149,"In order to do this, a better approach would be to use an asynchronous task queue alongside flask since you want to your application to be free to respond to client requests.

as mentioned in my above comment, celery could be an option. (http://www.celeryproject.org/)
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/34708435,Sqlalchemy Query Succeeds in Sublime Text IDE but fails when executed from the command line,"Hey everyone. I'm building an Albums DB application for a college project using sqlalchemy and pyqt (as both are libraries I'd like to gain a little experience with), all written in Python 3.
My IDE of choice is Sublime 3 and my database server is Mariadb running on an Ubuntu 14.04 64 bit variant (Elementary OS).

The problem I'm having is quiet strange and I don't know where to go from here (I actually thought I was nearly finished, duh!).

If I execute the application from inside Sublime (f7) then the program functions the way I would expect with all queries returning the desired values (hence why I thought I was nearly done).
However, If I make the script executable and execute it from the command line or from inside another IDE (Geany in this case) the thing falls apart but only when trying to make certain queries based on filtering arguments provided by user input. Queries elsewhere in the program succeed e.g. Songs.query.all() will return all songs in the DB.

The code below is a snippet of where the error occurs in the 'Read' section of the application. 'table' refers to a Table object (Songs, Albums or Artists) and the filter column &amp; clause refer to string arguments entered by the user.

    elif function == 'Read':

        # Debug Print - Display Details
        print('Table is ' + str(table))
        print('Clause is ' + clause)
        # Carry out the Query - Should only return 1 match - Fails Here
        r = table.query.filter(
            getattr(table, column.lower()) == clause).one()

        if self.db == 'Songs':

            # Query Corresponding Album &amp; Artist
            al = Albums.query.get(int(r.album_id))
            ar = Artists.query.get(int(r.artist_id))

            headers = 'Name;Artist;Album;Track Number;Genre'
            data = [r.name, ar.name, al.name,
                    r.track_num, r.genre]


My Models are as follows:

    class Artists(Base):
        __tablename__ = 'artists'

        artist_id = Column(Integer, autoincrement=True, primary_key=True)
        name = Column(String(30))

        albums = relationship(
            'Albums', backref='artists', cascade='all, delete-orphan')
        songs = relationship(
             'Songs', backref='artists', cascade='all, delete-orphan')

    class Albums(Base):
        __tablename__ = 'albums'

        album_id = Column(Integer, autoincrement=True, primary_key=True)
        name = Column(String(30), nullable=False)
        artist_id = Column(Integer, ForeignKey('artists.artist_id'))
        created_on = Column(
            DateTime(timezone=True), default=datetime.now, onupdate=datetime.now)

        songs = relationship(
            ""Songs"", backref=""albums"", cascade=""all, delete-orphan"")

    class Songs(Base):
        __tablename__ = 'songs'

        song_id = Column(Integer, autoincrement=True, primary_key=True)
        name = Column(String(50))
        track_num = Column(Integer)
        genre = Column(String(30))
        created_on = Column(
            DateTime(timezone=True), default=datetime.now, onupdate=datetime.now)

        album_id = Column(Integer, ForeignKey('albums.album_id'))
        artist_id = Column(Integer, ForeignKey('artists.artist_id'))


The Error when trying to match a song that is present in the DB is :


  sqlalchemy.exc.ProgrammingError: (_mysql_exceptions.ProgrammingError) (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '%s' at line 3"") [SQL: 'SELECT songs.song_id AS songs_song_id, songs.name AS songs_name, songs.track_num AS songs_track_num, songs.genre AS songs_genre, songs.created_on AS songs_created_on, songs.album_id AS songs_album_id, songs.artist_id AS songs_artist_id \nFROM songs \nWHERE songs.name = %s'] [parameters: ('WHAT WENT DOWN',)]




Any help on this issue would be wonderful, and if there is any additional information needed I'll be more than happy to provide. Thanks in advance
",0,225,"The error is complaining about the syntax of %s as a bind parameter. If I'm not mistaken, MySQL syntax uses ? for positional and :name for keyword bind params. %s is what the psycopg2 library uses because it does its own statement preparation. Your engine configuration is not shown here.

As for the difference between Sublime and running from console, look for (a) any potential difference in the Python interpreter being used and (b) any potential difference in imports due to PYTHONPATH issues.
",,
SQLAlchemy strange issue,https://stackoverflow.com/questions/9814632,running a bottle app from mod_wsgi handle results in maximum recursion depth exceeded while calling a Python object,,0,1907,,,
NLTK strange behavior,https://stackoverflow.com/questions/33157847,Lemmatizing words after POS tagging produces unexpected results,"I am using python3.5 with the nltk pos_tag function and the WordNetLemmatizer. My goal is to flatten words in our database to classify text. I am trying to test using the lemmatizer and I encounter strange behavior when using the POS tagger on identical tokens. In the example below, I have a list of three strings and when running them in the POS tagger every other element is returned as a noun(NN) and the rest are return as verbs (VBG). 

This affects the lemmatization. The out put looks like this:  

pos Of token: v
lemmatized token: skydive
pos Of token: n
lemmatized token: skydiving
pos Of token: v
lemmatized token: skydive


If I add more elements to the list of identical strings this same pattern continues. The code in full I am using is this:

tokens = ['skydiving', 'skydiving', 'skydiving']
lmtzr=WordNetLemmatizer()

def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return 'a'
    elif treebank_tag.startswith('V'):
        return 'v'
    elif treebank_tag.startswith('N'):
        return 'n'
    elif treebank_tag.startswith('R'):
        return 'r'
    elif treebank_tag.startswith('S'):
        return ''
    else:
        return ''

numTokens = (len(tokens))
for i in range(0,numTokens):
    tokens[i]=tokens[i].replace("" "","""")

noSpaceTokens = pos_tag(tokens)

for token in noSpaceTokens:
    tokenStr = str(token[1])
    noWhiteSpace = token[0].replace("" "", """")
    preLemmed = get_wordnet_pos(tokenStr)
    print(""pos Of token: "" + preLemmed)
    lemmed = lmtzr.lemmatize(noWhiteSpace,preLemmed)
    print(""lemmatized token: "" + lemmed)

",2,1531,"In short:

When POS tagging you need a context sentence not a list of ungrammatical tokens. 


E.g. to lemmatize a grammatical sentence, you can take a look at https://github.com/alvations/pywsd/blob/master/pywsd/utils.py#L100


When lemmatizing out of context sentence, the only way to get the right lemma is to manually specify the pos tags. 


E.g. from https://github.com/alvations/pywsd/blob/master/pywsd/utils.py#L66 , we have to specify the pos parameter for the lemmatize function. 
Otherwise it would be always assuming the n POS, see also WordNetLemmatizer not returning the right lemma unless POS is explicit - Python NLTK




In long:

POS tagger usually works on the full sentence and not individual words. When you try to tag a single word out of context, what you get is the most frequent tag.

To verify that when tagging a single word (i.e. a sentence with only 1 word), it always gives the same tag:

&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; ptb2wn_pos = {'J':'a', 'V':'v', 'N':'n', 'R':'r'}
&gt;&gt;&gt; sent = ['skydive']
&gt;&gt;&gt; most_frequent_tag = pos_tag(sent)[0][1]
&gt;&gt;&gt; most_frequent_tag
'JJ'
&gt;&gt;&gt; most_frequent_tag = ptb2wn_pos[most_frequent_tag[0]]
&gt;&gt;&gt; most_frequent_tag
'a'
&gt;&gt;&gt; for _ in range(1000): assert ptb2wn_pos[pos_tag(sent)[0][1][0]] == most_frequent_tag;
... 
&gt;&gt;&gt;


Now, since the tag is always 'a' by default if the sentence only have 1 word, then the WordNetLemmatizer will always return skydive:

&gt;&gt;&gt; wnl = WordNetLemmatizer()
&gt;&gt;&gt; wnl.lemmatize(sent[0], pos=most_frequent_tag)
'skydive'


Let's to to see the lemma of a word in context of a sentence:

&gt;&gt;&gt; sent2 = 'They skydrive from the tower yesterday'
&gt;&gt;&gt; pos_tag(sent2.split())
[('They', 'PRP'), ('skydrive', 'VBP'), ('from', 'IN'), ('the', 'DT'), ('tower', 'NN'), ('yesterday', 'NN')]
&gt;&gt;&gt; pos_tag(sent2.split())[1]
('skydrive', 'VBP')
&gt;&gt;&gt; pos_tag(sent2.split())[1][1]
'VBP'
&gt;&gt;&gt; ptb2wn_pos[pos_tag(sent2.split())[1][1][0]]
'v'


So the context of the input list of tokens matters when you do pos_tag.

In your example, you had a list ['skydiving', 'skydiving', 'skydiving'] meaning the sentence that you are pos-tagging is an ungrammatical sentence:


  skydiving skydiving skydiving


And the pos_tag function thinks is a normal sentence hence giving the tags:

&gt;&gt;&gt; sent3 = 'skydiving skydiving skydiving'.split()
&gt;&gt;&gt; pos_tag(sent3)
[('skydiving', 'VBG'), ('skydiving', 'NN'), ('skydiving', 'VBG')]


In which case the first is a verb, the second word a noun and the third word a verb, which will return the following lemma (which you do not desire):

&gt;&gt;&gt; wnl.lemmatize('skydiving', 'v')
'skydive'
&gt;&gt;&gt; wnl.lemmatize('skydiving', 'n')
'skydiving'
&gt;&gt;&gt; wnl.lemmatize('skydiving', 'v')
'skydive'


So if we have a valid grammatical sentence in your list of token, the output might look very different

&gt;&gt;&gt; sent3 = 'The skydiving sport is an exercise that promotes diving from the sky , ergo when you are skydiving , you feel like you are descending to earth .'
&gt;&gt;&gt; pos_tag(sent3.split())
[('The', 'DT'), ('skydiving', 'NN'), ('sport', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('exercise', 'NN'), ('that', 'IN'), ('promotes', 'NNS'), ('diving', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('sky', 'NN'), (',', ','), ('ergo', 'RB'), ('when', 'WRB'), ('you', 'PRP'), ('are', 'VBP'), ('skydiving', 'VBG'), (',', ','), ('you', 'PRP'), ('feel', 'VBP'), ('like', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('descending', 'VBG'), ('to', 'TO'), ('earth', 'JJ'), ('.', '.')]

",,
NLTK strange behavior,https://stackoverflow.com/questions/70286059,NLTK corpora download is hanging when run in AWS Lambda Python function,"I'm trying to download NLTK data onto the file storage of a Lambda function like so:
nltk.data.path.append(""/tmp"")
nltk.download(""popular"", download_dir=""/tmp"")

The Lambda function keeps timing out. When I check the Cloudwatch logs, I see no logs related to the download of different corpora files (e.g. Downloading package cmudict to /tmp...; instead the code seems to reach up to nltk.download(), then hang forever.
Has anyone seen this strange behavior?
",0,140,"Got it: My Lambda function was running in a VPC. I had to add an endpoint to enable the VPC to access S3.
",,
NLTK strange result,https://stackoverflow.com/questions/8842817,Selecting the most fluent text from a set of possibilities via grammar checking (Python),"Some background

I am a literature student at New College of Florida, currently working on an overly ambitious creative project. The project is geared towards the algorithmic generation of poetry. It's written in Python. My Python knowledge and Natural Language Processing knowledge come only from teaching myself things through the internet. I've been working with this stuff for about a year, so I'm not helpless, but at various points I've had trouble moving forward in this project. Currently, I am entering the final phases of development, and have hit a little roadblock.

I need to implement some form of grammatical normalization, so that the output doesn't come out as un- conjugated/inflected caveman-speak. About a month ago some friendly folks on SO gave me some advice on how I might solve this issue by using an ngram language modeller, basically -- but I'm looking for yet other solutions, as it seems that NLTK's NgramModeler is not fit for my needs. (The possibilities of POS tagging were also mentioned, but my text may be too fragmentary and strange for an implementation of such to come easy, given my amateur-ness.)

Perhaps I need something like AtD, but hopefully less complex

I think need something that works like After the Deadline or Queequeg, but neither of these seem exactly right. Queequeg is probably not a good fit -- it was written in 2003 for Unix and I can't get it working on Windows for the life of me (have tried everything). But I like that all it checks for is proper verb conjugation and number agreement.

On the other hand, AtD is much more rigorous, offering more capabilities than I need. But I can't seem to get the python bindings for it working. (I get 502 errors from the AtD server, which I'm sure are easy to fix, but my application is going to be online, and I'd rather avoid depending on another server. I can't afford to run an AtD server myself, because the number of ""services"" my application is going to require of my web host is already threatening to cause problems in getting this application hosted cheaply.)

Things I'd like to avoid

Building Ngram language models myself doesn't seem right for the task. my application throws a lot of unknown vocabulary, skewing all the results. (Unless I use a corpus that's so large that it runs way too slow for my application -- the application needs to be pretty snappy.)

Strictly checking grammar is neither right for the task. the grammar doesn't need to be perfect, and the sentences don't have to be any more sensible than the kind of English-like jibberish that you can generate using ngrams. Even if it's jibberish, I just need to enforce verb conjugation, number agreement, and do things like remove extra articles.

In fact, I don't even need any kind of suggestions for corrections. I think all I need is for something to tally up how many errors seem to occur in each sentence in a group of possible sentences, so I can sort by their score and pick the one with the least grammatical issues.

A simple solution? Scoring fluency by detecting obvious errors

If a script exists that takes care of all this, I'd be overjoyed (I haven't found one yet). I can write code for what I can't find, of course; I'm looking for advice on how to optimize my approach.

Let's say we have a tiny bit of text already laid out:

existing_text = ""The old river""

Now let's say my script needs to figure out which inflection of the verb ""to bear"" could come next. I'm open to suggestions about this routine. But I need help mostly with step #2, rating fluency by tallying grammatical errors:


Use the Verb Conjugation methods in NodeBox Linguistics to come up with all conjugations of this verb; ['bear', 'bears', 'bearing', 'bore', 'borne'].
Iterate over the possibilities, (shallowly) checking the grammar of the string resulting from existing_text + "" "" + possibility (""The old river bear"", ""The old river bears"", etc). Tally the error count for each construction. In this case the only construction to raise an error, seemingly, would be ""The old river bear"".
Wrapping up should be easy... Of the possibilities with the lowest error count, select randomly.

",30,3461,"Very cool project, first of all. 

I found a java grammar checker. I've never used it but the docs claim it can run as a server. Both java and listening to a port should be supported basically anywhere. 

I'm just getting into NLP with a CS background so I wouldn't mind going into more detail to help you integrate whatever you decide on using. Feel free to ask for more detail.
","The pylinkgrammar link provided above is a bit out of date.  It points to version 0.1.9, and the code samples for that version no longer work.  If you go down this path, be sure to use the latest version which can be found at:

https://pypi.python.org/pypi/pylinkgrammar
","Another approach would be to use what is called an overgenerate and rank approach.  In the first step you have your poetry generator generate multiple candidate generations.  Then using a service like Amazon's Mechanical Turk to collect human judgments of fluency.  I would actually suggest collecting simultaneous judgments for a number of sentences generated from the same seed conditions.  Lastly, you extract features from the generated sentences (presumably using some form of syntactic parser) to train a model to rate or classify question quality.  You could even thrown in the heuristics listed above.

Michael Heilman uses this approach for question generation.  For more details, read these papers:
Good Question! Statistical Ranking for Question Generation and
Rating Computer-Generated Questions with Mechanical Turk.
"
NLTK strange result,https://stackoverflow.com/questions/40480839,NLTK relation extraction returns nothing,"I am recently working on using nltk to extract relation from text. so i build a sample text:"" Tom is the cofounder of Microsoft."" and using following program to test and return nothing. I cannot figure out why.

I'm using NLTK version: 3.2.1, python version: 3.5.2.

Here is my code:

import re
import nltk
from nltk.sem.relextract import extract_rels, rtuple
from nltk.tokenize import sent_tokenize, word_tokenize


def test():
    with open('sample.txt', 'r') as f:
        sample = f.read()   # ""Tom is the cofounder of Microsoft""

    sentences = sent_tokenize(sample)
    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]
    tagged_sentences = [nltk.tag.pos_tag(sentence) for sentence in tokenized_sentences]

    OF = re.compile(r'.*\bof\b.*')

    for i, sent in enumerate(tagged_sentences):
        sent = nltk.chunk.ne_chunk(sent) # ne_chunk method expects one tagged sentence
        rels = extract_rels('PER', 'GPE', sent, corpus='ace', pattern=OF, window=10) 
        for rel in rels:
            print('{0:&lt;5}{1}'.format(i, rtuple(rel)))

if __name__ == '__main__':
    test()




1.After some debug, if found that when i changed the input as


  ""Gates was born in Seattle, Washington on October 28, 1955. ""


the nltk.chunk.ne_chunk() output is:


  (S
    (PERSON Gates/NNS)
    was/VBD
    born/VBN
    in/IN
    (GPE Seattle/NNP)
    ,/,
    (GPE Washington/NNP)
    on/IN
    October/NNP
    28/CD
    ,/,
    1955/CD
    ./.)


The test() returns:


  [PER: 'Gates/NNS'] 'was/VBD born/VBN in/IN' [GPE: 'Seattle/NNP']


2. After i changed the input as:


  ""Gates was born in Seattle on October 28, 1955. ""


The test() retuns nothing.

3. I digged into nltk/sem/relextract.py and find this strange

output is caused by function:
semi_rel2reldict(pairs, window=5, trace=False), which returns result only when len(pairs) &gt; 2, and that's why when one sentence with less than three NEs will return None.

Is this a bug or i used NLTK in wrong way?
",6,3435,"Firstly, to chunk NEs with ne_chunk, the idiom would look something like this 

&gt;&gt;&gt; from nltk import ne_chunk, pos_tag, word_tokenize
&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; chunked
Tree('S', [Tree('PERSON', [('Tom', 'NNP')]), ('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN'), Tree('ORGANIZATION', [('Microsoft', 'NNP')])])


(see also https://stackoverflow.com/a/31838373/610569)

Next let's look at the extract_rels function.

def extract_rels(subjclass, objclass, doc, corpus='ace', pattern=None, window=10):
    """"""
    Filter the output of ``semi_rel2reldict`` according to specified NE classes and a filler pattern.
    The parameters ``subjclass`` and ``objclass`` can be used to restrict the
    Named Entities to particular types (any of 'LOCATION', 'ORGANIZATION',
    'PERSON', 'DURATION', 'DATE', 'CARDINAL', 'PERCENT', 'MONEY', 'MEASURE').
    """"""


When you evoke this function:

extract_rels('PER', 'GPE', sent, corpus='ace', pattern=OF, window=10)


It performs 4 processes sequentially.

1. It checks whether your subjclass and objclassare valid

i.e. https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L202 :

if subjclass and subjclass not in NE_CLASSES[corpus]:
    if _expand(subjclass) in NE_CLASSES[corpus]:
        subjclass = _expand(subjclass)
    else:
        raise ValueError(""your value for the subject type has not been recognized: %s"" % subjclass)
if objclass and objclass not in NE_CLASSES[corpus]:
    if _expand(objclass) in NE_CLASSES[corpus]:
        objclass = _expand(objclass)
    else:
        raise ValueError(""your value for the object type has not been recognized: %s"" % objclass)


2. It extracts ""pairs"" from your NE tagged inputs:

if corpus == 'ace' or corpus == 'conll2002':
    pairs = tree2semi_rel(doc)
elif corpus == 'ieer':
    pairs = tree2semi_rel(doc.text) + tree2semi_rel(doc.headline)
else:
    raise ValueError(""corpus type not recognized"")


Now let's see given your input sentence Tom is the cofounder of Microsoft, what does tree2semi_rel() returns:

&gt;&gt;&gt; from nltk.sem.relextract import tree2semi_rel, semi_rel2reldict
&gt;&gt;&gt; from nltk import word_tokenize, pos_tag, ne_chunk
&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]


So it returns a list of 2 lists, the first inner list consist of a blank list and the Tree that contains the ""PERSON"" tag.

[[], Tree('PERSON', [('Tom', 'NNP')])] 


The second list consist of the phrase is the cofounder of and the Tree that contains ""ORGANIZATION"".

Let's move on.

3. extract_rel then tries to change the pairs to some sort of relation dictionary

reldicts = semi_rel2reldict(pairs)


If we look what the semi_rel2reldict function returns with your example sentence, we see that this is where the empty list gets returns:

&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[]


So let's look into the code of semi_rel2reldict https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L144:

def semi_rel2reldict(pairs, window=5, trace=False):
    """"""
    Converts the pairs generated by ``tree2semi_rel`` into a 'reldict': a dictionary which
    stores information about the subject and object NEs plus the filler between them.
    Additionally, a left and right context of length =&lt; window are captured (within
    a given input sentence).
    :param pairs: a pair of list(str) and ``Tree``, as generated by
    :param window: a threshold for the number of items to include in the left and right context
    :type window: int
    :return: 'relation' dictionaries whose keys are 'lcon', 'subjclass', 'subjtext', 'subjsym', 'filler', objclass', objtext', 'objsym' and 'rcon'
    :rtype: list(defaultdict)
    """"""
    result = []
    while len(pairs) &gt; 2:
        reldict = defaultdict(str)
        reldict['lcon'] = _join(pairs[0][0][-window:])
        reldict['subjclass'] = pairs[0][1].label()
        reldict['subjtext'] = _join(pairs[0][1].leaves())
        reldict['subjsym'] = list2sym(pairs[0][1].leaves())
        reldict['filler'] = _join(pairs[1][0])
        reldict['untagged_filler'] = _join(pairs[1][0], untag=True)
        reldict['objclass'] = pairs[1][1].label()
        reldict['objtext'] = _join(pairs[1][1].leaves())
        reldict['objsym'] = list2sym(pairs[1][1].leaves())
        reldict['rcon'] = _join(pairs[2][0][:window])
        if trace:
            print(""(%s(%s, %s)"" % (reldict['untagged_filler'], reldict['subjclass'], reldict['objclass']))
        result.append(reldict)
        pairs = pairs[1:]
    return result


The first thing that semi_rel2reldict() does is to check where there are more than 2 elements the output from tree2semi_rel(), which your example sentence doesn't:

&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; len(tree2semi_rel(chunked))
2
&gt;&gt;&gt; len(tree2semi_rel(chunked)) &gt; 2
False


Ah ha, that's why the extract_rel is returning nothing.

Now comes the question of how to make extract_rel() return something even with 2 elements from tree2semi_rel()? Is that even possible?

Let's try a different sentence:

&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft and now he is the founder of Marcohard""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; chunked
Tree('S', [Tree('PERSON', [('Tom', 'NNP')]), ('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN'), Tree('ORGANIZATION', [('Microsoft', 'NNP')]), ('and', 'CC'), ('now', 'RB'), ('he', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('founder', 'NN'), ('of', 'IN'), Tree('PERSON', [('Marcohard', 'NNP')])])
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])], [[('and', 'CC'), ('now', 'RB'), ('he', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('founder', 'NN'), ('of', 'IN')], Tree('PERSON', [('Marcohard', 'NNP')])]]
&gt;&gt;&gt; len(tree2semi_rel(chunked)) &gt; 2
True
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': 'and/CC now/RB he/PRP is/VBZ the/DT', 'subjtext': 'Tom/NNP'})]


But that only confirms that extract_rel can't extract when tree2semi_rel returns pairs of &lt; 2. What happens if we remove that condition of while len(pairs) &gt; 2?

Why can't we do while len(pairs) &gt; 1?

If we look closer into the code, we see the last line of populating the reldict, https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L169:

reldict['rcon'] = _join(pairs[2][0][:window])


It tries to access a 3rd element of the pairs and if the length of the pairs is 2, you'll get an IndexError.

So what happens if we remove that rcon key and simply change it to while len(pairs) &gt;= 2?

To do that we have to override the semi_rel2redict() function:

&gt;&gt;&gt; from nltk.sem.relextract import _join, list2sym
&gt;&gt;&gt; from collections import defaultdict
&gt;&gt;&gt; def semi_rel2reldict(pairs, window=5, trace=False):
...     """"""
...     Converts the pairs generated by ``tree2semi_rel`` into a 'reldict': a dictionary which
...     stores information about the subject and object NEs plus the filler between them.
...     Additionally, a left and right context of length =&lt; window are captured (within
...     a given input sentence).
...     :param pairs: a pair of list(str) and ``Tree``, as generated by
...     :param window: a threshold for the number of items to include in the left and right context
...     :type window: int
...     :return: 'relation' dictionaries whose keys are 'lcon', 'subjclass', 'subjtext', 'subjsym', 'filler', objclass', objtext', 'objsym' and 'rcon'
...     :rtype: list(defaultdict)
...     """"""
...     result = []
...     while len(pairs) &gt;= 2:
...         reldict = defaultdict(str)
...         reldict['lcon'] = _join(pairs[0][0][-window:])
...         reldict['subjclass'] = pairs[0][1].label()
...         reldict['subjtext'] = _join(pairs[0][1].leaves())
...         reldict['subjsym'] = list2sym(pairs[0][1].leaves())
...         reldict['filler'] = _join(pairs[1][0])
...         reldict['untagged_filler'] = _join(pairs[1][0], untag=True)
...         reldict['objclass'] = pairs[1][1].label()
...         reldict['objtext'] = _join(pairs[1][1].leaves())
...         reldict['objsym'] = list2sym(pairs[1][1].leaves())
...         reldict['rcon'] = []
...         if trace:
...             print(""(%s(%s, %s)"" % (reldict['untagged_filler'], reldict['subjclass'], reldict['objclass']))
...         result.append(reldict)
...         pairs = pairs[1:]
...     return result
... 
&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': [], 'subjtext': 'Tom/NNP'})]


Ah! It works but there's still a 4th step in extract_rels().

4. It performs a filter of the reldict given the regex you have provided to the pattern parameter, https://github.com/nltk/nltk/blob/develop/nltk/sem/relextract.py#L222:

relfilter = lambda x: (x['subjclass'] == subjclass and
                       len(x['filler'].split()) &lt;= window and
                       pattern.match(x['filler']) and
                       x['objclass'] == objclass)


Now let's try it with the hacked version of semi_rel2reldict:

&gt;&gt;&gt; text = ""Tom is the cofounder of Microsoft""
&gt;&gt;&gt; chunked = ne_chunk(pos_tag(word_tokenize(text)))
&gt;&gt;&gt; tree2semi_rel(chunked)
[[[], Tree('PERSON', [('Tom', 'NNP')])], [[('is', 'VBZ'), ('the', 'DT'), ('cofounder', 'NN'), ('of', 'IN')], Tree('ORGANIZATION', [('Microsoft', 'NNP')])]]
&gt;&gt;&gt; semi_rel2reldict(tree2semi_rel(chunked))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': [], 'subjtext': 'Tom/NNP'})]
&gt;&gt;&gt; 
&gt;&gt;&gt; pattern = re.compile(r'.*\bof\b.*')
&gt;&gt;&gt; reldicts = semi_rel2reldict(tree2semi_rel(chunked))
&gt;&gt;&gt; relfilter = lambda x: (x['subjclass'] == subjclass and
...                            len(x['filler'].split()) &lt;= window and
...                            pattern.match(x['filler']) and
...                            x['objclass'] == objclass)
&gt;&gt;&gt; relfilter
&lt;function &lt;lambda&gt; at 0x112e591b8&gt;
&gt;&gt;&gt; subjclass = 'PERSON'
&gt;&gt;&gt; objclass = 'ORGANIZATION'
&gt;&gt;&gt; window = 5
&gt;&gt;&gt; list(filter(relfilter, reldicts))
[defaultdict(&lt;type 'str'&gt;, {'lcon': '', 'untagged_filler': 'is the cofounder of', 'filler': 'is/VBZ the/DT cofounder/NN of/IN', 'objsym': 'microsoft', 'objclass': 'ORGANIZATION', 'objtext': 'Microsoft/NNP', 'subjsym': 'tom', 'subjclass': 'PERSON', 'rcon': [], 'subjtext': 'Tom/NNP'})]


It works! Now let's see it in tuple form:

&gt;&gt;&gt; from nltk.sem.relextract import rtuple
&gt;&gt;&gt; rels = list(filter(relfilter, reldicts))
&gt;&gt;&gt; for rel in rels:
...     print rtuple(rel)
... 
[PER: 'Tom/NNP'] 'is/VBZ the/DT cofounder/NN of/IN' [ORG: 'Microsoft/NNP']

",,
NLTK strange result,https://stackoverflow.com/questions/62735456,Understanding and using Coreference resolution Stanford NLP tool (in Python 3.7),"I am trying to understand the Coreference NLP Stanford tools.
This is my code and it is working:
import os
os.environ[""CORENLP_HOME""] = ""/home/daniel/StanfordCoreNLP/stanford-corenlp-4.0.0""

from stanza.server import CoreNLPClient

text = 'When he came from Brazil, Daniel was fortied with letters from Conan but otherwise did not know a soul except Herbert. Yet this giant man from the Northeast, who had never worn an overcoat or experienced a change of seasons, did not seem surprised by his past.'

with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],
               properties={'annotators': 'coref', 'coref.algorithm' : 'neural'},timeout=30000, memory='16G') as client:

    ann = client.annotate(text)

chains = ann.corefChain
chain_dict=dict()
for index_chain,chain in enumerate(chains):
    chain_dict[index_chain]={}
    chain_dict[index_chain]['ref']=''
    chain_dict[index_chain]['mentions']=[{'mentionID':mention.mentionID,
                                          'mentionType':mention.mentionType,
                                          'number':mention.number,
                                          'gender':mention.gender,
                                          'animacy':mention.animacy,
                                          'beginIndex':mention.beginIndex,
                                          'endIndex':mention.endIndex,
                                          'headIndex':mention.headIndex,
                                          'sentenceIndex':mention.sentenceIndex,
                                          'position':mention.position,
                                          'ref':'',
                                          } for mention in chain.mention ]


for k,v in chain_dict.items():
    print('key',k)
    mentions=v['mentions']
    for mention in mentions:
        words_list = ann.sentence[mention['sentenceIndex']].token[mention['beginIndex']:mention['endIndex']]
        mention['ref']=' '.join(t.word for t in words_list)
        print(mention['ref'])
    

I tried three algorithms:

statistical (as in the code above). Results:


he
this giant man from the Northeast , who had never worn an overcoat or experienced a change of seasons
Daniel
his



neural


this giant man from the Northeast , who had never worn an overcoat or experienced a change of seasons ,
his



deterministic (I got the error below)
 &gt; Starting server with command: java -Xmx16G -cp
 &gt; /home/daniel/StanfordCoreNLP/stanford-corenlp-4.0.0/*
 &gt; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout
 &gt; 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties
 &gt; corenlp_server-9fedd1e9dfb14c9e.props -preload
 &gt; tokenize,ssplit,pos,lemma,ner,parse,depparse,coref Traceback (most
 &gt; recent call last):
 &gt; 
 &gt;   File ""&lt;ipython-input-58-0f665f07fd4d&gt;"", line 1, in &lt;module&gt;
 &gt;     runfile('/home/daniel/Documentos/Working Papers/Leader traits/Code/20200704 - Modeling
 &gt; Organizing/understanding_coreference.py',
 &gt; wdir='/home/daniel/Documentos/Working Papers/Leader
 &gt; traits/Code/20200704 - Modeling Organizing')
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"",
 &gt; line 827, in runfile
 &gt;     execfile(filename, namespace)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"",
 &gt; line 110, in execfile
 &gt;     exec(compile(f.read(), filename, 'exec'), namespace)
 &gt; 
 &gt;   File ""/home/daniel/Documentos/Working Papers/Leader
 &gt; traits/Code/20200704 - Modeling
 &gt; Organizing/understanding_coreference.py"", line 21, in &lt;module&gt;
 &gt;     ann = client.annotate(text)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/stanza/server/client.py"",
 &gt; line 470, in annotate
 &gt;     r = self._request(text.encode('utf-8'), request_properties, **kwargs)
 &gt; 
 &gt;   File
 &gt; ""/home/daniel/anaconda3/lib/python3.7/site-packages/stanza/server/client.py"",
 &gt; line 404, in _request
 &gt;     raise AnnotationException(r.text)
 &gt; 
 &gt; AnnotationException: java.lang.RuntimeException:
 &gt; java.lang.IllegalArgumentException: No enum constant
 &gt; edu.stanford.nlp.coref.CorefProperties.CorefAlgorithmType.DETERMINISTIC



Questions:

Why am I getting this error with the deterministic?

Any piece of code using the NLP Stanford in Python seems to be much slower than the codes related with Spacy or NLTK. I know that there is no coreference in these other libraries. But for instance when I use import nltk.parse.stanford import StanfordDependencyParser for dependence parse it is much faster then this StanfordNLP library. Is there any way to acelerate this CoreNLPClient in Python?

I will use this library to work with long texts. Is it better to work with smaller pieces with the entire text? Long texts can cause wrong results for coreference resolution (I have found very strange results for this coreference library when I am using long texts)? Is there an optimal size?

Results:


The results from the statistical algorithm seems to be better. I expected that the best result would come from the neural algorithm. Do you agree with me? There are 4 valid mention in the statistical algorithm while only 2 when I am using the neural algorithm.
Am I missing something?
",2,1442,"
You may find the list of supported algorithms in Java documentation: link

You might want to start the server and then just use it, something like
# Here's the slowest partmodels are being loaded
client = CoreNLPClient(...)

ann = client.annotate(text)

...

client.stop()



But I cannot give you any clue regarding 3 and 4.
",,
NLTK strange result,https://stackoverflow.com/questions/48030920,"Tweet Feels: Always returns the same Sentiment Score, regardless tags","I am trying to use this library to generate sentiment score for cryptocurrencies:

https://github.com/uclatommy/tweetfeels/blob/master/README.md

When I use the code from the example trump, it returns a sentiment score of -0.00082536637608123106.

I have changed the tags to the following:

btc_feels = TweetFeels(login, tracking=['bitcoin'])
btc_feels.start(20)
btc_feels.sentiment.value


and it still gives me the same value.

I did notice something strange when I installed the library.

from the instructions:


  If for some reason pip did not install the vader lexicon:
  
  
    python3 -m nltk.downloader vader_lexicon
  


When I ran this, I got: 


  /anaconda/lib/python3.6/runpy.py:125: RuntimeWarning:
  'nltk.downloader' found in sys.modules after import of package 'nltk',
  but prior to execution of 'nltk.downloader'; this may result in
  unpredictable behaviour   warn(RuntimeWarning(msg))


Could this be why it appears not to be working?
",2,267,"By default, tweetfeels creates a database in your current directory. The next time you start the program, it will continue using the same database, and pick up where it left off. I don't know what tweetfeels does to handle you changing the keyword on it, but this behaviour of tweetfeels could be a problem. The solution would be to use a different database for different keywords, and then pass in the location of your database to the TweetFeels constructor.

I don't know that much about Tweetfeels, it just sounded interesting, so I've downloaded the project, and I have a working script that will perform the sentiment analysis on any keyword I give it. I can add a copy of the script here, if you're still having problems getting TweetFeels to work.



Edit: here the script I am using

I am currently having the following problems with the script.

1) I was getting some error that was different from the one you'd got, but I was able to fix the issue by replacing the tweetfeels library from pip with the latest code in their Github repository.

2) If a sentiment value does not get reported, sometimes tweetfeels fails to come to a complete stop, without forcefully sending a ctrl+c keyboard interrupt.

import os, sys, time
from threading import Thread
from pathlib import Path

from tweetfeels import TweetFeels

consumer_key = 'em...'
consumer_secret = 'aF...'
access_token = '25...'
access_token_secret = 'd3...'
login = [consumer_key, consumer_secret, access_token, access_token_secret]

try:
    kw = sys.argv[1]
except IndexError:
    kw = ""iota""

try:
    secs = int(sys.argv[2])
except IndexError:
    secs = 15

for arg in sys.argv:
    if (arg == ""-h"" or arg == ""--help""):
        print(""Gets sentiment from twitter.\n""
              ""Pass in a search term, and how frequently you would like the sentiment recalculated (defaults to 15 seconds).\n""
              ""The keyword can be a comma seperated list of keywords to look at."")
        sys.exit(0)

db = Path(f""~/tweetfeels/{kw}.sqlite"").expanduser()
if db.exists():
    print(""existing db detected. Continueing from where the last sentiment stream left off"")
else:
    #ensure the parent folder exists, the db will be created inside of this folder
    Path(f""~/tweetfeels"").expanduser().mkdir(exist_ok=True)

feels = TweetFeels(login, tracking=kw.split("",""), db=str(db))

go_on = True
def print_feels(feels, seconds):
    while go_on:
        if feels.sentiment:
            print(f""{feels.sentiment.volume} tweets analyzed from {feels.sentiment.start} to {feels.sentiment.end}"")
            print(f'[{time.ctime()}] Sentiment Score: {feels.sentiment.value}')
            print(flush=True)
        else:
            print(f""The datastream has not reported a sentiment value."")
            print(f""It takes a little bit for the first tweets to be analyzed (max of {feels._stream.retry_time_cap + seconds} seconds)."")
            print(""If this problem persists, there may not be anyone tweeting about the keyword(s) you used"")
            print(flush=True)
        time.sleep(seconds)


t = Thread(target=print_feels, kwargs={""feels"":feels,""seconds"":secs}, daemon=True)
print(f'Twitter posts containing the keyword(s) ""{kw}"" will be streamed, and a new sentiment value will be recalculated every {secs} seconds')
feels.start()
time.sleep(5)
t.start()

try:
    input(""Push enter at any time to stop the feed...\n\n"")
except (Exception, KeyboardInterrupt) as e:
    feels.stop()
    raise e

feels.stop()
go_on = False
print(f""Stopping feed. It may take up to {feels._stream.retry_time_cap} for the feed to shut down.\n"")
#we're waiting on the feels thread to stop

","No, the same sentiment value that you see printed is not related to the warning you've got when downloading the dataset.

The problem with the same sentiment score is coming from these lines:

for s in sentiments:
    pass
return s


I suspect that this unbound variable s remembers the previous value of the sentiment score.

But, the problem itself is that you are printing out the score right after you execute the start() function which starts a multi-threaded program to constantly update data from twitter - you should not expect the sentiment score to arrive right after you started the update. 

Note that the examples in the README are shown from the Python terminal where they wait after the execution of start() function until the Timer completed. Disconnecting now... message appears.
",
NLTK strange result,https://stackoverflow.com/questions/63508107,"What causes the problem: csv, pandas or nltk?","I have a strange problem resulting in wrong output delivered by NLTK collocations. In short, when I pass pandas object created in python envi (PyCharm or Jupyter) to the function I get correct result. When I save this object to csv and upload it to the pandas object, functions returns single letters and/or numbers instead of full words. Must be sth wrong with csv upload through pandas but I have no idea what is wrong...
here is the code.
Function that is applied:
def counts(x):
    trigram_measures = nltk.collocations.BigramAssocMeasures()
    finder = BigramCollocationFinder.from_documents(x)
    finder.nbest(trigram_measures.pmi, 100)

    s = pd.Series(x)
    ngram_list = [pair for row in s for pair in ngrams(row, 3)]
    c = Counter(ngram_list).most_common(3)

    return pd.DataFrame([(x.name, ) + element for element in c], columns=['group', 'Ngram', 'Frequency'])

Here is the object:
d = {'words' : pd.Series((['coffee', 'maker', 'brewing', 'properly', '2', '420', '420', '420'],
    ['galley', 'work', 'table', 'stuck'],
    ['cloth', 'stuck'],
    ['stuck', 'coffee'])),
    'group' : pd.Series([1, 2, 1, 2])}
df_cleaned = pd.DataFrame(d)

Then I apply function from above + some extra functions:
output = df_cleaned.groupby('group', as_index=False).words.apply(counts).reset_index(drop=True)

Result is correct:

But when pandas object is saved and uploaded result is sth like this:
here is a code for saving and uploading:
df.to_csv('test_file.csv', index=False, sep=',')

df = pd.read_csv('path/test_file.csv',
sep=',', usecols=['group','words']) 

I found quotes in uploaded pandas object therefore I had removed them before applying the fucntion""
df = df.replace({'\'': ''}, regex=True)

output = df_cleaned.groupby('group', as_index=False).words.apply(counts).reset_index(drop=True)

Now it returns wrong results.

Do have any suggestions which way shall I go?
",0,65,"I reproduced what you described in the following steps. I don't see any errors
import pandas as pd
d = {'words' : pd.Series((['coffee', 'maker', 'brewing', 'properly', '2','420', '420', '420'],
    ['galley', 'work', 'table', 'stuck'],
    ['cloth', 'stuck'],
    ['stuck', 'coffee'])),
    'group' : pd.Series([1, 2, 1, 2])}
df_cleaned = pd.DataFrame(d)
df_cleaned

The function you're using is
import nltk
from nltk.util import ngrams
from nltk.collocations import *
from collections import Counter
def counts(x):
    trigram_measures = nltk.collocations.BigramAssocMeasures()
    finder = BigramCollocationFinder.from_documents(x)
    finder.nbest(trigram_measures.pmi, 100)
    s = pd.Series(x)
    ngram_list = [pair for row in s for pair in ngrams(row, 3)]
    c = Counter(ngram_list).most_common(3)
    return pd.DataFrame([(x.name, ) + element for element in c], columns=['group', 'Ngram', 'Frequency'])

You then apply counts to the data
output = df_cleaned.groupby('group', 
            as_index=False).words.apply(counts).reset_index(drop=True)

and save the results to file
output.to_csv('test_file.csv', index=False, sep=',')
df = pd.read_csv('test_file.csv',sep=',')

I don't see any problems
",,
NLTK strange result,https://stackoverflow.com/questions/39603633,nltk semantic word substitution,"I'm trying to find different ways of writing ""events in [city]"" which are semantically similar. I am trying to do this by finding words that are semantically similar to ""events"" so I can substitute them in. 

To find these words I'm using nltk's wordnet corpus, but I'm getting some pretty strange results. For example, using the hyponyms of 'event.n.01', I'm getting ""Miracles in Ottawa"". 

co-hyponyms and hypernyms seem just as bad or worse. I wonder if anyone understands the structure better and can offer a potential solution?

Here's some sample code:

!/usr/bin/python3

import nltk

lemma = 'event.n.01'
synset = nltk.corpus.wordnet.synset(lemma)

print(""%s: %s"" % (synset.name(), synset.definition()))

print(""\nFinding hyponyms..."")
print([s.split('.')[0] for w in synset.hyponyms() for s in w.lemma_names()])

print(""\nFinding hypernym paths..."")
print([s.split('.')[0] for hyprs in synset.hypernym_paths() for hypr in hyprs for s in hypr.lemma_names()])

print(""\nFinding co-hyponyms..."")
for hypers in synset.hypernym_paths():
        for hyper in hypers:
                print(hyper.name())
                for hypos in hyper.hyponyms():
                        print(""\t%s"" % (', '.join(hypos.lemma_names())))

print(synset.similar())

",0,602,"You can take a deep learning approach. Train a word2vec model and get the most similar vectors to the ""event"" vector.

You can test a model here Word2Vec Demo
","The hyponyms of ""event"" are types of ""event"". One of them is ""miracle"", some others are:

&gt;&gt;&gt; [s for w in synset.hyponyms() for s in w.lemma_names][:7]  # is 7 enough? :)
['zap', 'act', 'deed', 'human_action', 'human_activity', 'happening', 'occurrence']


""Event's"" hypernyms are the oposite. Terms that ""event"" is a type of:

&gt;&gt;&gt; synset.hypernyms()
[Synset('psychological_feature.n.01')]


You can see that ""event"" is one of it's hyponyms:

&gt;&gt;&gt; synset.hypernyms()[0].hyponyms()
[Synset('motivation.n.01'), Synset('cognition.n.01'), Synset('event.n.01')]


Those are not really ""similar"" terms (""Psychological features in Ottawa"" may seem like a correct result to a robot, but not to humans).

Perhaps it is better to go at it from a completely different angle, e.g.

&gt;&gt;&gt; text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())
&gt;&gt;&gt; text.similar('event')
time day man order state way case house one place action night point
situation work year act and area audience


Now, take those and sort them e.g. by path_similarity:

&gt;&gt;&gt; words = 'time day man order state way case house one place action night point'\
...         ' situation work year act and area audience'.split()
&gt;&gt;&gt; 
&gt;&gt;&gt; def get_symilarity(synset, word):
...     return max([synset.path_similarity(synset2)
...                for synset2 in nltk.corpus.wordnet.synsets(word)]+[0])
&gt;&gt;&gt; 
&gt;&gt;&gt; sorted(words, key=lambda w: get_symilarity(synset, w), reverse=True)[:5]
['act', 'case', 'action', 'time', 'way']


Is that a good result? I don't know. I guess it could work: ""Acts in Ottawa"", ""Cases in New York"", ""Action in Rome"", ""Time in Tokyo"", ""Ways in Amsterdam""...
",
NLTK strange result,https://stackoverflow.com/questions/34491819,Semantic Clustering,"I am looking for advice on how to find clusters of terms that are all related to a single concept. 

The goal is to improve a tag or keyword search for images that describe concepts or processes or situations. An image may describe a brainstorming session, or a particular theme. These images which are meant to be used in PowerPoint or other presentation material have user contributed tags. 

The issue is  our tag based search may bring back completely unrelated images. Our goal is to find the clusters within the tags in order to refine the tags related to a central concept and remove the outliers that are not related to the clusters. 

For example if you have a you had the tags meeting, planning, brainstorming, and round table. Ideally we would want to remove round table from the cluster as it doesn't fit the theme. 

I have worked with WordNet Similarity but the results are quite strange. I was wondering if there are any other tools in python's NLTK that could help me solve this.

Thanks!
",0,684,"Your question is based in the area called ""topic modeling"" you can use:
gensim
https://radimrehurek.com/gensim/
or lda
https://pypi.python.org/pypi/lda
",,
NLTK strange issue,https://stackoverflow.com/questions/64945076,Using Natural Language Tool Kit with Django on Heroku - - Error: &#39;nltk.txt&#39; not found,"Ive got a basic Django project. One feature I am working on counts the number of most commonly occurring words in a .txt file, such as a large public domain book. Ive used the Python Natural Language Tool Kit to filter out stopwords (in SEO language, that means redundant words such as the, you, etc. ).
Anyways, Im getting this debug traceback when Django serves the template:

Resource [93mstopwords[0m not found. Please use the NLTK Downloader to
obtain the resource: [31m &lt;&lt;&lt; import nltk nltk.download('stopwords')
[0m For more information see: https://www.nltk.org/data.html

So I need to download the library of stopwords. To resolve the issue, I simply open a Python REPL on my remote server and invoke these two straightforward lines:
&lt;&lt;&lt; import nltk
&lt;&lt;&lt; nltk.download('stopwords')

That's covered at length elsewhere on SO. That resolves the issue, but only temporarily. As soon as the REPL session is terminated on my remote server, the error returns because the stopwords file just evaporates.
I noticed something strange when I use git to push my changes up to my remote server on Heroku. Check this:
remote: -----&gt; Python app detected
remote: -----&gt; No change in requirements detected, installing from cache
remote: -----&gt; Installing pip 20.1.1, setuptools 47.1.1 and wheel 0.34.2
remote: -----&gt; Installing SQLite3
remote: -----&gt; Installing requirements with pip
remote: -----&gt; Downloading NLTK corpora
remote:  !     'nltk.txt' not found, not downloading any corpora
remote:  !     Learn more: https://devcenter.heroku.com/articles/python-nltk 
remote: -----&gt; $ python manage.py collectstatic --noinput
remote:        122 static files copied to '/tmp/build_f2f9d10f/staticfiles', 388 post-processed.

That devcenter link is kind of like a stub, meaning that its not very detailed. Its sparse at best. The article says that to use Python nltk, you need to add an nltk.txt file to the project directory which specifies the list of objects for Heroku to download. So I went ahead and created an nltk text file which contained:

corpora

Here is this active nltk.txt currently located in my project directory. In addition to coprora, I also tried adding various combinations of the following three entries to nltk.txt:

corpus


stoplist


english

I tried adding all four, just two and just one. For example, here is an alternate nltk.txt that I tried verbatim. My feeling is that the main one I really need is just corpora, so that is the only entry in the nltk.txt that I am working with right now. With corpora there, when I push the change and Heroku builds the environment, I see this error and trace-back:
remote: -----&gt; Downloading NLTK corpora
remote: -----&gt; Downloading NLTK packages: corpora english stopwords corpus
remote: /app/.heroku/python/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour
remote:   warn(RuntimeWarning(msg))
remote: [nltk_data] Error loading corpora: Package 'corpora' not found in
remote: [nltk_data]     index
remote: Error installing package. Retry? [n/y/e]
remote: Traceback (most recent call last):
remote:   File ""/app/.heroku/python/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
remote:     ""__main__"", mod_spec)
remote:   File ""/app/.heroku/python/lib/python3.6/runpy.py"", line 85, in _run_code
remote:     exec(code, run_globals)
remote:   File ""/app/.heroku/python/lib/python3.6/site-packages/nltk/downloader.py"", line 2538, in &lt;module&gt;
remote:     halt_on_error=options.halt_on_error,
remote:   File ""/app/.heroku/python/lib/python3.6/site-packages/nltk/downloader.py"", line 790, in download

I am clearly not using nltk.txt properly because it isnt finding the corpora package. I can install nltk and have it run without issue in my local dev server but my remaining question is this: how do I make Heroku handle nltk properly remotely in this situation?
User Michael Godshall provides the same answer to more than one Stack Overflow question explaining that you can create a bin directory within the project root and add both a post_compile bash script and a install_nltk_data script. However this is no longer necessary because heroku-buildpack-python upstream maintainer Kenneth Reitz implemented an easy solution. All that is required now is to add an nltk.txt which contains the library you need. But I did that and I am still getting the error above.
The official nltk website documents how to use the library in general and how to install it which isnt helpful in the case of Heroku because Heroku seems to handle nltk differently.
",1,673,"Eureka! I got it working. My problem was with the name of the nltk library download. I tried stoplist when the actual name is stopwords. Ha! The contents of my nltk.txt is now simply: stopwords.  When I pushed to Heroku, the build succeeded and my website is now deployed and accessible on the web.
Special thanks goes out to @Darkknight for his patience and insight in the comment section of his answer.
","Yes, you need the nltk.txt file similar to the requirements.txt file properly. refer to the official doc here. if you still facing the same situation post the nltk.txt file here that will give us some way to find the solution
maybe this also will help you
",
NLTK strange issue,https://stackoverflow.com/questions/38523385,Not able to install numpy or nltk python-modules,"I am having a strange issue installing numpy or nltk python-modules in my windows-7 machine. I have successfully installed Python 2.7.12 but I get this error when I type pip install numpy as in this screenshot. I have also included the directory of pip.exe in the PATH. Any help would be appreciated.Thank you :)
",1,278,"Installing such these things in windows are sometime difficult, specially for someone new to python packages(also for some experts!)

Try to use Anaconda for windows: https://www.continuum.io/downloads#_windows
This install a python for you and many requirement packages(e.g Numpy, Scipy, Scikit and many more)

You can use older version of Anaconda, for python2.x if you want strictly python2.x



An alternative way is to download Numpy from github and then install it as a python package, that contain setup.py file

python setup.py install


Or you can download Numpy wheel package, then install it localy with pip
",,
SciPy unexpected behavior,https://stackoverflow.com/questions/49280404,Shift interpolation does not give expected behaviour,"When using scipy.ndimage.interpolation.shift to shift a numpy data array along one axis with periodic boundary treatment (mode = 'wrap'), I get an unexpected behavior. The routine tries to force the first pixel (index 0) to be identical to the last one (index N-1) instead of the ""last plus one (index N)"".

Minimal example:

# module import
import numpy as np
from scipy.ndimage.interpolation import shift
import matplotlib.pyplot as plt

# print scipy.__version__
# 0.18.1

a = range(10)

plt.figure(figsize=(16,12))

for i, shift_pix in enumerate(range(10)):
    # shift the data via spline interpolation
    b = shift(a, shift=shift_pix, mode='wrap')

    # plotting the data
    plt.subplot(5,2,i+1)
    plt.plot(a, marker='o', label='data')
    plt.plot(np.roll(a, shift_pix), marker='o', label='data, roll')
    plt.plot(b, marker='o',label='shifted data')
    if i == 0:
        plt.legend(loc=4,fontsize=12)
    plt.ylim(-1,10)
    ax = plt.gca()
    ax.text(0.10,0.80,'shift %d pix' % i, transform=ax.transAxes)


Blue line: data before the shift
Green line: expected shift behavior
Red line: actual shift output of scipy.ndimage.interpolation.shift

Is there some error in how I call the function or how I understand its behavior with mode = 'wrap'? The current results are in contrast to the mode parameter description from the related scipy tutorial page and from another StackOverflow post. Is there an off-by-one-error in the code?

Scipy version used is 0.18.1, distributed in anaconda-2.2.0


",7,985,"It is worth noting that this behavior appears to be a bug, as noted in this SciPy issue:
https://github.com/scipy/scipy/issues/2640

The issue appears to effect every extrapolation mode in scipy.ndimage other than mode='mirror'.
","It seems that the behaviour you have observed is intentional.

The cause of the problem lies in the C function map_coordinate which translates the coordinates after shift to ones before shift:

map_coordinate(double in, npy_intp len, int mode)


The function is used as the subroutine in NI_ZoomShift that does the actual shift. Its interesting part looks like this:



Example. Lets see how the output for output = shift(np.arange(10), shift=4, mode='wrap') (from the question) is computed.

NI_ZoomShift computes edge values output[0] and output[9] in some special way, so lets take a look at computation of output[1] (a bit simplified):

# input  =         [0,1,2,3,4,5,6,7,8,9]
# output = [ ,?, , , , , , , , ]          '?' == computed position
# shift  = 4
output_index = 1

in  = output_index - shift    # -3
sz  = 10 - 1                  # 9
in += sz * ((-5 / 9) + 1)
#  +=  9 * ((     0) + 1) == 9
# in == 6

return input[in]  # 6 


It is clear that sz = len - 1 is responsible for the behaviour you have observed. It was changed from sz = len in a suggestively named commit dating back to 2007: Fix off-by-on errors in ndimage boundary routines. Update tests.

I don't know why such change was introduced. One of the possible explanations that come to my mind is as follows:

Function 'shift' uses splines for interpolation.
A knot vector of an uniform spline on interval [0, k] is simply [0,1,2,...,k]. When we say that the spline should wrap, it is natural to require equality on values for knots 0 and k, so that many copies of the spline could be glued together, forming a periodic function:

0--1--2--3-...-k              0--1--2--3-...-k              0--1-- ...
               0--1--2--3-...-k              0--1--2--3-...-k      ...


Maybe shift just treats its input as a list of values for spline's knots?
",
SciPy unexpected behavior,https://stackoverflow.com/questions/40451203,Cython parallel loop problems,"I am using cython to compute a pairwise distance matrix using a custom metric as a faster alternative to scipy.spatial.distance.pdist.

My Motivation

My metric has the form 

def mymetric(u,v,w):
     np.sum(w * (1 - np.abs(np.abs(u - v) / np.pi - 1))**2)


and the pairwise distance using scipy can be computed as

x = sp.spatial.distance.pdist(r, metric=lambda u, v: mymetric(u, v, w))


Here, r is a m-by-n matrix of m vectors with dimension of n and w is a ""weight"" factor with dimmension n.

Since in my problem m is rather high, the computation is really slow. For m = 2000 and n = 10 this takes approx 20 sec.

Initial solution with Cython

I implemented a simple function in cython that computes the pairwise distance and immediately got very promising results -- speedup of over 500x.

import numpy as np
cimport numpy as np
import cython

from libc.math cimport fabs, M_PI

@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size
    cdef np.ndarray[np.double_t, ndim=1] ans
    size = r.shape[0] * (r.shape[0] - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    c = -1
    for i in range(r.shape[0]):
        for j in range(i + 1, r.shape[0]):
            c += 1
            for k in range(r.shape[1]):
                ans[c] += w[k] * (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))**2.0

    return ans


Problems using OpenMP

I wanted to speed up the computation some more using OpenMP, however, the following solution is roughly 3 times slower than the serial version.

import numpy as np
cimport numpy as np

import cython
from cython.parallel import prange, parallel

cimport openmp

from libc.math cimport fabs, M_PI

@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_omp(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size, m, n
    cdef np.double_t a
    cdef np.ndarray[np.double_t, ndim=1] ans
    m = r.shape[0]
    n = r.shape[1]
    size = m * (m - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    with nogil, parallel(num_threads=8):
        for i in prange(m, schedule='dynamic'):
            for j in range(i + 1, m):
                c = i * (m - 1) - i * (i + 1) / 2 + j - 1
                for k in range(n):
                    ans[c] += w[k] * (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))**2.0

    return ans


I don't know why is it actually slower, but I tried to introduce the following changes. This resulted not only in even slightly worse performance but also, the resulting distance ans is computed correctly only in the beginning of the array, the rest is just zeros. The speedup achieved through this is negligible. 

import numpy as np
cimport numpy as np

import cython
from cython.parallel import prange, parallel

cimport openmp

from libc.math cimport fabs, M_PI
from libc.stdlib cimport malloc, free

@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_omp_2(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int k, l, c, m, n
    cdef Py_ssize_t i, j, d
    cdef size_t size
    cdef int *ci, *cj

    cdef np.ndarray[np.double_t, ndim=1, mode=""c""] ans

    cdef np.ndarray[np.double_t, ndim=2, mode=""c""] data
    cdef np.ndarray[np.double_t, ndim=1, mode=""c""] weight

    data = np.ascontiguousarray(r, dtype=np.float64)
    weight = np.ascontiguousarray(w, dtype=np.float64)

    m = r.shape[0]
    n = r.shape[1]
    size = m * (m - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)

    cj = &lt;int*&gt; malloc(size * sizeof(int))
    ci = &lt;int*&gt; malloc(size * sizeof(int))

    c = -1
    for i in range(m):
        for j in range(i + 1, m):
            c += 1
            ci[c] = i
            cj[c] = j

    with nogil, parallel(num_threads=8):
        for d in prange(size, schedule='guided'):
            for k in range(n):
                ans[d] += weight[k] * (1.0 - fabs(fabs(data[ci[d], k] - data[cj[d], k]) / M_PI - 1.0))**2.0

    return ans


For all functions, I am using the following .pyxbld file

def make_ext(modname, pyxfilename):
    from distutils.extension import Extension
    return Extension(name=modname,
                     sources=[pyxfilename],
                     extra_compile_args=['-O3', '-march=native', '-ffast-math', '-fopenmp'],
                     extra_link_args=['-fopenmp'],
                     )


Summary

I have zero experience with cython and know only basics of C. I would appreciate any suggestion of what may be the cause of this unexpected behavior, or even, how to rephrase my question better.



Best serial solution (10 % faster than original serial)

@cython.cdivision(True)
@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_2(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size
    cdef np.ndarray[np.double_t, ndim=1] ans
    cdef np.double_t accumulator, tmp
    size = r.shape[0] * (r.shape[0] - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    c = -1
    for i in range(r.shape[0]):
        for j in range(i + 1, r.shape[0]):
            c += 1
            accumulator = 0
            for k in range(r.shape[1]):
                tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
                accumulator += w[k] * (tmp*tmp)
            ans[c] = accumulator

    return ans


Best parallel solution (1 % faster then original parallel, 6 times faster then best serial using 8 threads)

@cython.cdivision(True)
@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_omp_2d(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size, m, n
    cdef np.ndarray[np.double_t, ndim=1] ans
    cdef np.double_t accumulator, tmp
    m = r.shape[0]
    n = r.shape[1]
    size = m * (m - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    with nogil, parallel(num_threads=8):
        for i in prange(m, schedule='dynamic'):
            for j in range(i + 1, m):
                c = i * (m - 1) - i * (i + 1) / 2 + j - 1
                accumulator = 0
                for k in range(n):
                    tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
                    ans[c] += w[k] * (tmp*tmp)

    return ans




Unsolved issues:

When I try to apply the accumulator solution proposed in the answer, I get the following error:

Error compiling Cython file:
------------------------------------------------------------
...
                c = i * (m - 1) - i * (i + 1) / 2 + j - 1
                accumulator = 0
                for k in range(n):
                    tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
                    accumulator += w[k] * (tmp*tmp)
                ans[c] = accumulator
                                   ^
------------------------------------------------------------
pdist.pyx:207:36: Cannot read reduction variable in loop body


Full code:

@cython.cdivision(True)
@cython.wraparound(False)
@cython.boundscheck(False)
def pairwise_distance_omp(np.ndarray[np.double_t, ndim=2] r, np.ndarray[np.double_t, ndim=1] w):
    cdef int i, j, k, c, size, m, n
    cdef np.ndarray[np.double_t, ndim=1] ans
    cdef np.double_t accumulator, tmp
    m = r.shape[0]
    n = r.shape[1]
    size = m * (m - 1) / 2
    ans = np.zeros(size, dtype=r.dtype)
    with nogil, parallel(num_threads=8):
        for i in prange(m, schedule='dynamic'):
            for j in range(i + 1, m):
                c = i * (m - 1) - i * (i + 1) / 2 + j - 1
                accumulator = 0
                for k in range(n):
                    tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
                    accumulator += w[k] * (tmp*tmp)
                ans[c] = accumulator

    return ans

",5,3869,"I haven't timed this myself so it's possible this might not help too much, however:

If you run cython -a to get an annotated version of your initial attempt (pairwise_distance_omp) you'll find the ans[c] += ... line is yellow, suggesting it's got Python overhead. A look at that the C corresponding to that line suggests that it's checking for divide by zero. One key part of it starts:

if (unlikely(M_PI == 0)) {


You know this will never be true (and in any case you'd probably live with NaN values rather than an exception if it was). You can avoid this check by adding the following extra decorator to the function:

@cython.cdivision(True)
# other decorators
def pairwise_distance_omp # etc...


This cuts out quite a bit of C code, including bits that have to be run in a single thread. The flip-side is that most of that code should never be run, and the compiler should probably be able to work that out, so it isn't clear how much difference that will make.



Second suggestion:

# at the top
cdef np.double_t accumulator, tmp

    # further down later in the loop:
    c = i * (m - 1) - i * (i + 1) / 2 + j - 1
    accumulator = 0
    for k in range(r.shape[1]):
        tmp = (1.0 - fabs(fabs(r[i, k] - r[j, k]) / M_PI - 1.0))
        accumulator = accumulator + w[k] * (tmp*tmp)
    ans[c] = accumulator


This has two advantages hopefully: 1) tmp*tmp should probably be quicker than floating point exponent to the power of 2. 2) You avoid reading from the ans array, which might be a bit slow because the compiler always has to be careful that some other thread hasn't changed it (even though you know it shouldn't have).
",,
SciPy unexpected behavior,https://stackoverflow.com/questions/45255265,Unexpected behavior of Gaussian filtering with Scipy,"Given that I have an image f(x,y) loaded, for example,



I want to compute the Gaussian derivative /x /y G*f of the image f, where G is a Gaussian filter and * denotes convolution. This is easily done using Scipy:

from scipy.ndimage.filters import gaussian_filter
imshow(gaussian_filter(g, sigma, order=1))


With sigma=50 this produces the following result:



Now, for applicationary reasons, I need to do the computation with mode='constant':

imshow(gaussian_filter(g, sigma, order=1, mode='constant', cval=0))


Still, the result looks reasonable:



However, note that my image's background's intensity is 1 and not 0. Hence, it should be reasonable to use cval=1:

imshow(gaussian_filter(g, sigma, order=1, mode='constant', cval=1))




Now this is unexpected! This result makes no sense, does it?

For the record, I also checked the partial differentials /x G*f and /y G*f. Whereas

imshow(gaussian_filter(g, sigma, order=[0, 1], mode='constant', cval=1)


looks reasonable



the other one

imshow(gaussian_filter(g, sigma, order=[1, 0], mode='constant', cval=1)


does not:



Why is that?
",5,1015,"There is a bug in gaussian_filter that manifests itself when both order and cval are nonzero. Specifically, it's here: 

for axis, sigma, order, mode in axes:
    gaussian_filter1d(input, sigma, axis, order, output, mode, cval, truncate)
    input = output


The filter performs repeated 1d convolution, and each time it passes in cval to 1d filter. Problem is, if there were any derivatives taken, then cval should be set to 0 because the derivative of any constant is zero. This is why the result is wrong with order=[1, 0] but not with order=[0, 1]. Without testing (don't have SciPy dev environment), I think the following would be correct: 

for axis, sigma, order, mode in axes:
    gaussian_filter1d(input, sigma, axis, order, output, mode, cval, truncate)
    if order &gt; 0: 
        cval = 0.0
    input = output


Workaround

A nonzero cval can be emulated by subtracting it from the image before filtering (and adding back after filtering only if the order is zero). Example:

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage.filters import gaussian_filter

g = np.ones((500, 500))
g[200:300, 200:300] = 2
sigma = 50
cval = 1
gf = gaussian_filter(g-cval, sigma, order=1, mode='constant')
plt.matshow(gf)
plt.show()


returns



which is the expected result. (My original image is a bit different from yours, and I use a different visualization tool.)
",,
SciPy unexpected behavior,https://stackoverflow.com/questions/18155972,unexpected result in numpy array slicing (view vs copy),"I'm trying to reduce the amount of copying in my code and I came across surprising behavior when dealing with numpy array slicing and views, as explained in:
Scipy wiki page on copying numpy arrays
I've stumbled across the following behavior, which is unexpected for me:
Case 1.:
import numpy as np
a = np.ones((3,3))
b = a[:,1:2]
b += 5
print a
print b.base is a

As expected, this outputs:
array([[ 1.,  6.,  1.],
       [ 1.,  6.,  1.],
       [ 1.,  6.,  1.]])
True

Case 2: When performing the slicing and addition in one line, things look different:
import numpy as np
a = np.ones((3,3))
b = a[:,1:2] + 5
print a
print b.base is a

The part that's surprising to me is that a[:,1:2] does not seem to create a view, which is then used as a left hand side argument, so, this outputs:
array([[ 1.,  1.,  1.],
       [ 1.,  1.,  1.],
       [ 1.,  1.,  1.]])
False

Maybe someone can shed some light on why these two cases are different, I think I'm missing something.
Solution: I missed the obvious fact that the ""+"" operator, other than the in-place operator ""+="" will always create a copy, so it's in fact not related but slicing other than how in-place operators are defined for numpy arrays.
To illustrate this, the following generates the same output as Case 2:
import numpy as np
a = np.ones((3,3))
b = a[:,1:2]
b = b + 5
print a
print b.base is a

",4,3293,"The above is no different than:

&gt;&gt;&gt; a=np.arange(5)
&gt;&gt;&gt; b=a
&gt;&gt;&gt; b
array([0, 1, 2, 3, 4])

&gt;&gt;&gt; b+=5
&gt;&gt;&gt; a
array([5, 6, 7, 8, 9])
&gt;&gt;&gt; b
array([5, 6, 7, 8, 9])

&gt;&gt;&gt; b=b+5
&gt;&gt;&gt; b
array([10, 11, 12, 13, 14])
&gt;&gt;&gt; a
array([5, 6, 7, 8, 9])


Which, at least to me, seem like completely expected behavior. The b+=x operator calls __iadd__ which importantly first tries to modify the array in place, so it will update b which is still a view of a. While the b=b+x operator calls __add__ which creates new temporary data and then assigns it to b.

For a[i] +=b the sequence is (in numpy): 

a.__setitem__(i, a.__getitem__(i).__iadd__(b))

","a[:, 1:2] creates a view, but you don't modify the view in the second example. Instead, + creates a new array from its arguments. Suppose you do

a = np.ones((3, 3))
b = a + 5


In that case, you wouldn't expect a change to a, because this isn't an in-place addition. The operator is +, rather than +=. It's the same with the second example.

b = a[:, 1:2] + 5


doesn't modify a[:, 1:2], because this isn't in-place addition.
","The default thing to do when slicing a numpy array is to create b as a view of a, thus when you change b, a changes as well, which is confirmed by your first case. 

The second case is more tricky. You are not telling that b is a slice of a and then adding a number. What you are doing is creating b as something that does not coincide with a, so numpy is forced to copy the data instead of just creating a view. 
"
SciPy unexpected behavior,https://stackoverflow.com/questions/52450658,Python - unexpected shape parameter behavior in scipy genextreme fit,"I've been trying to fit the GEV distribution to some annual maximum river discharge using Scipy's stats.genextreme function, but I've found some weird behavior of the fit. Depending on how small your data is (i.e., 1e-5 vs. 1e-1), the shape parameter that is returned can be dramatically different. For example: 

import scipy as scipy
import numpy as np
from scipy.stats import genextreme as gev
from scipy.stats import gumbel_r as gumbel

#Set up arrays of values to fit curve to 
sample=np.random.rand(1,30) #Random set of decimal values 
smallVals = sample*1e-5     #Scale to smaller values 

#If the above is not creating different values, this instance of random numbers has:
bugArr = np.array([[0.25322987, 0.81952358, 0.94497455, 0.36295543, 0.72272746, 0.49482558,0.65674877, 0.40876558, 0.64952248, 0.23171052, 0.24645658, 0.35359126,0.27578928, 0.24820775, 0.69789187, 0.98876361, 0.22104156,0.40019593,0.0756707,  0.12342556, 0.3601186,  0.54137089,0.43477705, 0.44622486,0.75483338, 0.69766687, 0.1508741,  0.75428996, 0.93706003, 0.1191987]])
bugArr_small = bugArr*1e-5

#This array of random numbers gives the same shape parameter regardless 
fineArr = np.array([[0.7449611,  0.82376693, 0.32601009, 0.18544293, 0.56779629, 0.30495415,
        0.04670362, 0.88106521, 0.34013959, 0.84598841, 0.24454428, 0.57981437,
        0.57129427, 0.8857514,  0.96254429, 0.64174078, 0.33048637, 0.17124045,
        0.11512589, 0.31884749, 0.48975204, 0.87988863, 0.86898236, 0.83513966,
        0.05858769, 0.25889509, 0.13591874, 0.89106616, 0.66471263, 0.69786708]])
fineArr_small = fineArr*1e-5

#GEV fit for both arrays - shouldn't dramatically change distribution 
gev_fit      = gev.fit(sample)
gevSmall_fit = gev.fit(smallVals)

gevBug      = gev.fit(bugArr)
gevSmallBug = gev.fit(bugArr_small)

gevFine      = gev.fit(fineArr)
gevSmallFine = gev.fit(fineArr_small)


I get the following output for the GEV parameters estimated for the bugArr/bugArr_small and fineArr/fineArr_small:

Known bug array
Random values:         (0.12118250540401079, 0.36692231766996053, 0.23142400358716353)
Random values scaled:  (-0.8446554391074808, 3.0751769299431084e-06, 2.620390405092363e-06)

Known fine array
Random values:         (0.6745399522587823, 0.47616297212022757, 0.34117425062278584)
Random values scaled:  (0.6745399522587823, 4.761629721202293e-06, 3.411742506227867e-06)


Why would the shape parameter change so dramatically when the only difference in the data is a change in scaling? I would've expected the behavior to be consistent with the FineArr results (no change in shape parameter, and appropriate scaling of location and scale parameters). I've repeated the test in Matlab, but the results there are in line with what I expected (i.e., no change in shape parameter). 
",2,506,"I think I know why this might be happening. It is possible to pass initial shape parameter estimates when fitting, see the documentation for scipy.stats.rv_continuous.fit where it states ""Starting value(s) for any shape-characterizing arguments (those not provided will be determined by a call to _fitstart(data)). No default value."" Here is some extremely ugly, functional, code using my pyeq3 statistical distribution fitter which internally attempts to use different estimates, fit them, and return the parameters for best nnlf of the different fits. This example code does not show the behavior you observe, and gives the same shape parameters regardless of scaling. You would need to install pyeq3 with ""pip3 install pyeq3"" to run this code. The pyeq3 code is designed for text input from a web interface on zunzun.com, so hold you nose - here is the example code:

import numpy as np

#Set up arrays of values to fit curve to 
sample=np.random.rand(1,30) #Random set of decimal values 
smallVals = sample*1e-5     #Scale to smaller values 

#If the above is not creating different values, this instance of random numbers has:
bugArr = np.array([0.25322987, 0.81952358, 0.94497455, 0.36295543, 0.72272746, 0.49482558,0.65674877, 0.40876558, 0.64952248, 0.23171052, 0.24645658, 0.35359126,0.27578928, 0.24820775, 0.69789187, 0.98876361, 0.22104156,0.40019593,0.0756707,  0.12342556, 0.3601186,  0.54137089,0.43477705, 0.44622486,0.75483338, 0.69766687, 0.1508741,  0.75428996, 0.93706003, 0.1191987])
bugArr_small = bugArr*1e-5

#This array of random numbers gives the same shape parameter regardless 
fineArr = np.array([0.7449611,  0.82376693, 0.32601009, 0.18544293, 0.56779629, 0.30495415,
        0.04670362, 0.88106521, 0.34013959, 0.84598841, 0.24454428, 0.57981437,
        0.57129427, 0.8857514,  0.96254429, 0.64174078, 0.33048637, 0.17124045,
        0.11512589, 0.31884749, 0.48975204, 0.87988863, 0.86898236, 0.83513966,
        0.05858769, 0.25889509, 0.13591874, 0.89106616, 0.66471263, 0.69786708])
fineArr_small = fineArr*1e-5

bugArr_str = ''
for i in range(len(bugArr)):
    bugArr_str += str(bugArr[i]) + '\n'
bugArr_small_str = ''
for i in range(len(bugArr_small)):
    bugArr_small_str += str(bugArr_small[i]) + '\n'
fineArr_str = ''
for i in range(len(fineArr)):
    fineArr_str += str(fineArr[i]) + '\n'
fineArr_small_str = ''
for i in range(len(fineArr_small)):
    fineArr_small_str += str(fineArr_small[i]) + '\n'
import pyeq3

simpleObject_bugArr = pyeq3.IModel.IModel()
simpleObject_bugArr._dimensionality = 1
pyeq3.dataConvertorService().ConvertAndSortColumnarASCII(bugArr_str, simpleObject_bugArr, False)
solver = pyeq3.solverService()
result_bugArr = solver.SolveStatisticalDistribution('genextreme', simpleObject_bugArr.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')
simpleObject_bugArr_small = pyeq3.IModel.IModel()
simpleObject_bugArr_small._dimensionality = 1
pyeq3.dataConvertorService().ConvertAndSortColumnarASCII(bugArr_small_str, simpleObject_bugArr_small, False)
solver = pyeq3.solverService()
result_bugArr_small = solver.SolveStatisticalDistribution('genextreme', simpleObject_bugArr_small.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')

simpleObject_fineArr = pyeq3.IModel.IModel()
simpleObject_fineArr._dimensionality = 1
pyeq3.dataConvertorService().ConvertAndSortColumnarASCII(fineArr_str, simpleObject_fineArr, False)
solver = pyeq3.solverService()
result_fineArr = solver.SolveStatisticalDistribution('genextreme', simpleObject_fineArr.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')

simpleObject_fineArr_small = pyeq3.IModel.IModel()
simpleObject_fineArr_small._dimensionality = 1
pyeq3.dataConvertorService().ConvertAndSortColumnarASCII(fineArr_small_str, simpleObject_fineArr_small, False)
solver = pyeq3.solverService()
result_fineArr_small = solver.SolveStatisticalDistribution('genextreme', simpleObject_fineArr_small.dataCache.allDataCacheDictionary['IndependentData'][0], 'nnlf')

print('ba',result_bugArr[1]['fittedParameters'])
print('ba_s',result_bugArr_small[1]['fittedParameters'])
print()
print('fa',result_fineArr[1]['fittedParameters'])
print('fa_s',result_fineArr_small[1]['fittedParameters'])

",,
SciPy unexpected behavior,https://stackoverflow.com/questions/59281884,Unexpected behavior of `scipy.ndimage.zoom()` for `order=0`,"I have difficulties understanding the behavior of scipy.ndimage.zoom() when order=0.

Consider the following code:

import numpy as np
import scipy as sp
import scipy.ndimage

arr = np.arange(3) + 1
print(arr)
for order in range(5):
    zoomed = sp.ndimage.zoom(arr.astype(float), 4, order=order)
    print(order, np.round(zoomed, 3))


whose output is:

0 [1. 1. 1. 2. 2. 2. 2. 2. 2. 3. 3. 3.]
1 [1.    1.182 1.364 1.545 1.727 1.909 2.091 2.273 2.455 2.636 2.818 3.   ]
2 [1.    1.044 1.176 1.394 1.636 1.879 2.121 2.364 2.606 2.824 2.956 3.   ]
3 [1.    1.047 1.174 1.365 1.601 1.864 2.136 2.399 2.635 2.826 2.953 3.   ]
4 [1.    1.041 1.162 1.351 1.59  1.86  2.14  2.41  2.649 2.838 2.959 3.   ]


So, when order=0 the values are (expectedly) not interpolated.
However, I was expecting to have:

[1. 1. 1. 1. 2. 2. 2. 2. 3. 3. 3. 3.]


i.e. exactly the same number of elements for each value, since the zoom is a whole number.
Hence, I was expecting to get the same result as np.repeat():

print(np.repeat(arr.astype(float), 4))
[1. 1. 1. 1. 2. 2. 2. 2. 3. 3. 3. 3.]


Why is there a variation in the number of times each element gets repeated?



Note that np.repeat() does not directly work with multi-dimensional arrays and that is the reason why I would like to get the ""correct"" behavior from scipy.ndimage.zoom().



My NumPy and SciPy versions are:

print(np.__version__)
# 1.17.4
print(sp.__version__)
# 1.3.3




I found this:
`scipy.ndimage.zoom` vs `skimage.transform.rescale` with `order=0`
which points toward some unexpected behavior for scipy.ndimage.zoom() but I am not quite sure it is the same effect being observed.
",2,3324,"This is a bin/edge array interpretation issue.
The behavior of scipy.ndimage.zoom() is based on the edge interpretation of the array values, while the behavior that would produce equally-sized blocks for integer zoom factors (mimicking np.repeat()) is based on the bin interpretation.

Let's illustrate with some ""pictures"".

Bin Interpretation

Consider the array [1 2 3], and let's assign each value to a bin.
The edges of each bin would be: 0 and 1 for 1, 1 and 2 for 2, etc.

0 1 2 3
|1|2|3|


Now, let's zoom this array by a factor of 4:

                    1 1 1
0 1 2 3 4 5 6 7 8 9 0 1 2
|   1   |   2   |   3   |


Hence, the values to assign to the bins using the Next-door Neighbor method are:

                    1 1 1
0 1 2 3 4 5 6 7 8 9 0 1 2
|1 1 1 1|2 2 2 2|3 3 3 3|


Edge Interpretation

Consider the same array as before [1 2 3], but now let's assign each value to an edge:

0 1 2
| | |
1 2 3


Now, let's zoom this array by a factor of 4:

                    1 1
0 1 2 3 4 5 6 7 8 9 0 1
| | | | | | | | | | | |
1          2          3



Hence, the values to assign to the edges using the Next-door Neighbor method are:

                    1 1
0 1 2 3 4 5 6 7 8 9 0 1
| | | | | | | | | | | |
1 1 1 2 2 2 2 2 2 3 3 3


and edge 3 is assigned to 2 because 2 has position 5.5 while 1 has position 0 and (5.5 - 3 = 2.5) &lt; (3 - 0 = 3).
Similarly, edge 8 is assigned to 2 because (8 - 5.5 = 2.5) &lt; (11 - 8 = 3).



Comments

In Physics, the ""bin array interpretation"" is generally more useful, because measurements are typically ""the result of some integration over a certain bin in an appropriate domain"" (notably signal of any form -- including images -- collected at a given time interval), hence I was expecting a ""bin interpretation"" for scipy.ndimage.zoom() but I acknowledge that the ""edge interpretation"" is equally valid (although I am not sure which applications benefit the most from it).



(Thanks to @Patol75 for pointing me into the right direction)
","I think that this is the expected behaviour.

Consider your initial list, [1, 2, 3]. You ask scipy to zoom on it 4 times, which thereby creates a 4x3=12 elements list. The first element of the list has to be 1, the last one has to be 3. Then, for 2, well we have an even number of elements, so it would make sense to have 2 as both the 6th and 7th elements. This gives [1, , , , , 2, 2, , , , , 3]. From here, you provided zoom with order=0, which means zoom is going to fill in for the missing values with splines of order 0. First case, zoom needs to fill in for 4 missing values between 1 and 2. This has to be [1, 1, 2, 2]. Second case, 4 missing values between 2 and 3. Same logic, [2, 2, 3, 3]. Final result [1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3].

Now consider a 5x zoom, which generates a 15 elements array. Same story, except that there is a ""middle"" element, so that only one 2 is initially placed in the new list, at the 8th spot. With six elements to fill in between each pair, we get with the same logic [1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3].

Therefore, you get more 2s than 1s or 3s because 2 is involved in two interpolation operations, opposed to one for both 1 &amp; 3. 
",
SciPy unexpected behavior,https://stackoverflow.com/questions/29306538,Unexpected behavior in scipy isf,"I am using scipy's stats module to try and determine values of a distribution at which the upper tail probability reaches some small value, but I am getting some very unrealistic results. For example:

I fit a beta distribution to an array of the square of normalized correlation coefficients for a signal matching operation (correlation coefficient is always between -1 and 1 so its square is between 0 and 1). Using 

import scipy, numpy as np
bd=scipy.beta.fit(np.square(data),floc=0,fscale=1) #fitted beta dist


which gives me the beta distribution parameters of 
    (0.42119596435034012, 16939.046996018118, 0, 1) the data array is about 3 million elements long. 

Now when I plot the distribution it is clear that most the area of the distribution is very near 0 on the x axis

import matplotlib.pyplot as plt
x=x=np.linspace(0,1,num=1000000)
plt.plot(x,scipy.stats.beta.pdf(x,betaparams[0],betaparams[1]))
plt.xlim([0,.0001])




Now when I try to find the x value for which some upper tail probability remains I get some unexpected behavior. For example

for expon in [-1,-2,-3,-4,-5,-6,-7,-8,-9,-10]:
    print (expon,scipy.stats.beta.isf(10**expon,betaparams[0],betaparams[1]))


yeilds:

(-1, 6.9580465891063448e-05)
(-2, 0.00018124328968143608)
(-3, 0.00030250611696189104)
(-4, 0.00042796070123291116)
(-5, 0.0005557482540313166)
(-6, 0.00068501413697673774)
(-7, 0.99999966996999767)
(-8, 0.99999996699699967)
(-9, 0.99999999669970008)
(-10, 0.99999999966997)


Clearly scipy is returning poor estimates around 10**-7. My question is why, why it would express this behavior silently, and how to fix it. 

Thanks
",1,234,"This appears to be a bug in scipy.special.btdtri which is supposed to compute quantiles for the beta distribution.  Maybe you can file a bug report.

&gt;&gt;&gt; from scipy import special
&gt;&gt;&gt; special.btdtri (betaparams[0],betaparams[1], 1-1e-6)
0.00068501413697504238
&gt;&gt;&gt; special.btdtri (betaparams[0],betaparams[1], 1-1e-7)
0.99999966996999767


I can't figure out where btdtri is defined.

EDIT: For the record, here is the SciPy bug report: https://github.com/scipy/scipy/issues/4677
",,
SciPy unexpected behavior,https://stackoverflow.com/questions/54968954,scipy.optimize.curve_fit: Default value of max_nfev broken?,"I am experiencing unexpected behavior when calling scipy.optimize.curve_fit with the max_nfev argument. The documentation states that extra kwargs are passed on to leastsq for method='lm' and to least_squares otherwise. Also, method should default to 'trf' (i.e. not 'lm') if bounds are provided. Finally, least_squares takes the max_nfev argument, which defaults to 100*len(x) if not provided (or explicitly passed as max_nfev=None), x being one of the arrays on which to do the curve fitting.

I have a dataset (and matching function) on which scipy.optimize.curve_fit fails. The time it takes before the routine gives up depends linearly on max_nfev, as expected. However, this time is vastly different between specifying max_nfev=100*len(x) in the call to scipy.optimize.curve_fit and not passing in max_nfev at all, which seems to go against the documented behavior.

Here is a script which demonstrates this:

import time
import numpy as np
import scipy.optimize

x, y = np.loadtxt('data.txt', unpack=True)

# Attempt curve fit
initial_guess = (1, 0)
bounds = ([-np.inf, 0], [np.inf, 15])
for max_nfev in (None, 1*len(x), 10*len(x), 100*len(x)):
    print('\nRunning with max_nfev = {}:'.format(max_nfev))
    t0 = time.time()
    try:
        scipy.optimize.curve_fit(
            lambda x, factor, exponent: factor*x**exponent,
            x,
            y,
            initial_guess,
            bounds=bounds,
            ftol=1e-10,
            maxfev=max_nfev,
        )
        deltat = time.time() - t0
        print('Succeeded after', deltat, 's')
    except RuntimeError:
        deltat = time.time() - t0
        print('Failed after', deltat, 's')


The script needs the dataset in data.txt (24 KB), which you can download here.

On my system, the output of this script is


  Running with max_nfev = None:
  Failed after 0.10752344131469727 s
  
  Running with max_nfev = 441:
  Failed after 0.17525863647460938 s
  
  Running with max_nfev = 4410:
  Failed after 1.732572078704834 s
  
  Running with max_nfev = 44100:
  Failed after 17.796284437179565 s


I would expect the first (max_nfev=None) and last (max_nfev=100*len(x)) call to take roughly the same amount of time to fail. To add to the mystery, it seems that instead of passing max_nfev I might just as well pass maxfev, which is not a valid argument of least_squares, but instead the maxfev-equivalent argument taken by leastsq. 

Have I misunderstood something, or is the documentation or the implementation wrong?

I am experiencing this under both {SciPy 1.1.0, Python 3.6.5} and {SciPy 1.2.0, Python 3.7.1}.
",1,1487,"
  I would expect the first (max_nfev=None) and last (max_nfev=100*len(x)) call to take roughly the same amount of time


Set a breakpoint at line 250 of scipy.optimize._lsq.trf.py:

    if max_nfev is None:
        max_nfev = x0.size * 100


At this point x0 has just two elements, so your call that passed in None could equivalently have passed in 200. x0 came from p0, which defaulted to [1 0].

Based on this, your observed timings do make sense.
",,
SciPy unexpected behavior,https://stackoverflow.com/questions/57477723,Unexpected behaviour of scipy.integrate,"I want to use scipy.integrate for some numerical calculations. I just ran a little example to try it and ran across some unexpected behavior. 

I made some clean code to demonstrate the problem. I use a very simple exponential distribution to test. 

Here's my code:

import numpy as np
import sys
import scipy as sc
from scipy import integrate


print(sys.version)
print(np.version.version)
print(sc.version.version)
print()

r1 = integrate.quad(lambda x: sc.exp(-x), 0, 10)
r2 = integrate.quad(lambda x: sc.exp(-x), 0, 100000)
r3 = integrate.quad(lambda x: sc.exp(-x), 0, np.inf)

print(r1)
print(r2)
print(r3)

print()
r4 = integrate.quad(lambda x: sc.exp(-x), 0, 10000)
print(r4)


The output is

3.7.2 (default, Jan  2 2019, 17:07:39) [MSC v.1915 64 bit (AMD64)]
1.15.4
1.1.0

r1 (0.9999546000702375, 2.8326146575791917e-14)
r2 (2.0614532085314573e-45, 4.098798466247153e-45)
r3 (1.0000000000000002, 5.842606996763696e-11)

r4 (1.0, 1.6059202674761255e-14)


I expect all the output to be always approximately one. But in r2 i get an incredibly small value. Strangely, when integrating to infinity (r3), or a very small border (r1), the problem doesn't show up. Also, by decreasing the limit by one order of magnitude (r4) I also get a perfect result.

Does anyone know why this problem appears in scipy? 
I would call this a bug, but maybe I violated some restrictions?
How do I know in advance to prevent wrong results in my applied problems?

Thank you in advance

Output for full_output:

r2 (2.0614532085314573e-45, 4.098798466247153e-45, {'neval': 63, 'last': 2, 'iord': array([      1,       2,       3,       4,       5, 6357060, 6357108,
       4259932, 6357102, 7274595, 6553710, 3342433, 7077980, 6422633,
       7536732, 7602281, 2949221, 6357104, 7012451, 6750305, 7536741,
       7536732, 6881379, 7929968, 7274588, 7602288, 7143529, 7995497,
       6029413, 7209055, 7077998, 3014771, 7340131, 3604531, 7798829,
       7209065, 6357087, 6553709, 3407926, 7340078, 6553721, 3276846,
       5046318, 7209057, 6684777, 7536741,     116, 6619136, 7602291,
             0], dtype=int32), 'alist': array([0.00000000e+000, 5.00000000e+004, 0.00000000e+000, 0.00000000e+000,
       6.88436472e-272, 3.80218509e-136, 2.65902947e-068, 2.20016853e-034,
       1.04474528e-019, 3.09734336e-016, 9.03970673e-019, 8.23342652e-316,
       8.23342968e-316, 8.23343284e-316, 8.23343601e-316, 8.23343917e-316,
       8.23344233e-316, 8.23344549e-316, 8.23344865e-316, 8.23345182e-316,
       8.23345498e-316, 8.23345814e-316, 8.23346130e-316, 8.23346446e-316,
       8.23346763e-316, 8.23347079e-316, 8.23347395e-316, 8.23347711e-316,
       8.23348027e-316, 8.23348344e-316, 8.23348660e-316, 8.23348976e-316,
       8.23349292e-316, 8.23349608e-316, 8.23349925e-316, 8.23350241e-316,
       8.23350557e-316, 8.23350873e-316, 8.23351189e-316, 8.23351506e-316,
       8.23351822e-316, 8.23352138e-316, 8.23352454e-316, 8.23352770e-316,
       8.23353087e-316, 8.23353403e-316, 8.23353719e-316, 8.23354035e-316,
       8.23354351e-316, 8.23354668e-316]), 'blist': array([5.00000000e+004, 1.00000000e+005, 0.00000000e+000, 0.00000000e+000,
       6.88436472e-272, 3.80218509e-136, 2.65902947e-068, 2.20016853e-034,
       1.04474528e-019, 3.09734336e-016, 9.03970673e-019, 1.20736675e+285,
       1.05117823e-153, 1.05132391e-153, 1.05146958e-153, 3.79823888e-258,
       1.61465766e+184, 3.11517960e+161, 4.26137323e+257, 6.01346953e-154,
       6.01366349e-154, 1.19632546e-153, 3.64465882e-086, 1.31100174e-259,
       1.20679441e-153, 1.20679327e-153, 3.24245662e-086, 3.64465882e-086,
       6.01357764e-154, 1.20679441e-153, 5.75105581e+072, 2.20791354e+214,
       1.27734658e-152, 5.29444423e+160, 6.19633416e+223, 2.25563599e-153,
       8.21947530e+223, 6.09892510e-013, 1.06097757e-153, 2.86747940e-110,
       6.06154135e-154, 6.06445477e-154, 6.96312298e-077, 3.00226946e-067,
       6.03810921e-154, 1.30421760e-076, 1.21438942e-067, 4.61448322e-072,
       8.51221910e-053, 3.73237334e+069]), 'rlist': array([2.06145321e-045, 0.00000000e+000, 6.73898103e+149, 3.51023756e+151,
       4.50937881e-292, 9.43293441e-314, 4.65203811e+151, 6.99386802e-283,
       3.53886392e-308, 1.33360313e+241, 1.15420781e+171, 9.30281767e+242,
       1.17364463e+214, 3.12671297e+185, 2.85341794e-313, 8.18432962e-085,
       6.45840689e+170, 4.42638830e-239, 9.78681729e+199, 3.38460675e+125,
       3.11732880e+150, 9.78747303e+199, 2.27948172e-191, 1.04972250e+214,
       4.77402433e+180, 1.12985581e+277, 3.16464606e-307, 1.33360315e+241,
       1.76252970e-310, 1.02318154e-012, 1.15549302e-313, 1.03539814e-308,
       1.33360293e+241, 5.67421675e-311, 5.00120719e-162, 6.46048250e-313,
       1.68400738e-019, 1.10811151e-302, 1.66468912e-312, 1.09403545e-303,
       1.27613271e-303, 7.10020498e-270, 4.99875566e-111, 9.11927054e-304,
       9.11571045e-304, 9.11749048e-304, 9.11571042e-304, 9.60205653e+303,
       5.43239349e-312, 1.79972786e-304]), 'elist': array([4.09879847e-045, 0.00000000e+000, 6.47287707e+170, 5.98178835e-154,
       1.69375668e+190, 4.44389806e+252, 1.12297399e+219, 1.87673453e-152,
       7.20706153e+159, 1.27826731e-152, 2.43812981e-152, 5.52716101e+228,
       6.01346953e-154, 1.57761457e+214, 7.19938459e+252, 3.94357072e+180,
       3.44210870e+175, 3.62478142e+228, 1.23732543e-259, 3.53810655e+155,
       4.81222029e+233, 1.06843264e-258, 9.15000112e+199, 4.26614628e+180,
       3.53387914e+246, 2.35509149e+251, 1.69375944e+190, 1.57762309e+214,
       6.19634286e+223, 8.95533289e-106, 5.98148090e-154, 1.17914189e+195,
       5.42869734e+213, 6.72794695e+199, 5.30383390e+180, 1.02188594e-152,
       2.16452413e+233, 7.50052033e+247, 6.98907523e+096, 7.69843824e+218,
       3.23097122e+174, 9.84214185e-154, 1.36723829e+161, 1.19346501e+243,
       1.94670285e+227, 2.21366476e+214, 8.95533289e-106, 8.75378213e+247,
       1.87673453e-152, 2.50722129e-310])})



",1,1213,"It is not a bug, it has to do with the numerical precision of the integration, and the fact that you are integrating a function that is (almost) 0 in most of the interval.
From the docs:


  Be aware that pulse shapes and other sharp features as compared to the
  size of the integration interval may not be integrated correctly using
  this method.


Based on your output, the function is using only two (last=2) intervals, evaluating values for rlist=(2.06145321e-045, 0.00000000e+000, ..) on each (see the docs for more detail on the output)

You can add points to the interval to force the routine to use points closer to the left limit.

a = quad(lambda x: np.exp(-x), 0, 1e9, points=np.logspace(-10,3,10))
print(a)
(0.9999999999999997, 2.247900608926337e-09)


Adding to the explanation (thanks to @norok2): Note that points is a sequence of break points in the bounded integration interval where local difficulties of the integrand may occur (e.g., singularities, discontinuities). In this case, I'm not using it to point out discontinuities, but rather to force quad to perform more integration steps near the left boundary, using a log-spaced interval since a I have an exponential function (this is of course arbitrary and for this function, since I know its shape).
","There is no need to convert your integral into one over a (very large) interval. There is a specific integration scheme for integrals of the form

,

namely Gauss-Laguerre quadrature. It's also included in quadpy (a project of mine). Simply try out

import numpy
import quadpy

scheme = quadpy.e1r.gauss_laguerre(1)

val = scheme.integrate(lambda x: numpy.ones(x.shape[1:]))

print(val)


1.0

",
SciPy unexpected behavior,https://stackoverflow.com/questions/61594854,Packaged dags - Airflow can&#39;t find module installed,"I'm trying to use apache airlfow with packaged dags (https://airflow.apache.org/docs/stable/concepts.html#packaged-dags).

I've written my code as a python package and obviously my code depends on other popular libraries such as numpy, scipy etc.

EDIT:
This is setup.py of my custom python package:

from setuptools import setup, find_packages
from pathlib import Path
from typing import List

import distutils.text_file

def parse_requirements(filename: str) -&gt; List[str]:
    """"""Return requirements from requirements file.""""""
    # Ref: https://stackoverflow.com/a/42033122/
    return distutils.text_file.TextFile(filename=str(Path(__file__).with_name(filename))).readlines()


setup(name='classify_business',
      version='0.1',
      python_requires=""&gt;=3.6"",
      description='desc',
      url='https://urlgitlab/datascience/classifybusiness',
      author='Marco fumagalli',
      author_email='marco.fumagalli@mycompany.com',
      packages = find_packages(),
      license='MIT',
      install_requires=
      parse_requirements('requirements.txt'),
      zip_safe=False,
      include_package_data=True)


requirements.txt contains packages ( vertica_python, pandas, numpy etc) along with their version needed for my code.

I wrote a litte shell script based on the one provied in the doc:

set -eu -o pipefail

if [ $# == 0 ]; then
    echo ""First param should be /srv/user_name/virtualenvs/name_virtual_env""
    echo ""Second param should be name of temp_directory""
    echo ""Third param directory should be git url""
    echo ""Fourth param should be dag zip name, i.e dag_zip.zip to be copied into AIRFLOW__CORE__DAGS__FOLDER""
    echo ""Fifth param should be package name, i.e classify_business""
fi


venv_path=${1}
dir_tmp=${2}
git_url=${3}
dag_zip=${4}
pkg_name=${5}



python3 -m venv $venv_path
source $venv_path/bin/activate
mkdir $dir_tmp
cd $dir_tmp

python3 -m pip install --prefix=$PWD git+$git_url

zip -r $dag_zip *
cp $dag_zip $AIRFLOW__CORE__DAGS_FOLDER

rm -r $dir_tmp


The shell will install my package along with dependencies directly from gitlab, zip and then move to the dags folder.

This is the content of the folder tmp_dir before being zipped.

bin  
lib  
lib64  
predict_dag.py  
train_dag.py


Airflow doesn't seem to be able to import package installed in lib or lib64.
I'm getting this error


  ModuleNotFoundError: No module named 'vertica_python'


I even tried to move my custom package outside of lib:

bin
my_custom_package
lib  
lib64  
predict_dag.py  
train_dag.py


But still getting same error.

PS: One of the problem I think relies on how to use pip to install package in a specific location.
Airflow example use --install-option=""--install-lib=/path/"" but it's unsupported:


  Location-changing options found in --install-option: ['--install-lib']
  from command line. This configuration may cause unexpected behavior
  and is unsupported. pip 20.2 will remove support for this
  functionality. A possible replacement is using pip-level options like
  --user, --prefix, --root, and --target. You can find discussion regarding this at https://github.com/pypa/pip/issues/7309.


Using --prefix leads to a structure like above, with module not found error.

Using --target leads to every package installed in the directory specified.
In this case I have a pandas related error

C extension: No module named 'pandas._libs.tslibs.conversion' not built


I guess that it's related to dynamic libraries that should be available at a system level?

Any hint?

Thanks
",1,5544,"Ciao Marco,
I know this is an old question, but I had to go through the very same process and what worked for me was to use:
pip install -r ../requirements_dag.txt --target=""$PWD""

The same works for packages hosted on git. The key difference is the use of --target rather than --prefix.
","The Airflow documentation page you're referring to says this about packaged DAGs:


  To allow this you can create a zip file that contains the DAG(s) in the root of the zip file and have the extra modules unpacked in directories.


The way I interpret this is different from yours. I don't think Airflow handles these packaged DAGs as a real python package. It just seems like a custom zip folder that will be added to your DAGs folder. So the lib or lib64 folders you have are probably not real python modules (they don't have a __init__.py file). That's why they say that ""the extra modules should be unpacked in directories"".

Look at the example zip file they give:

my_dag1.py
my_dag2.py
package1/__init__.py
package1/functions.py


package1 has a __init__.py file. So in your case, your vertica_python library should be directly importable like this:

my_custom_package
vertica_python/
predict_dag.py  
train_dag.py


However, I don't think you should do this. I have the impression that the modules that you should add here are your own developed modules, not third party libraries.

So I suggest that you install the libraries you need to run your packaged DAGs beforehand.
",
SciPy unexpected behavior,https://stackoverflow.com/questions/29103540,Scipy Sparse: Unexpected Identity Behavior,"Following is my attempt to create a sparse matrix that has c as its diagonal. I know there is also alternative methods for this, but I'm rather curious why the following code is not working as expected:

import numpy as np
import scipy.sparse as sparse

c = np.arange(0,5)
&gt;&gt;&gt; np.identity(5)*c
array([[ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  2.,  0.,  0.],
       [ 0.,  0.,  0.,  3.,  0.],
       [ 0.,  0.,  0.,  0.,  4.]])
&gt;&gt;&gt; sparse.identity(5)*c
array([ 0.,  1.,  2.,  3.,  4.])
#expected output:
&lt;5x5 sparse matrix of type '&lt;type 'numpy.float64'&gt;'
with 5 stored elements (1 diagonals) in DIAgonal format&gt;
# and (sparse.identity(5)*c).todense() == np.identity(5)*c

",0,98,"There are 2 common types of array multiplication, element-by-element, and matrix.

In MATLAB, * is the matrix version, .* is the element by element one.

In numpy, * is element by element (with broadcasting), 'np.dot' is the basic form of matrix multiplication.  Python developers have approved @ as an operator that could be used for matrix multiplication (eventually).

For numpy matrix subclass, * is the matrix multiplication, np.multiply is used for element-by-element.  (np.multiply also works for ndarray.)  

scipy follows the np.matrix convention.  * is the matrix multiplication.  sparse.identity(5).multiply(c) does the element by element multiplication (though it returns a np.matrix, not a sparse one).

As to why, it comes down to conventions that the developers were used to.  For linear algebra problems, matrix multiplication is common, hence it's use in sparse.  np.matrix copies the MATLAB conventions.  MATLAB was created to give access to FORTRAN matrix libraries.

In physics there's another convention, Einstein notation.  This is a generalized matrix multiplication, extended to more dimensions. np.einsum does this. It can implement element by element multiplication, though at its core it uses a 'sum of products' method.  But it's not been implemented for np.matrix or sparse (and really isn't needed since those are always 2d).

Notice how similar the specifications are for your example

np.einsum('ij,j-&gt;ij',np.identity(5),c) # element by element
np.einsum('ij,j-&gt;i',np.identity(5),c)  # matrix (sum on j)

","In the expression sparse.identity(5)*c, the multiplication operator of the sparse matrix is used, which is the algebraic matrix multiplication (i.e. a matrix times a vector gives a vector).

You can create a sparse diagonal matrix with a given diagonal using scipy.sparse.diags:

In [18]: from scipy import sparse

In [19]: c = np.arange(5)

In [20]: d = sparse.diags(c, 0)

In [21]: d
Out[21]: 
&lt;5x5 sparse matrix of type '&lt;type 'numpy.float64'&gt;'
    with 5 stored elements (1 diagonals) in DIAgonal format&gt;

In [22]: d.A
Out[22]: 
array([[ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  2.,  0.,  0.],
       [ 0.,  0.,  0.,  3.,  0.],
       [ 0.,  0.,  0.,  0.,  4.]])

",
SciPy unexpected behavior,https://stackoverflow.com/questions/67142952,scipy weird unexpected behavior curve_fit large data set for sin wave,"For some reason when I am trying to large amount of data to a sin wave it fails and fits it to a horizontal line. Can somebody explain?
Minimal working code:
import numpy as np
import matplotlib.pyplot as plt
from scipy import optimize
# Seed the random number generator for reproducibility
import pandas

np.random.seed(0)

# Here it work as expected
# x_data = np.linspace(-5, 5, num=50)
# y_data = 2.9 * np.sin(1.05 * x_data + 2) + 250 + np.random.normal(size=50)

# With this data it breaks
x_data = np.linspace(0, 2500, num=2500)
y_data = -100 * np.sin(0.01 * x_data + 1) + 250 + np.random.normal(size=2500)

# And plot it

plt.figure(figsize=(6, 4))
plt.scatter(x_data, y_data)


def test_func(x, a, b, c, d):
    return a * np.sin(b * x + c) + d

# Used to fit the correct function
# params, params_covariance = optimize.curve_fit(test_func, x_data, y_data)

# making some guesses
params, params_covariance = optimize.curve_fit(test_func, x_data, y_data,
                                               p0=[-80, 3, 0, 260])

print(params)
plt.figure(figsize=(6, 4))
plt.scatter(x_data, y_data, label='Data')
plt.plot(x_data, test_func(x_data, *params),
         label='Fitted function')

plt.legend(loc='best')

plt.show()




Does anybody know, how to fix this issue. Should I use a different fitting method not least square? Or should I reduce the number of data points?
",0,184,"Given your data, you can use the more robust lmfit instead of scipy.
In particular, you can use SineModel (see here for details).
SineModel in lmfit is not for ""shifted"" sine waves, but you can easily deal with the shift doing
y_data_offset = y_data.mean()
y_transformed = y_data - y_data_offset
plt.scatter(x_data, y_transformed)
plt.axhline(0, color='r')


Now you can fit to sine wave
from lmfit.models import SineModel

mod = SineModel()

pars = mod.guess(y_transformed, x=x_data)
out = mod.fit(y_transformed, pars, x=x_data)

you can inspect results with print(out.fit_report()) and plot results with
plt.plot(x_data, y_data, lw=7, color='C1')
plt.plot(x_data, out.best_fit+y_data_offset, color='k')
#           we add the offset ^^^^^^^^^^^^^


or with the builtin plot method out.plot_fit(), see here for details.
Note that in SineModel all parameters ""are constrained to be non-negative"", so your defined negative amplitude (-100) will be positive (+100) in the parameters fit results. So the phase too won't be 1 but +1 (PS: they call shift the phase)
print(out.best_values)

{'amplitude': 99.99631403054289,
 'frequency': 0.010001193681616227,
 'shift': 4.1400215410836605}

",,
SciPy unexpected result,https://stackoverflow.com/questions/24242660,pymc3 : Multiple observed values,,11,5730,,,
SciPy unexpected result,https://stackoverflow.com/questions/50831551,parallel/multithread differential evolution in python,"I'm trying to model a biochemical process, and I structured my question as an optimization problem, that I solve using  differential_evolution from scipy.
So far, so good, I'm pretty happy with the  implementation of a simplified model with 15-19 parameters.
I expanded the model and now, with 32 parameters, is taking way too long. Not  totally unexpected, but still an issue, hence the question.

I've seen:
- an almost identical question for R  Parallel differential evolution
- and a github issue https://github.com/scipy/scipy/issues/4864 on the topic  

but it would like to stay in python (the model is within a python pipeline), and the pull request did not lead to and officially accepted solution yet, although some options have been suggested.

Also, I can't parallelize the code within the function to be optimised because is a series of sequential calculations each requiring the result of the previous step. The ideal option would be to have something that evaluates some individuals in parallel and return them to the population.

Summing up:
- Is there any option within scipy that allows parallelization of differential_evolution that I dumbly overlooked? (Ideal solution)
- Is there a suggestion for an alternative algorithm in scipy that is either (way) faster in serial or possible to parallelize?
- Is there any other good package that offers parallelized differential evolution funtions? Or other applicable optimization methods?
- Sanity check: am I overloading DE with 32 parameter and I need to radically change approach?

PS
I'm a biologist, formal math/statistics isn't really my strenght, any formula-to-english translation would be hugely appreciated :)

PPS
As an extreme option I could try to migrate to R, but I can't code C/C++ or other languages.
",5,4707,"Scipy differential_evolution can now be used in parallel extremely easily, by specifying the workers:


  workers int or map-like callable, optional
  
  If workers is an int the population is subdivided into workers
  sections and evaluated in parallel (uses multiprocessing.Pool). Supply
  -1 to use all available CPU cores. Alternatively supply a map-like callable, such as multiprocessing.Pool.map for evaluating the
  population in parallel. This evaluation is carried out as
  workers(func, iterable). This option will override the updating
  keyword to updating='deferred' if workers != 1. Requires that func be
  pickleable.
  
  New in version 1.2.0.


scipy.optimize.differential_evolution documentation
","Thanks to @jp2011 for pointing to pygmo

First, worth noting the difference from pygmo 1, since the fist link on google still directs to the older version.

Second, Multiprocessing island are available only for python 3.4+

Third, it works. The processes I started when I first asked the question are still running while I write, the pygmo archipelago running an extensive test of all the 18 possible DE variations present in saDE made in less than 3h. The compiled version using Numba as suggested here https://esa.github.io/pagmo2/docs/python/tutorials/coding_udp_simple.html will probably finish even earlier. Chapeau.

I personally find it a bit less intuitive than the scipy version, given the need to build a new class (vs a signle function in scipy) to define the problem but is probably just a personal preference. Also, the mutation/crossing over parameters are defined less clearly, for someone approaching DE for the first time might be a bit obscure.
But, since serial DE in scipy just isn't cutting it, welcome pygmo(2).

Additionally I found a couple other options claiming to parallelize DE. I didn't test them myself, but might be useful to someone stumbling on this question.  

Platypus, focused on multiobjective evolutionary algorithms 
https://github.com/Project-Platypus/Platypus

Yabox
https://github.com/pablormier/yabox

from Yabox creator a detailed, yet IMHO crystal clear, explaination of DE
https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/
","I suggest the batch mode of PyFDE.
https://pythonhosted.org/PyFDE/tutorial.html#batch-mode
In batch mode, the fitness function will be called only once per iteration to evaluate the fitness of all the population.
The example w/o the batch mode:
import pyfde
from math import cos, pi
import time
import numpy

t1=time.time()
def fitness(p):
    x, y = p[0], p[1]
    val = 20 + (x**2 - 10*cos(2*pi*x)) + (y**2 - 10*cos(2*pi*y))
    return -val
    
solver = pyfde.ClassicDE(fitness, n_dim=2, n_pop=40, limits=(-5.12, 5.12))
solver.cr, solver.f = 0.9, 0.45
best, fit = solver.run(n_it=150)
t2=time.time()
print(""Estimates: "",best)
print(""Normal mode elapsed time (s): "",t2-t1)

The batch mode example:
t1=time.time()
def vec_fitness(p,fit):
    x, y = numpy.array(p[:,0]), numpy.array(p[:,1])
    val = 20 + (x**2 - 10*numpy.cos(2*pi*x)) + (y**2 - 10*numpy.cos(2*pi*y))
    fit[:] = -val
    
solver = pyfde.ClassicDE(vec_fitness, n_dim=2, n_pop=40, limits=(-5.12, 5.12), batch=True)
solver.cr, solver.f = 0.9, 0.45
best, fit = solver.run(n_it=150)
t2=time.time()
print(""Estimates: "",best)
print(""Batch mode elapsed time (s): "",t2-t1)

The output is:
Estimates:  [1.31380987e-09 1.12832169e-09]
Normal mode elapsed time (s):  0.015959978103637695
Estimates:  [2.01733383e-10 1.23826873e-10]
Batch mode elapsed time (s):  0.006017446517944336
############################################################
It's 1.5x faster, but only for a simple question. You can see &gt;10x faster for a complex question.
The code runs on a single CPU core (no multi-processing), and the performance improvement comes from the use of vectorization and MIMD (multiple instruction, multiple data). Combining vectorization and parallel/multi-processing will result in a double-improvement.
"
SciPy unexpected result,https://stackoverflow.com/questions/28321286,pandas rolling_quantile bug?,"i recently bumped an unexpected issue with pandas rolling funcs. rolling_quantile for example:

&gt;&gt; row = 10
&gt;&gt; col = 5
&gt;&gt; idx = pd.date_range(20100101,periods=row,freq='B')
&gt;&gt; a = pd.DataFrame(np.random.rand(row*col).reshape((row,-1)),index=idx)
&gt;&gt; a
                   0           1           2           3           4
2010-01-01  0.341434    0.497274    0.596341    0.259909    0.872207
2010-01-04  0.222653    0.056723    0.064019    0.936307    0.785647
2010-01-05  0.179067    0.647165    0.931266    0.557698    0.713282
2010-01-06  0.049766    0.259756    0.945736    0.380948    0.282667
2010-01-07  0.385036    0.517609    0.575958    0.050758    0.850735
2010-01-08  0.628169    0.510453    0.325973    0.263361    0.444959
2010-01-11  0.099133    0.976571    0.602235    0.181185    0.506316
2010-01-12  0.987344    0.902289    0.080000    0.254695    0.753325
2010-01-13  0.759198    0.014548    0.139858    0.822900    0.251972
2010-01-14  0.404149    0.349788    0.038714    0.280568    0.197865

&gt;&gt; a.quantile([0.25,0.5,0.75],axis=0)
               0           1           2           3           4
0.25    0.189963    0.282264    0.094964    0.255999    0.323240
0.50    0.363235    0.503864    0.450966    0.271964    0.609799
0.75    0.572164    0.614776    0.600761    0.513510    0.777567

&gt;&gt; np.percentile(a,[25,50,75],axis=0)
[array([ 0.18996316,  0.28226404,  0.09496441,  0.25599853,  0.32323997]),
 array([ 0.36323529,  0.50386356,  0.45096554,  0.27196429,  0.60979881]),
 array([ 0.57216415,  0.61477607,  0.6007611 ,  0.51351021,  0.7775667 ])]

&gt;&gt; pd.rolling_quantile(a,row,0.25).tail(1)
                   0           1       2           3           4
2010-01-14  0.179067    0.259756    0.08    0.254695    0.282667


looks like pandas.DataFrame.quantile member func is consistent with the numpy.percentile func.  however the pandas.rolling_quantile func returns diff results.  reduce the row number to 5, the problem will be gone (all three methods return the same results).  any thoughts?

ps: i also tested rolling_std func which will ""random"" generate error with 10^-7 ~ 10^-8 scales for long (row-wise) pandas.DataFrames

python environment:


python 3.4.2
cython 0.21.1
numpy 1.8.2
scipy 0.14.0
pandas 0.15.1
statsmodels 0.6.0

",4,1150,,,
SciPy unexpected result,https://stackoverflow.com/questions/55428503,Scipy sparse.kron gives non-sparse matrix,"I am getting unexpected non-sparse results when using the kron method of Scipy's sparse module. Specifically, matrix elements that are equal to zero after performing the kronecker product are being kept in the result, and I'd like to understand what I should do to ensure the output is still fully sparse.

Here's an example of what I mean, taking the kronecker product of two copies of the identity:

import scipy.sparse as sp

s = sp.eye(2)

S = sp.kron(s,s)

S 
&lt;4x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
with 8 stored elements (blocksize = 2x2) in Block Sparse Row format&gt;

print(S)

(0, 0)  1.0
(0, 1)  0.0
(1, 0)  0.0
(1, 1)  1.0
(2, 2)  1.0
(2, 3)  0.0
(3, 2)  0.0
(3, 3)  1.0


The sparse matrix S should only contain the 4 (diagonal) non-zero entries, but here it also has other entries that are equal to zero. Any pointers on what I am doing wrong would be much appreciated.
",3,578,"In 

Converting from sparse to dense to sparse again decreases density after constructing sparse matrix

I point out that sparse.kron produces, by default a BSR format matrix.  That's what your display shows.  Those extra zeros are part of the dense blocks.

If you specify another format, kron will not produce  those zeros:

In [672]: sparse.kron(s,s,format='csr')                                         
Out[672]: 
&lt;4x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
    with 4 stored elements in Compressed Sparse Row format&gt;
In [673]: _.A                                                                   
Out[673]: 
array([[1., 0., 0., 0.],
       [0., 1., 0., 0.],
       [0., 0., 1., 0.],
       [0., 0., 0., 1.]])

",,
SciPy unexpected result,https://stackoverflow.com/questions/50932433,Scipy and Sklearn chi2 implementations give different results,"I an using sklearn.feature_selection.chi2 for feature selection and found out some unexpected results (check the code). Do anyone knows what is the reason or can point me to some documentation or pull request?

I include a comparison of the results I got and the expected ones obtained by hand and using scipy.stats.chi2_contingency.

The code:

import numpy as np
import pandas as pd
from scipy.stats import chi2_contingency
from sklearn.feature_selection import chi2, SelectKBest

x = np.array([[1, 1, 1, 0, 1], [1, 0, 1, 0, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 0], [0, 0, 0, 1, 1], [0, 0, 0, 1, 0]])
y = np.array([1, 1, 2, 2, 3, 3])

scores = []
for i in range(x.shape[1]):
    result = chi2_contingency(pd.crosstab(x[:, i], y))
    scores.append(result[0])

sel = SelectKBest(score_func=chi2, k=3)
sel.fit(x, y)

print(scores)
print(sel.scores_)
print(sel.get_support())


The results are:

[6., 2.4, 6.0, 6.0, 0.0] (Expected)
[4. 2. 2. 2. 0.] (Unexpected)
[ True  True False  True False]


Using scipy, it keeps features 0, 2, 3, while, with sklearn it keeps features 0,1,3.
",3,1485,"Yes, they do give different results. And I think you should trust the results from scipy, and reject the results from sklearn.
But let me provide details of my reasoning, because I could be wrong.
I lately observed a similar effect to what you describe, with a data set of 300 data points: the results of the two chi2 implementations differ indeed. In my case the difference was striking. I described the issue in details in this article , followed by this Cross Validated discussion thread and I also submitted a bug request to sklearn, available for review here.
The added value from my research, if any, seems to be that the results delivered by the scipy implementation seem correct, while the results from sklearn are incorrect. Please see the article for the details. But I only focused on my sample, so the conclusion may not be universally true. Sadly the source code analysis is beyond my capability, but I hope this input can help someone to possibly either improve the code, or disprove my reasoning if wrong.
",,
SciPy unexpected result,https://stackoverflow.com/questions/50142269,Why does scipy bessel root finding not return roots at zero?,"I am trying to use code which uses Bessel function zeros for other calculations. I noticed the following piece of code produces results that I consider unexpected.

    import scipy
    from scipy import special

    scipy.special.jn_zeros(1,2)


I would expect the result from this call to be

    array([0., 3.83170597])


instead of

    array([3.83170597, 7.01558667])


Is there a reason a reason why the root at x=0.0 is not being returned?

From what I can see the roots are symmetric along the x-axis except for any found at the origin, but I do not think this would be enough of a reason to leave off the root completely.

The computer I am using has python version 2.7.10 installed and is using scipy version 0.19.0

P.S. the following function is what I am trying to find the zeros of

    scipy.special.j1

",3,486,,,
SciPy unexpected result,https://stackoverflow.com/questions/46556376,Scipy&#39;s cut_tree() doesn&#39;t return requested number of clusters and the linkage matrices obtained with scipy and fastcluster do not match,"I'm doing an agglomerative hierarchical clustering (AHC) experiment using the fastcluster package in connection with scipy.cluster.hierarchy module functions, in Python 3, and I found a puzzling behaviour of the cut_tree() function.

I cluster data with no problem and get a linkage matrix, Z, using linkage_vector() with method=ward. Then, I want to cut the dendogram tree to get a fixed number of clusters (e.g. 33) and I do this properly using cut_tree(Z, n_clusters=33). (Remember that AHC is a deterministic method yielding a binary tree connecting all your datapoints, which sit at the leafs of the tree; you can look at this tree at any level to ""see"" the number of clusters you want in the end; all cut_tree() does is to return a set of 'n_cluster' integer labels from 0 to n_clusters - 1, attributed to every point of the dataset.)

I've done this many times in other experiments and I always get the number of clusters I request. The problem is that with this one dataset, when I ask cut_tree() for 33 clusters, it gives me only 32. I don't see why this is the case. Could it be a bug? Are you aware of any bug with cut_tree()? I tried to debug this behaviour and performed the same clustering experiment using scipy's linkage() function. With the resulting linkage matrix as input to cut_tree() I didn't get an unexpected number of clusters as output. I also verified that the linkage matrices output by the two methods are not equal.

The [dataset] I'm using consists of 10680 vectors, each with 20 dimensions. Check the following experiment:

import numpy as np
import fastcluster as fc
import scipy.cluster.hierarchy as hac
from scipy.spatial.distance import pdist

### *Load dataset (10680 vectors, each with 20 dimensions)*
X = np.load('dataset.npy')

### *Hierarchical clustering using traditional scipy method*
dists = pdist(X)
Z_1 = hac.linkage(dists, method='ward')

### *Hierarchical clustering using optimized fastcluster method*
Z_2 = fc.linkage_vector(X, method='ward')

### *Comparissons*

## Are the linkage matrices equal?
print(""Z_1 == Z_2 ? "", np.allclose(Z_1, Z_2))

## Is scipy's cut_tree() returning the requested number of clusters when using Z_2?
print(""Req.\tGot\tequal?"")
for i in range(1,50):
    cut = hac.cut_tree(Z_2, i)
    uniq = len(np.unique(cut))
    print(i,""\t"",uniq,""\t"",i==uniq)

## The same as before, but in condensed form. When requesting cut_tree() for clusters
#  in the range [1,50] does it return wrong results at some point?
print(""Any problem cutting Z_1 for n_clusters in [1,50]? "", not np.all([len(np.unique(
                                      hac.cut_tree(Z_1, i)))==i for i in range(1,50)]))
print(""Any problem cutting Z_2 for n_clusters in [1,50]? "", not np.all([len(np.unique(
                                      hac.cut_tree(Z_2, i)))==i for i in range(1,50)]))

#Output:
#
#Z_1 == Z_2 ?  False
#
#Req.    Got     equal?
#1        1       True
#2        2       True
#3        3       True
#4        4       True
#5        5       True
#6        6       True
#7        7       True
#8        8       True
#9        9       True
#10       10      True
#11       11      True
#12       12      True
#13       13      True
#14       14      True
#15       15      True
#16       16      True
#17       17      True
#18       18      True
#19       19      True
#20       20      True
#21       21      True
#22       22      True
#23       23      True
#24       24      True
#25       25      True
#26       26      True
#27       27      True
#28       28      True
#29       29      True
#30       30      True
#31       31      True
#32       32      True
#33       32      False
#34       33      False
#35       34      False
#36       35      False
#37       36      False
#38       37      False
#39       38      False
#40       39      False
#41       40      False
#42       41      False
#43       42      False
#44       43      False
#45       44      False
#46       45      False
#47       46      False
#48       47      False
#49       48      False
#
#Any problem cutting Z_1 for n_clusters in [1,50]?  False
#Any problem cutting Z_2 for n_clusters in [1,50]?  True


You might have noticed the dataset contains 37 vectors with at least an exact copy, and counting all the copies there is a total 55 vectors with at least a copy in the dataset.

For inspection, I decided to plot the dendrogram tree up to a shallow depth level for the two linkage matrices, which you can see on the image bellow (Z_1 at the top and Z_2 at the bottom). Numbers inside parenthesis indicate the population contained bellow in that branch; numbers without parenthesis are leafs of the tree (the number is the index of the vector in the X matrix). One can see the only difference (at the plotted level) is at the branches marked with the red square, which coalesce at 0 distance as they contain overlapping vectors.



So, I ran the clustering procedures as shown in the previous code again, but this time with only the subset of the data containing the 55 vectors which have at least a copy. I obtained X_subset with:

uniqs, uniqs_indices, uniqs_count = np.unique(X, axis=0, return_index=True, return_counts=True)
duplicate_rows_indices = list( set(range(len(X))) - set(uniqs_indices) )
number_of_duplicate_rows = len(X)-len(uniqs) # 37

all_duplicate_rows = set()
for i in duplicate_rows_indices:
    _rows = set(np.where(X == X[i])[0])
    for j in _rows:
        all_duplicate_rows.add(j)

rows_with_at_least_a_copy = list(all_duplicate_rows)
number_of_rows_with_at_least_a_copy = len(rows_with_at_least_a_copy)  # 55

X_subset = X[rows_with_at_least_a_copy]


and my output this time was:

#Z_1 == Z_2 ?  False
#Req.    Got     equal?
#1        1       True
#2        2       True
#3        2       False
#4        3       False
#5        4       False
#6        5       False
#7        6       False
#8        7       False
#9        8       False
#10       9       False
#11       10      False
#12       11      False
#13       12      False
#14       13      False
#15       14      False
#16       15      False
#17       16      False
#18       17      False
#19       18      False
#20       20      True
#21       21      True
#22       22      True
#23       23      True
#24       24      True
#25       25      True
#26       26      True
#27       27      True
#28       28      True
#29       29      True
#30       30      True
#31       31      True
#32       32      True
#33       33      True
#34       34      True
#35       35      True
#36       36      True
#37       37      True
#38       38      True
#39       39      True
#40       40      True
#41       41      True
#42       42      True
#43       43      True
#44       44      True
#45       45      True
#46       46      True
#47       47      True
#48       48      True
#49       49      True
#Any problem cutting Z_1 for n_clusters in [1,50]?  False
#Any problem cutting Z_2 for n_clusters in [1,50]?  True


Thus, fastcluster and scipy are not returning the same results, and if it is only due to the overlapping points this could be acceptable because of the ambiguity of that clustering situation. But the problem is cut_tree() which sometimes doesn't return the requested number of clusters in these cases when given the linkage matrix obtained by linkage_vector(). How can this be fixed?

Library versions used: scipy '0.19.1', numpy '1.13.3', fastcluster '1.1.24'

Edit: It's also posted here: https://github.com/scipy/scipy/issues/7977.
",3,1203,,,
SciPy unexpected result,https://stackoverflow.com/questions/76011928,Python: np.linalg.eigvalsh returning negatve eigen values,"I have a 7x7 covariance matrix (represented as an numpy array).
t = np.array(
    [
        [1.4, 0.3, 0.4, 0.8, 0.4, 0.9, 0.3],
        [0.3, 1.3, 0.4, 2.3, 0.4, 2.4, 0.4],
        [0.4, 0.4, 1.3, 2.8, 0.4, 1.0, 0.3],
        [0.8, 2.3, 2.8, 9.5, 1.0, 7.0, 1.0],
        [0.4, 0.4, 0.4, 1.0, 1.1, 1.2, 0.3],
        [0.9, 2.4, 1.0, 7.0, 1.2, 7.7, 1.0],
        [0.3, 0.4, 0.3, 1.0, 0.3, 1.0, 0.5],
    ],
    dtype=np.float64,
)

I have checked this matrix to be symmetric.
np.allclose(t, t.T)
True

And np.linalg.svd returns valid non-negative singular values. However, np.linalg.eigvalsh, is returning a negative eigenvalue.
min(np.linalg.eigvalsh(t))
-0.06473876145336957

This doesnt make too much sense to me as I have checked that the column of the matrix are linearly independent (getting the reduced row echelon form of the matrix).
import sympy
reduced_form, inds = sympy.Matrix(t.values).rref()

I see similar issues where people have reported eigvalsh to return negative eigenvalue for a well behaved matrix but none of the suggestions have helped.
Examples:

Why is scipy's eigh returning unexpected negative eigenvalues?
numpy.cov or numpy.linalg.eigvals gives wrong results

I was wondering if anyone has faced a similar issue with np.linalg.eigvalsh and have any recommendations on how to solve it?
Thank you so much.
",2,295,"It's totally normal for symmetric matrices to have negative eigenvalues. A matrix being Hermitian only guarantees that its eigenvalues are real. It doesn't guarantee that the eigenvalues are positive.
It's not normal for a covariance matrix to have negative eigenvalues, meaning this isn't actually a covariance matrix, despite what you thought. Perhaps you rounded the entries of an actual covariance matrix?
",,
SciPy unexpected result,https://stackoverflow.com/questions/60172680,Scipy LDL decomposition returning unexpected result,"I have generated a random 5*5 matrix x like so:

&gt;&gt;&gt; x = np.random.randn(5,5)


And decomposed it using the scipy.linalg.ldl decomposition like so:

&gt;&gt;&gt; l, d, p = la.ldl(x)


Using l, d and p I want to return x. I thought I would be able to do the following:

&gt;&gt;&gt; l[p,:] @ d @ l[p,:].transpose() - x


But this does not give me zero as I would have expected. Can anyone explain where I am going wrong?

My aim is to obtain the lower diagonal matrix L such that x = LDL^T without the need for the row permutation matrix p, but I am very confused as to what scipy is giving as output.
",2,1211,"The LDL decomposition algorithm only applies for Hermitian/symmetric matrices. You are passing it a matrix with random values which is very unlikely to be symmetric. In addition, the matrix multiplication should be performed without applying the permutation matrix to the lower triangular matrices.

When a non-symmetric matrix is passed to scipy.linalg.ldl, only the lower or upper triangular part of the matrix is referenced, depending on the value of the lower keyword argument, which defaults to True. We can see the effects of this with np.isclose():

&gt;&gt;&gt; x = np.random.randn(5,5)
&gt;&gt;&gt; l, d, p = la.ldl(x)
&gt;&gt;&gt; np.isclose(l.dot(d).dot(l.T) - x, 0)
[[ True False False False False]
 [ True  True False False False]
 [ True  True  True False False]
 [ True  True  True  True False]
 [ True  True  True  True  True]]


Here we see that the upper triangular part of the matrix has been assumed to be symmetric, and so the algorithm has returned values that would be correct if that were the case.

Below, we pass la.ldl an actual symmetric matrix, and obtain the expected result.

&gt;&gt;&gt; x = np.array([[1, 2, 3],
                  [2, 4, 5],
                  [3, 5, 6]])
&gt;&gt;&gt; l, d, p = la.ldl(x)
&gt;&gt;&gt; print(np.isclose(l.dot(d).dot(l.T) - x, 0))
[[ True  True  True]
 [ True  True  True]
 [ True  True  True]]


If you're looking for a decomposition into LDL^T in general, without having permutations, this reduces the field of matrices even further. Your matrix also needs to be positive definite.

Here is an example with one such matrix:

&gt;&gt;&gt; x = np.array([[2, -1, 0],
                  [-1, 3, -1],
                  [0, -1, 4]])
&gt;&gt;&gt; l, d, p = la.ldl(x)
&gt;&gt;&gt; l
array([[ 1. ,  0. ,  0. ],
       [-0.5,  1. ,  0. ],
       [ 0. , -0.4,  1. ]])
&gt;&gt;&gt; d
array([[2. , 0. , 0. ],
       [0. , 2.5, 0. ],
       [0. , 0. , 3.6]])
&gt;&gt;&gt; p
array([0, 1, 2], dtype=int64)


As you can see, the permutation p is just [0, 1, 2], and l is already lower triangular.
",,
SciPy unexpected result,https://stackoverflow.com/questions/58212061,calculating euclidean distance using scipy giving unexpected results,"I want to calculate euclidean distance between X and Y, where X is not static. So, X is define by combination of range.:

a= np.arange(0.2,0.41,0.1)
b= np.arange(2,8,1)
c= np.arange(40,61,5)
d= np.arange(40,61,5)
e= np.arange(0,11,5)
f= np.arange(25,71,5)

from itertools import product
X = list(product(a,b,c,d,e,f))
X = np.around(X,2)

from scipy.spatial.distance import euclidean
dis = []
for x in np.nditer(X):
    d = euclidean(x,[0.2,2,40,40,0,25]) #Inset Y here
    dis.append(d)

min(dis)


Since in this case, to test if everything was working as I wanted, I input Y which is one of possible value of X. I expected my minimum distance to be 0, however in this case, it was not so (~47).
",2,65,"The problem is that you're iterating over every individual number in X, rather than just iterating over the rows. Just remove the call to np.nditer(X) and iterate over X directly

for x in np.nditer(X):


should just be 

for x in X:

",,
SciPy unexpected result,https://stackoverflow.com/questions/40050369,Different eigenvalues between scipy.sparse.linalg.eigs and numpy/scipy.eig,,2,3125,,,
SciPy unexpected result,https://stackoverflow.com/questions/32920920,Stepsize control of dopri5 integrator,,2,1979,,,
SciPy unexpected result,https://stackoverflow.com/questions/77945259,"slurm didn&#39;t execute my Python code after running a few lines but also didn&#39;t stop, whereas it worked well on my local Linux","my code:
from datasets import load_dataset
MAX_LEN = 512
dataset = load_dataset(""glue"",""mrpc"")
from transformers import AutoTokenizer
from transformers import RobertaTokenizerFast

#tokenizer =AutoTokenizer.from_pretrained(""bert-base-uncased"")
tokenizer = RobertaTokenizerFast.from_pretrained(""/data/home//raw_roberta/Roberta_Tokenizer"", max_length=MAX_LEN, padding='max_length', return_tensors='pt')

print(""mapped_dataset"")

mapped_dataset = dataset.map(lambda x: tokenizer(x[""sentence1""], x[""sentence2""], max_length = MAX_LEN, truncation=True, padding='max_length', return_tensors='pt'), batched=True)

print(""completeed mapped_dataset"")

from transformers import DataCollatorWithPadding
data_collator= DataCollatorWithPadding(tokenizer=tokenizer)

from transformers import AutoModelForSequenceClassification
from transformers import RobertaForMaskedLM

#model = AutoModelForSequenceClassification.from_pretrained(""bert-base-uncased"",num_labels = 2)
#model = AutoModelForSequenceClassification.from_pretrained(""data/home//raw_roberta/Roberta_Model/checkpoint-90000"", num_labels = 2)
#model = AutoModelForSequenceClassification.from_pretrained(""data/home//raw_roberta/Roberta_Model/checkpoint-90000"")
base_model = RobertaForMaskedLM.from_pretrained('/data/home//raw_roberta/Roberta_Model/checkpoint-90000').roberta

from transformers import TrainingArguments
print(base_model.config)

I run above code on my local linux,it only takes about 2 minutes to execute.the log:
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization.
The tokenizer class you load from this checkpoint is 'BertTokenizer'.
The class this function is called from is 'RobertaTokenizer'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization.
The tokenizer class you load from this checkpoint is 'BertTokenizer'.
The class this function is called from is 'RobertaTokenizerFast'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
mapped_dataset
completeed mapped_dataset
RobertaConfig {
  ""_name_or_path"": ""/data/home//raw_roberta/Roberta_Model/checkpoint-90000"",
  ""architectures"": [
    ""RobertaForMaskedLM""
  ],
  ""attention_probs_dropout_prob"": 0.1,
  ""bos_token_id"": 0,
  ""classifier_dropout"": null,
  ""eos_token_id"": 2,
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 768,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 3072,
  ""layer_norm_eps"": 1e-12,
  ""max_position_embeddings"": 514,
  ""model_type"": ""roberta"",
  ""num_attention_heads"": 12,
  ""num_hidden_layers"": 12,
  ""pad_token_id"": 1,
  ""position_embedding_type"": ""absolute"",
  ""torch_dtype"": ""float32"",
  ""transformers_version"": ""4.33.2"",
  ""type_vocab_size"": 1,
  ""use_cache"": true,
  ""vocab_size"": 52000
}


but when I upload my code to slurm, it run for 4 hours,only get these log:
/data/home//anaconda3/envs/py38v1/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.24.4)
  warnings.warn(f""A NumPy version &gt;={np_minversion} and &lt;{np_maxversion} is required for this version of ""
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'BertTokenizer'. 
The class this function is called from is 'RobertaTokenizer'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'BertTokenizer'. 
The class this function is called from is 'RobertaTokenizerFast'.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
slurmstepd: error: *** JOB xxxxx ON compute-9-0 CANCELLED AT 2024-02-05T08:35:30 DUE TO TIME LIMIT ***

this problem truly confuses me,anyone know how to fix it? thanks!
",1,42,"check the python, scipy and numpy versions on the local workstation and the computing node(s) on the cluster. If you can create conda envs on the cluster, create a conda environment with just the python + relevant libs. First on the local workstation to check that it works, then replicate it on HPC.
",,
SciPy unexpected result,https://stackoverflow.com/questions/76667318,conda environment.yaml package conflict,"When I'm trying to build a git project, the environment,yml won't work properly. It seems like some package conflict, but other user doesn't seem to run into this issue, and I couldn't figure out a solution. My conda version is conda 4.8.2,and down below is the environment.yaml, requirment.txt and error
environment.yml:
name: oneposeplus
channels:
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - python=3.7
  - pytorch=1.8.0
  - torchvision=0.9.1
  - cudatoolkit=10.1
  - ipython
  - tqdm
  - matplotlib
  - pylint
  - conda-forge::jupyterlab
  - conda-forge::h5py=3.1.0
  - conda-forge::loguru=0.5.3
  - conda-forge::scipy
  - conda-forge::numba
  - conda-forge::ipdb
  - conda-forge::albumentations=0.5.1
  - pip
  - pip:
    - -r requirements.txt

requirment.txt:
pytorch-lightning==1.5.10
ray==1.13.0
aiohttp==3.7
aioredis==1.3.1
pydegensac==0.1.2
opencv_python==4.4.0.46
yacs&gt;=0.1.8
pytorch_memlab
joblib
pytorch3d
open3d
einops==0.3.0
kornia==0.4.1
autopep8
pickle5==0.0.11
timm&gt;=0.3.2
hydra-core
omegaconf
pycocotools
wandb
rich
transforms3d
natsort
plyfile
pycolmap==0.3.0

error:
rvl224@eervl224:~/OnePose_Plus_Plus-main$ conda env create -f environment.yaml
Collecting package metadata (repodata.json): done
Solving environment: \ 
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
Examining conflict for pylint numba python matplotlib jupyterlab ipython tqdm scExamining conflict for pylint numba python matplotlib torchvision jupyterlab ipyExamining conflict for pylint numba python matplotlib torchvision jupyterlab ipyExamining conflict for pylint jupyterlab ipython: : 20it [06:56, 20.25s/it]     Examining conflict for albumentations numba matplotlib torchvision h5py scipy pyExamining conflict for albumentations numba matplotlib torchvision h5py scipy pyExamining conflict for albumentations torchvision: : 24it [08:51,  8.89s/it]    Examining conflict for albumentations pytorch torchvision: : 25it [09:21, 37.46sExamining conflict for albumentations pytorch torchvision: : 26it [09:21, 35.40sExamining conflict for albumentations h5py: : 26it [09:35, 35.40s/it]           Examining conflict for albumentations matplotlib torchvision: : 27it [09:41, 28.Examining conflict for albumentations matplotlib torchvision: : 28it [09:41, 21.Examining conflict for albumentations matplotlib: : 28it [10:08, 21.91s/it]     Examining conflict for albumentations numba matplotlib torchvision scipy: : 29itExamining conflict for albumentations numba matplotlib torchvision scipy: : 30itExamining conflict for albumentations numba python matplotlib torchvision h5py cExamining conflict for albumentations numba python matplotlib torchvision h5py cExamining conflict for albumentations pytorch: : 31it [11:39, 25.74s/it]        failed                                                                  

# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

    Traceback (most recent call last):
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/exceptions.py"", line 1079, in __call__
        return func(*args, **kwargs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/cli/main.py"", line 80, in do_call
        exit_code = getattr(module, func_name)(args, parser)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/cli/main_create.py"", line 111, in execute
        result[installer_type] = installer.install(prefix, pkg_specs, args, env)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/installers/conda.py"", line 32, in install
        prune=getattr(args, 'prune', False), update_modifier=UpdateModifier.FREEZE_INSTALLED)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 117, in solve_for_transaction
        should_retry_solve)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 158, in solve_for_diff
        force_remove, should_retry_solve)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 281, in solve_final_state
        ssc = self._run_sat(ssc)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 808, in _run_sat
        should_retry_solve=ssc.should_retry_solve
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 1318, in solve
        self.find_conflicts(specs, specs_to_add, history_specs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 347, in find_conflicts
        bad_deps = self.build_conflict_map(specs, specs_to_add, history_specs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 507, in build_conflict_map
        root, search_node, dep_graph, num_occurances)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 369, in breadth_first_search_for_dep_graph
        last_spec = MatchSpec.union((path[-1], target_paths[-1][-1]))[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 481, in union
        return cls.merge(match_specs, union=True)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 475, in merge
        reduce(lambda x, y: x._merge(y, union), group) if len(group) &gt; 1 else group[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 475, in &lt;lambda&gt;
        reduce(lambda x, y: x._merge(y, union), group) if len(group) &gt; 1 else group[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 502, in _merge
        final = this_component.union(that_component)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 764, in union
        return '|'.join(options)
    TypeError: sequence item 0: expected str instance, Channel found

`$ /home/rvl224/anaconda3/bin/conda-env create -f environment.yaml`

  environment variables:
                 CIO_TEST=&lt;not set&gt;
        CMAKE_PREFIX_PATH=/opt/ros/noetic
  CONDA_AUTO_UPDATE_CONDA=false
                CONDA_EXE=/home/rvl224/anaconda3/bin/conda
         CONDA_PYTHON_EXE=/home/rvl224/anaconda3/bin/python
               CONDA_ROOT=/home/rvl224/anaconda3
              CONDA_SHLVL=0
            DEFAULTS_PATH=/usr/share/gconf/ubuntu.default.path
          LD_LIBRARY_PATH=/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-
                          gnu:/usr/local/cuda/lib64:
           MANDATORY_PATH=/usr/share/gconf/ubuntu.mandatory.path
                     PATH=/home/rvl224/anaconda3/bin:/home/rvl224/anaconda3/condabin:/opt/ros/no
                          etic/bin:/usr/local/cuda/bin:/home/rvl224/.local/bin:/home/rvl224/.loc
                          al/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/u
                          sr/games:/usr/local/games:/snap/bin
          PKG_CONFIG_PATH=/opt/ros/noetic/lib/pkgconfig:/opt/ros/noetic/lib/x86_64-linux-
                          gnu/pkgconfig
               PYTHONPATH=/opt/ros/noetic/lib/python3/dist-packages
       REQUESTS_CA_BUNDLE=&lt;not set&gt;
         ROS_PACKAGE_PATH=/opt/ros/noetic/share
            SSL_CERT_FILE=&lt;not set&gt;
               WINDOWPATH=2

     active environment : None
            shell level : 0
       user config file : /home/rvl224/.condarc
 populated config files : /home/rvl224/.condarc
          conda version : 4.8.2
    conda-build version : 3.18.11
         python version : 3.7.6.final.0
       virtual packages : __cuda=12.0
                          __glibc=2.31
       base environment : /home/rvl224/anaconda3  (writable)
           channel URLs : https://conda.anaconda.org/conda-forge/linux-64
                          https://conda.anaconda.org/conda-forge/noarch
                          https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /home/rvl224/anaconda3/pkgs
                          /home/rvl224/.conda/pkgs
       envs directories : /home/rvl224/anaconda3/envs
                          /home/rvl224/.conda/envs
               platform : linux-64
             user-agent : conda/4.8.2 requests/2.22.0 CPython/3.7.6 Linux/5.15.0-76-generic ubuntu/20.04.4 glibc/2.31
                UID:GID : 1000:1000
             netrc file : None
           offline mode : False


An unexpected error has occurred. Conda has prepared the above report.

If submitted, this report will be used by core maintainers to improve
future releases of conda.
Would you like conda to send this report to the core maintainers?

[y/N]: 
Timeout reached. No report sent.

down below are attempt updates to fix the error:
installing micromamba:
## install micromamba
curl micro.mamba.pm/install.sh | bash
export MAMBA_ROOT_PREFIX=/home/rvl224/anaconda3
micromamba update -n base conda

micromamba create -n oneposeplus -f environment.yaml

stuck at:
pkgs/main/linux-64                                            No change
pkgs/r/linux-64                                               No change
pkgs/r/noarch                                                 No change
pkgs/main/noarch                                              No change
pytorch/linux-64                                              No change
pytorch/noarch                                                No change
conda-forge/noarch                                  11.7MB @   2.3MB/s  5.2s
conda-forge/linux-64                                29.2MB @   4.4MB/s  6.9s

install mamba
#install mamba
micromamba install -c conda-forge mamba --root-prefix=/home/rvl224/anaconda3 -n base

mamba create -n oneposeplus -f environment.yaml

error:
Looking for: ['environment.yaml']

warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
pkgs/main/noarch                                   851.4kB @   1.3MB/s  0.7s
pkgs/r/linux-64                                      1.4MB @   1.6MB/s  0.9s
pkgs/r/noarch                                        1.3MB @   1.0MB/s  0.6s
pkgs/main/linux-64                                 @   2.5MB/s  2.4s
conda-forge/noarch                                 @   3.0MB/s  4.6s
conda-forge/linux-64                               @   4.8MB/s  7.4s
Encountered problems while solving:
  - nothing provides requested environment.yaml

tried installing packages one by one in a test environmemt,down below are some error while installing:
mamba install -n test_environment cudatoolkit=10.1 --yes

Encountered problems while solving:
  - nothing provides pytorch 1.10.2 cpu_py37h76afcab_0 needed by pytorch-cpu-1.10.2-cpu_py37h718b53a_0


mamba install -n test_environment ""conda-forge::h5py=3.1.0"" --yes

Encountered problems while solving:
  - package pulseaudio-daemon-16.1-ha8d29e2_3 requires openssl &gt;=3.1.0,&lt;4.0a0, but none of the providers can be installed

mamba install -n test_environment -c conda-forge -c pytorch -c defaults --file requirements.txt --yes


Encountered problems while solving:
  - nothing provides requested ray 1.13.0
  - nothing provides requested pydegensac 0.1.2
  - nothing provides requested opencv_python 4.4.0.46
  - nothing provides requested pytorch_memlab
  - nothing provides requested pytorch3d
  - nothing provides requested open3d
  - nothing provides requested kornia 0.4.1
  - package pycolmap-0.3.0-cpu_py39h5202583_1 requires python &gt;=3.9,&lt;3.10.0a0, but none of the providers can be installed

change into:
cudatoolkit=10.2
h5py (without version)

still figuring what to do with requirement.txt
",1,986,"Had to manually install every package,idk why environment.yml doesn't work
mamba create -n oneposeplus python=3.7 --yes
conda activate oneposeplus

pip install torch==1.8.0 --no-input
pip install torchvision==0.9.1 --no-input
pip install matplotlib --no-input
pip install ipython --no-input
pip install tqdm --no-input
pip install pylint --no-input
pip install jupyterlab --no-input
pip install ""h5py==3.1.0"" --no-input
pip install loguru==0.5.3 --no-input
pip install scipy --no-input
pip install numba --no-input
pip install ipdb --no-input
pip install ""albumentations==0.5.1"" --no-input

pip install pytorch-lightning==1.5.10 --no-input
pip install ray==1.13.0 --no-input
pip install aiohttp==3.7 --no-input
pip install aioredis==1.3.1 --no-input
pip install pydegensac==0.1.2 --no-input
pip install opencv-python==4.4.0.46 --no-input
pip install ""yacs&gt;=0.1.8"" --no-input
pip install pytorch_memlab --no-input
pip install joblib --no-input
pip install pytorch3d --no-input
pip install open3d --no-input
pip install ""einops==0.3.0"" --no-input
pip install ""kornia==0.4.1"" --no-input
pip install autopep8 --no-input
pip install ""pickle5==0.0.11"" --no-input
pip install ""timm&gt;=0.3.2"" --no-input
pip install hydra-core --no-input
pip install omegaconf --no-input
pip install pycocotools --no-input
pip install wandb --no-input
pip install rich --no-input
pip install transforms3d --no-input
pip install natsort --no-input
pip install plyfile --no-input
pip install ""pycolmap==0.3.0"" --no-input

Thanks for @merv for helping.
",,
SciPy unexpected result,https://stackoverflow.com/questions/68080031,numpy.any(axis=i) for scipy.sparse,"import numpy
a = numpy.array([
    [0, 1, 0, 0],
    [1, 0, 0, 0],
    [0, 0, 1, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0],
])
numpy.any(a, axis=0)
numpy.any(a, axis=1)

produces
array([ True,  True,  True, False])
array([ True,  True,  True, False, False])

However, after
from scipy import sparse
a = sparse.csr_matrix(a)

the same numpy.any(a, axis) calls produces
&lt;5x4 sparse matrix of type '&lt;class 'numpy.intc'&gt;'
        with 3 stored elements in Compressed Sparse Row format&gt;

and
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""&lt;__array_function__ internals&gt;"", line 5, in any
  File ""C:\Users\user\.conda\envs\py385\lib\site-packages\numpy\core\fromnumeric.py"", line 2330, in any
    return _wrapreduction(a, np.logical_or, 'any', axis, None, out, keepdims=keepdims)
  File ""C:\Users\user\.conda\envs\py385\lib\site-packages\numpy\core\fromnumeric.py"", line 87, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
numpy.AxisError: axis 1 is out of bounds for array of dimension 0

Of course, a is actually so large a sparse matrix that converting to normal numpy array is not an option.  How can I acquire the same (or equivalent) result for a csr_matrix and other scipy.sparse matrices?
ADDED:
According to Usage information in official scipy documentation,

Despite their similarity to NumPy arrays, it is strongly discouraged to use NumPy functions directly on these matrices because NumPy may not properly convert them for computations, leading to unexpected (and incorrect) results. If you do want to apply a NumPy function to these matrices, first check if SciPy has its own implementation for the given sparse matrix class, or convert the sparse matrix to a NumPy array (e.g., using the toarray() method of the class) first before applying the method.

I'm looking for ""its own implementation"" or equivalent.
",1,543,"you can use sum instead of any on bool arrays
import numpy
a = numpy.array([
    [0, 1, 0, 0],
    [1, 0, 0, 0],
    [0, 0, 1, 0],
    [0, 0, 0, 0],
    [0, 0, 0, 0],
])

from scipy import sparse
a = sparse.csr_matrix(a.astype(bool))
# Use sum instead of any on a bool array
print(a.sum(axis=0).astype(bool))
print(a.sum(axis=1).flatten().astype(bool))

output:
[[ True  True  True False]]
[[ True  True  True False False]]

If you want to do 'all' that would be a little tricky since scipy doesn't appear to have an implementation for 'prod'.
But this post has an answer for that case.
",,
SciPy unexpected result,https://stackoverflow.com/questions/66640632,Python Kolmogorov-Smirnov (KS) Test Inconsistent Results,"I am trying to compute the KS test specifying the CDF as a array, however, I encountered unexpected results. Upon further evaluation, I found different results based on whether I specified the CDF as a callable, string or array. My code is as follows:
import scipy.stats as st

random_variables = st.norm.rvs(loc=1, scale=1,size=1000000)
cdf_data = st.norm.cdf(random_variables, loc=1,scale=1)
params = st.norm.fit(data=random_variables)
display(params)
print('\n')

#test 1
out = kstest(rvs=random_variables,cdf='norm',args=params)
display(out, out[0], out[1])
print('\n')

#test 2
out = kstest(rvs=random_variables,cdf=st.norm.cdf,args=params)
display(out, out[0], out[1])
print('\n')

#test 3
out = kstest(rvs=random_variables,cdf=cdf_data)
display(out, out[0], out[1])

The results from this code are:
(1.0004825310590526, 0.9996641807017618)


KstestResult(statistic=0.0007348981302804924, pvalue=0.6523439724424506)
0.0007348981302804924
0.6523439724424506


KstestResult(statistic=0.0007348981302804924, pvalue=0.6523439724424506)
0.0007348981302804924
0.6523439724424506


KstestResult(statistic=0.500165, pvalue=0.0)
0.500165
0.0

Given the large sample data is compared against its the exact distribution from which the sample was generated, I expect a failure to reject the null hypothesis. This is the case in test 1 and 2, but it is not the case in test 3. I want to be able to replicate this test using an array argument for the ""cdf"" argument. Any help as to what I am doing wrong for test 3 would be very helpful. My numpy is version 1.19.2 and scipy is 1.5.2. Thank you!
",1,667,"I think there are two things that may be contributing to your confusion.

I don't think you want to be comparing to cdf_data = st.norm.cdf(random_variables, loc=1,scale=1). This is returning the value of the cumulative distribution function at all the x values of random-variables. In a KS test you are comparing two distributions, and your cdf_data and random_variable are two very different distributions, so you would expect to get a p-value of 0. I suggest you replace cdf_data with something like random_variable_2 = st.norm.rvs(loc=1,scale=1, size=size)
Additionally you are performing two different KS tests between your first two (one sample) and third (two sample) test. In the first two you compare your data to a fixed functional form to check if the data is consistent with that functional distribution. Since you have the same data and distribution between cases one and two, you would expect the output to be the same. However in case three you are testing two independent distributions to see if they are consistent with each other. Since the cdf_data should just be other normally distributed data points, you should find that the two distributions are consistent, but it should not necessarily give you the exact same answer as the previous two cases, just KS test statistic and p-value that suggests the two data sets come from the same underlying distribution.

",,
SciPy unexpected result,https://stackoverflow.com/questions/65010384,Differing results in Scipy vs Matlab transfer functions,"I can't understand why the scipy.signal.ss2tf() and scipy.signal.StateSpace().to_tf() give (the same) unexpected result.
Example:
A=[[0, 1, 0], [0, 0, 1], [-3, -4, -2]]
B=[[0], [0], [1]]
C=[[5, 1, 0]]
D=[[0]]

The result for scipy is
num = array([[0, 0, 0, 4]]),
den = array([1., 2., 4., 3.])

in Matlab the result is
num = [0,0,1,5],
den = [1,2,4,3]

It seems that the denominator is always right, I tried other examples, but the numerator of the transfer function doesn't correspond.
Am I using scipy in an incorrect way?
(another example)
A=[[0, 1, 0], [0, 0, 1], [-8, -14, -7]]
B=[[0], [0], [1]]
C=[[15, 5, 0]]
D=[[0]]

",1,311,"This is a bug in SciPy that was fixed in SciPy 1.6.0.  When ss2tf creates an array to hold the numerator, it uses the data types of the inputs to determine the data type of the numerator array.  In your case, the values are all integers, so the data type of the numerator is integer.  However, the coefficients for the numerator are the result of a floating point calculation, and therefore are subject to loss of precision.  When the computed values are copied into the numerator array, those values are truncated to integers.  In your example, that results in a large error.  When I reproduce the floating point calculation of the numerator, I get [0.0, 0.0, 0.9999999999999947, 4.999999999999995], and when those values are copied into the integer array, the result is [0, 0, 0, 4].
A work-around is to ensure that your inputs (or at least A) contain floating point values.  For example,
In [33]: A = [[0., 1., 0.], [0., 0., 1.], [-8., -14., -7.]] 
    ...: B = [[0], [0], [1]] 
    ...: C = [[15, 5, 0]] 
    ...: D = [[0]]                                                              

In [34]: num, den = ss2tf(A, B, C, D)                                           

In [35]: num                                                                    
Out[35]: array([[0.00000000e+00, 1.77635684e-15, 5.00000000e+00, 1.50000000e+01]])

",,
SciPy unexpected result,https://stackoverflow.com/questions/54766766,OpenCV filter2d gives incorrect result,"I am currently trying to filter an image with a Laplacian kernel that I have constructed myself. However, when filtering an input image with this kernel it gives an unexpected result compared to the implementation in SciPy.

The Laplacian kernel I have constructed should be verified by the following images

 


The code for filtering the image:

im = cv2.imread(""test.png"",0)
im = im.astype(np.float32)

def lkern(t=1.):
    ax = np.arange(np.round(-5*np.sqrt(t),0),np.round(5*np.sqrt(t),0)+1)
    xx, yy = np.meshgrid(ax, ax)

    kernel = -1/(np.sqrt(2*np.pi*t)*t)*np.exp(-(xx**2+yy**2)/(2*t))+
        (xx**2+yy**2)/(np.sqrt(2*np.pi*t)*t**2)*np.exp(-(xx**2+yy**2)/(2*t))


    return kernel.astype(np.float)

t = 25**2/2
l = lkern(t)

L = cv2.filter2D(im/255,-1,l)

plt.figure()
plt.imshow(L,cmap=""gray"")
plt.show()


which results in



Comparing to SciPy's ndimage.gaussian_laplace, the result should have been



which is very different, and I cannot figure out how to do this properly.
",1,1299,"The code in the OP seems to take the equation for a 1D Laplace of Gaussian and use that to construct a 2D radially symmetric function. That is, along any of the diameters of the kernel the function looks like a 1D Laplace of Gaussian. This is not the correct way to create a 2D Laplace of Gaussian.

The Laplace of Gaussian is defined as the sum of the second order derivative of the Gaussian kernel along each of the axes. That is,

LoG = d/dx G + d/dy G


With G the Gaussian kernel.

With Numpy, you can construct this kernel as follows. I'm using the separability of the Gaussian to reduce the computational complexity.

s = 5;
x = np.arange(np.floor(-4*s),np.ceil(4*s)+1)
g = 1/(np.sqrt(2*np.pi)*s)*np.exp(-x**2/(2*s**2))
d2g = (x**2 - s**2)/(s**4) * g
log = g * d2g[:,None] + g[:,None] * d2g


Tricks here: g and d2g are 1D functions. g[:,None] turns the 1D function on its side, so that the multiplication causes broadcasting, leading to a 2D output.

I have written the kernel this way, rather than expressing the full 2D equation in one go, because this leads to large efficiencies in your code: The convolution of an image f with the kernel log can be written as:

conv(f, log) = conv(f, g * d2g[:,None] + g[:,None] * d2g)
             = conv(conv(f, g), d2g[:,None]) + conv(conv(f, g[:,None]), d2g)


That is, instead of one convolution with a large 2D kernel, we compute 4 convolutions with relatively small 1D kernels. Note that the actual order here does not matter:


One applies a 1D kernel g and on the result a 1D kernel d2g along the other axis. These two operations can be reversed.
Then one repeats this process changing the axes along which each of the operations is applied.
Finally one adds the two results.


(It is OK to use cv2.filter2D where I wrote conv. conv just indicates any convolution function, but a correlation function like filter2D is fine because the kernels are all symmetric.)
",,
SciPy unexpected result,https://stackoverflow.com/questions/31040188,Dot product between 1D numpy array and scipy sparse matrix,,1,4408,,,
SciPy unexpected result,https://stackoverflow.com/questions/77619352,Voronoi diagram gives unexpected results in scipy,"I have the following pandas dataframe:
import pandas as pd
pd.DataFrame({'cl': {0: 'A', 1: 'C', 2: 'H', 3: 'M', 4: 'S'},
 'd': {0: 245.059986986012,
  1: 320.49044143557785,
  2: 239.79023081978914,
  3: 263.38325791238833,
  4: 219.53334398353175},
 'p': {0: 10.971011721360075,
  1: 10.970258360366753,
  2: 13.108487516946218,
  3: 12.93241352743668,
  4: 13.346107628161008}})

    cl  d           p
0   A   245.059987  10.971012
1   C   320.490441  10.970258
2   H   239.790231  13.108488
3   M   263.383258  12.932414
4   S   219.533344  13.346108

I want to create a Voronoi diagram. To do so I am using the package from scipy.
I am using the following code:
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial import Voronoi, voronoi_plot_2d

centers2 = np.array(
    centers_dt[['d', 'p']]
)

scatter_x = np.array(centers_dt['d'])
scatter_y = np.array(centers_dt['p'])
group = np.array(centers_dt['cl'])
cdict = {'C': 'red', 'A': 'blue', 'H': 'green', 'M': 'yellow', 'S': 'black'}

fig, ax = plt.subplots()
for g in np.unique(group):
    ix = np.where(group == g)
    ax.scatter(scatter_x[ix], scatter_y[ix], c = cdict[g], label = g, s = 100)
ax.legend()

vor = Voronoi(centers2)
fig = voronoi_plot_2d(vor,plt.gca())

plt.show()
plt.close()

But the result I am getting is unexpected:

Since there is a boarder missing plus the boarders seem a bit off.
Any ideas ?
",0,69,"The code in voronoi_plot_2d draws segments with finite length, even for borders that should extend to infinity.  It is possible that with the default axis limits chosen by voronoi_plot_2d, a border segment doesn't appear in the plot, because the finite length chosen by vorono_plot_2d is too small for it to extend into the displayed window.
Here's the plot that I get from your code if I add plt.ylim(-400, 400).

Note that the dashed line extending upwards from the lowest intersection point should continue upwards to infinity, but the code draws only a finite segment.
Also note that the axis scales are not equal, so the dividing lines between points appear to be not perpendicular to the lines that would connect the points.  Here's the plot with equal axis scales:

",,
SciPy unexpected result,https://stackoverflow.com/questions/76564629,Cannot import protobuf builder when deploying Azure function,"The Problem
I have an Azure function built using the Python v1 programming model which builds a basic Keras model and trains it using some data from an Azure file share.
Tested locally and everything works.
I'm deploying to Azure using Azure Pipelines. It's invoking correctly, but then failing. Checking the detailed invocation history reveals the following error:
Result: Failure Exception: ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/internal/__init__.py). 
Please check the requirements.txt file for the missing module. For more info, please refer the troubleshooting guide: https://aka.ms/functions-modulenotfound
Stack: File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/dispatcher.py"", line 380, in _handle__function_load_request func = loader.load_function( 
File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/utils/wrappers.py"", line 48, in call raise extend_exception_message(e, message) File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/utils/wrappers.py"", line 44, in call return func(*args, **kwargs)
File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/loader.py"", line 132, in load_function mod = importlib.import_module(fullmodname)
File ""/usr/local/lib/python3.9/importlib/__init__.py"", line 127, in import_module return _bootstrap._gcd_import(name[level:], package, level) File ""&lt;frozen importlib._bootstrap&gt;"", line 1030, in _gcd_import
File ""&lt;frozen importlib._bootstrap&gt;"", line 1007, in _find_and_load
File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _find_and_load_unlocked 
File ""&lt;frozen importlib._bootstrap&gt;"", line 680, in _load_unlocked
File ""&lt;frozen importlib._bootstrap_external&gt;"", line 850, in exec_module
File ""&lt;frozen importlib._bootstrap&gt;"", line 228, in _call_with_frames_removed 
File ""/home/site/wwwroot/func-gc-imgorientation-train/__init__.py"", line 20, in &lt;module&gt; from core.data.training_image_filestore import TrainingImageFilestore
File ""/home/site/wwwroot/core/data/training_image_filestore.py"", line 4, in &lt;module&gt; import tensorflow as tf
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/__init__.py"", line 37, in &lt;module&gt; from tensorflow.python.tools import module_util as _module_util
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/python/__init__.py"", line 37, in &lt;module&gt; from tensorflow.python.eager import context
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/python/eager/context.py"", line 28, in &lt;module&gt; from tensorflow.core.framework import function_pb2
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/core/framework/function_pb2.py"", line 5, in &lt;module&gt; from google.protobuf.internal import builder as _builder

Well Documented Error with a Solution that Works Outside of Azure
This error is well documented and understood as in the following version:
ImportError: cannot import name 'builder' from 'google.protobuf.internal'
In general, arising from API changes in v3.20:
https://stackoverflow.com/a/71984564/1928761
TF adopted these changes, but there were bugs when using TF with protobuf &gt;= v3.20 which were resolved in the most recent Tensorflow release so protobuf 4.23.3 should work with TF 2.12.0 as here:
https://github.com/tensorflow/tensorflow/issues/59221
Problem is resolved locally
I resolved this issue locally and have tested my function. All works fine. The problem only occurs in Azure.
Suspected Root Cause in Azure
Looking through the error above I noticed that all my custom modules, and Tensorflow, are installed to the same root: /home/site/wwwroot/
For instance, my custom core.data module is installed at /home/site/wwwroot/core/data/
Tensorflow is installed at /home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/
I've confirmed that all modules in my requirements.txt are being installed as expected in /home/site/wwwroot/.python_packages/lib/site-packages/ - including the latest protobuf with the builder module as expected.
However, protobuf is being imported from /azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/internal/init.py
Presumably the implication is that TF is using the version of protobuf that's bundled with Azure's python distribution on the host, rather than the version in my site packages.
To test this theory I added the following code to the top of my init.py for the function:
import sys
print(sys.path)
import google.protobuf
print(google.protobuf.__version__)
print(google.protobuf.__path__)

This confirmed that protobuf version 3.19 was being loaded from the following path and not from site packages: /azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/
The Question
The question therefore is can I upgrade the version of protobuf that's included in this python bundle?
Alternatively, can I force TF to use the version of protobuf in my site packages rather than the ones in the Azure python bundle?
Attempted Solutions
ADO is building and installing the correct versions of both protobuf and TF, and the builder file is definitely in the installed site-packages. I've confirmed this by downloading the package from my storage account and unzipping it.
To resolve the issue, I've tried the following:

Confirmed that I can import other modules and that this is specific to importing the protobuf builder
Confirmed that I can import google.protobuf.internal.
Tried clearing the protobuf pycache by adding an rm -rvf command to my bash script immediately after the pip install --target....
Using subprocess to run pip install --upgrade protobuf==4.23.3 from within my function and before importing TF (which runs without error, but does not do the job).

At this point, I'm all out of ideas.
requirements.txt
# DO NOT include azure-functions-worker in this file
# The Python Worker is managed by Azure Functions platform
# Manually managing azure-functions-worker may cause unexpected issues

# Protobuf comes first to force the very latest version
protobuf==4.23.3

# Tensorflow, Dotenv, Pillow
absl-py==1.4.0
astunparse==1.6.3
cachetools==5.3.1
certifi==2023.5.7
charset-normalizer==3.1.0
contourpy==1.1.0
cycler==0.11.0
flatbuffers==23.5.26
fonttools==4.40.0
gast==0.4.0
google-auth==2.20.0
google-auth-oauthlib==1.0.0
google-pasta==0.2.0
graphviz==0.20.1
grpcio==1.54.2
h5py==3.8.0
idna==3.4
jax==0.4.12
keras==2.12.0
kiwisolver==1.4.4
libclang==16.0.0
Markdown==3.4.3
MarkupSafe==2.1.3
matplotlib==3.7.1
ml-dtypes==0.2.0
numpy==1.23.5
oauthlib==3.2.2
opt-einsum==3.3.0
packaging==23.1
Pillow==9.5.0
pyasn1==0.5.0
pyasn1-modules==0.3.0
pydot==1.4.2
pyparsing==3.0.9
python-dateutil==2.8.2
python-dotenv
requests==2.31.0
requests-oauthlib==1.3.1
rsa==4.9
scipy==1.10.1
six==1.16.0
tensorboard==2.12.3
tensorboard-data-server==0.7.1
tensorflow==2.12.0
tensorflow-estimator==2.12.0
termcolor==2.3.0
typing_extensions==4.6.3
urllib3==1.26.16
Werkzeug==2.3.6
wrapt==1.14.1

# Azure libraries
azure-functions
azure-identity
azure-keyvault

Deployment pipeline YAML
# Python Function App to Linux on Azure
# Build a Python function app and deploy it to Azure as a Linux function app.
# Add steps that analyze code, save build artifacts, deploy, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/python

trigger:
- main

variables:
  # Azure Resource Manager connection created during pipeline creation
  azureSubscription: '4ae24131-0b22-421c-8e3e-6d766e891ece'

  # Function app name
  functionAppName: 'func-xxxx-dev'

  # Agent VM image name
  vmImageName: 'ubuntu-latest'

  # Working Directory
  workingDirectory: '$(System.DefaultWorkingDirectory)'

stages:
- stage: Build
  displayName: Build stage

  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName)

    steps:
    - bash: |
        if [ -f extensions.csproj ]
        then
            dotnet build extensions.csproj --runtime ubuntu.16.04-x64 --output ./bin
        fi
      workingDirectory: $(workingDirectory)
      displayName: 'Build extensions'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.9'
      inputs:
        versionSpec: 3.9 # Functions V2 supports Python 3.6 as of today

    - bash: |
        python -m pip install --upgrade pip
        pip install --target=""./.python_packages/lib/site-packages"" -r ./requirements.txt
      workingDirectory: $(workingDirectory)
      displayName: 'Install application dependencies'

    - task: ArchiveFiles@2
      displayName: 'Archive files'
      inputs:
        rootFolderOrFile: '$(workingDirectory)'
        includeRootFolder: false
        archiveType: zip
        archiveFile: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
        replaceExistingArchive: true

    - publish: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
      artifact: drop

- stage: Deploy
  displayName: Deploy stage
  dependsOn: Build
  condition: succeeded()

  jobs:
  - deployment: Deploy
    displayName: Deploy
    environment: 'development'
    pool:
      vmImage: $(vmImageName)

    strategy:
      runOnce:
        deploy:

          steps:
          - task: AzureFunctionApp@1
            displayName: 'Azure functions app deploy'
            inputs:
              azureSubscription: '$(azureSubscription)'
              appType: functionAppLinux
              appName: $(functionAppName)
              package: '$(Pipeline.Workspace)/drop/$(Build.BuildId).zip'

Log stream
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778401,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778404,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778801,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:51:00Z   [Information]   Executing 'Functions.func-xxxx-train' (Reason='Timer fired at 2023-06-27T11:51:00.0016150+00:00', Id=3795600b-379e-423c-b29f-65fec390289a)
2023-06-27T11:51:00Z   [Verbose]   Sending invocation id: '3795600b-379e-423c-b29f-65fec390289a
2023-06-27T11:51:00Z   [Verbose]   Posting invocation id:3795600b-379e-423c-b29f-65fec390289a on workerId:250ae2e1-4416-41ef-b355-0684a59d0a91
2023-06-27T11:51:00Z   [Error]   Executed 'Functions.func-xxxx-train' (Failed, Id=3795600b-379e-423c-b29f-65fec390289a, Duration=2ms)
2023-06-27T11:51:00Z   [Verbose]   Function 'func-xxx-train' updated status: Last='2023-06-27T11:51:00.0015724+00:00', Next='2023-06-27T11:52:00.0000000+00:00', LastUpdated='2023-06-27T11:51:00.0015724+00:00'
2023-06-27T11:51:00Z   [Verbose]   Timer for 'func-gc-imgorientation-train' started with interval '00:00:59.9674482'.
2023-06-27T11:51:05Z   [Information]   Host Status: {
  ""id"": ""func-gc-imgorientation-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2800545,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}

",0,1441,"I had a similar issue and found a different solution. There is an app setting that isolates the application's dependencies from the azure function runtime dependencies:
https://learn.microsoft.com/en-us/azure/azure-functions/functions-app-settings#python_isolate_worker_dependencies
Adding the PYTHON_ISOLATE_WORKER_DEPENDENCIES setting with a value of 1 to my function in azure fixed the issue for me.
","The root cause of the problem, as highlighted by @SiddheshDesai, turned out to be that the Azure Functions host was loading a version of protobuf (3.19.6) into the cache which was earlier than the version that the latest Tensorflow needed (4.23.3).
Even when placing my protobuf/TF imports at the top of my function app module, the same error arose.
I was, however, able to reload protobuf from my site-packages using importlib.
This cause was proven by adding the following code to my function:
import logging
import importlib
import google.protobuf

logging.info(google.protobuf.__version__)
logging.info(google.protobuf.__file__)

importlib.reload(google.protobuf)

logging.info(google.protobuf.__version__)
logging.info(google.protobuf.__file__)

However, the reload then interfered with the Azure Functions library and host causing a heap of other exceptions.
My conclusion therefore is that this is an issue with Azure Functions rather than an issue with my code.
Rather than trying to work around these issues, I've now containerised my function and re-deployed. This has worked as expected.
The answer, therefore, turns out to be - as SiddheshDesai suggested above - that any application needing to use versions of protobuf &gt;= 3.20.0 need to be containerised rather than deployed as code/run from packages.
","If you want to use Protobuf module in Azure Functions, You need to downgrade it to 3.20.* and add it in your requirements.txt.
I added protobuf==3.20.* in my requirements.txt and the Http Trigger got deployed successfully in Azure Functions via DevOps YAML pipeline, Refer below:-
My requirements.txt:-
azure-functions
protobuf==3.20.*

My init.py:-
import logging

import azure.functions as func


def main(req: func.HttpRequest) -&gt; func.HttpResponse:
    logging.info('Python HTTP trigger function processed a request.')

    name = req.params.get('name')
    if not name:
        try:
            req_body = req.get_json()
        except ValueError:
            pass
        else:
            name = req_body.get('name')

    if name:
        return func.HttpResponse(f""Hello, {name}. This HTTP triggered function executed successfully."")
    else:
        return func.HttpResponse(
             ""This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response."",
             status_code=200
        )


My YAML pipeline:-
trigger:
- master

variables:
  
  azureSubscription: 'xxxxxxxx-xxxxx-xxx9bbd4354dd'

 
  functionAppName: 'valleyfunc541'

 
  vmImageName: 'ubuntu-latest'

 
  workingDirectory: '$(System.DefaultWorkingDirectory)'

stages:
- stage: Build
  displayName: Build stage

  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName)

    steps:
    - bash: |
        if [ -f extensions.csproj ]
        then
            dotnet build extensions.csproj --runtime ubuntu.16.04-x64 --output ./bin
        fi
      workingDirectory: $(workingDirectory)
      displayName: 'Build extensions'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.10'
      inputs:
        versionSpec: 3.10 # Functions V2 supports Python 3.6 as of today

    - bash: |
        pip install --target=""./.python_packages/lib/site-packages"" -r ./requirements.txt
      workingDirectory: $(workingDirectory)
      displayName: 'Install application dependencies'

    - task: ArchiveFiles@2
      displayName: 'Archive files'
      inputs:
        rootFolderOrFile: '$(workingDirectory)'
        includeRootFolder: false
        archiveType: zip
        archiveFile: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
        replaceExistingArchive: true

    - publish: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
      artifact: drop

- stage: Deploy
  displayName: Deploy stage
  dependsOn: Build
  condition: succeeded()

  jobs:
  - deployment: Deploy
    displayName: Deploy
    environment: 'development'
    pool:
      vmImage: $(vmImageName)

    strategy:
      runOnce:
        deploy:

          steps:
          - task: AzureFunctionApp@1
            displayName: 'Azure functions app deploy'
            inputs:
              azureSubscription: '$(azureSubscription)'
              appType: functionAppLinux
              appName: $(functionAppName)
              package: '$(Pipeline.Workspace)/drop/$(Build.BuildId).zip'

Output:-

The HTTP Trigger got deployed successfully:-


Reference:- My SO thread answer
"
SciPy unexpected result,https://stackoverflow.com/questions/75879324,How to use kwargs in scipy.optimize.curve_fit() to pass a parameter that isn&#39;t being fitted,"The problem originates from fitting a diffraction pattern to data, where the number of slits is known beforehand. I have given a simplified version below that highlights the same issue. The function should fit the values of a and b in the data while passing n to the function. I could use a global n which would solve my issues, however, I would like to do this using **kwargs as shown in the scipy.optimize.curve_fit() reference.
Here is an example of the issue. The code generates the curve of 4sin(2x)+3cos(2x) with some noise as the data:

import numpy as np
import scipy
import matplotlib.pyplot as plt

def curve(x,a,b,**kwargs):
    n = kwargs[""n""]
    return a*np.sin(n*x)+b*np.cos(n*x)

x = np.linspace(-5,5,1000)
y = np.random.normal(loc=curve(x, 4, 3, n=2), scale=0.2, size=None)
result = scipy.optimize.curve_fit(curve, x, y, n = 2)
y2 = curve(x, *result[0], n=2)

plt.plot(x, y2)
plt.plot(x,y)
plt.show()


This returns the error
  File ""C:\Users\HP\OneDrive\Documents\Uni\lab year 2\diffraction\kwargs.py"", line 13, in &lt;module&gt;
    result = scipy.optimize.curve_fit(curve, x, y, n = 2)

  File ""C:\Users\HP\anaconda3\lib\site-packages\scipy\optimize\_minpack_py.py"", line 834, in curve_fit
    res = leastsq(func, p0, Dfun=jac, full_output=1, **kwargs)

TypeError: leastsq() got an unexpected keyword argument 'n'

",0,333,"You might consider using lmfit for this (main author here). Your example would look like:
import numpy as np
import matplotlib.pyplot as plt
from lmfit import Model

def curve(x, a, b, n=2):
    return a*np.sin(n*x)+b*np.cos(n*x)

x = np.linspace(-5,5,1000)
y = np.random.normal(loc=curve(x, 4, 3, n=2), scale=0.2, size=None)

# turn your curve function into an lmfit Model:
model = Model(curve)

# create parameters for the fit, giving decent initial values
# note that n will use the default function value by default
params = model.make_params(a=3.2, b=2.5)

# fix n so that it is not varied in the fit:
params['n'].vary = False

# do the fit
result = model.fit(y, params, x=x)

# print out fit report
print(result.fit_report())

# plot data and best fit
plt.plot(x, y, label='data')
plt.plot(x, result.best_fit, label='fit')
plt.legend()
plt.show()


which will print a report of
[[Model]]
    Model(curve)
[[Fit Statistics]]
    # fitting method   = leastsq
    # function evals   = 7
    # data points      = 1000
    # variables        = 2
    chi-square         = 39.7903631
    reduced chi-square = 0.03987010
    Akaike info crit   = -3220.13053
    Bayesian info crit = -3210.31502
    R-squared          = 0.99678449
[[Variables]]
    a:  4.00080256 +/- 0.00914251 (0.23%) (init = 3.2)
    b:  3.00787199 +/- 0.00873116 (0.29%) (init = 2.5)
    n:  2 (fixed)

and a plot of

","I had a similar issue with curve_fit not accepting the args argument and after following the suggestion
here:, managed to get something working with a curried function
import numpy as np
import scipy
import matplotlib.pyplot as plt

def curve_curry(n):
    def curve(x, a, b):
        return a * np.sin(n * x) + b * np.cos(n * x)
    
    return curve

n = 2
x = np.linspace(-5, 5, 1000)
y = np.random.normal(loc=curve_curry(n)(x, 4, 3), scale=0.2, size=None)
result = scipy.optimize.curve_fit(curve_curry(n=2), x, y)
y2 = curve_curry(n)(x, *result[0])

plt.plot(x, y, '.', color='lightgrey')
plt.plot(x, y2)
plt.show()

In my case, I had fit parameters [p0, p1, p2], constants c1, c2, c3  and pre-defined functions f1, f2.  My code looked something like this:
def func_curry(c1, c2, c3, f1, f2):
    def func(x, *p):
        
        yinv = f1(c1) + x * c2 * (c3 + p[1] + p[2] * x ) / p[0]
        y = f2(yinv)
        
        return y
    
    return func 


# Curve fit using scipy

args = (c1, c2, c3, f1, f2)
p0 = [p0, p1, p2]
popt, pcov = scipy.optimize.curve_fit(func_curry(*args), xdata, ydata, p0=p0)

",
SciPy unexpected result,https://stackoverflow.com/questions/71634466,Curve fitting using Scipy leads to unexpected result,"I have a data set which I can fit to a given function. I did it using Desmos graphing calculator and got the expected results. But when I implement it using Matplotlib, I get a totally different curve and I can't figure out the error in my code. Here is my code:
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

data = np.loadtxt('data.txt')


def func(x, a, b):
    return a/(np.sin((x-b)/2))**4

x = data[:, 0]
y = data[:, 1]

popt, pcov = curve_fit(func, x, y)

plt.plot(x, y, 'ko', label=""Observed data points"")


plt.plot(x, func(x, *popt), 'r-', label=""Fitted Curve"")


plt.legend()
plt.show()

Here is the expected plot (plotted using Desmos): 
And here is what I got from Matplotlib: 
Any help would be appreciated. This is the data.txt for reference:




X
Y




-0.523598776
0.530580093


-0.436332313
1.016423844


-0.34906585
6.38245854


-0.261799388
18.70139225


-0.174532925
30.7389007


0.174532925
13.59465343


0.261799388
4.011313119


0.34906585
0.805865977


0.436332313
0.50894953


0.523598776
0.253654518



",0,127,"You have a singularity at x==b (your function tends to infinity). That makes it very difficult for curve_fit to find a value of b that ""crosses"" any of your data points. Since your data is about zero centered, you'll have better luck providing a an initial value such as
popt, pcov = curve_fit(func, x, y, [0.0, 0.0])

",,
SciPy unexpected result,https://stackoverflow.com/questions/67284779,Scipy rv_continuous fit does not check input data bounds,"I am fitting synthetic data to various distributions in scipy, however, I am observing some unexpected results. My data contains negative numbers and I do not get an error when I fit this data to distributions with non negative supports when fixing location and scale. My code is as follows:
import scipy.stats as st
import numpy as np
import pandas as pd

np.random.seed(7)
test_data = pd.Series(0.5 + 0.1*np.sin(np.linspace(0, 3*np.pi, 100)) + 0.5*np.random.normal(0,1,size=100))
print(np.min(test_data))

Which returns:
-0.5900934692403015

Confirming I have generated negative observations. When I fit scipy lognorm, which has a non inclusive non negative support, I get the expected result of an error where the data bounds are violated:
st.lognorm.fit(test_data, floc=0, fscale=1)

---------------------------------------------------------------------------
FitDataError                              Traceback (most recent call last)
&lt;ipython-input-13-fbeaae8f3c2e&gt; in &lt;module&gt;
----&gt; 1 st.lognorm.fit(test_data, floc=0, fscale=1)

~\Miniconda3\lib\site-packages\scipy\stats\_continuous_distns.py in fit(self, data, *args, **kwds)
   5087             data = data - floc
   5088         if np.any(data &lt;= 0):
-&gt; 5089             raise FitDataError(""lognorm"", lower=floc, upper=np.inf)
   5090         lndata = np.log(data)
   5091 

FitDataError: Invalid values in `data`.  Maximum likelihood estimation with 'lognorm' requires that 0.0 &lt; x &lt; inf for each x in `data`.

However, with the following distributions, I am able to fit the data, despite the fact that all of these distributions have non negative data bounds (as defined by their scipy documentation) and fixed location and scale.
st.burr.fit(test_data, floc=0, fscale=1)

st.expon.fit(test_data)

st.chi2.fit(test_data, floc=0, fscale=1)

st.invgauss.fit(test_data, floc=0, fscale=1)

st.invgamma.fit(test_data, floc=0, fscale=1)

Which yield:
(4.435119987970436, 0.32475585134451646, 0, 1)
(-0.5900934692403015, 1.1171187649605647)
(1.349414062500001, 0, 1)
(0.6815429687499996, 0, 1)
(2.301074218750003, 0, 1)

Additionally, the distribution expon without any shape parameters is able parameters was able to execute which was surprising. If someone could explain how these distributions are able to fit to the data despite the fact that their support bounds have been violated I would really appreciate it.
I am running numpy 1.19.2 and scipy 1.5.2
Thank you!
",0,1013,"The fact that those fit didn't throw any error doesn't mean that they have been a good fit or that they can describe your data.
I'm using scipy==1.6.1.
You can check plotting results
x = np.linspace(test_data.min(), test_data.max(), 100)

Burr: no error, bu cannot describe data &lt;0
burr_pars = sps.burr.fit(test_data, floc=0, fscale=1)
y = sps.burr(*burr_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);


Expon: no error, but very bad fit
expon_pars = sps.expon.fit(test_data)
y = sps.expon(*expon_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);


Chi2: no error but very bad fit and cannot describe data &lt;0
chi2_pars = sps.chi2.fit(test_data, floc=0, fscale=1)
y = sps.chi2(*chi2_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);


Invgauss: error
invgauss_pars = sps.invgauss.fit(test_data, floc=0, fscale=1)
FitDataError: Invalid values in `data`.  Maximum likelihood estimation with 'invgauss' requires that 0 &lt; (x - loc)/scale  &lt; inf for each x in `data`.

If you don't set loc and scale, works best for x&gt;=0, but given the formula of its PDF there is no reason why it should throw an error for x&lt;0
invgauss_pars = sps.invgauss.fit(test_data)
y = sps.invgauss(*invgauss_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);


Invgamma: a warning, bad fit and cannot describe for x&lt;0
invagamm_pars = sps.invgamma.fit(test_data, floc=0, fscale=1)
y = sps.invgauss(*invagamm_pars).pdf(x)
plt.plot(x, y)
plt.hist(test_data, alpha=.5, density=True);
RuntimeWarning: invalid value encountered in double_scalars
  Lhat = muhat - Shat*mu


EDIT
From https://github.com/scipy/scipy/blob/v1.6.3/scipy/stats/_continuous_distns.py you see that FitDataError is called only by beta, expon (but if floc is None then floc = data_min), gamma,  invgauss (but only np.any(data - floc &lt; 0)), lognorm, pareto, rayleigh, uniform.
For other distributions FitDataError is not implemented.
",,
SciPy unexpected result,https://stackoverflow.com/questions/65152041,Using sp.ndimage.label on Xarray DataArray with apply_ufunc,"Lets say I have the simple array:
data = [1,1,0,0,1,1,1]

I can apply labeling to this data with the scipy ndimage module with:
groups, _ = sp.ndimage.label(data)

Resulting in
In [68]: print(groups)
[1 1 0 0 2 2 2]

Now, I would like to do the same labeling function on a  xarray DataArray.
xr_data = xr.DataArray([1,1,0,0,1,1,1], coords = [(""x"", [0,1,2,3,4,5,6])])

I know I could call the same function as before on the xr_data, but the output of doing this call is a numpy array, which in my actual dataset, is too large to fit in memory.
It seems like the xr.apply_ufunc function is what I need. However, I am having trouble getting it to work.
def xr_label(arr):
    return xr.apply_ufunc(sp.ndimage.label, arr)

xr_groups, _ = xr_label(xr_data)

This results in:
""ValueError: applied function returned data with unexpected number of dimensions. Received 0 dimension(s) but expected 1 dimensions with names: ('x',)""
I'm finding the documentation on the apply_ufunc method difficult to interpret. Can someone help me out with this?
",0,203,"You have to define input_core_dims and output_core_dims as parameters to apply_ufunc. See the documentation at: http://xarray.pydata.org/en/stable/generated/xarray.apply_ufunc.html
In your case I think this will be:
xr.apply_ufunc(sp.ndimage.label, arr, input_core_dims=[['x']], output_core_dims=[['x']])

I also recently struggled with understanding apply_ufunc (to be fair, I still don't have a full understanding), however the example at http://xarray.pydata.org/en/stable/examples/apply_ufunc_vectorize_1d.html helped me a lot.
",,
SciPy unexpected result,https://stackoverflow.com/questions/50960016,What is the cosine distance of something and itself?,"I'm playing with scipy's cosine distance. From what I've gathered, the closer a cosine distance is to 1, the more similar the vectors are. I got some unexpected results in a text mining project, so I decided to investigate the simplest case.

import numpy as np
import scipy.spatial
arr1 = np.array([1,1])
arr2 = np.array([1,1])
print scipy.spatial.distance.cosine(arr1, arr2)


My program prints 0.0.

Shouldn't the result be 1.0? Why or why not?
",0,608,"It is the cosine distance, not the cosine similarity.  A basic requirement for a function d(u, v) to be a distance is that d(u, u) = 0.

See the definition of the formula in the docstring of scipy.spatial.distance.cosine, and notice that the formula begins 1 - (...).  Your expectation of the function is probably based on the quantity in (...), but that expression is the cosine similarity.
",,
SciPy unexpected result,https://stackoverflow.com/questions/48675322,How to make all elements in many columns of a sparse csc matrix 0,"my_csr_matrix


&lt;338232x1783504 sparse matrix of type ''
    with 15740456 stored elements in Compressed Sparse Column format&gt;

my_csr_matrix[:,736225:1783504] = 0

Traceback (most recent call last):
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-135-b0e125d5d27e&gt;"", line 1, in &lt;module&gt;
    my_csr_matrix[:,736225:1783504] = 0
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\scipy\sparse\compressed.py"", line 695, in __setitem__
    i, j = self._swap((i.ravel(), j.ravel()))
MemoryError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Users\abhatia\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2879, in run_code
    self.showtraceback(running_compiled_code=True)
TypeError: showtraceback() got an unexpected keyword argument 'running_compiled_code'

",0,97,,,
SciPy unexpected result,https://stackoverflow.com/questions/45036419,Solver (fsolve in python.scipy) not working,"I have been trying to solve the following system of equations using a scipy solver:

    from scipy.optimize import fsolve
    import math
    import numpy as np

    S0 = 1000
    u  = 1
    d  = 1

    delta_t = 1
    r = 0
    psi_r = (np.exp(-r*delta_t))**(-1)
    fi_r = np.exp(-r*delta_t)
    fi_2r = np.exp(-2*r*delta_t)

    def equations(p):
        p_u,p_m,p_d = p
        return (p_u+p_m+p_d - 1, fi_r*(p_u*(S0+u) + p_m*S0 + p_u*(S0-d)) -S0,fi_2r*
(p_u*p_u*(S0+2*u) + 2*p_m*p_u*(S0+u) + (2*p_u*p_d+p_m*p_m)*S0 + 2*p_m*p_d*(S0-d) + p_d*p_d*(S0-2*d)) - S0)

    p_u,p_m,p_d = fsolve(equations,(0.3,0.5,0.4))

    print(equations((p_u,p_m,p_d)))


The problem is that, despite the first equation stating that the sum of my unknowns should be 1, it never gives a result that would satisfy this. What I get is unexpected numbers on the order of 10 to the -12, or sometimes even negative numbers, which I know cannot be the correct solution.

I know I have to try several initial guesses, but what concerns me is that none of the guesses so far has given me probabilities which sum up to 1. 
",0,2026,,,
SciPy unexpected result,https://stackoverflow.com/questions/40377810,"interpolate.griddata shifts data northwards, is it a bug?","I observe unexpected results from scipy.interpolate.griddata. I am trying to visualize a set of irregularly spaced points using matplotlib.basemap and scipy.interpolate.griddata.

The data is given as three lists: latitudes, longitudes and values. To get them on the map I interpolate the data onto a regular grid and visualize it using Basemap's imshow function.

I observe that the interpolated data is shifted northwards from true positions.

Here is an example. Here I want to highlight a cell formed by two meridians and two parallels. I expect to get something like this:



However what I get is something like this:



You can see that the red rectangle is visibly shifted northwards.

I have tried to vary the grid resolution and the number of points, however this does not seem to have any effect on this observed shift.

Here is an IPython notebook that illustrates the issue.

Also below is the complete code:

import numpy as np
from numpy import random
from scipy import interpolate
import matplotlib.pyplot as plt
from mpl_toolkits.basemap import Basemap

# defining the region of interest
r = {'lon':[83.0, 95.5], 'lat':[48.5,55.5]}
# initializing Basemap
m = Basemap(projection='merc', 
            llcrnrlon=r['lon'][0],
            llcrnrlat=r['lat'][0],
            urcrnrlon=r['lon'][1],
            urcrnrlat=r['lat'][1],
            lon_0=r['lon'][0], 
            ellps='WGS84',
            fix_aspect=True,
            resolution='h')
# defining the highlighted block
block = {'lon':[89,91],'lat':[50.5,52.5]}
# generating the data
npixels = 100000
lat_range = r['lat'][1] - r['lat'][0]
lats = lat_range * random.random(npixels) + r['lat'][0]
lon_range = r['lon'][1] - r['lon'][0]
lons = lon_range * random.random(npixels) + r['lon'][0]
values = np.zeros(npixels)
for p in range(npixels):
    if block['lat'][0] &lt; lats[p] &lt; block['lat'][1] \
    and block['lon'][0] &lt; lons[p] &lt; block['lon'][1]:
        values[p] = 1.0 
# plotting the original data without interpolation
plt.figure(figsize=(5, 5))
m.drawparallels(np.arange(r['lat'][0], r['lat'][1] + 0.25, 2.0),
                    labels=[True,False,True,False])
m.drawmeridians(np.arange(r['lon'][0], r['lon'][1] + 0.25, 2.0), 
                    labels=[True,True,False,True])
m.scatter(lons,lats,c=values,latlon=True,edgecolors='none')
# interpolating on the regular grid
nx = ny = 500
mapx = np.linspace(r['lon'][0],r['lon'][1],nx)
mapy = np.linspace(r['lat'][0],r['lat'][1],ny)
mapgridx,mapgridy = np.meshgrid(mapx,mapy)
mapdata = interpolate.griddata(list(zip(lons,lats)),values,
                   (mapgridx,mapgridy),method='nearest')
# plotting the interpolated data
plt.figure(figsize=(5, 5))
m.drawparallels(np.arange(r['lat'][0], r['lat'][1] + 0.25, 2.0),
                    labels=[True,False,True,False])
m.drawmeridians(np.arange(r['lon'][0], r['lon'][1] + 0.25, 2.0), 
                    labels=[True,True,False,True])
m.imshow(mapdata)


I am seeing this with SciPy 0.17.0
",0,288,,,
SciPy unexpected result,https://stackoverflow.com/questions/25033995,How do I calculate expected values of a Poisson distributed random variable in Python using Scipy?,,0,3194,,,
SciPy unexpected issue,https://stackoverflow.com/questions/10518729,add2virtualenv (virtualenv wrapper) does not work with scipy,"I want to create a virtualenv without global python packages, but with the scipy distribution that is shared; installing scipy takes quite a while and I don't want to go through the motions too often.

So I run add2virtualenv /Library/Python/2.7/site-packages/scipy and after running add2virtualenv it shows the directory is added. (I doublechecked, it is the right directory).
Then I issue workon myfile to be sure the working directories are reloaded. 
However, when I try to load scipy, it is an ImportError: No module named scipy. This is unexpected.

Has anyone used a global scipy in a non-global-sitepackages virtualenv?
",6,4997,"So, to summarize, the actual problem here is that the directory including the packages to be imported must be used, instead of the specific package. That is, instead of

add2virtualenv /Library/Python/2.7/site-packages/scipy


It should be

add2virtualenv /Library/Python/2.7/site-packages


Beware: this solution has the drawback that you do not only include scipy, but any other packages in /Library/Python/2.7/site-packages.



An alternate, space-efficent solution could be symlinking scipy directory inside the virtual env's site-package. This can be done, in your virtual env, through:

cdsitepackages
ln -s /Library/Python/2.7/site-packages/scipy scipy




All credits go to @rubik (see the comments)

Check out this answer to find your site-packages path in case it is different than the one used here.
",,
SciPy unexpected issue,https://stackoverflow.com/questions/40690045,conda update failed in windows 10,"In Anaconda prompt as an administrator (Window 10 - 64 bit) after this command : conda update --all I see these errors as below:
I want to update and install packages but after enter every command these errors appear.

conda update --all
Fetching package metadata ..........An unexpected error has occurred.
Please consider posting the following information to the
conda GitHub issue tracker at:

    https://github.com/conda/conda/issues



Current conda install:

               platform : win-64
          conda version : 4.2.9
       conda is private : False
      conda-env version : 4.2.9
    conda-build version : 2.0.2
         python version : 3.5.2.final.0
       requests version : 2.11.1
       root environment : D:\Anaconda3  (writable)
    default environment : D:\Anaconda3
       envs directories : D:\Anaconda3\envs
          package cache : D:\Anaconda3\pkgs
           channel URLs : http://www.lfd.uci.edu/~gohlke/pythonlibs/win-64/
                          http://www.lfd.uci.edu/~gohlke/pythonlibs/noarch/
                          http://github.com/scipy/scipy.git/win-64/
                          http://github.com/scipy/scipy.git/noarch/
                          https://repo.continuum.io/pkgs/free/win-64/
                          https://repo.continuum.io/pkgs/free/noarch/
                          https://repo.continuum.io/pkgs/pro/win-64/
                          https://repo.continuum.io/pkgs/pro/noarch/
                          https://repo.continuum.io/pkgs/msys2/win-64/
                          https://repo.continuum.io/pkgs/msys2/noarch/
            config file : C:\Users\Behnaz Eslami\.condarc
           offline mode : False



`$ D:\Anaconda3\Scripts\conda-script.py update --all`




    Traceback (most recent call last):
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 131, in fetch_repodata
        cache = json.loads(json_str)
      File ""D:\Anaconda3\lib\json\__init__.py"", line 319, in loads
        return _default_decoder.decode(s)
      File ""D:\Anaconda3\lib\json\decoder.py"", line 339, in decode
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
      File ""D:\Anaconda3\lib\json\decoder.py"", line 357, in raw_decode
        raise JSONDecodeError(""Expecting value"", s, err.value) from None
    json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

    During handling of the above exception, another exception occurred:

    Traceback (most recent call last):
      File ""D:\Anaconda3\lib\site-packages\conda\exceptions.py"", line 473, in conda_exception_handler
        return_value = func(*args, **kwargs)
      File ""D:\Anaconda3\lib\site-packages\conda\cli\main.py"", line 144, in _main
        exit_code = args.func(args, p)
      File ""D:\Anaconda3\lib\site-packages\conda\cli\main_update.py"", line 65, in execute
        install(args, parser, 'update')
      File ""D:\Anaconda3\lib\site-packages\conda\cli\install.py"", line 238, in install
        prefix=prefix)
      File ""D:\Anaconda3\lib\site-packages\conda\api.py"", line 24, in get_index
        index = fetch_index(channel_urls, use_cache=use_cache, unknown=unknown)
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 293, in fetch_index
        for url in urls]
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 293, in &lt;listcomp&gt;
        for url in urls]
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 70, in func
        res = f(*args, **kwargs)
      File ""D:\Anaconda3\lib\site-packages\conda\fetch.py"", line 137, in fetch_repodata
        .format(url, filename, e))
    conda.exceptions.CondaRuntimeError: Runtime error: Invalid index file: http://www.lfd.uci.edu/~gohlke/pythonlibs/win-64/repodata.json: Expecting value: line 1 column 1 (char 0)


How can I solve this issue?
",2,10297,"Try to run conda update conda on command prompt (admin)

or Go to your anaconda installation folder, right-click on ""Anaconda3"", go to Security, in ""Group or user names"" click on Users, then check all under ""Allow"".

See this Image
","You need to add the following reference to your (environment variables) path:
&lt;AnacondaInstallationDirectory&gt;\condabin

It's important to add it before any other existing Anaconda references there, so that conda.exe can be executed with the right privileges.
Also you make sure that you have these as well:
&lt;AnacondaInstallationDirectory&gt;\Library\bin
&lt;AnacondaInstallationDirectory&gt;\Scripts
&lt;AnacondaInstallationDirectory&gt;

","instead of running running in a normal terminal window or as administrator, open an anaconda terminal.

In the windows 10 search bar, type cmd (don't press enter)
In the results, you should see one titled ""Anaconda prompt (anaconda 3)""
type your command in this anaconda terminal e.g. conda update conda

"
SciPy unexpected issue,https://stackoverflow.com/questions/61973962,pypy3 dyld can not be found in homebrew,"I got the following error when I use homebrew. Does anybody know how to fix the problem?

dyld: Library not loaded: /usr/local/opt/libffi/lib/libffi.6.dylib
  Referenced from: /usr/local/Cellar/pypy/7.1.1_1/libexec/lib/libpypy-c.dylib
  Reason: image not found


Here is the output of brew doctor.

$ brew doctor
Please note that these warnings are just used to help the Homebrew maintainers
with debugging if you file an issue. If everything you use Homebrew for is
working fine: please don't worry or file an issue; just ignore this. Thanks!

Warning: Some installed formulae were deleted!
You should find replacements for the following formulae:
  inchi
  python@2
  libmpc@0.8
  pygtk
  isl@0.11
  f2c
  indigo
  mpfr@2
  gmp@4
  phantomjs
  phantomjs
  rdkit
  casperjs
  mumps
  ecj
  pygobject
  isl@0.12
  gcc@4.8
  gcc@4.8
Warning: Calling a Formula#patches definition is deprecated! Use 'patch do' block calls instead.
Please report this issue to the docmunch/pdftk tap (not Homebrew/brew or Homebrew/core), or even better, submit a PR to fix it:
  /usr/local/Homebrew/Library/Taps/docmunch/homebrew-pdftk/pdftk.rb:15


Warning: You have the following deprecated, official taps tapped:
  Homebrew/homebrew-dupes
  Homebrew/homebrew-fuse
  Homebrew/homebrew-head-only
  Homebrew/homebrew-versions
  Homebrew/homebrew-x11
Untap them with `brew untap`.

Warning: ""config"" scripts exist outside your system or Homebrew directories.
`./configure` scripts often look for *-config scripts to determine if
software packages are installed, and which additional flags to use when
compiling and linking.

Having additional scripts in your path can confuse software installed via
Homebrew if the config script overrides a system or Homebrew-provided
script of the same name. We found the following ""config"" scripts:
  /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7-config
  /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7m-config
  /Library/Frameworks/Python.framework/Versions/3.7/bin/python3-config
  /Library/Frameworks/Python.framework/Versions/2.7/bin/python2-config
  /Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7-config
  /Library/Frameworks/Python.framework/Versions/2.7/bin/python-config

Warning: Putting non-prefixed coreutils in your path can cause gmp builds to fail.

Warning: Putting non-prefixed findutils in your path can cause python builds to fail.

Warning: Unbrewed dylibs were found in /usr/local/lib.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected dylibs:
  /usr/local/lib/libgcc_ext.10.4.dylib
  /usr/local/lib/libgcc_ext.10.5.dylib
  /usr/local/lib/libgcc_s.10.4.dylib
  /usr/local/lib/libgcc_s.10.5.dylib
  /usr/local/lib/libgcc_s_x86_64.1.dylib
  /usr/local/lib/libgfortran.2.0.0.dylib
  /usr/local/lib/libgfortran.3.dylib
  /usr/local/lib/libgomp.1.dylib
  /usr/local/lib/libpocketsphinx.3.dylib
  /usr/local/lib/libpostal.1.dylib
  /usr/local/lib/libquadmath.0.dylib
  /usr/local/lib/libsphinxad.3.dylib
  /usr/local/lib/libsphinxbase.3.dylib
  /usr/local/lib/libtcl8.6.dylib
  /usr/local/lib/libtk8.6.dylib
  /usr/local/lib/libwkhtmltox.0.12.1.dylib
  /usr/local/lib/libwkhtmltox.0.12.2.dylib
  /usr/local/lib/libwkhtmltox.0.12.4.dylib

Warning: Unbrewed header files were found in /usr/local/include.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected header files:
  /usr/local/include/fakemysql.h
  /usr/local/include/fakepq.h
  /usr/local/include/fakesql.h
  /usr/local/include/itcl.h
  /usr/local/include/itcl2TclOO.h
  /usr/local/include/itclDecls.h
  /usr/local/include/itclInt.h
  /usr/local/include/itclIntDecls.h
  /usr/local/include/itclMigrate2TclCore.h
  /usr/local/include/itclTclIntStubsFcn.h
  /usr/local/include/libpostal/libpostal.h
  /usr/local/include/mysqlStubs.h
  /usr/local/include/node/js_native_api.h
  /usr/local/include/node/js_native_api_types.h
  /usr/local/include/node/libplatform/libplatform-export.h
  /usr/local/include/node/libplatform/libplatform.h
...  /usr/local/include/tclPlatDecls.h
  /usr/local/include/tclThread.h
  /usr/local/include/tclTomMath.h
  /usr/local/include/tclTomMathDecls.h
  /usr/local/include/tdbc.h
  /usr/local/include/tdbcDecls.h
  /usr/local/include/tdbcInt.h
  /usr/local/include/tk.h
  /usr/local/include/tkDecls.h
  /usr/local/include/tkPlatDecls.h
  /usr/local/include/wkhtmltox/image.h
  /usr/local/include/wkhtmltox/pdf.h

Warning: Unbrewed .la files were found in /usr/local/lib.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected .la files:
  /usr/local/lib/libgfortran.la
  /usr/local/lib/libgomp.la
  /usr/local/lib/libpocketsphinx.la
  /usr/local/lib/libpostal.la
  /usr/local/lib/libquadmath.la
  /usr/local/lib/libsphinxad.la
  /usr/local/lib/libsphinxbase.la

Warning: Unbrewed .pc files were found in /usr/local/lib/pkgconfig.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected .pc files:
  /usr/local/lib/pkgconfig/libpostal.pc
  /usr/local/lib/pkgconfig/pocketsphinx.pc
  /usr/local/lib/pkgconfig/sphinxbase.pc
  /usr/local/lib/pkgconfig/tcl.pc
  /usr/local/lib/pkgconfig/tk.pc

Warning: Unbrewed static libraries were found in /usr/local/lib.
If you didn't put them there on purpose they could cause problems when
building Homebrew formulae, and may need to be deleted.

Unexpected static libraries:
  /usr/local/lib/libgfortran.a
  /usr/local/lib/libgomp.a
  /usr/local/lib/libpocketsphinx.a
  /usr/local/lib/libpostal.a
  /usr/local/lib/libquadmath.a
  /usr/local/lib/libsphinxad.a
  /usr/local/lib/libsphinxbase.a
  /usr/local/lib/libtclstub8.6.a
  /usr/local/lib/libtkstub8.6.a

Warning: You have unlinked kegs in your Cellar.
Leaving kegs unlinked can lead to build-trouble and cause brews that depend on
those kegs to fail to run properly once built. Run `brew link` on these:
  inchi
  mpich
  ssed
  python
  libmpc@0.8
  nss
  isl@0.11
  numpy
  mpfr@2
  gmp@4
  poppler
  nspr
  scipy
  gcc@4.8

Warning: Some installed formulae are not readable:
  indigo: Unsupported special dependency :python

  rdkit: Unsupported special dependency :python3

Warning: Your Xcode (10.3) is outdated.
Please update to Xcode 11.3.1 (or delete it).
Xcode can be updated from the App Store.


Warning: Broken symlinks were found. Remove them with `brew cleanup`:
  /usr/local/share/man/man5/package-lock.json.5
  /usr/local/share/man/man7/removing-npm.7

Warning: Some installed formulae are missing dependencies.
You should `brew install` the missing dependencies:
  brew install imlib2 libcaca opusfile

Run `brew missing` for more details.

",0,630,"This is a duplicate of issue 3229 on the PyPy issue tracker. It seems our buildbot is using a brew installed version of libffi. Until we solve this, you should be able to do brew install pypy3. Help from someone who can make the PyPy build look more like the CPython one (which apparently does not have this problem) are welcome.
",,
SciPy unexpected issue,https://stackoverflow.com/questions/25161815,scipy curve_fit returns error for keyword absolute_sigma,"I am using ubuntu 14.04 on python 2.7 with numpy version 1.8.1 and scipy version 0.13.3. When I do a curve_fit with the keyword absolute_sigma=True, I get the message:


  TypeError: leastsq() got an unexpected keyword argument 'absolute_sigma'


I updated numpy and scipy to the versions mentioned above as the keyword absolute_sigma seems to be a new feature. However, no success!

any ideas how to solve this issue? Do I need scipy 0.14... ?

Thanks a lot
",0,2500,"The absolute_sigma parameter was added in version 0.14. Compare the call signatures:


version 0.14
version 0.13

",,
SciPy strange behavior,https://stackoverflow.com/questions/3812896,Can some explain this strange behavior of the hypergeometric distribution in scipy?,"I am running Python 2.6.5 on Mac OS X 10.6.4 (this is not the native version, I installed it myself) with Scipy 0.8.0. If I do the following:

&gt;&gt;&gt; from scipy.stats import hypergeom
&gt;&gt;&gt; hypergeom.sf(5,10,2,5)


I get an IndexError. Then I do:

&gt;&gt;&gt; hypergeom.sf(2,10,2,2)
-4.44....


I suspect the negative value is due to bad floating point precision. Then I do the first one again:

&gt;&gt;&gt; hypergeom.sf(5,10,2,5)
0.0


Now it works! Can someone explain this? Are you seeing this behavior too?
",11,1399,"The problem seems to arise based if the first call to the survival function is in the range that should obviously be zero (see my comment to the previous answer).  E.g., for calls to hypergeom.sf(x,M,n,N) it fails if the first call to a hypergeometric function to the function is a situation where x &gt; n, where the survival function will always be zero.  

You could trivially fix this temporarily by:

def new_hypergeom_sf(k, *args, **kwds):
    from scipy.stats import hypergeom
    (M, n, N) = args[0:3]
    try:
        return hypergeom.sf(k, *args, **kwds)
    except Exception as inst:
        if k &gt;= n and type(inst) == IndexError:
            return 0 ## or conversely 1 - hypergeom.cdf(k, *args, **kwds)
        else:
            raise inst


Now if you have no problem editing the /usr/share/pyshared/scipy/stats/distributions.py (or equivalent file), the fix is likely on line 3966 where right now it reads:

    place(output,cond,self._sf(*goodargs))
    if output.ndim == 0:
        return output[()]
    return output


But if you change it to:

    if output.ndim == 0:
        return output[()]
    place(output,cond,self._sf(*goodargs))
    if output.ndim == 0:
        return output[()]
    return output


It now works without the IndexError.  Basically if the output is zero dimensional because it fails the checks, it tries to call place, fails, and doesn't generate the distribution.  (This doesn't happen if a previous distribution has already been created which is likely why this wasn't caught on earlier tests.)  Note that place (defined in numpy's function_base.py) will change elements of the array (though I'm not sure if it changes the dimensionality) so it may be best to still have it leave the 0 dim check after place too.  I haven't fully tested this to see if this change breaks anything else (and it applies to all discrete random variable distributions), so it maybe its best to do the first fix.

It does break it; e.g., stats.hypergeom.sf(1,10,2,5) returns as zero (instead of 2/9).  

This fix seems to work much better, in the same section:

class rv_discrete(rv_generic):
...
    def sf(self, k, *args, **kwds):
    ...
        if any(cond):
            place(output,cond,self._sf(*goodargs))
        if output.ndim == 0:
            return output[()]
        return output

","I don't know python, but the function is defined like this:
hypergeom.sf(x,M,n,N,loc=0) 

M is the number of interesting objects, N the total number of objects, and n is how often you ""pick one"" (Sorry, German statistician).

If you had a bowl with 20 balls, 7 of those yellow (an interesting yellow), then N is 20 and M is 7.

Perhaps the function behaves undefined for the (nonsense) case when M&gt;N ?
",
SciPy strange behavior,https://stackoverflow.com/questions/65585639,Eigenvalues in Python: A Bug?,"Here are two assumptions about eigenvectors and eigenvalues of square matrices. I believe that both are true:

If a matrix is symmetric and contains only real values, then it is a Hermitian matrix, and then all eigenvalues should be real numbers and all components of all eigenvectors should also be real numbers. No complex numbers should appear in the results when you calculate eigenvectors and eigenvalues from Hermitian matrices.

The eigenvector of a given eigenvalue, calculated from a given matrix should always point into a direction that is determined only by the matrix and the eigenvalue. The algorithm used to calculate it has no influence on the result, as long as the algorithm is implemented correctly.


But both assumptions do not hold when you use standard libraries in Python to calculate eigenvectors and eigenvalues. Do those methods contain bugs?
There are four different methods to calculate eigenvalues and eigenvectors from Hermitian matrices:

numpy.linalg.eig
scipy.linalg.eig
numpy.linalg.eigh
scipy.linalg.eigh

#1 and #2 can be used for any square matrix (including Hermitian matrices).
#3 and #4 are made for Hermitian matrices only. As far as I did understand their purpose is just that they run faster, but the results should be the same (as long as the input is really Hermitian).
But the four methods deliver three different results for the very same input. Here is the program that I used to test all four methods:
#!/usr/bin/env python3

import numpy as np
import scipy.linalg as la

A = [
    [19, -1, -1, -1, -1, -1, -1, -1],
    [-1, 19, -1, -1, -1, -1, -1, -1],
    [-1, -1, 19, -1, -1, -1, -1, -1],
    [-1, -1, -1, 19, -1, -1, -1, -1],
    [-1, -1, -1, -1, 19, -1, -1, -1],
    [-1, -1, -1, -1, -1, 19, -1, -1],
    [-1, -1, -1, -1, -1, -1, 19, -1],
    [-1, -1, -1, -1, -1, -1, -1, 19]
]

A = np.array(A, dtype=np.float64)

delta = 1e-12
A[5,7] += delta
A[7,5] += delta

if np.array_equal(A, A.T):
    print('input is symmetric')
else:
    print('input is NOT symmetric')

methods = {
    'np.linalg.eig'  : np.linalg.eig,
    'la.eig'         : la.eig,
    'np.linalg.eigh' : np.linalg.eigh,
    'la.eigh'        : la.eigh
}

for name, method in methods.items():

    print('============================================================')
    print(name)
    print()

    eigenValues, eigenVectors = method(A)
    
    for i in range(len(eigenValues)):
        print('{0:6.3f}{1:+6.3f}i '.format(eigenValues[i].real, eigenValues[i].imag), end=' |  ')
        line = eigenVectors[i]
        for item in line:
            print('{0:6.3f}{1:+6.3f}i '.format(item.real, item.imag), end='')
        print()

    print('---------------------')

    for i in range(len(eigenValues)):
        if eigenValues[i].imag == 0:
            print('real    ', end=' |  ')
        else:
            print('COMPLEX ', end=' |  ')
        line = eigenVectors[i]
        for item in line:
            if item.imag == 0:
                print('real    ', end='')
            else:
                print('COMPLEX ', end='')
        print()

    print()

And here is the output it produces:
input is symmetric
============================================================
np.linalg.eig

12.000+0.000i  |  -0.354+0.000i  0.913+0.000i  0.204+0.000i -0.013+0.016i -0.013-0.016i  0.160+0.000i -0.000+0.000i  0.130+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.183+0.000i  0.208+0.000i  0.379-0.171i  0.379+0.171i -0.607+0.000i  0.000+0.000i -0.138+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.468-0.048i -0.468+0.048i  0.153+0.000i  0.001+0.000i -0.271+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i  0.657+0.000i  0.657-0.000i  0.672+0.000i -0.001+0.000i  0.617+0.000i 
20.000-0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.276+0.101i -0.276-0.101i -0.361+0.000i  0.001+0.000i -0.644+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.001+0.000i -0.612+0.000i -0.001+0.000i -0.001-0.000i  0.001+0.000i  0.706+0.000i -0.000+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.276+0.101i -0.276-0.101i -0.018+0.000i -0.000+0.000i  0.306+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.001+0.000i -0.612+0.000i -0.001+0.000i -0.001-0.000i  0.001+0.000i -0.708+0.000i  0.000+0.000i 
---------------------
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
COMPLEX  |  real    real    real    real    real    real    real    real    
COMPLEX  |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    

============================================================
la.eig

12.000+0.000i  |  -0.354+0.000i  0.913+0.000i  0.204+0.000i -0.013+0.016i -0.013-0.016i  0.160+0.000i -0.000+0.000i  0.130+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.183+0.000i  0.208+0.000i  0.379-0.171i  0.379+0.171i -0.607+0.000i  0.000+0.000i -0.138+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.468-0.048i -0.468+0.048i  0.153+0.000i  0.001+0.000i -0.271+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i  0.657+0.000i  0.657-0.000i  0.672+0.000i -0.001+0.000i  0.617+0.000i 
20.000-0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.276+0.101i -0.276-0.101i -0.361+0.000i  0.001+0.000i -0.644+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.001+0.000i -0.612+0.000i -0.001+0.000i -0.001-0.000i  0.001+0.000i  0.706+0.000i -0.000+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.182+0.000i  0.203+0.000i -0.276+0.101i -0.276-0.101i -0.018+0.000i -0.000+0.000i  0.306+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.001+0.000i -0.612+0.000i -0.001+0.000i -0.001-0.000i  0.001+0.000i -0.708+0.000i  0.000+0.000i 
---------------------
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
COMPLEX  |  real    real    real    real    real    real    real    real    
COMPLEX  |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    
real     |  real    real    real    COMPLEX COMPLEX real    real    real    

============================================================
np.linalg.eigh

12.000+0.000i  |  -0.354+0.000i  0.000+0.000i  0.000+0.000i -0.086+0.000i  0.905+0.000i -0.025+0.000i  0.073+0.000i  0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.000+0.000i -0.374+0.000i  0.149+0.000i -0.236+0.000i -0.388+0.000i  0.682+0.000i  0.206+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.001+0.000i  0.551+0.000i  0.136+0.000i -0.180+0.000i  0.616+0.000i  0.317+0.000i  0.201+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.001+0.000i -0.149+0.000i  0.719+0.000i -0.074+0.000i -0.042+0.000i -0.534+0.000i  0.207+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.005+0.000i  0.505+0.000i -0.386+0.000i -0.214+0.000i -0.556+0.000i -0.274+0.000i  0.203+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.707+0.000i -0.004+0.000i  0.002+0.000i  0.001+0.000i  0.002+0.000i -0.000+0.000i -0.612+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.003+0.000i -0.529+0.000i -0.535+0.000i -0.203+0.000i  0.398+0.000i -0.262+0.000i  0.203+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.707+0.000i  0.001+0.000i  0.001+0.000i  0.000+0.000i -0.005+0.000i -0.001+0.000i -0.612+0.000i 
---------------------
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    

============================================================
la.eigh

12.000+0.000i  |  -0.354+0.000i  0.000+0.000i  0.000+0.000i -0.225+0.000i  0.882+0.000i  0.000+0.000i  0.065+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.000+0.000i -0.395+0.000i  0.332+0.000i -0.156+0.000i  0.227+0.000i  0.701+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.001+0.000i  0.612+0.000i  0.011+0.000i -0.204+0.000i -0.597+0.000i  0.250+0.000i -0.200+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.001+0.000i -0.086+0.000i  0.689+0.000i  0.030+0.000i -0.054+0.000i -0.589+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.005+0.000i  0.413+0.000i -0.264+0.000i -0.245+0.000i  0.711+0.000i -0.165+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i -0.707+0.000i -0.004+0.000i -0.000+0.000i  0.001+0.000i -0.002+0.000i -0.001+0.000i  0.612+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.003+0.000i -0.540+0.000i -0.542+0.000i -0.309+0.000i -0.290+0.000i -0.261+0.000i -0.205+0.000i 
20.000+0.000i  |  -0.354+0.000i  0.707+0.000i  0.001+0.000i -0.000+0.000i  0.001+0.000i  0.005+0.000i -0.001+0.000i  0.612+0.000i 
---------------------
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real    
real     |  real    real    real    real    real    real    real    real 

As you can see, numpy.linalg.eig and scipy.linalg.eig produce complex numbers in their output, but they shouldn't. This could be accepted as some kind of rounding error, if the magnitude of the imaginary part would by tiny compared to the magnitude of the real part. But this is not the case. One of the numbers that are produced is -0.013+0.016i. Here the imaginary part has an even higher magnitude than the real part.
Even worse: The four methods produce three different results.
All four methods calculate only once an eigenvalue of 12 and 7 times an eigenvalue of 20. And all eigenvectors always have the length 1. This means, all four methods should produce the very same eigenvector for eigenvalue 12. But only numpy.linalg.eig and scipy.linalg.eig produce the same output.
Here are the components of the eigenvector for eigenvalue 12. Have a closer look to the lines marked with an arrow (&lt;==). Here you find three different values, but the values should be exactly equal. And if you have a second look, you will see, that only in the 1st line all three values are equal. In all other lines you will find 2 or 3 different values.
numpy.linalg.eig  |                     |
scipy.linalg.eig  |  numpy.linalg.eigh  |  scipy.linalg.eigh
------------------+---------------------+-------------------
   -0.354+0.000i  |      -0.354+0.000i  |      -0.354+0.000i
    0.913+0.000i  |       0.000+0.000i  |       0.000+0.000i
    0.204+0.000i  |       0.000+0.000i  |       0.000+0.000i
   -0.013+0.016i  |      -0.086+0.000i  |      -0.225+0.000i   &lt;===
   -0.013-0.016i  |       0.905+0.000i  |       0.882+0.000i   &lt;===
    0.160+0.000i  |      -0.025+0.000i  |       0.000+0.000i   &lt;===
   -0.000+0.000i  |       0.073+0.000i  |       0.065+0.000i   &lt;===
    0.130+0.000i  |       0.205+0.000i  |      -0.205+0.000i

Here are my questions:

How is this possible?
Are these bugs?
Is one of the results correct?
If there is a method that delivers correct results: Which is it?


p.s: Here are relevant version informations:

I did run this code on an iMac (macOS Catalina Version 10.15.7)
The python version is 3.8.5
The version of numpy is 1.19.5
The version of scipy is 1.6.0


This is the output of numpy.show_config()
(as requested in a comment):
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
None


ADDENDUM (added 1 day after the question was asked)
Reaction to comments:

complex eigenvectors of real symmetric matrices
@Rethipher: Thank you! I did read and understand the question you linked to (Can a real symmetric matrix have complex eigenvectors?), and I also did read the answers, but I didnt understand them. Did they say yes or no? (rhetoric question, no need to answer, see next line)
@Mark Dickinson &amp; @bnaecker: Thank you for making clear, that my assumption was wrong.

real symmetric matrices vs. Hermitian matrices
@bnaecker: The set of real numbers is a subset of the set of complex numbers. Those complex numbers which are equal to their own complex conjugate are called real. So, the set of real symmetric matrices is a subset of Hermitian matrices. This is important, because numpy.linalg.eigh and scipy.linalg.eigh are designed to handle Hermitian matrices. And because every real symmetric matrix is a Hermitian matrix, those modules also can be used for my purposes.

mixing up rows and columns
@Mark Dickinson &amp; @bnaecker: Thank you, I think you are right. Also the documentations says so, I should have read it more carefully. But even if you compare columns instead of rows you will still find that the 4 methods produce 3 different results. But if the result contains a 7-dimensional subspace that can be described with 7 real basis vectors only, I still find it strange, that an algorithm produces a complex basis.

a bug would be surprising
@bnaecker: This is true, but surprising bugs do exist. (Like Heartbleed and some others.) So, this is not really an argument.

I get reals - your sample matrix doesn't contain floats
@Stef &amp; @JohanC: Sorry, you didnt read my program carefully enough. I added a value of 1e-12 to A[5,7] and A[7,5] to simulate tiny rounding errors that appear inevitably in my real app before it comes to the calculation of eigenvalues and eigenvectors. (What Ive posted here is just a tiny test program, just big enough to demonstrate the issue.)
And you are right, Stef: Without adding this tiny noise, I also get real results. But only a tiny change of one millionth of one millionth makes such a big difference, and I can't understand why.


Reaction to DavidB2013s answer :
I tried the tool you suggested, and I got different results. I think you also forgot to add that little noise of 1e-12 to A[5,7] and A[7,5]. However, all results are still real. I did get these eigenvalues:
12.000000000000249
20
20.00000000000075
19.999999999999
20
20
20
20

and these eigenvectors:
0.3535533905932847   0.9128505045937204      0.20252576206455747  0.002673672081814904   -0.09302397289286794   -0.09302397289286794   -0.09302397289286794   -0.09302397289286794     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406 -0.20415317121194954   -0.20415317121194954   -0.20415317121194954   -0.20415317121194954     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406 -0.20415317121194954   -0.20415317121194954   -0.20415317121194954    0.9080920678356449     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406 -0.20415317121194954    0.9080920678356449    -0.20415317121194954   -0.20415317121194954     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406  0.9080920678356449    -0.20415317121194954   -0.20415317121194954   -0.20415317121194954     
0.35355339059324065 -0.00011103276380548543 -0.6116010247648269   0.7060012169461334      0.0005790869815273477  0.0005790869815273477  0.0005790869815273477  0.0005790869815273477     
0.3535533905932848  -0.18259457246238117     0.20444330131542393 -0.00009386949436945406 -0.20415317121194954   -0.20415317121194954    0.9080920678356449    -0.20415317121194954     
0.35355339059324054  0.0002333904819895115  -0.6131412438770024  -0.7082055415560993      0.0009655029234935232  0.0009655029234935232  0.0009655029234935232  0.0009655029234935232     

Only the vector for eigenvalue 12 has the same values as your calculation: (There is a difference of approx. 1.1e-14 in 6 dimensions and 3.3e-14 in the two other dimensions, but I count this as rounding error.) All other vectors are significantly different (the smallest differences are of the size of 0.02). It puzzles me, that a tiny rounding error of 1e-12 in just 2 elements of the input matrix can produce so big differences.

I calculated the eigenvalues with another method (with the help of https://www.wolframalpha.com), and when I didnt add the tiny delta values, which should simulate rounding errors, I only get two different eigenvalues which are 12 and 20.
The characteristic polynomial of the given matrix is:
(20 - )^7 * (12 - )

So, it has one root at =12 and 7 roots at =20 and these 8 roots are the 8 eigenvalues. All of them real numbers.
When I add the tiny delta values, I get this characteristic polynomial:
(20 - )^5 * (19999999999999/1000000000000 - ) * (1000000000000 ^2 - 32000000000001  + 240000000000014)/1000000000000

It has these roots:
=12.00000000000024999999999998 (rounded)
=19.999999999999 (exact value)
=20 (exact value)  
=20 (exact value)  
=20 (exact value)  
=20 (exact value)  
=20 (exact value)  
=20.00000000000075000000000002 (rounded)

And again all 8 eigenvalues are real numbers.
Then I calculated the eigenvectors. Without adding 1e-12 I get this results:
Vector for eigenvalue 12:
v = (1,1,1,1,1,1,1,1)

The length of this vector is sqrt(8), and if you multiply the vector with 1/sqrt(8), you get exactly the result from the other calculations (0.35355339 in each dimension).
But the seven eigenvectors for eigenvalue 20 are very different. They are:
(-1,1,0,0,0,0,0,0)
(-1,0,1,0,0,0,0,0)
(-1,0,0,1,0,0,0,0)
(-1,0,0,0,1,0,0,0)
(-1,0,0,0,0,1,0,0)
(-1,0,0,0,0,0,1,0)
(-1,0,0,0,0,0,0,1)

Even if you bring them to the length 1, they are different from all other results and it is very easy to see that they are correct. The other results are also correct, but I would prefer these simple results.
I also calculated the eigenvalues for the version with the tiny noise. All 8 vectors are so close to the noise-less results, that even Wolfram Alpha rounded them to exactly the same values as before. And this is exactly the behavior that I would expect from an algorithm that calculates eigenvalues and eigenvectors:

Tiny variations in the input should - when ever it is possible - return tiny variations in the results.

",10,1316,"As far as I know, assumption 1 is correct, but assumption 2 is not.
A Real Symmetric matrix produces eigenvalues and eigenvectors that are real only.
However, for a given eigenvalue, the associated eigenvector isn't necessarily unique.
Furthermore, round-off error shouldn't be so significant for a matrix that actually isn't that big, or contain numbers that aren't very small.
For comparison, I ran your test matrix through a JavaScript version of RG.F (Real General, from the EISPACK Library):  Eigenvalues and Eigenvectors Calculator
Here is the output:
Eigenvalues:
   20
   12
   20
   20
   20
   20
   20
   20

Eigenvectors:
 0.9354143466934854     0.35355339059327395     -0.021596710639534     -0.021596710639534     -0.021596710639534     -0.021596710639534     -0.021596710639533997     -0.021596710639533997
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999623     -0.15117697447673797     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999622     -0.15117697447673797
-0.1336306209562122     0.3535533905932738     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     -0.15117697447673797     0.9286585574999622

No imaginary components.
To confirm, or deny, the validity of results, you could always write a small program that plugs the results back into the original equation. Simple matrix and vector multiplication. Then you'd know for sure whether or not the outputs are correct. Or, if they are wrong, how far away from correct answers they are.
",,
SciPy strange behavior,https://stackoverflow.com/questions/25752444,"Scipy error: numpy.dtype size changed, may indicate binary incompatibility (and associated strange behavior)","I am installing numpy/scipy/scikit-learn on OS X 10.9.4, and am getting errors about ""numpy.dtype size changed, may indicate binary incompatibility"".

Here's what I did to construct the repo:

mkvirtualenv thm
workon thm
pip install numpy scipy pandas ipython # and some other stuff
cd /path/to/our/repo
# run tests


Here's a traceback excerpt of a relevant warning (turned into an error because we use warnings.simplefilter('error') at the beginning of our tests):

======================================================================
ERROR: Failure: RuntimeWarning (numpy.dtype size changed, may indicate binary in
compatibility)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/nose/loader.py"",
 line 414, in loadTestsFromName
    addr.filename, addr.module)
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/nose/importer.py
"", line 47, in importFromPath
    return self.importFromDir(dir_path, fqname)
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/nose/importer.py
"", line 94, in importFromDir
    mod = load_module(part_fqname, fh, filename, desc)
  File ""/Users/ben/code/thm/alpha/prosper/base/stats/test_auc.py"", line 3, in &lt;m
odule&gt;
    import sklearn.metrics
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/sklearn/metrics/
__init__.py"", line 6, in &lt;module&gt;
    from .metrics import (accuracy_score,
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/sklearn/metrics/metrics.py"", line 27, in &lt;module&gt;
    from scipy.spatial.distance import hamming as sp_hamming
  File ""/Users/ben/.virtualenvs/thm/lib/python2.7/site-packages/scipy/spatial/__init__.py"", line 90, in &lt;module&gt;
    from .ckdtree import *
  File ""__init__.pxd"", line 155, in init scipy.spatial.ckdtree (scipy/spatial/ckdtree.c:20570)
RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility


I'm told that this warning is caused by scipy being compiled against a different version of numpy than the one installed. However, I installed them all with pip in what I thought was a pretty standard way, so this shouldn't be a problem, I would think.

Weirdly, although running our entire test suite as a whole (via python -m unittest discover) gives these errors, running the individual tests (via python -m unittest &lt;module&gt;) works fine.

According to the tests, here's some relevant version info:

numpy version 1.9.0 (rev 07601a64cdfeb1c0247bde1294ad6380413cab66)
scipy version 0.14.0 (built against numpy 1.9.0)
sklearn version 0.15.2
pandas version 0.14.1


Happy to provide more info on request!
",10,16174,"How did you build sklearn 0.14.1? Did you build it against the same version of numpy as you did for scipy?

Recent versions of scikit-learn, scipy and numpy have prebuilt-packages. In particular scikit-learn 0.15.2 should be binary compatible with numpy 1.7+. I think the same is true with scipy 0.14.0 but you said you built it yourself from source, which is not what pip should do by default (it should just install the prebuilt whl package).

Edit: have you tried to do:

pip install -U scipy scikit-learn pandas


to make sure that you are using the latest stable versions of the whl for those packages?

Edit: The comment below has the actual answer that works and is presumably why this answer was accepted.  Namely:

pip uninstall -y scipy scikit-learn
pip install --no-binary scipy scikit-learn

",,
SciPy strange behavior,https://stackoverflow.com/questions/17130795,Periodogram in Octave/Matlab vs Scipy,"I am porting some matlab code to python using scipy and got stuck with the following line:

Matlab/Octave code

[Pxx, f] = periodogram(x, [], 512, 5)


Python code

f, Pxx = signal.periodogram(x, 5, nfft=512)


The problem is that I get different output on the same data. More specifically, Pxx vectors are different. I tried different windows for signal.periodogram, yet no luck (and it seems that default scypy's boxcar window is the same as default matlab's rectangular window) Another strange behavior is that in python, first element of Pxx is always 0, no matter what data input is.

Am i missing something? Any advice would be greatly appreciated!  



Simple Matlab/Octave code with actual data: http://pastebin.com/czNeyUjs
Simple Python+scipy code with actual data: http://pastebin.com/zPLGBTpn
",10,4456,"After researching octave's and scipy's periodogram source code I found that they use different algorithm to calculate power spectral density estimate. Octave (and MATLAB) use FFT, whereas scipy's periodogram use the Welch method. 

As @georgesl has mentioned, the output looks quite alike, but still, it differs. And for porting reason it was critical. In the end, I simply wrote a small function to calculate PSD estimate using FFT, and now output is the same. According to timeit testing, it works ~50% faster (1.9006s vs 2.9176s on a loop with 10.000 iterations). I think it's due to the FFT being faster than Welch in scipy's implementation, of just being faster. 

Thanks to everyone who showed interest.
","I faced the same problem but then I came across the documentation of scipy's periodogram  

As you would see there that detrend='constant' is the default argument. This means that python automatically subtracts the mean of the input data from each point. (Read here). While Matlab/Octave do no such thing. I believe that is the reason why the outputs are different. Try specifying detrend=False, while calling scipy's periodogram you should get the same output as Matlab.
","After reading the Matlab and Scipy documentation, another contribution to the different values could be that they use different default window function. Matlab uses a Hamming window, and Scipy uses a Hanning. The two window functions and similar but not identical.
"
SciPy strange behavior,https://stackoverflow.com/questions/20037444,scipy.signal.resample behaves strangely,"I am currently working on some signal processing (using scipy), but I encountered a strange problem and can't figure out what's wrong. Namely, I am reading some audio data from a .wav file, but have to resample before further processing. The signal has more than 500,000 samples.

Now, scipy.signal.resample takes more than 10 minutes on just one of the channels. OK, I thought, this might be normal because there are a lot of samples. However, then I decided to experiment with two other ""signals"" (i.e. a randomly generated array of numbers and an array of zeros) with 1,000,000 samples and resample these ones. Strangely, resampling in this case takes only a few milliseconds, so the size is obviously not a problem.

My final experiment was extracting the zeros from my original signal (there are about 50,000 samples that are zero-valued) and resampling them. I was totally surprised to see that resampling only 50,000 zeros takes about a minute. Previously, I resampled an array of zeros that had 1,000,000 samples in a few milliseconds and now I have to wait about a minute for an array of 50,000 samples. Something has to be wrong, but I can't figure out what.

I really don't see any reason for this behavior; especially the zeros (1,000,000 and just a few milliseconds vs 50,000 and a minute) surprise me a lot.

Here's a sample code, so that you know what I'm talking about:

import scipy.io.wavfile as wavfile
import numpy
import scipy.signal as signal

sample_rate, signal_data = wavfile.read('file.wav')

test_channel = numpy.array(signal_data[:,0], dtype=float)
channel_zeros = numpy.array(signal_data[numpy.where(signal_data[:,0]==0)[0],0], dtype=float)
test_signal = numpy.random.rand((1000000))
test_signal_2 = numpy.zeros((1000000))

number_of_samples = 500

#both of these are executed in less than a second
resampled_random = signal.resample(test_signal, number_of_samples)
resampled_zeros = signal.resample(test_signal_2, number_of_samples)

#this takes minutes
resamples_original_signal = signal.resample(test_channel, number_of_samples)

#this takes about a minute
resampled_original_zeros = signal.resample(channel_zeros, number_of_samples)


Do you have any idea what might be wrong with this? Thanks in advance.
",9,3527,"The numpy implementation of FFT (based on FFTPACK) is fastest when the length of the data is a power of 2 (e.g. 2, 4, 8, 16, 32) and slowest when it is a prime. To speed up processing of the signal, you can zero-pad the data to a power of 2 length.

In Python you can use the following code to find the next largest power of 2 for a given number:

y = np.floor(np.log2(n))
nextpow2 = np.power(2, y+1)


You can use this with numpy.pad to pad your data array to this size:

sample_rate, signal_data = wavfile.read('file.wav')
n = signal_data.shape[0]

y = np.floor(np.log2(n))
nextpow2  = np.power(2, y+1)

signal_data  = np.pad(signal_data , ((0, nextpow2-n), (0,0)), mode='constant')


For more background on scipy/numpy and FFT in general in see this question.
",,
SciPy strange behavior,https://stackoverflow.com/questions/37592643,Scipy interpolate returns a &#39;dimensionless&#39; array,"I understand that interp1d expects an array of values to interpolate, but the behavior when passing it a float is strange enough to ask what is going on and what exactly is being returned

import numpy as np
from scipy.interpolate import interp1d

x = np.array([1,2,3,4])
y = np.array([5,7,9,15])
f = interp1d(x,y, kind='cubic')
a = f(2.5)

print(repr(a))
print(""type is {}"".format(type(a)))
print(""shape is {}"".format(a.shape))
print(""ndim is {}"".format(a.ndim))
print(a)


Output:

array(7.749999999999992)
type is &lt;class 'numpy.ndarray'&gt;
shape is ()
ndim is 0
7.749999999999992


EDIT: To clarify, I would not expect numpy to even have a dimensionless, shapeless array much less a scipy function return one. 

print(""Numpy version is {}"".format(np.__version__))
print(""Scipy version is {}"".format(scipy.__version__))

Numpy version is 1.10.4
Scipy version is 0.17.0

",4,3305,"The interp1d returns a value that matches the input in shape - after wrapping in np.array() if needed:

In [324]: f([1,2,3])
Out[324]: array([ 5.,  7.,  9.])

In [325]: f([2.5])
Out[325]: array([ 7.75])

In [326]: f(2.5)
Out[326]: array(7.75)

In [327]: f(np.array(2.5))
Out[327]: array(7.75)


Many numpy operations do return scalars instead of 0d arrays.

In [330]: np.arange(3).sum()
Out[330]: 3


though actually it returns a numpy object

In [341]: type(np.arange(3).sum())
Out[341]: numpy.int32


which does have a shape () and ndim 0.

Whereas interp1d returns an array. 

In [344]: type(f(2.5))
Out[344]: numpy.ndarray


You can extract the value with [()] indexing

In [345]: f(2.5)[()]
Out[345]: 7.75

In [346]: type(f(2.5)[()])
Out[346]: numpy.float64


This may just be an oversight in the scipy code.  How often do people want to interpolate at just one point?  Isn't interpolating over a regular grid of points more common?

==================

The documentation for f.__call__ is quite explicit about returning an array.

Evaluate the interpolant

Parameters
----------
x : array_like
    Points to evaluate the interpolant at.

Returns
-------
y : array_like
    Interpolated values. Shape is determined by replacing
    the interpolation axis in the original array with the shape of x.


===============

The other side to the question is why does numpy even have a 0d array.  The linked answer probably is sufficient.  But often the question is asked by people who are used to MATLAB.  In MATLAB nearly everything is 2d.  There aren't any (true) scalars.  Now MATLAB has structures and cells, and matrices with more than 2 dimensions.  But I recall a time (in the 1990s) when it didn't have those.  Everything, literal, was a 2d matrix.

The np.matrix approximates that MATLAB case, fixing its arrays at 2d. But it does have a _collapse method that can return a 'scalar'.
",,
SciPy strange behavior,https://stackoverflow.com/questions/27004245,Numpy array bug or feature (catsing to int behind the scenes)?,"Today I have noticed strange behavior of Numpy/Scipy arrays. It looks like adding array cell with integer inside to float can have two different results, depending on the variable to which the result is assigned. Instead of long explanations, below I present the code:

import scipy as sp
array_int = sp.array([[0], [0]])
float_operand = 0.1
print array_int[0, 0] + float_operand #gives 0.1


But

import scipy as sp
array_int = sp.array([[0], [0]])
float_operand = 0.1
array_int[0, 0] = array_int[0, 0] + float_operand 
print array_int[0, 0] #gives 0


I could understand if this kind of behavior was inherited from Python, but:

In contrary to behavior of ""bare"" Python (2.7):

integer_for_sure = int(0) 
integer_for_sure = integer_for_sure + 0.1
print integer_for_sure #gives 0.1 as expected


Is this kind of feature somwhere documented? Has anybody encounterd it before?
",3,135,"Henry Keiter has explained it well enough. I would only add one technical detail.

In contrast to the regular assignment which simply rewires integer_for_sure to refer to the float object that results from integer_for_sure + 0.1, thus changing the type of the variable, assignment to array elements such as

array_int[0, 0] = array_int[0, 0] + float_operand


is actually syntactic sugar for the more verbose

array_int.__setitem__((0,0), array_int.__getitem__((0,0)) + float_operand)


(this applies to old-style classes; it looks a bit different for new-style classes but the idea stays the same)

The __setitem__ method for each array type performs a typecast of its value argument to the type of the array. The actual C code that implements the assignment is kind of ugly and involves a custom preprocessor.

On the other side

print array_int[0, 0] + float_operand


is

print array_int.__getitem__((0,0)) + float_operand


i.e. it fetches the integer value from array_int, sums it with float_operand and the resulting float object is passed to print. There is no intermediate assignment to array_int[0, 0] and therefore no typecast.
","This is not a behavior ""inherited from Python"" -- as you can see, adding a float to an int in pure Python produces a float result. Rather, you can think of this behavior as ""inherited from C."" Unlike Python lists, numpy arrays have strong element types. The array constructor includes a telltale optional keyword argument that alludes to this:

dtype : data-type, optional
The desired data-type for the array. If not given, then the type will be determined as the minimum type required to hold the objects in the sequence. This argument can only be used to upcast the array.

The emphasis is mine. When you create an array with np.array([[0], [0]]), you get a two-dimentional integer array, because an integer is the smallest datatype that can contain 0. Once the integer array is created, it may only contain integers. If you attempt to insert a float, it will be cast to an integer as you noticed in order to be placed in the array.

If you want to store floats in the array eventually, you need to initialize your array as a float array ('upcast' it). This can be accomplished by using the dtype argument mentioned above, or simply by putting a float value into the initial array (e.g. 0.0 instead of the integer 0).
import scipy as sp
array_int = sp.array([[0], [0]])
array_float = sp.array([[0.0], [0]])  # Note the extra .0 to make one element a float

array_int[0, 0] = array_int[0, 0] + 0.1
print array_int[0, 0] # 0

array_float[0, 0] = array_float[0, 0] + 0.1
print array_float[0, 0] # 0.1

",
SciPy strange behavior,https://stackoverflow.com/questions/65513251,Scipy coo_matrix.max() alters data attribute,"I am building a recommendation system using an open source library, LightFM. This library requires certain pieces of data to be in a sparse matrix format, specifically the scipy coo_matrix. It is here that I am encountering strange behavior. It seems like a bug, but it's more likely that I am doing something wrong.
Basically, I let LightFM.Dataset build me a sparse matrix, like so:
interactions, weights = dataset.build_interactions(data=_get_interactions_data())

The method, build_interactions, returns ""Two COO matrices: the interactions matrix and the corresponding weights matrix"" -- LightFM Official Doc.
When I inspect the contents of this sparse matrix (in practice, I use a debugger), like so:
for i in interactions.data:
    print(i, end=', ')

1, 1, 1, 1, 1, ....
It prints a long list of 1s, which indicates that the sparse matrix's nonzero elements are only 1s.
However, when I first check the max of the sparse matrix, it indicates that the maximum values in the sparse matrix is not a 1, its a 3. Furthermore, printing the matrix after that check will print a long list of 1s, 2s, and 3s. This is the code for that:
print(interactions.max())
for i in interactions.data:
    print(i, end=', ')

3 
1, 1, 3, 2, 1, 2, ...
Any idea what is going on here? Python is 3.6.8. Scipy is 1.5.4. CentOS7.
Thank you.
",1,199,"A 'raw' coo_matrix can have duplicate elements (repeats of the same row and col values), but when converted to csr format for calculations those duplicates are summed. It must be doing the same, but in-place, in order to find that max.
In [9]: from scipy import sparse
In [10]: M = sparse.coo_matrix(([1,1,1,1,1,1],([0,0,0,0,0,0],[0,0,1,0,1,2])))
In [11]: M.data
Out[11]: array([1, 1, 1, 1, 1, 1])
In [12]: M.max()
Out[12]: 3
In [13]: M.data
Out[13]: array([3, 2, 1])
In [14]: M
Out[14]: 
&lt;1x3 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    with 3 stored elements in COOrdinate format&gt;

Tracing through the max code I find it uses sum_duplicates
In [33]: M = sparse.coo_matrix(([1,1,1,1,1,1],([0,0,0,0,0,0],[0,0,1,0,1,2])))
In [34]: M.data
Out[34]: array([1, 1, 1, 1, 1, 1])
In [35]: M.sum_duplicates?
Signature: M.sum_duplicates()
Docstring:
Eliminate duplicate matrix entries by adding them together

This is an *in place* operation
File:      /usr/local/lib/python3.8/dist-packages/scipy/sparse/coo.py
Type:      method
In [36]: M.sum_duplicates()
In [37]: M.data
Out[37]: array([3, 2, 1])

",,
SciPy strange behavior,https://stackoverflow.com/questions/58164513,Problems with bisplrep and bisplev from scipy.interpolate,"I receive some quite strange behavior with bisplrep and bisplev from the scipy.interpolate module. I try to reproduce the example from the scipy homepage (https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html). I have some function f(x,y) which calculates me some z-values which I then interpolate using bisplrep. If I recalculate the data using bisplev and plot the data the resulting values appear rotated about 90. Even if I exchange the x and y values the plot is still rotated. Can somebody tell me if I am doing something completely wrong here? The following code should be sufficient to reproduce the error. I am using the most recent version of scipy and the error occurs in Jupyter, Spyder and in IDLE. 

import matplotlib.pyplot as plt
import numpy as np
from scipy import interpolate

def f(x, y):
    return x**2 + y**2

x, y = np.linspace(0, 5, 15), np.linspace(-2*np.pi, 2*np.pi, 15)
xx, yy = np.meshgrid(x, y)
zz = f(xx, yy)

tck = interpolate.bisplrep(xx, yy, zz)

plt.pcolor(xx, yy, zz)

x_new, y_new = np.linspace(0, 5, 100), np.linspace(-2*np.pi, 2*np.pi, 100)

z_new = interpolate.bisplev(x_new, y_new, tck)

plt.figure()    
plt.pcolor(x_new, y_new, z_new)

plt.figure()    
plt.pcolor(y_new, x_new, z_new)

plt.show()

",1,1443,"I used another definition for the grid. Now, it should work:

import matplotlib.pyplot as plt
import numpy as np
from scipy import interpolate

def f(x, y):
    return x**2 + y**2

#x, y = np.linspace(0, 5, 15), np.linspace(-2*np.pi, 2*np.pi, 15)
xx, yy = np.mgrid[0:5:15j, -2*np.pi:2*np.pi:15j]
zz = f(xx, yy)

tck = interpolate.bisplrep(xx, yy, zz)

plt.pcolor(xx, yy, zz)

#x_new, y_new = np.linspace(0, 5, 100), np.linspace(-2*np.pi, 2*np.pi, 100)
xx_new, yy_new = np.mgrid[0:5:100j, -2*np.pi:2*np.pi:100j]

zz_new = interpolate.bisplev(xx_new[:,0], yy_new[0,:], tck)

plt.figure()    
plt.pcolor(xx_new, yy_new, zz_new)

plt.figure()    
plt.pcolor(yy_new, xx_new, zz_new)

plt.show()

",,
SciPy strange behavior,https://stackoverflow.com/questions/31685156,Normalizing vector produces nan in Numpy,"I'm getting some strange behavior from scipy/numpy that I suspect is a bug but someone may know better?  I've got a pair of long arrays which I'm breaking into frames which are of length 2-4 for debugging purposes.  I want to normalize each pair of frames and take the dot product.  The code that does it (with some debugging output) is:

   tf = numpy.copy(t_frame) / norm(t_frame)
   pf = numpy.copy(p_frame) / norm(p_frame)
   print ""OPF:""
   print p_frame
   print ""PF: ""
   print pf
   print ""TF norm is: "" + str(norm(tf))
   print ""PF norm is: "" + str(norm(pf))
   print numpy.dot(tf, pf)
   return numpy.dot(tf, pf)


This does what I'd expect for a while (specifically giving a norm of 1 for tf and pf) but then I start to see lines like this:


  OPF:
  
  [ -91 -119 -137 -132]
  
  PF: 
  
  [ nan  nan  nan  nan]


What??  This can be normalized fine in a new Python window:

&gt;&gt;&gt; p = [ -91, -119, -137, -132] 
&gt;&gt;&gt; p / norm(p)
array([-0.37580532, -0.49143773, -0.56577285, -0.54512421])


For what it's worth, I've tried numpy.linalg.norm, scipy.linalg.norm, and defining a function to return the square root of the dot product.

Any ideas?

UPDATE:
Thanks for the suggestions!  I tried switching the dtype to float128 and am sadly getting similar behavior.  I'm actually inclined to believe that it's a bug in Python rather than numpy at this point:


If it were a straightforward overflow issue, it seems like I'd get it consistently with a given list.  But the norm computes fine if I do it in a new python session.
I tried rolling my own:

def norm(v):
   return (  sum(numpy.array(v)*numpy.array(v)))**(0.5)


This only uses numpy  to represent the arrays.  I still get the same issue, but later in the data set (and no runtime warnings).  It's doing about 37000 of these computations.
I'm actually computing the norm on two frames, a t_frame and a p_frame.  The computation of one chokes if and only if the computation for the other one does.


Put together, I think there's some weird buffer overflow somewhere in the bowels of Python (2.7.9)???  I ultimately need these computations to be fast as well; so I'm thinking of just switching over to Cython for that computation.

Update 2:
I tried really rolling my own:

def norm(v):
  sum = float(0)
  for i in range(len(v)):
    sum += v[i]**2
  return sum**(0.5)


and the problem disappears.  So I would guess that it is a bug in numpy (1.9.0 on Gentoo Linux).
",1,5875,"It looks like this is a bug in numpy.  I can reproduce the problem if the data type of the array is np.int16:

In [1]: np.__version__
Out[1]: '1.9.2'

In [2]: x = np.array([ -91, -119, -137, -132], dtype=np.int16)

In [3]: x
Out[3]: array([ -91, -119, -137, -132], dtype=int16)

In [4]: np.linalg.norm(x)
/Users/warren/anaconda/lib/python2.7/site-packages/numpy/linalg/linalg.py:2061: RuntimeWarning: invalid value encountered in sqrt
  return sqrt(sqnorm)
Out[4]: nan


The problem also occurs in the master branch of the development version of numpy.  I created an issue here: https://github.com/numpy/numpy/issues/6128

If p_frame is, in fact, a 16 bit integer array, a simple work-around is something like:

x = np.asarray(p_frame, dtype=np.float64)
pf = x / norm(x)

","Following one of Warren's links, I get this warning:

In [1016]: np.linalg.norm(100000*np.ones(2).astype('int16'))
/usr/local/lib/python2.7/site-packages/numpy/linalg/linalg.py:2051: RuntimeWarning: invalid value encountered in sqrt
  return sqrt(add.reduce((x.conj() * x).real, axis=None))


For this x2, the inner expression is negative - the result of overflow in a small dtype.

In [1040]: x2=100000*np.ones(2).astype('int16')
In [1041]: np.add.reduce((x2.conj()*x2).real,axis=None)
Out[1041]: -1474836480


similarly with an x1:

In [1042]: x1
Out[1042]: array([ -9100, -11900, -13700, -13200], dtype=int16)
In [1043]: np.add.reduce((x1.conj()*x1).real,axis=None)
Out[1043]: -66128


If the sum of the 'dot' becomes too large for the dtype, it can be negative, producing a nan when passed through sqrt.

(I'm using 1.8.2 and 1.9.0 under linux).
",
SciPy strange behavior,https://stackoverflow.com/questions/77227994,Parallel scipy milp optimizing with multiprocessing,"I want to solve a linear programming problem using the scipy. Everything works in one process. If you try to do this in several processes, then everything works too.
However, if you first do optimization using multiprocessing, then without it, and then with it, then everything hangs. Any idea why?
if you run the code below, it will run for an infinite time:
import numpy as np
from multiprocessing import Pool

from scipy.optimize import LinearConstraint, milp

def task(identifier):
    c = -np.array([1., 2])
    A = np.array([[-1., 1], [-3, 2], [2, 3]])
    b_u = np.array([1., 12, 12])
    b_l = np.full_like(b_u, -np.inf)

    constraints = LinearConstraint(A, b_l, b_u)
    integrality = np.ones_like(c)

    res = milp(c=c, constraints=constraints, integrality=integrality)
    return res.x


with Pool() as pool:
    for result in pool.map(task, range(3)):
        print(f'Before: Got result: {result}')

print(task(4)) # if you comment out - everything works, otherwise it hangs

with Pool() as pool:
    for result in pool.map(task, range(3)):
        print(f'After: Got result: {result}')

Python version and Libraries:
Python 3.10.11, Numpy 1.24.2, SciPy 1.11.3
Who knows why such strange behavior and how it can be fixed?
",0,68,"I was able to run your code from the command line with a script containing:
import numpy as np
from multiprocessing import Pool

from scipy.optimize import LinearConstraint, milp


def task(identifier):
    c = -np.array([1.0, 2])
    A = np.array([[-1.0, 1], [-3, 2], [2, 3]])
    b_u = np.array([1.0, 12, 12])
    b_l = np.full_like(b_u, -np.inf)

    constraints = LinearConstraint(A, b_l, b_u)
    integrality = np.ones_like(c)

    res = milp(c=c, constraints=constraints, integrality=integrality)
    return res.x


if __name__ == ""__main__"":
    with Pool() as pool:
        for result in pool.map(task, range(3)):
            print(f""Before: Got result: {result}"")

    print(task(4))  # if you comment out - everything works, otherwise it hangs

    with Pool() as pool:
        for result in pool.map(task, range(3)):
            print(f""After: Got result: {result}"")

Result:
Before: Got result: [3. 2.]
Before: Got result: [3. 2.]
Before: Got result: [3. 2.]
[3. 2.]
After: Got result: [3. 2.]
After: Got result: [3. 2.]
After: Got result: [3. 2.]

If you are trying to run your code within Jupyter/IPython, there are known complications with using multiprocessing in these environments. Take a look at this question for some possible solutions.
",,
SciPy strange behavior,https://stackoverflow.com/questions/51089817,scipy rankdata with masked array,"I notice the following strange behavior with rankdata with maksed_array. Here is the code:

import numpy as np
import scipy.stats as stats

m = [True, False]
print(stats.mstats.rankdata(np.ma.masked_array([1.0, 100], mask=m)))
# result [0. 1.]

print(stats.mstats.rankdata(np.ma.masked_array([1.0, np.nan], mask=m)))
# result [1. 0.]

print(stats.mstats.rankdata([1.0, np.nan]))
# result [1. 2.]


According the scipy doc, masked values will be assigned 0 (use_missing=False). So why it outputs [1 0] in the 2nd one? Bug?
",0,336,"After tracing I find it is related to the argsort method of masked_array. When mstats.rankdata calls argsort, it does not specify fill_value, and endwith input parameters, which defaults to np.nan and True respectively. Based on the following code from numpy, the fill_value is np.nan.

if fill_value is None:
    if endwith:
        # nan &gt; inf
        if np.issubdtype(self.dtype, np.floating):
            fill_value = np.nan


So in the case of masked_array of [1, 100], it is argsorting [nan, 100], which is [1, 0]. In the case of masked_array of [1, np.nan], it is argsoring [nan, nan], which can be [0,1]. Then in the rankdata function, it assume first n (n=1) from argsort is valid, which is not correct here.

n = data.count()
rk = np.empty(data.size, dtype=float)
idx = data.argsort()
rk[idx[:n]] = np.arange(1,n+1)

",,
SciPy strange behavior,https://stackoverflow.com/questions/23897118,pylab.plot &quot;cannot convert float NaN to integer&quot; after calling scipy.stats.multivariate_normal,"While testing a regression algorithm I found this strange behavior: for some covariance matrices, the multivariate_normal function gives correct samples but then an exception is raised (only) the first time pylab.plot() is called:

ValueError: cannot convert float NaN to integer

The following code reproduces the error:
import numpy as np
from scipy.stats import multivariate_normal as mnorm
from matplotlib import pyplot as plt

B = np.array([ 0, 0, 0])

# works fine
v1 = np.array([[1, 0, 0],
              [0, 1, 0],
              [0, 0, 1]])


# OK. non positive semidefinite, well raised exception
v2 = np.array([[ 0.2 , -0.2, -0.3],
              [-0.2,  0.4, -0.9],
              [-0.3, -0.9,  0.7]])

# KO. exception (?)
v3 = np.array([[ 0.2 , -0.02, -0.026],
              [-0.02,  0.014, -0.009],
              [-0.026, -0.009,  0.017]])



w = mnorm(mean=B, cov=v3).rvs()
print w

plt.plot(w)
plt.show()

And if plt.plot(w) is called a second time, then it works. Any ideas?
Versions:

python 2.7.5 Anaconda 1.9.1 (64-bit)
scipy 0.14.0
matplotlib 1.3.1
numpy 1.8.1

",0,1457,"Well, it works fine here, and says :

[-0.72849048  0.15439657  0.00146853]


and shows :



I use python 2.7.6

other packages are same as yours.

Hope it helped. Good luck !
",,
SciPy strange result,https://stackoverflow.com/questions/17097236,Replace invalid values with None in Pandas DataFrame,"Is there any method to replace values with None in Pandas in Python?

You can use df.replace('pre', 'post') and can replace a value with another, but this can't be done if you want to replace with None value, which if you try, you get a strange result.

So here's an example:

df = DataFrame(['-',3,2,5,1,-5,-1,'-',9])
df.replace('-', 0)


which returns a successful result.

But,

df.replace('-', None)


which returns a following result:

0
0   - // this isn't replaced
1   3
2   2
3   5
4   1
5  -5
6  -1
7  -1 // this is changed to `-1`...
8   9


Why does such a strange result be returned?

Since I want to pour this data frame into MySQL database, I can't put NaN values into any element in my data frame and instead want to put None. Surely, you can first change '-' to NaN and then convert NaN to None, but I want to know why the dataframe acts in such a terrible way.


  Tested on pandas 0.12.0 dev on Python 2.7 and OS X 10.8. Python is a
  pre-installed version on OS X and I installed pandas by using SciPy
  Superpack script, for your information.

",119,291916,"With Pandas version 1.0.0, I would use DataFrame.replace or Series.replace:
df.replace(old_val, pd.NA, inplace=True)

This is better for two reasons:

It uses pd.NA instead of None or np.nan.
It optionally works in-place which could be more memory efficient depending upon the internal implementation.

","Alternatively you can also use mask:
df.mask(df=='-', None)

","df.replace('-', np.nan).astype(""object"")


This will ensure that you can use isnull() later on your dataframe
"
SciPy strange result,https://stackoverflow.com/questions/25328818,python 2.7: cannot pip on windows &quot;bash: pip: command not found&quot;,"I am trying to install the SciPy stack located at https://scipy.org/stackspec.html [I am only allowed 2 links; trying to use them wisely].  I realize that there are much easier ways to do this, but I think there is a lot to be learned by doing it manually.  I am relatively new to a lot of this stuff, so I apologize if I sound ignorant at any point.
I am running  Windows 7 Enterprise - 64 bit.  Here is what I have done so far:

Installed python-2.7.8.msi (32-bit) from https://www.python.org/download/releases/2.7.8/

Installed numpy-1.8.1-win32-superpack-python2.7 from
http://sourceforge.net/projects/numpy/files/
Test: import numpy as np ---&gt; no errors

Installed scipy library,
scipy-0.14.0-win32-superpack-python2.7.exe from
(SCIPY DOT ORG LINK REMOVED)
Test: import scipy as sp ---&gt; no errors

Installed matplotlib: matplotlib-1.3.1.win32-py2.7.exe from
(MATPLOTLIB DOT ORG LINK REMOVED)

Installed PIP by running script here:
https://raw.githubusercontent.com/pypa/pip/master/contrib/get-pip.py
I just copied-pasted script to a new file in IDLE,
saved as C:\Python27\Scripts\pip_install.py and clicked Run&gt;module. No errors reported.


Does the path on which I saved
pip_install.py matter?




HERE IS WHERE I FAIL
Attempted to install matlibplot dependency dateutil: Opened a
Cygwin Shell, and typed
        cd C:\Python27          ! is it necessary to cd to python directtory?
        pip install python-dateutil

This results in the error:
    bash: pip: command not found

I get the same error attempting from cmd.
Any help  is appreciated; the closest I found was bash: pip: command not found.  But the OSX nature of it is just enough to confise me further.

UPDATE:
I added the pip-path per Paul H's suggestion below.  It made the error go away, but strangely, nothing I pip actually installs. For example, in Cygwin, I type:
cbennett2&gt; pip install python-dateutil
cbennett2&gt;                            

You can see that there is no output or feedback from the shell (which I think there should be).  Then when I go to a new python shell:
&gt;&gt;&gt; from dateutil.parser import parse
Traceback (most recent call last):
  File ""&lt;pyshell#12&gt;"", line 1, in &lt;module&gt;
    from dateutil.parser import parse
ImportError: No module named dateutil.parser
&gt;&gt;&gt;&gt;

This happens with all of the modules that I thought I had pip'd ... pandas, tornado, etc.
",45,166470,"The problem is that your Python version and the library you want to use are not same versionally (Python). Even if you install Python's latest version, your PATH might not change properly and automatically. Thus, you should change it manually.After matching their version, it will work.

Ex: When I tried to install Django3, I got same error. I noticed that my PATH still seems C:\python27\Scripts though I already install Python3.8, so that I manually edited my PATH C:\python38\Scripts and reinstalled pip install Django and everything worked well. 
",,
SciPy strange result,https://stackoverflow.com/questions/34890585,In scipy why doesn&#39;t idct(dct(a)) equal to a?,"I am trying to implement JPEG compression using python. When I tried to apply the DCT, quantization, IDCT process for a tiff image, I found something strange for scipy.fftpack.dct/idct.

Since there is only 1D dct/idct within scipy package, I was doing this for a 2D dct

import numpy as np
from scipy.fftpack import dct, idct

def dct2(block):
    return dct(dct(block.T).T)

def idct2(block):
    return idct(idct(block.T).T)


I tested the 2D dct/idct using a simple 3x3 matrix. I was expecting to get a True matrix with this test case.

a = np.random.randint(0,255,9).reshape(3,3)
print a == idct2(dct2(a))


However it turned out that after idct2(dct2(a)) the result was scaled by a constant factor compared with the original a matrix.

I would like to ask if there is a way to implement a set of 2D dct/idct such that after a idct(dct(a)) operation I can get the same output as the input.
",10,7085,"You need to normalize idct2(dct2(a)) and convert the data type to uint8. One option is to use cv2 for this normalization:
import numpy as np
from scipy.fftpack import dct, idct

def dct2(block):
    return dct(dct(block.T).T)

def idct2(block):
    return idct(idct(block.T).T)
a = np.random.randint(0,255,9).reshape(3,3)

# convert to uint8
norm_image = cv2.normalize(idct2(dct2(a)), None, 
                           alpha = a.min(), beta = a.max(), 
                           norm_type = cv2.NORM_MINMAX, 
                           dtype = cv2.CV_32F).astype(np.uint8)
print(a == norm_image)
# [[ True  True  True]
#  [ True  True  True]
#  [ True  True  True]]

",,
SciPy strange result,https://stackoverflow.com/questions/16746974,Scipy Sparse Matrix - Dense Vector Multiplication Performance - Blocks vs Large Matrix,,7,3319,,,
SciPy strange result,https://stackoverflow.com/questions/64324685,Why my PCA is not invariant to rotation and axis swap?,"I have a voxel (np.array) with size 3x3x3, filled with some values, this setup is essential for me. I want to have rotation-invariant representation of it. For this case, I decided to try PCA representation which is believed to be invariant to orthogonal transformations. another
For simplicity, I took some axes swap, but in case I'm mistaken there can be np.rot90.
I have interpereted my 3d voxels as a set of weighted 3d cube point vectors which I incorrectly called ""basis"", total 27 (so that is some set of 3d point in space, represented by the vectors, obtained from cube points, scaled by voxel values).
import numpy as np

voxel1 = np.random.normal(size=(3,3,3))
voxel2 =  np.transpose(voxel1, (1,0,2)) #np.rot90(voxel1) #


basis = []
for i in range(3):
    for j in range(3):
        for k in range(3):
            basis.append([i+1, j+1, k+1]) # avoid 0
basis = np.array(basis)


voxel1 = voxel1.reshape((27,1))
voxel2 = voxel2.reshape((27,1))

voxel1 = voxel1*basis # weighted basis vectors
voxel2 = voxel2*basis

print(voxel1.shape)
(27, 3)

Then I did PCA to those 27 3-dimensional vectors:
def pca(x):
    center = np.mean(x, 0)
    x = x - center

    cov = np.cov(x.T) / x.shape[0]

    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)

    v = e_vectors[:, order].transpose()

    return x.dot(v)

vp1 = pca(voxel1)
vp2 = pca(voxel2)

But the results in vp1 and vp2 are different. Perhaps, I have a mistake (though I beleive this is the right formula), and the proper code must be
x.dot(v.T)
But in this case the results are very strange. The upper and bottom blocks of the transofrmed data are the same up to the sign:
&gt;&gt;&gt; np.abs(np.abs(vp1)-np.abs(vp2)) &gt; 0.01
array([[False, False, False],
       [False, False, False],
       [False, False, False],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True, False,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [False, False, False],
       [False, False, False],
       [False, False, False],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True, False,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [False, False, False],
       [False, False, False],
       [False, False, False]])

What I'm doing wrong?
What I want to do is to find some invariant representation of my weighted voxel, something like positioning according to the axes of inertia or principal axes. I would really appreciate if someone helps me.
UPD: Found the question similar to mine, but code is unavailable
EDIT2: Found the code InertiaRotate and managed to monkey-do the following:
import numpy as np

# https://github.com/smparker/orient-molecule/blob/master/orient.py

voxel1 = np.random.normal(size=(3,3,3))
voxel2 =  np.transpose(voxel1, (1,0,2))

voxel1 = voxel1.reshape((27,))
voxel2 = voxel2.reshape((27,))


basis = []
for i in range(3):
    for j in range(3):
        for k in range(3):
            basis.append([i+1, j+1, k+1]) # avoid 0
basis = np.array(basis)
basis = basis - np.mean(basis, axis=0)



def rotate_func(data, mass):

    #mass = [ masses[n.lower()] for n in geom.names ]

    inertial_tensor = -np.einsum(""ax,a,ay-&gt;xy"", data, mass, data)
    # negate sign to reverse the sorting of the tensor
    eig, axes = np.linalg.eigh(-inertial_tensor)
    axes = axes.T

    # adjust sign of axes so third moment moment is positive new in X, and Y axes
    testcoords = np.dot(data, axes.T) # a little wasteful, but fine for now
    thirdmoment = np.einsum(""ax,a-&gt;x"", testcoords**3, mass)

    for i in range(2):
        if thirdmoment[i] &lt; 1.0e-6:
            axes[i,:] *= -1.0

    # rotation matrix must have determinant of 1
    if np.linalg.det(axes) &lt; 0.0:
        axes[2,:] *= -1.0

    return axes

axes1 = rotate_func(basis, voxel1)
v1 = np.dot(basis, axes1.T)
axes2 = rotate_func(basis, voxel2)
v2 = np.dot(basis, axes2.T)


print(v1)
print(v2)

It seems to use basis (coordinates) and mass separately. The results are quite similar to my problem above: some parts of the transformed data match up to the sign, I believe those are some cube sides
print(np.abs(np.abs(v1)-np.abs(v2)) &gt; 0.01)
[[False False False]
 [False False False]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [False False False]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [False False False]
 [False False False]]


Looking for some explanation. This code is designed for molecules, and must work...
UPD: Tried to choose 3 vectors as a new basis from those 24 - the one with biggest norm, the one with the smallest and their cross product. Combined them into the matrix V, then used the formula V^(-1)*X to transform coordinates, and got the same problem - the resulting sets of vectors are not equal for rotated voxels.

UPD2: I agree with meTchaikovsky that my idea of multiplying voxel vectors by weights and thus creating some non-cubic point cloud was incorrect. Probably, we indeed need to take the solution for rotated ""basis""(yes, this is not a basis, but rather a way to determine point cloud) which will work later when ""basis"" is the same, but the weights are rotated according to the 3D rotation.
Based on the answer and the reference provided by meTchaikovsky, and finding other answers we together with my friend came to conclusion that rotate_func from molecular package mentioned above tries to invent some convention for computing the signs of the components. Their solution tries to use 3rd moment for the first 2 axes and determinant for the last axis (?). We tried a bit another approach and succeeded to have half of the representations matching:
# -*- coding: utf-8 -*-
""""""
Created on Fri Oct 16 11:40:30 2020

@author: Dima
""""""


import numpy as np
from numpy.random import randn
from numpy import linalg as la
from scipy.spatial.transform import Rotation as R
np.random.seed(10)

rotate_mat = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])

def pca(feat, x):
    # pca with attemt to create convention on sign changes
    
    x_c =x- np.mean(x,axis=0)
    x_f= feat*x
    x_f-= np.mean(x_f, axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)[::-1]
    #print(order)
    v = e_vectors[:,order]
    v= v/np.sign(v[0,:])
    if(la.det(v)&lt;0):
        v= -v
    return x_c @ v

def standardize(x):
    # take vector with biggest norm, with smallest and thir cross product as basis
    x -= np.mean(x,axis=0)
    nrms= la.norm(x, axis=1)
    imin= argmin(nrms)
    imax= argmax(nrms)
    vec1= x[imin, :]
    vec2= x[imax, :]
    vec3= np.cross(vec1, vec2)
    Smat= np.stack([vec1, vec2, vec3], axis=0)
    if(la.det(Smat)&lt;0):
        Smat= -Smat
    return(la.inv(Smat)@x.T)

    

angles = np.linspace(0.0,90.0,91)
voxel1 = np.random.normal(size=(3,3,3))    
res = []
for angle in angles:

    
    voxel2 = voxel1.copy()
    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)
    
    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    basis1 = basis1+1e-4*randn(27,3) # perturbation
    basis2 = basis1 @rotate_mat(np.deg2rad(angle))
    #voxel1 = voxel1*basis1
    #voxel2 = voxel2*basis2

    #print(angle,(np.abs(pca(voxel1) - pca(voxel2) )))
    #gg= np.abs(standardize(basis1) - standardize(basis2) )
    gg= np.abs(pca(voxel1, basis1) - pca(voxel1, basis2) )
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4) 
           
    print(angle,ss,  bl)
    #res.append(np.all(np.abs(pca(voxel1) - pca(voxel2) &lt; 1e-6)))
    del basis1, basis2

The results are good up to 58 degree angle (yet we're still experimenting with rotation of x, y axes). After that we have constant difference which indicates some uncounted sign reverse. This is better than the less consistent result of rotate_func:
0.0 0.0 True
1.0 1.1103280567106161e-13 True
2.0 5.150139890290964e-14 True
3.0 8.977126225544196e-14 True
4.0 5.57341699240722e-14 True
5.0 4.205149954378956e-14 True
6.0 3.7435437643664957e-14 True
7.0 1.2943967187158123e-13 True
8.0 5.400185371573149e-14 True
9.0 8.006410204958181e-14 True
10.0 7.777189536904011e-14 True
11.0 5.992073021576436e-14 True
12.0 6.3716122222085e-14 True
13.0 1.0120048110065158e-13 True
14.0 1.4193029076233626e-13 True
15.0 5.32774440341853e-14 True
16.0 4.056702432878251e-14 True
17.0 6.52062429116855e-14 True
18.0 1.3237663595853556e-13 True
19.0 8.950259695710006e-14 True
20.0 1.3795067925438317e-13 True
21.0 7.498727794307339e-14 True
22.0 8.570866862371226e-14 True
23.0 8.961510590826412e-14 True
24.0 1.1839169916779899e-13 True
25.0 1.422193407555868e-13 True
26.0 6.578778015788652e-14 True
27.0 1.0042963537887101e-13 True
28.0 8.438153062569065e-14 True
29.0 1.1299103064863272e-13 True
30.0 8.192453876745831e-14 True
31.0 1.2618492405483406e-13 True
32.0 4.9237819394886296e-14 True
33.0 1.0971028569666842e-13 True
34.0 1.332138304559801e-13 True
35.0 5.280024600049296e-14 True

From the code above, you can see that we tried to use another basis: vector with the biggest norm, vector with the smallest and their cross product. Here we should have only two variants (direction of the cross product) which could be later fixed, but I couldn't manage this alternative solution to work.
I hope that someone can help me finish this and obtain rotation-invariant representation for voxels.

EDIT 3. Thank you very much meTchaikovsky, but the situation is still unclear. My problem initially lies in processing 3d voxels which are (3,3,3) numpy arrays. We reached the conclusion that for finding invariant representation, we just need to fix 3d voxel as weights used for calculating cov matrix, and apply rotations on the centered ""basis"" (some vectors used for describing point cloud).
Therefore, when we achieved invariance to ""basis"" rotations, the problem should have been solved: now, when we fix ""basis"" and use rotated voxel, the result must be invariant. Surprisingly, this is not so. Here I check 24 rotations of the cube with basis2=basis1 (except small perturbation):
import scipy.ndimage

def pca(feat, x):

    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]

    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_c @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign

    return proj

def rotate_3d(image1, alpha, beta, gamma):
    # z
    # The rotation angle in degrees.
    image2 = scipy.ndimage.rotate(image1, alpha, mode='nearest', axes=(0, 1), reshape=False)

    # rotate along y-axis
    image3 = scipy.ndimage.rotate(image2, beta, mode='nearest', axes=(0, 2), reshape=False)

    # rotate along x-axis
    image4 = scipy.ndimage.rotate(image3, gamma, mode='nearest', axes=(1, 2), reshape=False)
    return image4



voxel10 = np.random.normal(size=(3,3,3))

angles = [[x,y,z] for x in [-90,0,90] for y in [-90,0,90] for z in [-90,0,90]]
res = []
for angle in angles:

    voxel2 = rotate_3d(voxel10, angle[0], angle[1], angle[2])
    voxel1 = voxel10.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)

    basis1 += 1e-4*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1
    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel2, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))


difference before pca 0.000, difference after pca 45.738 False
difference before pca 0.000, difference after pca 12.157 False
difference before pca 0.000, difference after pca 26.257 False
difference before pca 0.000, difference after pca 37.128 False
difference before pca 0.000, difference after pca 52.131 False
difference before pca 0.000, difference after pca 45.436 False
difference before pca 0.000, difference after pca 42.226 False
difference before pca 0.000, difference after pca 18.959 False
difference before pca 0.000, difference after pca 38.888 False
difference before pca 0.000, difference after pca 12.157 False
difference before pca 0.000, difference after pca 26.257 False
difference before pca 0.000, difference after pca 50.613 False
difference before pca 0.000, difference after pca 52.132 False
difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 52.299 False

Here basis1=basis2 (hence basis difference before pca=0), and you can see 0 for (0,0,0) rotation. But rotated voxels give different result. In case scipy does something wrong, I've checked the approach with numpy.rot90 with the same result:
rot90 = np.rot90

def rotations24(polycube):
    # imagine shape is pointing in axis 0 (up)

    # 4 rotations about axis 0
    yield from rotations4(polycube, 0)

    # rotate 180 about axis 1, now shape is pointing down in axis 0
    # 4 rotations about axis 0
    yield from rotations4(rot90(polycube, 2, axis=1), 0)

    # rotate 90 or 270 about axis 1, now shape is pointing in axis 2
    # 8 rotations about axis 2
    yield from rotations4(rot90(polycube, axis=1), 2)
    yield from rotations4(rot90(polycube, -1, axis=1), 2)

    # rotate about axis 2, now shape is pointing in axis 1
    # 8 rotations about axis 1
    yield from rotations4(rot90(polycube, axis=2), 1)
    yield from rotations4(rot90(polycube, -1, axis=2), 1)

def rotations4(polycube, axis):
    """"""List the four rotations of the given cube about the given axis.""""""
    for i in range(4):
        yield rot90(polycube, i, axis)



def rot90(m, k=1, axis=2):
    """"""Rotate an array k*90 degrees in the counter-clockwise direction around the given axis""""""
    m = np.swapaxes(m, 2, axis)
    m = np.rot90(m, k)
    m = np.swapaxes(m, 2, axis)
    return m


voxel10 = np.random.normal(size=(3,3,3))

gen = rotations24(voxel10)

res = []
for voxel2 in gen:

    #voxel2 = rotate_3d(voxel10, angle[0], angle[1], angle[2])
    voxel1 = voxel10.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)

    basis1 += 1e-4*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1

    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel2, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

I tried to investigate this case, and the only perhaps irrelevant thing I found the following:
voxel1 = np.ones((3,3,3))
voxel1[0,0,0] = 0 # if I change 0 to 0.5 it stops working at all

# mirrored around diagonal
voxel2 = np.ones((3,3,3))
voxel2[2,2,2] = 0

for angle in range(1):

    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1) 

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)

    basis1 = basis1 + 1e-4 * randn(27,3) # perturbation
    basis2 = basis1

# If perturbation is used we have 

# difference before pca 0.000, difference after pca 0.000 True
# correct for 100.0 percent of time

# eigenvalues for both voxels
# [1.03417495 0.69231107 0.69235402]
# [0.99995368 0.69231107 0.69235402]


# If no perturbation applied for basis, difference is present

# difference before pca 0.000, difference after pca 55.218 False
# correct for 0.0 percent of time

# eignevalues for both voxels (always have 1.):
# [0.69230769 1.03418803 0.69230769]
# [1.         0.69230769 0.69230769]




Currently don't know how to proceed from there.

EDIT4:
I'm currently thinking that there is some problem with voxel rotations transformed into basis coefficients via voxel.reshape()
Simple experiment with creating array of indices
indices = np.arange(27)
indices3d = indices.reshape((3,3,3))
voxel10 = np.random.normal(size=(3,3,3))
assert voxel10[0,1,2] == voxel10.ravel()[indices3d[0,1,2]]

And then using it for rotations
gen = rotations24(indices3d)

res = []
for ind2 in gen:

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    voxel1 = voxel10.copy().reshape(27,1) #np.array([voxel10[i,j,k] for k in range(3) for j in range(3) for i in range(3)])[...,np.newaxis]

    voxel2 = voxel1[ind2.reshape(27,)]

    basis1 += 1e-4*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1[ind2.reshape(27,)]

    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel2, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

Shows that those rotations are not correct, because on my opinion rotated voxel and basis should match:
difference before pca 0.000, difference after pca 0.000 True
difference before pca 48.006, difference after pca 87.459 False
difference before pca 72.004, difference after pca 70.644 False
difference before pca 48.003, difference after pca 71.930 False
difference before pca 72.004, difference after pca 79.409 False
difference before pca 84.005, difference after pca 36.177 False


EDIT 5: Okaaay, so here we go at least for 24 rotations. At first, we had a slight change of logic lurked into our pca function. Here we center x_c (basis) and forget about it, further centering x_f (features*basis) and transforming it with pca. This does not work perhaps because our basis is not centered and multiplication by features further increased the bias. If we center x_c first, and multiply it by features, everything will be Ok. Also, previously we had proj = x_c @ v with v computed from x_f which was totally wrong in this case, as x_f and x_c were centered around different centers.
def pca(feat, x):
    
    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    
    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    
    
    return proj

Secondly, as we already found, we need to sort vectors obtained by pca, for example by the first column:
    basis2 = basis1

    original_diff = np.sum(np.abs(basis1-basis2))

    a = pca(voxel1, basis1)
    t1 = a[a[:,0].argsort()]

    a = pca(voxel2, basis2)
    t2 = a[a[:,0].argsort()]

    gg= np.abs(t1-t2)

And the last thing we also discovered already, is that simple reshape is wrong for voxel, it must correspond to rotation:
voxel2 = voxel1[ind2.reshape(27,)] #np.take(voxel10, ind2).reshape(27,1).
One more important comment to understand the solution. When we perform PCA on the 3d vectors (point cloud, defined by our basis) with weights assigned (analogously to the inertia of the rigid body), the actual assignment of the weights to the points is sort of external information, which becomes hard-defined for the algorithm. When we rotated basis by applying rotation matrices, we did not change the order of the vectors in the array, hence the order of the mass assignments wasn't changed too. When we start to rotate voxel, we change the order of the masses, so in general PCA algorithm will not work without the same transformation applied to the basis. So, only if we have some array of 3d vectors, transformed by some rotation AND the list of masses re-arranged accordingly, we can detect the rotation of the rigid body using PCA. Otherwise, if we detach masses from points, that would be another body in general.
So how does it work for us then? It works because our points are fully symmetric around the center after centering basis. In this case reassignment of the masses does not change ""the body"" because vector norms are the same. In this case we can use the same (numerically) basis2=basis1 for testing 24 rotations and rotated voxel2 (rotated point cloud cubes match, just masses migrate). This correspond to the rotation of the point cloud with mass points around the center of the cube. PCA will transform vectors with the same lengths and different masses in the same way according to the body's ""inertia"" then (after we reached convention on the signs of the components). The only thing left is to sort the pca transformed vectors in the end, because they have different position in the array (because our body was rotated, mass points changed their positions). This makes us lose some information related to the order of the vectors but it looks inevitable.
Here is the code which checks the solution for 24 rotations. If should theoretically work in the general case as well, giving some closer values for more complicated objects rotated inside a bigger voxel:
import numpy as np
from numpy.random import randn

#np.random.seed(20)

def pca(feat, x):
    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x_c
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign
    return proj


# must be correct https://stackoverflow.com/questions/15230179/how-to-get-the-linear-index-for-a-numpy-array-sub2ind
indices = np.arange(27)
indices3d = indices.reshape((3,3,3))
voxel10 = np.random.normal(size=(3,3,3))
assert voxel10[0,1,2] == voxel10.ravel()[indices3d[0,1,2]]

rot90 = np.rot90

def rotations24(polycube):
    # imagine shape is pointing in axis 0 (up)

    # 4 rotations about axis 0
    yield from rotations4(polycube, 0)

    # rotate 180 about axis 1, now shape is pointing down in axis 0
    # 4 rotations about axis 0
    yield from rotations4(rot90(polycube, 2, axis=1), 0)

    # rotate 90 or 270 about axis 1, now shape is pointing in axis 2
    # 8 rotations about axis 2
    yield from rotations4(rot90(polycube, axis=1), 2)
    yield from rotations4(rot90(polycube, -1, axis=1), 2)

    # rotate about axis 2, now shape is pointing in axis 1
    # 8 rotations about axis 1
    yield from rotations4(rot90(polycube, axis=2), 1)
    yield from rotations4(rot90(polycube, -1, axis=2), 1)

def rotations4(polycube, axis):
    """"""List the four rotations of the given cube about the given axis.""""""
    for i in range(4):
        yield rot90(polycube, i, axis)



def rot90(m, k=1, axis=2):
    """"""Rotate an array k*90 degrees in the counter-clockwise direction around the given axis""""""
    m = np.swapaxes(m, 2, axis)
    m = np.rot90(m, k)
    m = np.swapaxes(m, 2, axis)
    return m


gen = rotations24(indices3d)

res = []

for ind2 in gen:

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    voxel1 = voxel10.copy().reshape(27,1)

    voxel2 = voxel1[ind2.reshape(27,)] #np.take(voxel10, ind2).reshape(27,1)

    basis1 += 1e-6*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1

    original_diff = np.sum(np.abs(basis1-basis2))
    a = pca(voxel1, basis1)
    t1 = a[a[:,0].argsort()]
    a = pca(voxel2, basis2)
    t2 = a[a[:,0].argsort()]
    gg= np.abs(t1-t2)
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 0.000 True


PS. I want to propose better ordering theme to take into account zero values in the voxel which might confuse previous approach when entire first column of PCA vectors is zero, etc. I propose to sort by vector norms, multiplied by the sign of the sum of elements. Here is tensorflow 2 code:

def infer_shape(x):
    x = tf.convert_to_tensor(x)

    # If unknown rank, return dynamic shape
    if x.shape.dims is None:
        return tf.shape(x)

    static_shape = x.shape.as_list()
    dynamic_shape = tf.shape(x)

    ret = []
    for i in range(len(static_shape)):
        dim = static_shape[i]
        if dim is None:
            dim = dynamic_shape[i]
        ret.append(dim)

    return ret

def merge_last_two_dims(tensor):
    shape = infer_shape(tensor)
    shape[-2] *= shape[-1]
    #shape.pop(1)
    shape = shape[:-1]
    return tf.reshape(tensor, shape)


def pca(inpt_voxel):
        patches = tf.extract_volume_patches(inpt_voxel, ksizes=[1,3,3,3,1], strides=[1, 1,1,1, 1], padding=""VALID"")
        features0 = patches[...,tf.newaxis]*basis
        # centered basises
        basis1_ = tf.ones(shape=tf.shape(patches[...,tf.newaxis]), dtype=tf.float32)*basis
        basis1 = basis1_ - tf.math.divide_no_nan(tf.reduce_sum(features0, axis=-2), tf.reduce_sum(patches, axis=-1)[...,None])[:,:,:,:,None,:]
        features = patches[...,tf.newaxis]*basis1
        features_centered_basis = features - tf.reduce_mean(features, axis=-2)[:,:,:,:,None,:]
        x = features_centered_basis
        m = tf.cast(x.get_shape()[-2], tf.float32)
        cov = tf.matmul(x,x,transpose_a=True)/(m - 1)
        e,v = tf.linalg.eigh(cov,name=""eigh"")
        proj = tf.matmul(x,v,transpose_b=False)
        asign = tf.sign(proj)
        max_ind = tf.argmax(tf.abs(proj),axis=-2)[:,:,:,:,None,:]
        sign = tf.gather(asign,indices=max_ind, batch_dims=4, axis=-2)
        sign = tf.linalg.diag_part(sign)
        proj = proj * sign
        # But we can have 1st coordinate zero. In this case,
        # other coordinates become ambiguous
        #s = tf.argsort(proj[...,0], axis=-1)
        # sort by l2 vector norms, multiplied by signs of sums
        sum_signs = tf.sign(tf.reduce_sum(proj, axis=-1))
        norms = tf.norm(proj, axis=-1)
        s = tf.argsort(sum_signs*norms, axis=-1)
        proj = tf.gather(proj, s, batch_dims=4, axis=-2)
        return merge_last_two_dims(proj)

",6,931,"Firstly, your pca function is not correct, it should be
def pca(x):
    
    x -= np.mean(x,axis=0)
    cov = np.cov(x.T)
    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    
    return x @ v

You shouldn't transpose the e_vectors[:,order] because we want each column of the v array is an eigenvector, therefore, x @ v will be projections of x on those eigenvectors.
Secondly, I think you misunderstand the meaning of rotation. It is not voxel1 that should be rotated, but the basis1. If you rotate (by taking transposition) voxel1, what you really do is to rearrange the indices of grid points, while the coordinates of the points basis1 are not changed.
In order to rotate the points (around the z axis for example), you can first define a function to calculate the rotation matrix given an angle
rotate_mat = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])

with the rotation matrix generated by this function, you can rotate the array basis1 to create another array basis2
basis2 = basis1 @ rotate_mat(np.deg2rad(angle))

Now it comes to the title of your question ""Why my PCA is not invariant to rotation and axis swap?"", from this post, the PCA result is not unique, you can actually run a test to see this
import numpy as np

np.random.seed(10)

rotate_mat = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])

def pca(x):
    
    x -= np.mean(x,axis=0)
    cov = np.cov(x.T)
    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    return x @ v


angles = np.linspace(0,90,91)
    
res = []
for angle in angles:

    voxel1 = np.random.normal(size=(3,3,3))
    voxel2 = voxel1.copy()
    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)
    
    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)])
    # basis2 = np.hstack((-basis1[:,1][:,None],basis1[:,0][:,None],-basis1[:,2][:,None]))
    basis2 = basis1 @ rotate_mat(np.deg2rad(angle))
    voxel1 = voxel1*basis1
    voxel2 = voxel2*basis2

    print(angle,np.all(np.abs(pca(voxel1) - pca(voxel2) &lt; 1e-6)))
    res.append(np.all(np.abs(pca(voxel1) - pca(voxel2) &lt; 1e-6)))
    
print()
print(np.sum(res) / len(angles))

After you run this script, you will see that in only 21% of times the two PCA results are the same.

UPDATE
I think instead of focusing on the eigenvectors of the principal components, you can instead focus on the projections. For two clouds of points, even though they are essentially the same, the eigenvectors can be drastically different. Therefore, hardcoding in order to somehow let the two sets of eigenvectors to be the same is a very difficult task.
However, based on this post, for the same cloud of points, two sets of eigenvectors can be different only up to a minus sign. Therefore, the projections upon the two sets of eigenvectors are also different only up to a minus sign. This actually offers us an elegant solution, for the projections along an eigenvector (principal axis), all we need to do is to switch the sign of the projections so that the projection with the largest absolute value along that principal axis is positive.
import numpy as np
from numpy.random import randn

#np.random.seed(20)

rotmat_z = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])
rotmat_y = lambda theta: np.array([[np.cos(theta),0.,np.sin(theta)],[0.,1.,0.],[-np.sin(theta),0.,np.cos(theta)]])
rotmat_x = lambda theta: np.array([[1.,0.,0.],[0.,np.cos(theta),-np.sin(theta)],[0.,np.sin(theta),np.cos(theta)]])
# based on https://en.wikipedia.org/wiki/Rotation_matrix
rot_mat = lambda alpha,beta,gamma: rotmat_z(alpha) @ rotmat_y(beta) @ rotmat_x(gamma)

deg2rad = lambda alpha,beta,gamma: [np.deg2rad(alpha),np.deg2rad(beta),np.deg2rad(gamma)]

def pca(feat, x):
    
    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    
    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign
    
    return proj

ref_angles = np.linspace(0.0,90.0,10)
angles = [[alpha,beta,gamma] for alpha in ref_angles for beta in ref_angles for gamma in ref_angles]


voxel1 = np.random.normal(size=(3,3,3))
res = []
for angle in angles:

    voxel2 = voxel1.copy()
    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    basis1 = basis1 + 1e-4 * randn(27,3) # perturbation
    basis2 = basis1 @ rot_mat(*deg2rad(*angle))
   
    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel1, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

As you can see by running this script, the projections on the principal axis are the same, this means we have resolved the issue of PCA results being not unique.

Reply to EDIT 3
As for the new issue you raised, I think you missed an important point, it is the projections of the cloud of points onto the principal axes that are invariant, not anything else. Therefore, if you rotate voxel1 and obtain voxel2, they are the same in the sense that their own respective projections onto the principal axes of the cloud of points are the same, it actually does not make too much sense to compare pca(voxel1,basis1) with pca(voxel2,basis1).
Furthermore, the method rotate of scipy.ndimage actually changes information, as you can see by running this script
image1 = np.linspace(1,100,100).reshape(10,10)
image2 = scipy.ndimage.rotate(image1, 45, mode='nearest', axes=(0, 1), reshape=False)
image3 = scipy.ndimage.rotate(image2, -45, mode='nearest', axes=(0, 1), reshape=False)

fig,ax = plt.subplots(nrows=1,ncols=3,figsize=(12,4))
ax[0].imshow(image1)
ax[1].imshow(image2)
ax[2].imshow(image3)

The output image is
As you can see the matrix after rotation is not the same as the original one, some information of the original matrix is changed.


Reply to EDIT 4
Actually, we are almost there, the two pca results are different because we are comparing pca components for different points.
indices = np.arange(27)
indices3d = indices.reshape((3,3,3))
# apply rotations to the indices, it is not weights yet
gen = rotations24(indices3d)

# construct the weights
voxel10 = np.random.normal(size=(3,3,3))

res = []
count = 0
for ind2 in gen:
    count += 1
    # ind2 is the array of indices after rotation
    # reindex the weights with the indices after rotation 
    voxel1 = voxel10.copy().reshape(27,1) 
    voxel2 = voxel1[ind2.reshape(27,)]

    # basis1 is the array of coordinates where the points are
    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    basis1 += 1e-4*np.random.normal(size=(27, 1))
    # reindex the coordinates with the indices after rotation
    basis2 = basis1[ind2.reshape(27,)]

    # add a slight modification to pca, return the axes also 
    pca1,v1 = pca(voxel1,basis1)
    pca2,v2 = pca(voxel2,basis2)
    # sort the principal components before comparing them 
    pca1 = np.sort(pca1,axis=0)
    pca2 = np.sort(pca2,axis=0)
    
    gg= np.abs(pca1 - pca2)
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference after pca %.3f' % ss,bl)
    res.append(bl)
    
    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

Running this script, you will find, for each rotation, the two sets of principal axes are different only up to a minus sign. The two sets of pca results are different because the indices of the cloud of points before and after rotation are different (since you apply rotation to the indices). If you sort the pca results before comparing them, you will find the two pca results are exactly the same.

Summary
The answer to this question can be divided into two parts. In the first part, the rotation is applied to the basis (the coordinates of points), while the indices and the corresponding weights are unchanged. In the second part, the rotation is applied to the indices, then the weights and the basis are rearranged with the new indices. For both of the two parts, the solution pca function is the same
def pca(feat, x):

    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]

    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign

    return proj

The idea of this function is, instead of matching the principal axes, we can match the principal components since it is the principal components that are rotationally invariant after all.
Based on this function pca, the first part of this answer is easy to understand, since the indices of the points are unchanged while we only rotate the basis. In order to understand the second part of this answer (Reply to EDIT 5), we must first understand the function rotations24. This function rotates the indices rather than the coordinates of the points, therefore, if we stay at the same position observing the points, we will feel that the positions of the points are changed.

With this in mind, it is not hard to understand Reply to EDIT 5.
Actually, the function pca in this answer can be applied to more general cases, for example (we rotate the indices)
num_of_points_per_dim = 10
num_of_points = num_of_points_per_dim ** 3

indices = np.arange(num_of_points)
indices3d = indices.reshape((num_of_points_per_dim,num_of_points_per_dim,num_of_points_per_dim))
voxel10 = 100*np.random.normal(size=(num_of_points_per_dim,num_of_points_per_dim,num_of_points_per_dim))

gen = rotations24(indices3d)

res = []
for ind2 in gen:

    voxel1 = voxel10.copy().reshape(num_of_points,1)
    voxel2 = voxel1[ind2.reshape(num_of_points,)]

    basis1 = 100*np.random.rand(num_of_points,3)
    basis2 = basis1[ind2.reshape(num_of_points,)]

    pc1 = np.sort(pca(voxel1, basis1),axis=0)
    pc2 = np.sort(pca(voxel2, basis2),axis=0)
    
    gg= np.abs(pc1-pc2)
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

",,
SciPy strange result,https://stackoverflow.com/questions/23699301,Scipy strange results using curve fitting,,6,860,,,
SciPy strange result,https://stackoverflow.com/questions/76439150,Scipy banded eigensolver much slower than standard eigensolver,"I'm observing a strange behaviour concerning the scipy.linalg.eig_banded eigensolver.
I am generating banded matrices of size N=p*f that have a specific structure. The matrices are symmetric tri-block-diagonal with p blocks of size fxf on the main diagonal and p-1 identity matrices of size f*f on the off diagonals.
Example with p=3 and f=3:
 [2 2 2 1 0 0 0 0 0]
 [2 2 2 0 1 0 0 0 0]
 [2 2 2 0 0 1 0 0 0]
 [1 0 0 3 3 3 1 0 0]
 [0 1 0 3 3 3 0 1 0]
 [0 0 1 3 3 3 0 0 1]
 [0 0 0 1 0 0 4 4 4]
 [0 0 0 0 1 0 4 4 4]
 [0 0 0 0 0 1 4 4 4]

Usually these matrices are of size p = 100, f=30, N=p*f=3000 but can easily grow much larger.
Given the structure of these matrices I was hoping that the banded eigensolver in scipy was going to be much faster than the dense eigensolver, however it seems like this is not the case.
I am benchmarking the solvers with the following code:
# Set dimension of problem
f = 50
p = 80
a = 1

print(f""p={p}, f={f}, size={f*p, f*p}"")

print(f""Matrix containing random numbers in {(-a, a)}"")
A = generate_matrix(p, f, -a, a)

# Benchmark standard eigensolver
start = time()
D, Q = linalg.eigh(A)
end = time()

# Test correctness
D = np.diag(D)
print(f""Time for dense solver {end - start}"")
print(f""||AQ - QD|| = {np.linalg.norm(A@Q - Q@D)}"")


# Convert A to banded format
A_banded = banded_format(A, upper = f)

# Benchmark banded eigensolver
start = time()
D, Q = linalg.eig_banded(A_banded)
end = time()

# Test correctness
D = np.diag(D)
print(f""Time for banded solver {end - start}"")
print(f""||AQ - QD|| = {np.linalg.norm(A@Q - Q@D)}"")


The results I get indicate that the banded eigensolver is much slower than the dense one:
p=80, f=50, size=(4000, 4000)
Matrix containing random numbers in (-1, 1)

Time for dense solver 13.475645780563354
||AQ - QD|| = 3.1334336527852233e-12

Time for banded solver 24.427151203155518
||AQ - QD|| = 1.589349711533356e-11

I have already tried storing the matrix in lower diagonal format and passing the overwrite_a_band=True option, but the performance remains the same.
Numpy configuration:
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']

Scipy configuration:
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
lapack_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
blas_opt_info:
    libraries = ['openblas', 'openblas']
    library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None)]
    runtime_library_dirs = ['/cluster/apps/gcc-8.2.0/openblas-0.2.20-5gatj7a35vypgjekzf3ibbtz54tlbk3m/lib']

I also tried running the same benchmark on a different cluster using MKL as a backend instead of OpenBLAS and I observed very similar results. Also setting the number of threads with OMP_NUM_THREADS and/or MKL_NUM_THREADS has a very small effect on performance.
Does anyone have any ideas on why this is happening?
Thanks
",5,106,"I did some digging into the source code of SciPy and the Intel MKL documentation and I have figured out why this is happening.
The scipy eig_banded solver delegates the problem to the LAPACK dsbevd routine which computes all eigenvalues and eigenvectors of a matrix in banded format using a variation of the Cuppen divide and conquer algorithm. This offers an advantage in terms of memory usage because of the banded storage format, but the actual algorithm scales in O(n^3) flops after tridiagonalization with respect to matrix size.
On the other hand, the scipy dense eigensolver delegates the problem to the dsyev routine which for real symmetric matrices calls the dsyevr routine which computes the eigenvalues and eigenvectors using the MRRR algorithm in O(n^2) flops after tridiagonalization.
I am still unsure why there is no MRRR implementation for banded matrix format in MKL.
",,
SciPy strange result,https://stackoverflow.com/questions/59406477,Correct normalization of discrete power spectral density in python for a real problem,"I am struggling with the correct normalization of the power spectral density (and its inverse).

I am given a real problem, let's say the readings of an accelerometer in the form of the power spectral density (psd) in units of Amplitude^2/Hz. I would like to translate this back into a randomized time series. However, first I want to understand the ""forward"" direction, time series to PSD.

According to [1], the PSD of a time series x(t) can be calculated by:

PSD(w) = 1/T * abs(F(w))^2 = df * abs(F(w))^2


in which T is the sampling time of x(t) and F(w) is the Fourier transform of x(t) and df=1/T is the frequency resolution in the Fourier space. However, the results I am getting are not equal to what I am getting using the scipy Welch method, see code below.

This first block of code is taken from the scipy.welch documentary:

from scipy import signal
import matplotlib.pyplot as plt

fs = 10e3
N = 1e5
amp = 2*np.sqrt(2)
freq = 1234.0
noise_power = 0.001 * fs / 2
time = np.arange(N) / fs
x = amp*np.sin(2*np.pi*freq*time)
x += np.random.normal(scale=np.sqrt(noise_power), size=time.shape)

f, Pxx_den = signal.welch(x, fs, nperseg=1024)
plt.semilogy(f, Pxx_den)
plt.ylim(\[0.5e-3, 1\])
plt.xlabel('frequency \[Hz\]')
plt.ylabel('PSD \[V**2/Hz\]')
plt.show()


First thing I noticed is that the plotted psd changes with the variable fs which seems strange to me. (Maybe I need to adjust the nperseg argument then accordingly? Why is nperseg not set to fs automatically then?)

My code would be the following: (Note that I defined my own fft_full function which already takes care of the correct fourier transform normalization, which I verified by checking Parsevals theorem).

import scipy.fftpack as fftpack

def fft_full(xt,yt):
    dt = xt[1] - xt[0]
    x_fft=fftpack.fftfreq(xt.size,dt)
    y_fft=fftpack.fft(yt)*dt
    return (x_fft,y_fft)

xf,yf=fft_full(time,x)
df=xf[1] - xf[0]
psd=np.abs(yf)**2 *df
plt.figure()
plt.semilogy(xf, psd)
#plt.ylim([0.5e-3, 1])
plt.xlim(0,)
plt.xlabel('frequency [Hz]')
plt.ylabel('PSD [V**2/Hz]')
plt.show()


Unfortunately, I am not yet allowed to post images but the two plots do not look the same!

I would greatly appreciate if someone could explain to me where I went wrong and settle this once and for all :)


  [1]: Eq. 2.82. Random Vibrations in Spacecraft Structures Design
  Theory and Applications, Authors: Wijker, J. Jaap, 2009

",5,3027,"The scipy library uses the Welch's method to estimate a PSD. This method is more complex than just taking the squared modulus of the discrete Fourier transform. In short terms, it proceeds as follows:

Let x be the input discrete signal that contains N samples.

Split x into M overlapping segments, such that each segment sm contains nperseg samples and that each two consecutive segments overlap in noverlap samples, so that nperseg = K * (nperseg - noverlap), where K is an integer (usually K = 2). Note also that:
N = nperseg + (M - 1) * (nperseg - noverlap) = (M + K - 1) * nperseg / K

From each segment sm, subtract its mean (this removes the DC component):
tm = sm - sum(sm) / nperseg

Multiply the elements of the obtained zero-mean segments tm by the elements of a suitable (nonsymmetric) window function, h (such as the Hann window):
um = tm * h

Calculate the Fast Fourier Transform of all vectors um. Before performing these transformations, we usually first append so many zeros to each vector um that its new dimension becomes a power of 2 (the nfft argument of the function welch is used for this purpose). Let us suppose that len(um) = 2p. In most cases, our input vectors are real-valued, so it is best to apply FFT for real data. Its results are then complex-valued vectors vm = rfft(um), such that len(vm) = 2p - 1 + 1.

Calculate the squared modulus of all transformed vectors:
am = abs(vm) ** 2,
or more efficiently:
am = vm.real ** 2 + vm.imag ** 2

Normalize the vectors am as follows:
bm = am / sum(h * h)
bm[1:-1] *= 2 (this takes into account the negative frequencies),
where h is a real vector of the dimension nperseg that contains the window coefficients. In case of the Hann window, we can prove that
sum(h * h) = 3 / 8 * len(h) = 3 / 8 * nperseg

Estimate the PSD as the mean of all vectors bm:
psd = sum(bm) / M
The result is a vector of the dimension len(psd) = 2p - 1 + 1. If we wish that the sum of all psd coefficients matches the mean squared amplitude of the windowed input data (rather than the sum of squared amplitudes), then the vector psd must also be divided by nperseg. However, the scipy routine omits this step. In any case, we usually present psd on the decibel scale, so that the final result is:
psd_dB = 10 * log10(psd).


For a more detailed description, please read the original Welch's paper. See also Wikipedia's page and chapter 13.4 of Numerical Recipes in C
",,
SciPy strange result,https://stackoverflow.com/questions/44710838,Calling BLAS / LAPACK directly using the SciPy interface and Cython,,4,2232,,,
SciPy strange result,https://stackoverflow.com/questions/39196057,Libraries in &quot;virtualenv&quot; much bigger than system libraries,,4,933,,,
SciPy strange result,https://stackoverflow.com/questions/36706163,python multiprocessing module: strange behaviour and processor load when using Pool,,4,871,,,
SciPy strange result,https://stackoverflow.com/questions/71146140,Using RNN Trained Model without pytorch installed,"I have trained an RNN model with pytorch. I need to use the model for prediction in an environment where I'm unable to install pytorch because of some strange dependency issue with glibc. However, I can install numpy and scipy and other libraries. So, I want to use the trained model, with the network definition, without pytorch.
I have the weights of the model as I save the model with its state dict and weights in the standard way, but I can also save it using just json/pickle files or similar.
I also have the network definition, which depends on pytorch in a number of ways. This is my RNN network definition.
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import random

torch.manual_seed(1)
random.seed(1)
device = torch.device('cpu')

class RNN(nn.Module):
  def __init__(self, input_size, hidden_size, output_size,num_layers, matching_in_out=False, batch_size=1):
    super(RNN, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.num_layers = num_layers
    self.batch_size = batch_size
    self.matching_in_out = matching_in_out #length of input vector matches the length of output vector 
    self.lstm = nn.LSTM(input_size, hidden_size,num_layers)
    self.hidden2out = nn.Linear(hidden_size, output_size)
    self.hidden = self.init_hidden()
  def forward(self, feature_list):
    feature_list=torch.tensor(feature_list)
    
    if self.matching_in_out:
      lstm_out, _ = self.lstm( feature_list.view(len( feature_list), 1, -1))
      output_space = self.hidden2out(lstm_out.view(len( feature_list), -1))
      output_scores = torch.sigmoid(output_space) #we'll need to check if we need this sigmoid
      return output_scores #output_scores
    else:
      for i in range(len(feature_list)):
        cur_ft_tensor=feature_list[i]#.view([1,1,self.input_size])
        cur_ft_tensor=cur_ft_tensor.view([1,1,self.input_size])
        lstm_out, self.hidden = self.lstm(cur_ft_tensor, self.hidden)
        outs=self.hidden2out(lstm_out)
      return outs
  def init_hidden(self):
    #return torch.rand(self.num_layers, self.batch_size, self.hidden_size)
    return (torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device),
            torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device))

I am aware of this question, but I'm willing to go as low level as possible. I can work with numpy array instead of tensors, and reshape instead of view, and I don't need a device setting.
Based on the class definition above, what I can see here is that I only need the following components from torch to get an output from the forward function:

nn.LSTM
nn.Linear
torch.sigmoid

I think I can easily implement the sigmoid function using numpy. However, can I have some implementation for the nn.LSTM and nn.Linear using something not involving pytorch? Also, how will I use the weights from the state dict into the new class?
So, the question is, how can I ""translate"" this RNN definition into a class that doesn't need pytorch, and how to use the state dict weights for it?
Alternatively, is there a ""light"" version of pytorch, that I can use just to run the model and yield a result?
EDIT
I think it might be useful to include the numpy/scipy equivalent for both nn.LSTM and nn.linear. It would help us compare the numpy output to torch output for the same code, and give us some modular code/functions to use. Specifically, a numpy equivalent for the following would be great:
rnn = nn.LSTM(10, 20, 2)
input = torch.randn(5, 3, 10)
h0 = torch.randn(2, 3, 20)
c0 = torch.randn(2, 3, 20)
output, (hn, cn) = rnn(input, (h0, c0))

and also for linear:
m = nn.Linear(20, 30)
input = torch.randn(128, 20)
output = m(input)

",3,934,"You should try to export the model using torch.onnx. The page gives you an example that you can start with.
An alternative is to use TorchScript, but that requires torch libraries.
Both of these can be run without python. You can load torchscript in a C++ application https://pytorch.org/tutorials/advanced/cpp_export.html
ONNX is much more portable and you can use in languages such as C#, Java, or Javascript
https://onnxruntime.ai/ (even on the browser)
A running example
Just modifying a little your example to go over the errors I found
Notice that via tracing any if/elif/else, for, while will be unrolled
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import random

torch.manual_seed(1)
random.seed(1)
device = torch.device('cpu')

class RNN(nn.Module):
  def __init__(self, input_size, hidden_size, output_size,num_layers, matching_in_out=False, batch_size=1):
    super(RNN, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.num_layers = num_layers
    self.batch_size = batch_size
    self.matching_in_out = matching_in_out #length of input vector matches the length of output vector 
    self.lstm = nn.LSTM(input_size, hidden_size,num_layers)
    self.hidden2out = nn.Linear(hidden_size, output_size)
  def forward(self, x, h0, c0):
    lstm_out, (hidden_a, hidden_b) = self.lstm(x, (h0, c0))
    outs=self.hidden2out(lstm_out)
    return outs, (hidden_a, hidden_b)
  def init_hidden(self):
    #return torch.rand(self.num_layers, self.batch_size, self.hidden_size)
    return (torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device).detach(),
            torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device).detach())

# convert the arguments passed during onnx.export call
class MWrapper(nn.Module):
    def __init__(self, model):
        super(MWrapper, self).__init__()
        self.model = model;
    def forward(self, kwargs):
        return self.model(**kwargs)

Run an example
rnn = RNN(10, 10, 10, 3)
X = torch.randn(3,1,10)
h0,c0  = rnn.init_hidden()
print(rnn(X, h0, c0)[0])

Use the same input to trace the model and export an onnx file

torch.onnx.export(MWrapper(rnn), {'x':X,'h0':h0,'c0':c0}, 'rnn.onnx', 
                  dynamic_axes={'x':{1:'N'},
                               'c0':{1: 'N'},
                               'h0':{1: 'N'}
                               },
                  input_names=['x', 'h0', 'c0'],
                  output_names=['y', 'hn', 'cn']
                 )

Notice that you can use symbolic values for the dimensions of some axes of some inputs. Unspecified dimensions will be fixed with the values from the traced inputs. By default LSTM uses dimension 1 as batch.
Next we load the ONNX model and pass the same inputs
import onnxruntime
ort_model = onnxruntime.InferenceSession('rnn.onnx')
print(ort_model.run(['y'], {'x':X.numpy(), 'c0':c0.numpy(), 'h0':h0.numpy()}))

","Basically implementing it in numpy and copying weights from your pytorch model can do the trick.  For your usecase you will only need to do a forward pass so we just need to implement that only
#Set Parameters for a small LSTM network
input_size  = 2 # size of one 'event', or sample, in our batch of data
hidden_dim  = 3 # 3 cells in the LSTM layer
output_size = 1 # desired model output

num_layers=3
torch_lstm = RNN( input_size, 
                 hidden_dim ,
                 output_size,
                 num_layers,
                 matching_in_out=True
                 )

state = torch_lstm.state_dict() # state will capture the weights of your model

Now for LSTM in numpy these functions will be used:
got the below code from this link: https://towardsdatascience.com/the-lstm-reference-card-6163ca98ae87
### NOT MY CODE
import numpy as np 
from scipy.special import expit as sigmoid

def forget_gate(x, h, Weights_hf, Bias_hf, Weights_xf, Bias_xf, prev_cell_state):
    forget_hidden  = np.dot(Weights_hf, h) + Bias_hf
    forget_eventx  = np.dot(Weights_xf, x) + Bias_xf
    return np.multiply( sigmoid(forget_hidden + forget_eventx), prev_cell_state )

def input_gate(x, h, Weights_hi, Bias_hi, Weights_xi, Bias_xi, Weights_hl, Bias_hl, Weights_xl, Bias_xl):
    ignore_hidden  = np.dot(Weights_hi, h) + Bias_hi
    ignore_eventx  = np.dot(Weights_xi, x) + Bias_xi
    learn_hidden   = np.dot(Weights_hl, h) + Bias_hl
    learn_eventx   = np.dot(Weights_xl, x) + Bias_xl
    return np.multiply( sigmoid(ignore_eventx + ignore_hidden), np.tanh(learn_eventx + learn_hidden) )


def cell_state(forget_gate_output, input_gate_output):
    return forget_gate_output + input_gate_output

  
def output_gate(x, h, Weights_ho, Bias_ho, Weights_xo, Bias_xo, cell_state):
    out_hidden = np.dot(Weights_ho, h) + Bias_ho
    out_eventx = np.dot(Weights_xo, x) + Bias_xo
    return np.multiply( sigmoid(out_eventx + out_hidden), np.tanh(cell_state) )


We would need the sigmoid function as well so
def sigmoid(x):
    return 1/(1 + np.exp(-x))

Because pytorch stores weights in stacked manner so we need to break it up for that we would need the below function
def get_slices(hidden_dim):
    slices=[]
    breaker=(hidden_dim*4)
    slices=[[i,i+3] for i in range(0, breaker, breaker//4)]
    return slices

Now we have the functions ready for lstm, now we create an lstm class to copy the weights from pytorch class and get the output from it.
class numpy_lstm:
    def __init__( self, layer_num=0, hidden_dim=1, matching_in_out=False):
        self.matching_in_out=matching_in_out
        self.layer_num=layer_num
        self.hidden_dim=hidden_dim
        
    def init_weights_from_pytorch(self, state):
        slices=get_slices(self.hidden_dim)
        print (slices)

        #Event (x) Weights and Biases for all gates
        
        lstm_weight_ih='lstm.weight_ih_l'+str(self.layer_num)
        self.Weights_xi = state[lstm_weight_ih][slices[0][0]:slices[0][1]].numpy()  # shape  [h, x]
        self.Weights_xf = state[lstm_weight_ih][slices[1][0]:slices[1][1]].numpy()  # shape  [h, x]
        self.Weights_xl = state[lstm_weight_ih][slices[2][0]:slices[2][1]].numpy()  # shape  [h, x]
        self.Weights_xo = state[lstm_weight_ih][slices[3][0]:slices[3][1]].numpy() # shape  [h, x]

        
        lstm_bias_ih='lstm.bias_ih_l'+str(self.layer_num)
        self.Bias_xi = state[lstm_bias_ih][slices[0][0]:slices[0][1]].numpy()  #shape is [h, 1]
        self.Bias_xf = state[lstm_bias_ih][slices[1][0]:slices[1][1]].numpy()  #shape is [h, 1]
        self.Bias_xl = state[lstm_bias_ih][slices[2][0]:slices[2][1]].numpy()  #shape is [h, 1]
        self.Bias_xo = state[lstm_bias_ih][slices[3][0]:slices[3][1]].numpy() #shape is [h, 1]
        
        
        lstm_weight_hh='lstm.weight_hh_l'+str(self.layer_num)

        #Hidden state (h) Weights and Biases for all gates
        self.Weights_hi = state[lstm_weight_hh][slices[0][0]:slices[0][1]].numpy()  #shape is [h, h]
        self.Weights_hf = state[lstm_weight_hh][slices[1][0]:slices[1][1]].numpy()  #shape is [h, h]
        self.Weights_hl = state[lstm_weight_hh][slices[2][0]:slices[2][1]].numpy()  #shape is [h, h]
        self.Weights_ho = state[lstm_weight_hh][slices[3][0]:slices[3][1]].numpy() #shape is [h, h]
        
        
        lstm_bias_hh='lstm.bias_hh_l'+str(self.layer_num)

        self.Bias_hi = state[lstm_bias_hh][slices[0][0]:slices[0][1]].numpy()  #shape is [h, 1]
        self.Bias_hf = state[lstm_bias_hh][slices[1][0]:slices[1][1]].numpy()  #shape is [h, 1]
        self.Bias_hl = state[lstm_bias_hh][slices[2][0]:slices[2][1]].numpy()  #shape is [h, 1]
        self.Bias_ho = state[lstm_bias_hh][slices[3][0]:slices[3][1]].numpy() #shape is [h, 1]
    def forward_lstm_pass(self,input_data):
        h = np.zeros(self.hidden_dim)
        c = np.zeros(self.hidden_dim)
        
        output_list=[]
        for eventx in input_data:
            f = forget_gate(eventx, h, self.Weights_hf, self.Bias_hf, self.Weights_xf, self.Bias_xf, c)
            i =  input_gate(eventx, h, self.Weights_hi, self.Bias_hi, self.Weights_xi, self.Bias_xi, 
                        self.Weights_hl, self.Bias_hl, self.Weights_xl, self.Bias_xl)
            c = cell_state(f,i)
            h = output_gate(eventx, h, self.Weights_ho, self.Bias_ho, self.Weights_xo, self.Bias_xo, c)
            if self.matching_in_out: # doesnt make sense but it was as it was in main code :(
                output_list.append(h)
        if self.matching_in_out:
            return output_list
        else:
            return h


Similarly for fully connected layer,
    
    
class fully_connected_layer:
    def __init__(self,state, dict_name='fc', ):
        self.fc_Weight = state[dict_name+'.weight'][0].numpy()
        self.fc_Bias = state[dict_name+'.bias'][0].numpy() #shape is [,output_size]
        
    def forward(self,lstm_output, is_sigmoid=True):
        res=np.dot(self.fc_Weight, lstm_output)+self.fc_Bias
        print (res)
        if is_sigmoid:
            return sigmoid(res)
        else:
            return res
        

Now we would need one class to call all of them together and generalise them with respect to multiple layers
You can modify the below class if you need more Fully connected layers or want to set false condition for sigmoid etc.
        
class RNN_model_Numpy:
    def __init__(self, state, input_size, hidden_dim, output_size, num_layers, matching_in_out=True):
        self.lstm_layers=[]
        for i in range(0, num_layers):
            lstm_layer_obj=numpy_lstm(layer_num=i, hidden_dim=hidden_dim, matching_in_out=True)
            lstm_layer_obj.init_weights_from_pytorch(state) 
            self.lstm_layers.append(lstm_layer_obj)
        
        self.hidden2out=fully_connected_layer(state, dict_name='hidden2out')
        
    def forward(self, feature_list):
        for x in self.lstm_layers:
            lstm_output=x.forward_lstm_pass(feature_list)
            feature_list=lstm_output
            
        return self.hidden2out.forward(feature_list, is_sigmoid=False)

Sanity check on a numpy variable:
data = np.array(
           [[1,1],
            [2,2],
            [3,3]])



check=RNN_model_Numpy(state, input_size, hidden_dim, output_size, num_layers)
check.forward(data)

EXPLANATION:
Since we just need forward pass, we would need certain functions that are required in LSTM, for that we have the forget gate, input gate, cell gate and output gate. They are just some operations that are done on the input that you give.
For get_slices function, this is used to break down the weight matrix that we get from pytorch state dictionary (state dictionary) is the dictionary which contains the weights of all the layers that we have in our network.
For LSTM particularly have it in this order  ignore, forget, learn, output. So for that we would need to break it up for different LSTM cells.
For numpy_lstm class, we have init_weights_from_pytorch function which must be called, what it will do is that it will extract the weights from state dictionary which we got earlier from pytorch model object and then populate the numpy array weights with the pytorch weights. You can first train your model and then save the state dictionary through pickle and then use it.
The fully connected layer class just implements the hidden2out neural network.
Finally our rnn_model_numpy class is there to ensure that if you have multiple layers then it is able to send the output of one layer of lstm to other layer of lstm.
Lastly there is a small sanity check on data variable.
IMPORTANT NOTE: PLEASE NOTE THAT YOU MIGHT GET DIMENSION ERROR AS PYTORCH WAY OF HANDLING INPUT IS COMPLETELY DIFFERENT SO PLEASE ENSURE THAT YOU INPUT NUMPY IS OF SIMILAR SHAPE AS DATA VARIABLE.
Important references:
https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
https://christinakouridi.blog/2019/06/19/backpropagation-lstm/
",
SciPy strange result,https://stackoverflow.com/questions/54671752,"Why does glmnet.py throw a TypeError at cvglmnet when I provide floats, if glmnet does require floats?","I am trying to run a glm for logistic regression using Glmnet package for python following Glmnet Vignette and I am having problems with the type of my response variable 'y'.
I want to perform both the glmnet function and the cvglmnet function. The vignette says that ""For binomial logistic regression, the response variable y should be either a factor with two levels, or a two-column matrix of counts or proportions."".
I got my y values from one column of a pandas dataframe. Similarly my input matrix x is from several columns of a pandas dataframe. So my y.values is a numpy.ndarray of shape (290,) made of 1's and 2's, so an array of integers.

To check my types, if I do:

isinstance(y.values,scipy.ndarray)
True
isinstance(y.values,np.ndarray)
True
isinstance(y.values,int)
False
type(y.values[1])
numpy.int64
isinstance(y.values, float)
False


If I run

fit = glmnet(x = fold1_sp.copy(), y = y.values, family = 'binomial')


I get an error 

ValueError: y input must be a scipy float64 ndarray


I found in https://github.com/bbalasub1/glmnet_python/issues/15 that they tried to address this issue mentioning that any search for scipy float 64 ndarrays returns numpy float ndarray results because they are basically the same object type, so I convert to numpy float: 

fit = glmnet(x = fold1_sp.copy(), y = np.float64(y.values), family = 'binomial')


and it runs just fine.
But this way my y array is made of floats 1.'s and 2.'s

If I now try using the cvglmnet function, it complains it needs an integer

cvfit = cvglmnet(x = fold1_sp.copy(), y = np.float64(y.values), family = 'binomial', ptype = 'class')


throwing the following error:

TypeError: 'numpy.float64' object cannot be interpreted as an integer


which is strange at least to me since it asked for a float before, and if I use my y as integers using the original y.values, then it complains again it needs scipy float64.

So how can I overcome this TypeError problem?.

Thanks ever so much
",3,973,"I was having a similar problem but with X data, and I solve it by putting
cvglmnet(x=X.astype(np.float64), y=y,family = ""gaussian"",alpha=1)

",,
SciPy strange result,https://stackoverflow.com/questions/24824192,Redisplaying plots in Matplotlib in conjunction with PyQt,,3,313,,,
SciPy strange result,https://stackoverflow.com/questions/17753178,howto emulate 2-sample t-test in scipy,,3,4496,,,
SciPy strange result,https://stackoverflow.com/questions/10283053,Downloading a .csv file from the web (with redirects) in python,,3,9459,,,
SciPy strange result,https://stackoverflow.com/questions/76787449,SciPy&#39;s lsim not behaving as expected compared to Python Control Systems Library,"I am doing some control engineering in Python, and I have come accross a very strange discrepency between SciPy and the Python Control Systems Library for the implementation of the lsim command. To the best of my knowledge, these commands should behave identically, but it seems they are not: more particularly, the output of the SciPy command seems like nonsense. I have included a MWE below.
""""""
Minimal working example (MWE) replicating the discrepency between lsim for the SciPy module and control module
""""""

# Import modules
import numpy as np
import scipy.signal as sgnl
import matplotlib.pyplot as plt
import control as ctrl
import control.matlab as ctrl_mtl

# Number of samples
N_k = 100    # Total number of outputs

# Continous-time (unstable) open loop system dynamics
A = np.array([[0, 1],
              [-2, 3]])
B = np.array([[0],
              [1]])
n_x = A.shape[0]
n_u = B.shape[1]

# Initial state
x_0 = np.array([[1], [1]], dtype=float)

# Sampling period
h = 0.5

# Controller (full-state feedback)
K = np.array([[0, -4]])

# Closed-loop (stable) continous-time system
sys_cl_scipy = sgnl.StateSpace(A + B @ K, np.zeros((n_x, 1)), np.eye(n_x), np.zeros((n_x, 1)))
sys_cl_ctrl = ctrl.StateSpace(A + B @ K, np.zeros((n_x, 1)), np.eye(n_x), np.zeros((n_x, 1)))

# Simulate dynamics
# NOTE: The only difference is that for SciPy to work I need to add np.ravel(), as otherwise I get the error
# ""ValueError: could not broadcast input array from shape (2,1) into shape (2,)""
_, _, x_scipy = sgnl.lsim(sys_cl_scipy, None, np.linspace(0, h * (N_k - 1), N_k), X0=np.ravel(x_0), interp=False)
_, _, x_ctrl = ctrl_mtl.lsim(sys_cl_ctrl, T=np.linspace(0, h * (N_k - 1), N_k), X0=x_0)
x_scipy = x_scipy.T
x_ctrl = x_ctrl.T

# Plot the state through state-space (verification)
fig_verify, ax = plt.subplots()
ax.plot(np.ravel(x_scipy[0, :]), np.ravel(x_scipy[1, :]), label=""x SciPy"", color=""green"")
ax.plot(np.ravel(x_ctrl[0, :]), np.ravel(x_ctrl[1, :]), label=""x control"", color=""blue"")
plt.legend()
plt.title(""State trajectory"")

# Display all plots
plt.show()

This gives this as result.

The control implementation works as expected, but the SciPy implementation seems completely wrong. Is there something I am missing, or how should I interpret this? Thanks in advance.
I tried to compare both methods on an identical setup, and they produce different results, whilst they both should simulate the output of a continous-time LTI system.
",2,132,"You appear to be running into a typing problem (as in the numerical type of the variables, not hitting keys with your fingers ). Your array A is an array of integers, and lsim uses the data type of A to determine the data type of the output array.  You can fix this by ensuring that the data type of A is floating point:
A = np.array([[0.0, 1.0],
              [-2.0, 3.0]])

P.S. I'm pretty sure anyone running lsim--which simulates a continuous-time system--would expect the calculation to use floating point values, so I'd call this a bug.  I created an issue for this in the SciPy github repo: https://github.com/scipy/scipy/issues/18982
",,
SciPy strange result,https://stackoverflow.com/questions/55169294,Convert AsciiMath/MathML to SVG/PNG,"I'm looking for a way to convert my AsciiMath (or MathML) sources to SVG and/or PNG. I've found a nodeJS library for SVG conversion but calling that from Python is not very convenient and the output is not entirely satisfying.

Taking the fact I'd like to render mathematical formulas to svg/png it seems logical to look for a solution in math libraries (NumPy, SciPy, Pandas, Matplotlib, Sympy, etc...) but to no avail. All my google results combining all possible permutations of asciimath+mathml+svg+png lead to nothing which is strange.

Please recommend me either search patterns to find a solution or share your experiences/ideas to get this seemingly simple job done in Python.

All help would be highly appreciated!
",2,2498,"I just created ziamath for exactly this purpose. It comes bundled with the STIX math font, so no setup is required beyond the pip install, but it should also work with other math-enabled fonts. Pure-Python, so it does not need a Latex installation or anything else to work. This first version doesn't quite cover the full MathML specification, but the most useful parts are in there.
To bundle it into an app with something like PyInstaller, you'll need to make sure the STIX font gets included, but that should just be one line in the PyInstaller config.
",,
SciPy strange result,https://stackoverflow.com/questions/54810650,Strange behaviour in scipy.solve_ivp when using an implicit method,"I recently ran into a question about integration and encountered a strange bug. I attempt a very simple problem using solve_ivp:

from scipy.integrate import solve_ivp
import numpy as np

def f(y, t):
    return y

y0 = [1,1,1,1]
method = 'RK23'
s = solve_ivp(f, (0,1), y0, method=method, t_eval=np.linspace(0,1))


And it works fine. When I change to method='BDF' or method='Radau' I get an error:

Traceback (most recent call last):

  File ""&lt;ipython-input-222-f11c4406e92c&gt;"", line 10, in &lt;module&gt;
    s = solve_ivp(f, (0,1), y0, method=method, t_eval=np.linspace(0,1))

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\ivp.py"", line 455, in solve_ivp
    solver = method(fun, t0, y0, tf, vectorized=vectorized, **options)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\radau.py"", line 299, in __init__
    self.jac, self.J = self._validate_jac(jac, jac_sparsity)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\radau.py"", line 345, in _validate_jac
    J = jac_wrapped(t0, y0, self.f)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\radau.py"", line 343, in jac_wrapped
    sparsity)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\common.py"", line 307, in num_jac
    return _dense_num_jac(fun, t, y, f, h, factor, y_scale)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\scipy\integrate\_ivp\common.py"", line 318, in _dense_num_jac
    diff = f_new - f[:, None]

IndexError: too many indices for array


I also get an error with method = 'LSODA', although different (i.e. all implicit integrators). I do not get an error with any of the explicit integrators.

I tried this in spyder with scipy version 1.0.0 and in google colab (scipy version 1.1.0), with the same results.

Is this a bug or am I missing some argument I need for implicit integrators??
",2,900,"It appears that the Radau and BDF methods do not handle single-valued RHS functions. Making the function f above output a 1-D list solves your issue. Additionally, as mentioned by Weckesser in the comments, solve_ivp expects the RHS to be f(t, y) and not f(y, t).
Like this
def f(t, y):
    return [y]

",,
SciPy strange result,https://stackoverflow.com/questions/47931749,Why is Scipy&#39;s percentileofscore returning a different result than Excel&#39;s PERCENTRANK.INC?,,2,972,,,
SciPy strange result,https://stackoverflow.com/questions/37487532,Strange Result from FFT using Scipy,"I'm trying to do an FFT of some data (a Gaussian pulse), but I'm finding a strange result. The real and imaginary components of the resultant FFT alternate in sign every index of the array. The absolute values of the arrays, however, are continuous. So, I get something that looks like this:



Does anybody have an idea on what is causing this? Thanks!
",2,212,"Alternating signs in the frequency domain corresponds to an exp(j*pi*n) complex factor which by the shift theorem corresponds to a time domain circular shift of N/2 samples. Looking at your time domain Gaussian pulse you should notice that the peak indeed appears at N/2 instead of index 0.
Shifting back your time domain Gaussian pulse with ifftshift should give you a pulse centered at 0 whose frequency domain representation does not have this sign alternation.
",,
SciPy strange result,https://stackoverflow.com/questions/20061141,pymc 3.0 Predictive Posterior Distribution,,2,1176,,,
SciPy strange result,https://stackoverflow.com/questions/17527869,Curve fit fails with exponential but zunzun gets it right,,2,1944,,,
SciPy strange result,https://stackoverflow.com/questions/16929179,Strange result with python&#39;s (scipy) curve fitting,,2,1153,,,
SciPy strange result,https://stackoverflow.com/questions/74073866,SciPy Minimize doesn&#39;t pass all guesses on?,"I am trying to minimize a function of two variables using SciPy. The function itself is a chain of multiple lambda functions (makes it complicated but unfortunately it is the easiest way to write the expressions I need).
However, when using SciPy's minimize routine, I get the error ""TypeError: () missing 1 required positional argument: 'labour'""
Strangely enough, if I pass the arguments to the function directly, there is no error, so I assume that my chaining was correct.
Here is a minimum reproducible example:
# Preliminaries 0: import packages
import numpy as np
from scipy import optimize

# Preliminaries 1: Set parameters

alpha = 0.4
gamma = 0.4
delta = 0.05
beta = 0.95

# Preliminaries 2: Define functions

production_f = lambda capital, labour : (capital** alpha) * (labour ** (1-alpha))
utility_f_uni = lambda consumption, labour : np.log(consumption) + gamma * np.log(1-labour) if (consumption &gt; 0 and labour &gt; 0 and labour &lt; 1) else -5000
law_of_motion_f = lambda current_capital, next_capital, labour : production_f(current_capital, labour) - next_capital + (1-delta) * current_capital
utility_f_multi = lambda current_capital, next_capital, labour : utility_f_uni(law_of_motion_f(current_capital, next_capital, labour), labour)

optimization_f = lambda current_capital, next_capital, labour, value_f: utility_f_multi(current_capital, next_capital, labour) + beta * value_f(next_capital)

max_capital = lambda capital : production_f(capital, 1) + (1 - delta) * capital

For those knowledgeable of Dynamic Programming, I am trying to derive the value function of a growth model using Value Function iteration, but I didn't get so far yet. The period payoff is given by utility_f_multi. The Value function guess is given by optimization_f, which takes in four arguments including the previous value function guess.
In my example, I generate an interpolation that is closer to the true value function, but for all intents and purposes the constant 0 function also suffices. I then go on to create the optimization problem given our state variable
initial_value = lambda x : 0
current_optimization_f = lambda next_capital, labour: -optimization_f(3, next_capital, labour, initial_value)

Finally, I pass on the problem to the minimize function, from which the error results:
optimized_problem = optimize.minimize(current_optimization_f, [2,0.2])

The message, as mentioned previously, is ""() missing 1 required positional argument: 'labour'""
However, if I just pass on the two arguments to the function by hand, I receive no issue
print(current_optimization_f(2, 0.3))

which returns a value without problems.
Any help on this issue would be appreciated!
",1,37,"The function that you pass to scipy.minimize must use a singular argument for all the numerical inputs.
Imaging you wrote your function like:
def current_optimization_f(next_capital, labour):
   return optimization_f(3, next_capital, labour, initial_value)

scipy will call:
current_optimization([2, 0.3])

rather than
current_optimization(2, 0.3)

You can use an additional lambda to unpack the arguments:
obj_func = lambda x: current_optimization(*x)

rtn = optimize.minimize(obj_func, [2, 0.3])

And also, unrelated to your question, you should look at CasADI to solve these types of questions
",,
SciPy strange result,https://stackoverflow.com/questions/70371720,scipy curve_fit strange result matplotlib,"inbefore my code:
def func (x,a,b):
    return a*np.exp(b*x)

xFit= np.arange(0.0, 20, 0.01)
dev_Fluenz1= np.array([68.9, 21.81, 9.38, 3.73])
dev_Fluenz2= np.array([137.68 , 42.34, 18.75, 7.47 ])
dev_Fluenz3= np.array([80.34, 23.82 , 10.06, 3.76 ])
dev_Fluenz4= np.array([203.7, 61.67 , 10.06, 10.33 ])
dev_Fluenz5= np.array([135.74, 46.23 , 19.42 , 11.21 ])


dev_Fluenz6= np.array([382.83, 112.95, 50.02, 14.95])
dev_Fluenz7= np.array([382.45 , 117.62 , 50.01, 14.95 ])
dev_Fluenz8= np.array([147.32 , 43.67 , 17.88 , 5.01 ])
dev_Fluenz9= np.array([282.91, 85.64 , 35.63 , 13.77 ])
dev_Fluenz10= np.array([150.82, 51.37 , 20.5 , 11.21 ])


dev_x2= np.array([2,5,10,20])

plt.plot(dev_x2,dev_Fluenz1, 'bo')
popt, pcov = curve_fit(func, dev_x2, dev_Fluenz1)
plt.plot(xFit, func(xFit,*popt),color='b', linestyle='--',label=f'Reales DOE Sa &lt;= 0,3 Fluenz 1 J/cm**2 \u03bcm F(x) = {round(popt[0])} * e^({round(popt[1])}*x)')

results in : enter image description here
changing xFit= np.arange(0.0, 20, 0.01) to xFit= np.arange(0.0, 200, 0.01)
and
`dev_x2= np.array([2,5,10,20])` to `dev_x2= np.array([20,50,100,200])`

results in enter image description here
Why?
Thank you for your help!
",1,54,"curve_fit takes an initial guess as a starting point of the fit, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html.
If you do not provide it, as you did not, the default is used:

p0array_like, optional
Initial guess for the parameters (length N). If None, then the initial values will all be 1 (if the number of parameters for the function can be determined using introspection, otherwise a ValueError is raised).

For your second test case that blows up, it matters because with the default initial value of the exponent b in func, for the given range of dev_x2`, you get very large values and the optimizer cannot find a good solution
So you need to provide a sensible starting value. For example, note the p0 = [... bit below
xFit= np.arange(0.0, 200, 0.01)
dev_x2= np.array([20,50,100,200])

plt.plot(dev_x2,dev_Fluenz1, 'bo')
popt, pcov = curve_fit(func, dev_x2, dev_Fluenz1, p0 = [100,  -0.01])
plt.plot(xFit, func(xFit,*popt),color='b', linestyle='--',label=f'Reales DOE Sa &lt;= 0,3 Fluenz 1 J/cm**2 \u03bcm F(x) = {round(popt[0])} * e^({round(popt[1])}*x)')


you get a sensible result:

",,
SciPy strange result,https://stackoverflow.com/questions/69420714,Python deconvolution giving unexpected result,"Below I have plotted the signal (Lifetime decay) I am trying to deconvolve from an impulse response function, i.e. the divider (IRF). So I should just get the decay a bit sharper.
Here is an example of a topic I look at that gives what I need:
Understanding scipy deconvolve

Please not for my code, I am using only the peak of the divider (IRF), not the entire array sequence as shown on the image.
I am using the following code to do that:
IRF = IRF * (max(decay)/max(IRF))
# replace 0s to avoid error message 
IRF = np.where(IRF == 0, 0.1, IRF)    
decay = np.where(decay == 0, 0.1, decay)  
# take only the quotient part of the result 
deconv = scipy.signal.deconvolve(decay, IRF)[0]
# ""padding"" the deconvolved signal so it has the same size as the original signal 
s = int((len(decay)-(len(deconv)))/2)  ## difference on each side 
deconv_res = np.zeros(len(decay))   
end = int(len(decay)-s-1)  # final index
deconv_res[s:end] = deconv
deconv = deconv_res
# convolved normalized to decay height for plotting 
deconv_n = deconv * (max(decay)/max(deconv))   

The IRF is an array of float64, the signal is an array of uint16.
I admit I'm not so familiar with the maths of deconvolution, so I am trying blindly different things, like trying different divider functions, but nothing is producing anywhere near as expected.
The last result I got looks like this (see plot of the original signal and what the signal it tried to deconvolve..)

Could anyone give me some idea if it's something in scipy.deconvolve I don't understand, what the reason could be for this strange behaviour, or even some high-level reading material that might help me out?
Or if you think this problem is more physics-y than coding-related, a better place to ask such a question?
",1,762,"The problem is that deconvolve is a sort of polynomial division, it decomposes the output signal in $conv(h, x) + r$, if your signal is noisy it may give strange results. Also if the first sample in the inpulse response is small it tends to produce the exponentially growing output.
What I would do for this problem is the division of FFTs.
N = 2**(ceil(log2(len(IRF) + len(decay)))
filtered = ifft(fft(decay, N) / fft(IRF, N))

",,
SciPy strange result,https://stackoverflow.com/questions/69077658,Doubts on what scipy.optimize.minimize is really doing,"I am trying to minimize a cost function and I got very strange results from scipy.optimize.minimize (with methods and 'SLSQP', 'L-BFGS-B').
I print the value of the cost function after each evaluation. First it performs the small perturbations before going into the supposedly right direction (ok). But then occurs something strange: it seems to change the initial cost function by something like value of the cost function at first evaluation - value of the cost function in the current evaluation and converges towards the value of the first evaluation of the cost function.
To illustrate that I created a toy function of 2 parameters (0.25 + 1000 * x1 ** 2 + 100 * x2 ** 2 + 0.1 * random()). x1 and x2 are restricted to the interval [0, 1] (bounds). X0 is set to (0.5, 0.5). Here is what i get:
cost function: 275.3414617153509 x1: 0.5 x2: 0.5
cost function: 275.34428666473536 x1: 0.5000000149011612 x2: 0.5
cost function: 275.3542128554434 x1: 0.5 x2: 0.5000000149011612
cost function: 0.2665482586461191 x1: 0.0 x2: 0.0
cost function: 68.9989043756609 x1: 0.24986835289808013 x2: 0.24986835289808013
cost function: 154.87646326641064 x1: 0.374835397734792 x2: 0.374835397734792
cost function: 210.70119869030185 x1: 0.4373600232007103 x2: 0.4373600232007103
cost function: 241.8621094503892 x1: 0.4686490613793924 x2: 0.4686490613793924
cost function: 258.36597245010955 x1: 0.4843084999840323 x2: 0.4843084999840323
cost function: 266.6807722679986 x1: 0.4921461216177911 x2: 0.4921461216177911
cost function: 270.96794190195914 x1: 0.49606891372760337 x2: 0.49606891372760337
cost function: 273.0999396362265 x1: 0.49803236262951744 x2: 0.49803236262951744
cost function: 274.23903284113646 x1: 0.4990151079476797 x2: 0.4990151079476797
cost function: 274.7564047455383 x1: 0.4995070260788122 x2: 0.4995070260788122

 fun: 274.7564047455383
 jac: array([189579.1440506 , 855714.52631378])
 message: 'Optimization terminated successfully'
nfev: 14
 nit: 1
njev: 1
status: 0
success: True
x: array([0.49950703, 0.49950703])

So I do not understand:

why the final result is 2.74.756... and not 0.2666
why it starts to converge towards X0

What makes me think that the cost function is ""modified"" (i.e., what it tries to minimize is not the cost function but initial cost function evaluation - current cost function evaluation) is that, sometimes, due the random() part of the toy function, the first guessed evaluation is a higher value than the perturbation evaluations and it also converges towards X0.
I am using Python 3.9.6 and scipy 1.6.1
Edit:
Here is the full code:
def toto(X):
   val  = 0.25 + 1000 * X[0] ** 2 + 100 * X[1] ** 2 + 0.1 * random();
   print(""cost function:"", val, 'x1:', X[0], 'x2:', X[1])
   return val

optimization = minimize(toto, [0.5, 0.5], method=SLSQP, bounds= [[0.0, 1.0], [0.0, 1.0]])
print(optimization)

Mathieu
",1,880,"Trying your code, I get basically the same results.
I can't say I have a full solution to your problem, but I can point out a few issues. One is that scipy.optimize.minimize defaults to using a very small step to compute numerical gradients (e.g. for L-BFGS-B, the default step size eps  equals 1e-8). To see why this is a problem, consider if you computed a numerical derivative from the optimal solution (0,0). The deterministic part of the derivative will be roughly 0, but what will be the stochastic part. It should the difference of the two random values divided by 1e-8. The most likely value for the difference will be 0.05 (based on the difference having a triangular distribution), so your derivative will roughly on the order of 1e6. So while the function doesn't differ much with this random noise, it has a substantial effect on the numerical derivative.
But if the gradients are so large, why is it saying it converged? Both the methods you listed also have an ftol convergence criteria, which causes convergence when the relative change in the function value between steps is below the threshold. SLSQP doesn't provide any description in it's convergence message, but L-BFGS-B at least gives a brief description of why it converged. For the cases where it was far away from (0,0), convergence was related to this ftol criteria. I don't think there is anything in the code specifically drawing it back towards your initial point; rather it just seems like a random step away from this doesn't lead to much of a change in the function value. If I ran the code repeatedly, it would converge to a lot of different solutions and not always come back to near this initial point.
You can't entirely fix this with just a numerical gradient based optimizer, but you can at least improve your result by changing the value of eps. I found that if I changed eps to 1e-4, it tended to converge to (0,0) or near it. Increasing eps doesn't entirely fix, as the gradient can still be significantly altered by random portion.
Other options are discussed in this prior post and include methods for denoising your function before evaluating gradients or evaluating the function across the range, fitting it with splines, and then optimizing the fitting function.
If your interest is in diagnosing the problems with this specific code, someone who knows more about the technical details of the scipy implementation can probably help. However, if your interest is generally about finding minima of noisy functions, I think this example makes clear that a numerical gradient based optimizer won't be sufficient for this purpose.
",,
SciPy strange result,https://stackoverflow.com/questions/65825332,Scipy Coordinate system,"i cannot find the definition of scipy's coordinate system.
i have tried several values (assuming a right hand system) but got a strange result.
for example:
from scipy.spatial.transform import Rotation as R
R.from_euler('zyx', angles=np.array([90,0,0]), degrees=True).as_matrix()
[ [ 0., -1.,  0.], [ 1.,  0.,  0.],  [ 0.,  0.,  1.]]

meaninig the counterclockwise rotation about the z axis (true for a right hand system) is inverse (meaning a left coordinate system)...
where can i find the definition??
Thanks!!!
",1,972,"The full documentation for Scipy's Rotation module can be found here. For your problem in particular, I am not sure there actually is a problem. Looking at Wikipedia, a 90-degree rotation is indeed counter-clockwise so that a vector originally aligned with the x-axis becomes aligned with the y-axis. This, I believe, is in agreement with the result of the code below.
from scipy.spatial.transform import Rotation as R

point = (5, 0, -2)
print(R.from_euler('z', angles=90, degrees=True).as_matrix() @ point)
# [0, 5, -2]

","In short, I think giving positive angle means negative rotation about the axis, since it makes sense with the result.
Normally, positive direction of rotation about z-axis is rotating from x-axis to y-axis; negative direction is from y to x.
The Documentation shows that using from_euler to initial a counter-clockwise rotation of 90 degrees about the z-axis is
R.from_euler('z', 90, degrees=True)

I guess ""the counter-clockwise rotation about z-axis"" from doc means ""negative direction about z-axis"" instead of ""positive direction about z-axis"".
",
SciPy strange result,https://stackoverflow.com/questions/61091824,scipy normal distribution with scale greater and less than 1,"I'm using the normal distribution from numpy and having a hard time understanding its documentation. Let's say I have a normal distribution with mean of 5 and standard deviation of 0.5:

import numpy as np
from matplotlib import pyplot as plt
from scipy.stats import norm

mean = 5
std = 0.25

x = np.linspace(mean - 3*std, mean + 3*std, 1000)
y = norm(loc=mean, scale=std).pdf(x)
plt.plot(x,y)




The resulting chart is the familiar bell curve but with its peak at around 1.6. How can the probability of any value exceed 1? If I multiply it by scale  then the probabilities are correct.

No such problem when std (and scale) are greater than 1 however:

mean = 5
std = 10

x = np.linspace(mean - 3*std, mean + 3*std, 1000)
y = norm(loc=mean, scale=std).pdf(x)
plt.plot(x,y)




The documentation on norm says loc is the mean and scale is the standard deviation. Why does it behave so strangely with scale greater and less than 1?

Python 3.8.2. Scipy 1.4.1
",1,2546,,,
SciPy strange result,https://stackoverflow.com/questions/60787710,"Scipy Optimization TNC method, what does scale do ? solution to &quot;unable to progress&quot;?","I am running multiple bounded optimizations with known gradient (100 to 300 Variables). Sometimes, TNC is returning ""unable to progress"".

For my objective function L-BFGS-B is much slower and outputs poor results compared to TNC. (Maybe because TNC is better when number of variables are large)
Using Basinhopping with L-BFGS-B and niter_success to 10, I am getting results close to TNC with 20x slower speed. When TNC returns ""unable to progress"", ""L-BFGS-B"" returns better results. So my current solution is to run Basinhopping when TNC fails with status 6 - ""unable to progress"".

It seems that ""unable to progress"" is returned when TNC is unable to reduce the objective function for x number of iterations. I played a little with the scale factor and inconsistently I got better results.

To my knowledge, Scale in an optimization problem let's the optimizer know which variable is more effective. I have this information and I believe this will reduce number of ""unable to progress"" I am getting. According to the docs,
https://docs.scipy.org/doc/scipy/reference/optimize.minimize-tnc.html
""The default scale array are up-low for interval bounded variables and 1+|x] fo the others"". So it's upper bound - lower bound for variables which are bounded, unable to understand how unbounded is treated. what is 1+|x]  ?

Also, I manually calculated up - low and set unbounded to 1, this is returning different results every time I run the optimization with the same input. (Strange ?)

I also tried to look into the code, how TNC is handling scale, the spicy wrapper sends an empty array or the input array to C code https://github.com/scipy/scipy/blob/master/scipy/optimize/tnc/moduleTNC.c. In the C code I am unable to find where the scale array is created or how it's used. Also could not find when ""unable to progress"" is triggered. Can someone point me where I should look into ?
",1,1201,,,
SciPy strange result,https://stackoverflow.com/questions/53000952,ImportError: cannot import name &#39;LinearNDInterpolator&#39;,,1,1576,,,
SciPy strange result,https://stackoverflow.com/questions/52027309,calculating p value from pearson r different to scipy,,1,563,,,
SciPy strange result,https://stackoverflow.com/questions/45356848,scipy curve_fit not producing smooth graph when fitting fourier function,,1,609,,,
SciPy strange result,https://stackoverflow.com/questions/44474327,Real 1D DFT in fftw,,1,1718,,,
SciPy strange result,https://stackoverflow.com/questions/40008017,scipy curve_fit strange result,,1,728,,,
SciPy strange result,https://stackoverflow.com/questions/77686694,SVD Decomposition for linear equation solving,"looking for Algo here &amp; trying to implement this code, I'm getting different l2-norms for resulting vectors of params for linear equation. Where am I mistaken in my attempt to adopt the code?
import numpy as np
from scipy import linalg

np.random.seed(123)
v = np.random.rand(4)
A = v[:,None] * v[None,:]
b = np.random.randn(4)

x = linalg.inv(A.T.dot(A)).dot(A.T).dot(b) #Usually not recommended because of Numerical Instability of the Normal Equations  https://johnwlambert.github.io/least-squares/
l2_0= linalg.norm(A.dot(x) - b)
print(""manually: "", l2_0)

x = linalg.lstsq(A, b)[0]
l2_1= linalg.norm(A.dot(x) - b)
print(""scipy.linalg.lstsq: "", l2_1)

# 2-norm of two calculations compared
print(np.allclose(l2_0, l2_1, rtol=1.3e-1))

def direct_ls_svd(x,y):
  # append a columns of 1s (these are the biases)
  x = np.column_stack([np.ones(x.shape[0]), x])

  # calculate the economy SVD for the data matrix x
  U,S,Vt = linalg.svd(x, full_matrices=False)

  # solve Ax = b for the best possible approximate solution in terms of least squares
  x_hat = Vt.T @ linalg.inv(np.diag(S)) @ U.T @ y
  #print(x_hat)

  # perform train and test inference
  #y_pred = x @ x_hat
    
  return y-x @ x_hat     #x_hat

x= direct_ls_svd(A, b)
l2_svd= linalg.norm(A.dot(x) - b)
print(""svd: "", l2_svd)

# LU
x= linalg.solve(A.T@A, A.T@b)
l2_solve= linalg.norm(A.dot(x) - b)
print(""scipy.linalg.solve: "", l2_solve)

# manually:  2.9751344995811313
# scipy.linalg.lstsq:  2.9286130558050654
# True
# svd:  6.830550019041984
# scipy.linalg.solve:  2.928613055805065

if my error is in SVD-decomposition Algorithm imlementation for solving Least-Squares problem or perhaps in Numpy relative Scipy rounding or precision differences ? How to correct svd-algo for Least-Squares to become compareable with scipy's? And will this algorithm be faster &amp; less memory-consuming than Iterative Least-Squares methods?
P.S.
SVD applications or here, SVD for PCA &amp; PLS-SVD is my final goal -- will be the algo the same as for Least-Squares approximation ? I'm confused with such a question in general (even with code examples). Can somebody add some clarity for newbie like me, please?
P.P.S.
applying such implementation - I'm getting even worse result: svd:  10.031259300735462 for l2-norm
P.P.P.S.
also I lack some understanding of svd in singular spectral decomposition if exists ref, as 1st for unsupervised dim.reduction &amp; 2nd for non-parametric TS analisys, for practice
P.P.P.P.S. ! PCA is used preliminary estimating if Multicollinearity exists, otherwise strange results can get (biased etc.)... (if no collinearity =&gt; no sensitivity to error of estimation, aka shown in small condition number of OLS analysis, - vc.vs huge cond.num.==collinearity for multivariable/multidimensional regression)
",0,134,"The most important part here is to filter out the singular vallues that are 0. For your example data S is [9.22354602e-01 3.92705914e-17 1.10667017e-17 5.55744006e-18], notice that you have one singular value close of ~9.22 and the other three are tiny (&lt; 1e-16).
If you attempt to reconstruct the solution using the small errors some elements of Vt and U, that should be about the same magnitude will get divided by these small values and will add up to a significant error to the result.
What can be done in this case is, you assume that any singular value that are small enough are an exact zero. In the following modified version of your function I am assuming all the singular values that are less than rcond times the maximum singular value should be zero. Then I compute a mask m and drop the corresponding rows and columns of the U and Vt matrices.
def direct_ls_svd(A,b,rcond=1e-7):
  # calculate the economy SVD for the data matrix x
  U,S,Vt = linalg.svd(A, full_matrices=False)
  # Mask to remove the zeroes
  m = (abs(S) / np.max(abs(S))) &gt; rcond
  U, S, Vt = U[:,m], S[m], Vt[m, :]
  assert np.allclose( U @ np.diag(S) @ Vt, A)
  # solve Ax = b for the best possible approximate solution
  # in terms of least squares
  x_hat = Vt.T @ ((U.T @ b) / S)
    
  return x_hat

An alternative solution is to set S[m]=0 then you could avoid an extra copy in the worst case, but you lose the potential savings from the reduction of the number of multiplications in the very low rank cases.
",,
SciPy strange result,https://stackoverflow.com/questions/76610841,Why does the p-value of the scipy.stats.ketest drop when the number of samples increases?,"Now days I am working on a time-series event, and want to show it is NOT randomly generated events. For this purpose, I am trying to use the kstest in scipy. By the way, I have a question about the cipy.stats.kstest for the poisson distribution as follows,
--- Code 1---
from scipy.stats import poisson, kstest

noPts = 100 # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;---------------
lambdaPoisson = 10

my_data = poisson.rvs(size = noPts, mu = lambdaPoisson)

ks_statistic, p_value = kstest(my_data, 'poisson', args=(lambdaPoisson,0))
print(ks_statistic, p_value)

0.18677614630310613 0.0015821590670650476

--- code 2 ---
from scipy.stats import poisson, kstest

noPts = 1000 #&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;--------------
lambdaPoisson = 10

my_data = poisson.rvs(size = noPts, mu = lambdaPoisson)

ks_statistic, p_value = kstest(my_data, 'poisson', args=(lambdaPoisson,0))
print(ks_statistic, p_value)

0.13477614630310608 2.6511802749311937e-16

I expected that the larger number of data points would result in a more precise statistical match between the two distributions (poisson.rvs vs. kstest(poisson). Yes, the ks_statistics is getting smaller. BUT the p-value becomes very small which rejects the null hypothesis, that is very strange because the two distributions are the same 'Poisson' with the same parameter. What am I doing wrong?
Can someone understand and explain to me the discrepancy here?
",0,55,"The documentation for kstest says it is only valid for continuous distributions. Poissons are discrete.
",,
SciPy strange result,https://stackoverflow.com/questions/75586614,Different results when computing integral analytically/with scipy VS approximating integral,"I'm trying to calculate the integral of a variant of the Hill Equation shown here. 
When I try implementing this with the integrate function in SciPy, I get the following:
from scipy import integrate

Top = 0.9015038230670139
Bottom = 0.5972679490151096
ic50 = 17561.998143066336
Coef = -1.245569789770613

def Hill_formula(X):
    return Bottom + (Top - Bottom)/(1 + ((10**np.log10(ic50))/10**X)**Coef)

integrate.quad(Hill_formula, 0.001, 40.0)

with the result coming out to (25.18116866489653, 1.3873362345430754e-08)
However, looking at the graph of this equation with 100 evenly spaced points from 0.001 to 40.0, it clearly looks nothing close to the given answer of ~25: (EDIT: It actually does look right on hindsight because I misread the y-axis of the plot. Still would appreciate help on why this discrepancy exists though) 
Checking with an approximation method for the area under the curve gives a similar result:
from scipy.integrate import simpson
points = np.linspace(0.001, 40.0, num=100)
curve_results = []
for val in points:
    curve_results.append(Hill_formula(val))

np.abs(simpson(points, curve_results))

resulting in 1.2913519756923537. Is there a reason why this discrepancy exists?
I tried calculating the analytical solution of the integral and using that resulting equation to directly obtain the area under the curve between the bounds, but ran into that same strange ~25 answer.
",0,73,"From the documentation of scipy.simpson:
scipy.integrate.simpson(y, x=None, dx=1.0, axis=-1, even='avg')

Integrate y(x) using samples along the given axis and the composite
Simpson's rule. If x is None, spacing of dx is assumed.

If there are an even number of samples, N, then there are an odd
number of intervals (N-1), but Simpson's rule requires an even number
of intervals. The parameter 'even' controls how this is handled.

Parameters
----------
y : array_like
    Array to be integrated.
x : array_like, optional
    If given, the points at which `y` is sampled.
...

That means that you've probably meant to write simpson(curve_results, points) instead of np.abs(simpson(points, curve_results)). That results in 25.18116860801739, which has the same first 9 digits as the one you've obtained and the the analytical result evaluating to 25.181168664896531....
",,
SciPy strange result,https://stackoverflow.com/questions/72985838,Scipy Gumbel Fit does not Fit - What is the correct use?,"I'm trying to fit various distributions onto my data and test (chi-squared?) which fits best. I started out by using the gumbel_r distribution of scipy, as this is the one often used in literature.
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import scipy.stats as ss

data = pd.read_csv(""data.csv"")
data

sns.histplot(data[""score""], kde=True, stat='probability')
plt.show()

x = np.linspace(0,1,101)
hist, bins = np.histogram(data[""score""], bins=x, density=True)
loc, scale = ss.gumbel_r.fit(hist)
dist = ss.gumbel_r(loc=loc,scale=scale)
plt.plot(x, dist.pdf(x))
plt.show()

Inspecting the plots yields strange results. For example my data has a peak at ~0.09 of around ~0.025. However, the plotted gumbel looks completely off.
My questions are now:

Why are the plots not looking similar? I'm also suspecting stat='probability' could be the culprit here?
What do I need to do, such that the second plot will look somewhat similar to the first one?
Optimally I would get another hist for the same bins of the fitted distribution and input into scipy.stats.chisquare to quantify how good the fit of the distribution is and see which fits best. Is that correct?

",0,815,"Don't give hist to gumbel_r.fit().  It expects the original data.  Change the line that calls fit() to
loc, scale = ss.gumbel_r.fit(data['score'].to_numpy())

Also, to get the Seaborn plot on the same scale as the plot of the PDF, change stat='probability' to stat='density' in the histplot() call.
",,
SciPy strange result,https://stackoverflow.com/questions/66856730,LAPACK&#39;s zgesvd results different than scipy.linalg&#39;s SVD,"So I'm trying to compute SVD of an NxN matrix. Strangely, for all the cases of 2x2 matrices, the SVD from both lapack and scipy match but they differ when I go for a 3x3 or 4x4 matrices.
// LAPACK (C) Case: 2x2

#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;Accelerate/Accelerate.h&gt;

/* Complex datatype */
struct _dcomplex { double re, im; };
typedef struct _dcomplex dcomplex;

/* ZGESVD prototype */
extern void zgesvd( char* jobu, char* jobvt, int* m, int* n, dcomplex* a,
                   int* lda, double* s, dcomplex* u, int* ldu, dcomplex* vt, int* ldvt,
                   dcomplex* work, int* lwork, double* rwork, int* info );
/* Auxiliary routines prototypes */
extern void print_matrix( char* desc, int m, int n, dcomplex* a, int lda );
extern void print_rmatrix( char* desc, int m, int n, double* a, int lda );

/* Parameters */
#define M 2
#define N 2
#define LDA M
#define LDU M
#define LDVT N

/* Main program */
int main() {
    /* Locals */
    int m = M, n = N, lda = LDA, ldu = LDU, ldvt = LDVT, info, lwork;
    dcomplex wkopt;
    dcomplex* work;
    /* Local arrays */
    /* rwork dimension should be at least max( 1, 5*min(m,n) ) */
    double s[M], rwork[5*M];
    dcomplex u[LDU*M], vt[LDVT*N];
//    dcomplex a[LDA*N] = {
//        {0, 0}, {0, 0}, {1,  0},
//        {-0.36599657,  -0.27449743}, {-0.27449743,  0.36599657}, {0.76249285, 0},
//        {-0.36599657, 0.27449743}, {-0.27449743, -0.36599657}, {0.76249285, 0},
//    };
    dcomplex a[LDA*N] = {
        {0.70710678, 0}, {0, -0.70710678},
        {0.70710678,  0}, {0,  0.70710678},
    };
    /* Executable statements */
    printf( "" ZGESVD Example Program Results\n"" );
    /* Query and allocate the optimal workspace */
    lwork = -1;
    zgesvd( ""All"", ""All"", &amp;m, &amp;n, a, &amp;lda, s, u, &amp;ldu, vt, &amp;ldvt, &amp;wkopt, &amp;lwork,
           rwork, &amp;info );
    lwork = (int)wkopt.re;
    work = (dcomplex*)malloc( lwork*sizeof(dcomplex) );
    /* Compute SVD */
    zgesvd( ""All"", ""All"", &amp;m, &amp;n, a, &amp;lda, s, u, &amp;ldu, vt, &amp;ldvt, work, &amp;lwork,
           rwork, &amp;info );
    /* Check for convergence */
    if( info &gt; 0 ) {
        printf( ""The algorithm computing SVD failed to converge.\n"" );
        exit( 1 );
    }
    /* Print singular values */
    print_rmatrix( ""Singular values"", 1, m, s, 1 );
    /* Print left singular vectors */
    print_matrix( ""Left singular vectors (stored columnwise)"", m, m, u, ldu );
    /* Print right singular vectors */
    print_matrix( ""Right singular vectors (stored rowwise)"", m, n, vt, ldvt );
    /* Free workspace */
    free( (void*)work );
    exit( 0 );
} /* End of ZGESVD Example */

/* Auxiliary routine: printing a matrix */
void print_matrix( char* desc, int m, int n, dcomplex* a, int lda ) {
    int i, j;
    printf( ""\n %s\n"", desc );
    for( i = 0; i &lt; m; i++ ) {
        for( j = 0; j &lt; n; j++ )
        printf( "" (%6.2f,%6.2f)"", a[i+j*lda].re, a[i+j*lda].im );
        printf( ""\n"" );
    }
}

/* Auxiliary routine: printing a real matrix */
void print_rmatrix( char* desc, int m, int n, double* a, int lda ) {
    int i, j;
    printf( ""\n %s\n"", desc );
    for( i = 0; i &lt; m; i++ ) {
        for( j = 0; j &lt; n; j++ ) printf( "" %6.2f"", a[i+j*lda] );
        printf( ""\n"" );
    }
}

Yields
&lt;!-- language: lang-bash --&gt;
ZGESVD Example Program Results

 Singular values
   1.00   1.00

 Left singular vectors (stored columnwise)
 ( -0.71,  0.00) ( -0.71,  0.00)
 ( -0.00,  0.71) (  0.00, -0.71)

 Right singular vectors (stored rowwise)
 ( -1.00, -0.00) ( -0.00, -0.00)
 ( -0.00, -0.00) ( -1.00, -0.00)

and scipy.linalg.SVD yields
&lt;!-- language: lang-bash --&gt;
Singular values: [1. 1.]
Left singular vectors: [[-0.70710678+0.j         -0.70710678+0.j        ]
                        [ 0.        +0.70710678j  0.        -0.70710678j]]
Right singular vectors: [[-1.+0.j -0.+0.j]
                         [-0.+0.j -1.+0.j]]

So far so good. Now when I try to input a 3x3 or NxN matrix, the results are like
// LAPACK C Case: 3x3

#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;Accelerate/Accelerate.h&gt;

/* Complex datatype */
struct _dcomplex { double re, im; };
typedef struct _dcomplex dcomplex;

/* ZGESVD prototype */
extern void zgesvd( char* jobu, char* jobvt, int* m, int* n, dcomplex* a,
                   int* lda, double* s, dcomplex* u, int* ldu, dcomplex* vt, int* ldvt,
                   dcomplex* work, int* lwork, double* rwork, int* info );
/* Auxiliary routines prototypes */
extern void print_matrix( char* desc, int m, int n, dcomplex* a, int lda );
extern void print_rmatrix( char* desc, int m, int n, double* a, int lda );

/* Parameters */
#define M 3
#define N 3
#define LDA M
#define LDU M
#define LDVT N

/* Main program */
int main() {
    /* Locals */
    int m = M, n = N, lda = LDA, ldu = LDU, ldvt = LDVT, info, lwork;
    dcomplex wkopt;
    dcomplex* work;
    /* Local arrays */
    /* rwork dimension should be at least max( 1, 5*min(m,n) ) */
    double s[M], rwork[5*M];
    dcomplex u[LDU*M], vt[LDVT*N];
    dcomplex a[LDA*N] = {
        {0, 0}, {0, 0}, {1,  0},
        {-0.36599657,  -0.27449743}, {-0.27449743,  0.36599657}, {0.76249285, 0},
        {-0.36599657, 0.27449743}, {-0.27449743, -0.36599657}, {0.76249285, 0},
    };
//    dcomplex a[LDA*N] = {
//        {0.70710678, 0}, {0, -0.70710678},
//        {0.70710678,  0}, {0,  0.70710678},
//    };
    /* Executable statements */
    printf( "" ZGESVD Example Program Results\n"" );
    /* Query and allocate the optimal workspace */
    lwork = -1;
    zgesvd( ""All"", ""All"", &amp;m, &amp;n, a, &amp;lda, s, u, &amp;ldu, vt, &amp;ldvt, &amp;wkopt, &amp;lwork,
           rwork, &amp;info );
    lwork = (int)wkopt.re;
    work = (dcomplex*)malloc( lwork*sizeof(dcomplex) );
    /* Compute SVD */
    zgesvd( ""All"", ""All"", &amp;m, &amp;n, a, &amp;lda, s, u, &amp;ldu, vt, &amp;ldvt, work, &amp;lwork,
           rwork, &amp;info );
    /* Check for convergence */
    if( info &gt; 0 ) {
        printf( ""The algorithm computing SVD failed to converge.\n"" );
        exit( 1 );
    }
    /* Print singular values */
    print_rmatrix( ""Singular values"", 1, m, s, 1 );
    /* Print left singular vectors */
    print_matrix( ""Left singular vectors (stored columnwise)"", m, m, u, ldu );
    /* Print right singular vectors */
    print_matrix( ""Right singular vectors (stored rowwise)"", m, n, vt, ldvt );
    /* Free workspace */
    free( (void*)work );
    exit( 0 );
} /* End of ZGESVD Example */

/* Auxiliary routine: printing a matrix */
void print_matrix( char* desc, int m, int n, dcomplex* a, int lda ) {
    int i, j;
    printf( ""\n %s\n"", desc );
    for( i = 0; i &lt; m; i++ ) {
        for( j = 0; j &lt; n; j++ )
        printf( "" (%6.2f,%6.2f)"", a[i+j*lda].re, a[i+j*lda].im );
        printf( ""\n"" );
    }
}

/* Auxiliary routine: printing a real matrix */
void print_rmatrix( char* desc, int m, int n, double* a, int lda ) {
    int i, j;
    printf( ""\n %s\n"", desc );
    for( i = 0; i &lt; m; i++ ) {
        for( j = 0; j &lt; n; j++ ) printf( "" %6.2f"", a[i+j*lda] );
        printf( ""\n"" );
    }
}


Yeilds
&lt;!-- language: lang-bash --&gt;
 ZGESVD Example Program Results

 Singular values
   1.55   0.65   0.42

 Left singular vectors (stored columnwise)
 (  0.26,  0.00) (  0.49, -0.34) ( -0.75,  0.00)
 (  0.20, -0.00) ( -0.66,  0.46) ( -0.57,  0.00)
 ( -0.94, -0.00) (  0.00,  0.00) ( -0.33,  0.00)

 Right singular vectors (stored rowwise)
 ( -0.61, -0.00) ( -0.56, -0.00) ( -0.56, -0.00)
 (  0.00,  0.00) (  0.41, -0.58) ( -0.41,  0.58)
 ( -0.79, -0.00) (  0.43, -0.00) (  0.43, -0.00)

&lt;!-- language: lang-bash --&gt;
# Python
Singular values: [1.55161905 0.64699664 0.41698163]
Left singular vectors: [[ 0.26480555-9.68622857e-18j  0.57973136-1.54633603e-01j
  -0.75490266+2.76133169e-17j]
 [ 0.19860416+1.15286199e-17j -0.77297515+2.06178138e-01j
  -0.56617699-3.28655711e-17j]
 [-0.94362832+0.00000000e+00j  0.        +0.00000000e+00j
  -0.33100694+0.00000000e+00j]]
Right singular vectors: [[-0.60815722+0.j         -0.5613131 +0.j         -0.5613131 +0.j        ]
 [ 0.        +0.j          0.18223745-0.68321996j -0.18223745+0.68321996j]
 [-0.7938166 +0.j          0.43003209+0.j          0.43003209+0.j        ]]

Now for a fact I know that the results computed by Scipy are perfect as the purpose that I'm using SVD for is point perfect and gives perfect results and my goal is to generate results like scipy. Now I know that Scipy also uses LAPACK's Drivers but why the difference then? Where am I messing it up.
",0,155,"The conditions you should be testing is not that the output matrices are the same.
The condition u @ np.diag(s) @ vh = A with u, s, vh = np.linalg.svd(A), is where you should focus.
Also pay atention to the order of the singular values, if you have two set of singular values s1 and s2 that are the same when sorted, you can construct a matrix P such that P @ np.diag(s1) @ P.T = s2 if you are lucky you can then use u1 @ P.inv() = u2 and P.T.inv() @ vh1 = vh2 since (u1 @ P.inv()) @ P @ np.diag(s1) @ P.T @ (P.T.inv() @ vh1) = u1 @ np.diag(1) @ vh1.
",,
SciPy strange result,https://stackoverflow.com/questions/65678500,"How to use print() command and make the shape of a numpy array consistent, during integration using scipy?","I tried examining how spicy.integrate.ode works. The code below is simple code to do this.
def func(t, z, p):
    x = z[0]
    y = z[1]
    print('x :', x)
    print('x.shape :', x.shape)
    print('y :', y)
    print('y.shape :', y.shape)
    return [x*0, y*0]

t_ini = 0
t_fin = 1
x_ini = np.array([[2, 2]])
y_ini = np.array([[2, 2]])

solver = ode(func)
solver.set_integrator('dopri5')
solver.set_initial_value([x_ini, y_ini], t_ini)
solver.set_f_params([0])
solver.integrate(t_fin)
x_fin, y_fin = solver.y
print('x_fin :', x_fin)
print('y_fin :', y_fin)

However,
print('x :', x)
print('x.shape :', x.shape)
print('y :', y)
print('y.shape :', y.shape)
return [x*0, y*0]

didn't work. The result of the code was
x_fin : [[2. 2.]]
y_fin : [[2. 2.]]

.
Interestingly, when I changed x_ini and y_ini into
x_ini = np.array([[2]])
y_ini = np.array([[2]])

, the print() command worked and the result of the code was the repetition of
x : 2.0
x.shape : ()
y : 2.0
y.shape : ()

with the two lines after the repetition which are
x_fin : [[2.]]
y_fin : [[2.]]

.
It was strange that even if I put x_ini and y_ini having (1, 1) shape, both print(x.shape) and print(y.shape) showed ().
So the questions are:

Why the print() didn't worked for x_ini = y_ini = np.array([[2, 2]]) and what I should to to make them work?
Why the shape of the numpy arrays which are x and y became () instead of (1, 1).
How to make the shape of the numpy arrays which are x and y be (1, 1) during the integration using scipy. What should I do if the shape of both x_ini and y_ini is (2, 2) and I want to make the shape consistent during the integration using scipy.

Is there any guys who know about these?
",0,70,"I get a warning when using your initial value array:
In [9]: x_ini = np.array([[2, 2]])
   ...: y_ini = np.array([[2, 2]])
In [10]: solver.set_initial_value([x_ini, y_ini], 0)
Out[10]: &lt;scipy.integrate._ode.ode at 0x7f1ab8953d60&gt;
In [11]: solver.integrate(.1)
/usr/local/lib/python3.8/dist-packages/scipy/integrate/_ode.py:1181: UserWarning: dopri5: input is not consistent
  warnings.warn('{:s}: {:s}'.format(self.__class__.__name__,
Out[11]: 
array([[[2., 2.]],

       [[2., 2.]]])

The output is the same as the input
In [12]: np.array([x_ini, y_ini])
Out[12]: 
array([[[2, 2]],

       [[2, 2]]])

With
x_ini = np.array([[2]])
y_ini = np.array([[2]])

The initial value is a (2,1,1) array
In [18]: np.array([x_ini, y_ini])
Out[18]: 
array([[[2]],

       [[2]]])

That does run, but the values passed to your function are 0d arrays
x : 2.0
x.shape : ()
y : 2.0
y.shape : ()

===
Let's simplify the func:
In [20]: def func(t, z, p):
    ...:     print(type(z), z.shape, z)
    ...:     return z*0
    ...: 
In [21]: solver = ode(func)
    ...: solver.set_integrator('dopri5')
Out[21]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [22]: solver.set_f_params([0])
Out[22]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [23]: solver.set_initial_value([1,2], 0)
Out[23]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [24]: solver.integrate(.1)
&lt;class 'numpy.ndarray'&gt; (2,) [1. 2.]
...

If I change the initial value to a (2,1,1), func gets the same inputs:
In [27]: solver.set_initial_value([[[1]],[[2]]], 0)
Out[27]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [28]: solver.integrate(.1)
&lt;class 'numpy.ndarray'&gt; (2,) [1. 2.]

Change the input to a 3 element array:
In [31]: solver.set_initial_value([1,2,3], 0)
Out[31]: &lt;scipy.integrate._ode.ode at 0x7f1ab6debe50&gt;
In [32]: solver.integrate(.1)
&lt;class 'numpy.ndarray'&gt; (3,) [1. 2. 3.]

From the docs:
f : callable ``f(t, y, *f_args)``
    Right-hand side of the differential equation. t is a scalar,
    ``y.shape == (n,)``.
    ``f_args`` is set by calling ``set_f_params(*args)``.
    `f` should return a scalar, array or list (not a tuple).

f returns dy/dt.  The y will be a 1d array, and it's supposed to return a like size array.  Note the y.shape requirement.
The y that ode passes to the function is derived from the initial value array.  A (2,1,1) input is flattened to (2,).  A (2,1,2) produces the warning.
",,
SciPy strange result,https://stackoverflow.com/questions/64392302,"Scipy, Numpy, Django, Docker Issue","Having issues with scipy and numpy.
This code, part of a Django app, runs perfectly when run on my windows 10 system:
try:
    # sparse_load is a scipy.sparse.csr_matrix
    sparse_load = scipy.sparse.load_npz(cache)
    logger.info('Got sparse_load')
    concept_alias_tfidfs = sparse_load.astype(numpy.float32)
except:
    logger.exception('Something went wrong!' ) 
# code continues here ...

It also runs perfectly when running inside a docker container deployed on a Linux server.
The issue is that the type conversion (sparse_load.astype(numpy.float32)) crashes my app when running in a docker container deployed on Docker Desktop running on Windows 10.  The strange thing is that logger.exception is never executed!  I've tried other type conversions with the same result and also tried removing the astype altogether which resulted in another crash further down in the code (again w/o hitting the exception handler placed around that piece of code.
Thoughts?
",0,146,"Even though OOMKIlled is false (Killed due to Out of memory), I increased the memory from 2 to 8 GB on docker desktop and voila my app worked!
",,
SciPy strange result,https://stackoverflow.com/questions/62479689,Butterworth filters look very strange as increasing order,"I'm trying to design a simple Butterworth bandpass filter in SciPy, and I'm running into some strange results. 

import scipy.signal as signal
import numpy as np
import matplotlib.pyplot as plt

def butter_bandpass(lowcut, highcut, fs, freqs,order=3, label=None):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    sos = signal.butter(order, [low, high], btype='band', output='sos')

    w, h = signal.sosfreqz(sos,worN=freqs,whole=True,fs=fs)

    return w,h

freqs = 650

for i in np.arange(1,10):
    w,h = butter_bandpass(0.01, 0.1, fs=1/0.68, freqs=freqs, order=i)
    plt.plot(h)


This is giving strange results, as can be seen from the image below (Butterworth filters from order 1-10). I thought the filter was supposed to become increasingly rectangular as the order increased? 



Does anyone know how to design a simple Butterworth filter in SciPy?
",0,413,"The frequency response of the Butterworth filter is not real-valued. When plotting the complex-valued response using plt.plot(), only the real component is shown. You should see a warning:

ComplexWarning: Casting complex values to real discards the imaginary part


To examine the filter's gain, plot the magnitude of the frequency response:

plt.plot(np.abs(h))


You will see an increasingly square response, as expected:


",,
SciPy strange result,https://stackoverflow.com/questions/54369255,Signal processing - Why a signal is not completely filtered out at my cutoff frequency?,,0,355,,,
SciPy strange result,https://stackoverflow.com/questions/53187760,scipy.linalg.sparse.eigsh returns negative and non-consistent eigenvalues for positive semi-definite matrix,,0,765,,,
SciPy strange result,https://stackoverflow.com/questions/40540229,Python Scipy Optimize Error &quot;ValueError: Lengths must match to compare&quot;,,0,2280,,,
SciPy strange result,https://stackoverflow.com/questions/17543432,"Matlab calling python, returning vector, str2num not working on returned vector",,0,494,,,
SciPy strange issue,https://stackoverflow.com/questions/39350557,Running scipy.integrate.ode in multiprocessing Pool results in huge performance hit,"I'm using python's scipy.integrate to simulate a 29-dimensional linear system of differential equations. Since I need to solve several problem instances, I thought I could speed it up by doing computations in parallel using multiprocessing.Pool. Since there is no shared data or synchronization necessary between threads (the problem is embarrassingly parallel), I thought this should obviously work. After I wrote the code to do this, however, I got very strange performance measurements:


Single-threaded, without jacobian: 20-30 ms per call
Single-threaded, with jacobian: 10-20 ms per call
Multi-threaded, without jacobian: 20-30 ms per call
Multi-threaded, with jacobian: 10-5000 ms per call


What's shocking is that what I thought should be the fastest setup, was actually the slowest, and the variability was two orders of magnitude. It's a deterministic computation; computers aren't supposed to work this way. What could possibly be causing this?

Effect seems system-dependent

I tried the same code on another computer and I didn't see this effect. 

Both machines were using Ubuntu 64 bit, Python 2.7.6, scipy version 0.18.0, and numpy version 1.8.2. I didn't see the variability with an Intel(R) Core(TM) i5-5300U CPU @ 2.30GHz processor. I did see the issue with an Intel(R) Core(TM) i7-2670QM CPU @ 2.20GHz.

Theories

One thought was that there might be a shared cache among processors, and by running it in parallel I can't fit two instances of the jacobian matrix in the cache, so they constantly battle each other for the cache slowing each other down compared with if they are run serially or without the jacobian. But it's not a million variable system. The jacobian is a 29x29 matrix, which takes up 6728 bytes. The level 1 cache on the processor is 4 x 32 KB, much larger. Are there any other shared resources between processors that might be to blame? How can we test this?

Another thing I noticed is that each python process seems to take several hundred percent of the CPU as it's running. This seems to mean that the code is already parallelized at some point (perhaps in the low-level library). This could mean that further parallelization wouldn't help, but I wouldn't expect such a dramatic slowdown.

Code

It would be good to try out the on more machines to see if (1) other people can experience the slowdown at all and (2) what are the common features of systems where the slowdown occurs. The code does 10 trials of two parallel computations using a multiprocessing pool of size two, printing out the time per scipy.ode.integrate call for each of the 10 trials. 

'odeint with multiprocessing variable execution time demonsrtation'

from numpy import dot as npdot
from numpy import add as npadd
from numpy import matrix as npmatrix
from scipy.integrate import ode
from multiprocessing import Pool
import time

def main():
    ""main function""

    pool = Pool(2) # try Pool(1)
    params = [0] * 2

    for trial in xrange(10):
        res = pool.map(run_one, params)
        print ""{}. times: {}ms, {}ms"".format(trial, int(1000 * res[0]), int(1000 * res[1]))

def run_one(_):
    ""perform one simulation""

    final_time = 2.0
    init_state = [0.1 if d &lt; 7 else 0.0 for d in xrange(29)]
    (a_matrix, b_vector) = get_dynamics()

    derivative = lambda dummy_t, state: npadd(npdot(a_matrix, state), b_vector)
    jacobian = lambda dummy_t, dummy_state: a_matrix
    #jacobian = None # try without the jacobian

    #print ""jacobian bytes:"", jacobian(0, 0).nbytes

    solver = ode(derivative, jacobian)
    solver.set_integrator('vode')
    solver.set_initial_value(init_state, 0)

    start = time.time()
    solver.integrate(final_time)
    dif = time.time() - start

    return dif

def get_dynamics():
    ""return a tuple (A, b), which are the system dynamics x' = Ax + b""

    return \
    (
        npmatrix([
        [0, 0, 0, 0.99857378006, 0.053384274244, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 1, -0.003182219341, 0.059524655342, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, -11.570495605469, -2.544637680054, -0.063602626324, 0.106780529022, -0.09491866827, 0.007107574493, -5.20817921341, -23.125876742495, -4.246931301528, -0.710743697134, -1.486697327603, -0.044548215175, 0.03436637817, 0.022990248611, 0.580153205353, 1.047552018229, 11.265023544535, 2.622275290571, 0.382949404795, 0.453076470454, 0.022651889536, 0.012533628369, 0.108399390974, -0.160139432044, -6.115359574845, -0.038972389136, 0, ],
        [0, 0, 0.439356565475, -1.998182296753, 0, 0.016651883721, 0.018462046981, -0.001187470742, -10.778778281386, 0.343052863546, -0.034949331535, -3.466737362551, 0.013415853489, -0.006501746896, -0.007248032248, -0.004835912875, -0.152495086764, 2.03915052839, -0.169614300211, -0.279125393264, -0.003678218266, -0.001679708185, 0.050812027754, 0.043273505033, -0.062305315646, 0.979162836629, 0.040401368402, 0.010697028656, 0, ],
        [0, 0, -2.040895462036, -0.458999156952, -0.73502779007, 0.019255757332, -0.00459562242, 0.002120360732, -1.06432932386, -3.659159530947, -0.493546966858, -0.059561101143, -1.953512259413, -0.010939065041, -0.000271004496, 0.050563886711, 1.58833954495, 0.219923768171, 1.821923233098, 2.69319056633, 0.068619628466, 0.086310028398, 0.002415425662, 0.000727041422, 0.640963888079, -0.023016712545, -1.069845542887, -0.596675149197, 0, ],
        [-32.103607177734, 0, -0.503355026245, 2.297859191895, 0, -0.021215811372, -0.02116791904, 0.01581159234, 12.45916782984, -0.353636907076, 0.064136531117, 4.035326800046, -0.272152744884, 0.000999589868, 0.002529691904, 0.111632959213, 2.736421830861, -2.354540136198, 0.175216915979, 0.86308171287, 0.004401276193, 0.004373406589, -0.059795009475, -0.051005479746, 0.609531777761, -1.1157829788, -0.026305051933, -0.033738880627, 0, ],
        [0.102161169052, 32.057830810547, -2.347217559814, -0.503611564636, 0.83494758606, 0.02122657001, -0.037879735231, 0.00035400386, -0.761479736492, -5.12933410588, -1.131382179292, -0.148788337148, 1.380741054924, -0.012931029503, 0.007645723855, 0.073796656681, 1.361745395486, 0.150700793731, 2.452437244444, -1.44883919298, 0.076516270282, 0.087122640348, 0.004623192159, 0.002635233443, -0.079401941141, -0.031023369979, -1.225533436977, 0.657926151362, 0, ],
        [-1.910972595215, 1.713829040527, -0.004005432129, -0.057411193848, 0, 0.013989634812, -0.000906753354, -0.290513515472, -2.060635522957, -0.774845915178, -0.471751979387, -1.213891560083, 5.030515136324, 0.126407660877, 0.113188603433, -2.078420624662, -50.18523312358, 0.340665548784, 0.375863242926, -10.641168797333, -0.003634153255, -0.047962774317, 0.030509705209, 0.027584169642, -10.542357589006, -0.126840767097, -0.391839285172, 0.420788121692, 0, ],
        [0.126296110212, -0.002898250629, -0.319316070797, 0.785201711657, 0.001772374259, 0.00000584372, 0.000005233812, -0.000097899495, -0.072611454126, 0.001666291957, 0.195701043078, 0.517339177294, 0.05236528267, -0.000003359731, -0.000003009077, 0.000056285381, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [-0.018114066432, 0.077615035084, 0.710897211118, 2.454275059389, -0.012792968774, 0.000040510624, 0.000036282541, -0.000678672106, 0.010414324729, -0.044623231468, 0.564308412696, -1.507321670112, 0.066879720068, -0.000023290783, -0.00002085993, 0.000390189123, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [-0.019957254425, 0.007108972111, 122.639137999354, 1.791704310155, 0.138329792976, 0.000000726169, 0.000000650379, -0.000012165459, -8.481152717711, -37.713895394132, -93.658221074435, -4.801972165378, -2.567389718833, 0.034138340146, -0.038880106034, 0.044603217363, 0.946016722396, 1.708172458034, 18.369114490772, 4.275967542224, 0.624449778826, 0.738801257357, 0.036936909247, 0.020437742859, 0.176759579388, -0.261128576436, -9.971904607075, -0.063549647738, 0, ],
        [0.007852964982, 0.003925745426, 0.287856349997, 58.053471054491, 0.030698062827, -0.000006837601, -0.000006123962, 0.000114549925, -17.580742026275, 0.55713614874, 0.205946900184, -43.230778067404, 0.004227082975, 0.006053854501, 0.006646690253, -0.009138926083, -0.248663457912, 3.325105302428, -0.276578605231, -0.455150962257, -0.005997822569, -0.002738986905, 0.082855748293, 0.070563187482, -0.101597078067, 1.596654829885, 0.065879787896, 0.017442923517, 0, ],
        [0.011497315687, -0.012583019909, 13.848373855148, 22.28881517216, 0.042287331657, 0.000197558695, 0.000176939544, -0.003309689199, -1.742140233901, -5.959510415282, -11.333020298294, -14.216479234895, -3.944800806497, 0.001304578929, -0.005139259078, 0.08647432259, 2.589998222025, 0.358614863147, 2.970887395829, 4.39160430183, 0.111893402319, 0.140739944934, 0.003938671797, 0.001185537435, 1.045176603318, -0.037531801533, -1.744525005833, -0.972957942438, 0, ],
        [-16.939142002537, 0.618053512295, 107.92089190414, 204.524147386814, 0.204407545189, 0.004742101706, 0.004247169746, -0.079444150933, -2.048456967261, -0.931989524708, -66.540858220883, -116.470289129818, -0.561301215495, -0.022312225275, -0.019484747345, 0.243518778973, 4.462098610572, -3.839389874682, 0.285714413078, 1.40736916669, 0.007176864388, 0.007131419303, -0.097503691021, -0.083171197416, 0.993922379938, -1.819432085819, -0.042893874898, -0.055015718216, 0, ],
        [-0.542809857455, 7.081822285872, -135.012404429101, 460.929268260027, 0.036498617908, 0.006937238413, 0.006213200589, -0.116219147061, -0.827454697348, 19.622217613195, 78.553728334274, -283.23862765888, 3.065444785639, -0.003847616297, -0.028984525722, 0.187507140282, 2.220506417769, 0.245737625222, 3.99902408961, -2.362524402134, 0.124769923797, 0.142065016461, 0.007538727793, 0.004297097528, -0.129475392736, -0.050587718062, -1.998394759416, 1.072835822585, 0, ],
        [-1.286456393795, 0.142279456389, -1.265748910581, 65.74306027738, -1.320702989799, -0.061855995532, -0.055400100872, 1.036269854556, -4.531489334771, 0.368539277612, 0.002487097952, -42.326462719738, 8.96223401238, 0.255676968878, 0.215513465742, -4.275436802385, -81.833676543035, 0.555500345288, 0.612894852362, -17.351836610113, -0.005925968725, -0.078209662789, 0.049750119549, 0.044979645917, -17.190711833803, -0.206830688253, -0.638945907467, 0.686150823668, 0, ],
        [0, 0, 0, 0, 0, -0.009702263896, -0.008689641059, 0.162541456323, 0, 0, 0, 0, 0, 0, 0, 0, -0.012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [-8.153162937544, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.005, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, -3.261265175018, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.005, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 0, 0.17441246156, -3.261265175018, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.01, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, -3.261265175018, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -8.5, -18, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 0, -8.153162937544, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -8.5, -18, 0, 0, 0, 0, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 0.699960862226, 0.262038222227, 0.159589891262, 0.41155156501, -1.701619176699, -0.0427567124, -0.038285155304, 0.703045934017, 16.975651534025, -0.115788018654, -0.127109026104, 3.599544290134, 0.001229743857, 0.016223661959, -0.01033400498, -0.00934235613, -6.433934989563, 0.042639567847, 0.132540852847, -0.142338323726, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, -37.001496211974, 0.783588795613, -0.183854784348, -11.869599790688, -0.106084318011, -0.026306590251, -0.027118088888, 0.036744952758, 0.76460150301, 7.002366574508, -0.390318898363, -0.642631203146, -0.005701671024, 0.003522251111, 0.173867535377, 0.147911422248, 0.056092715216, -6.641979472328, 0.039602243105, 0.026181724138, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 1.991401999957, 13.760045912368, 2.53041689113, 0.082528789604, 0.728264862053, 0.023902766734, -0.022896554363, 0.015327568208, 0.370476566397, -0.412566245022, -6.70094564846, -1.327038338854, -0.227019235965, -0.267482033427, -0.008650986307, -0.003394359441, 0.098792645471, 0.197714179668, -6.369398456151, -0.011976840769, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 1.965859332057, -3.743127938662, -1.962645156793, 0.018929412474, 11.145046656101, -0.03600197464, -0.001222148117, 0.602488409354, 11.639787952728, -0.407672972316, 1.507740702165, -12.799953897143, 0.005393102236, -0.014208764492, -0.000915158115, -0.000640326416, -0.03653528842, 0.012458973237, -0.083125038259, -5.472831842357, 0, ],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],
        ])
    , 
        npmatrix([1.0 if d == 28 else 0.0 for d in xrange(29)])
    )



if __name__ == ""__main__"":
    main()


Example Output

Here's an example of the output that demonstrates the problem (each run is slightly different). Notice the large variability in execution times (over two orders of magnitude!). Again, this all goes away if I either use a pool of size 1 (or run the code without a pool), or if I don't use an explicit jacobian in the call to integrate.


  
  times: 5847ms, 5760ms
  times: 4177ms, 3991ms
  times: 229ms, 36ms
  times: 1317ms, 1544ms
  times: 87ms, 100ms
  times: 113ms, 102ms
  times: 4747ms, 5077ms
  times: 597ms, 48ms
  times: 9ms, 49ms
  times: 135ms, 109ms
  

",14,2467,"Based on the variability of the execution times you posted for the machine showing the problem, I wonder what else that computer is doing at the time you are running your test.  Here are times I saw when I ran your code on an AWS r3.large server (2 cores, 15 GB of RAM) that normally runs interactive R sessions but is currently mostly idle:


  
  times: 11ms, 11ms
  times: 9ms, 9ms
  times: 9ms, 9ms
  times: 9ms, 9ms
  times: 10ms, 10ms
  times: 10ms, 10ms
  times: 10ms, 10ms
  times: 11ms, 10ms
  times: 11ms, 10ms
  times: 9ms, 9ms
  


Is it possible your machine is swapping and you do not know it?  vmstat 5 will give you a lot of information about swap in and outs, but not about cache evictions.

Intel makes some very nice tools for monitoring--two at a time--thousands of different types of operations and errors going on in a processor--including L2 cache evictions--but they are a bit of a firehose: there is information generated every microsecond--or more frequently--and you have to decide what you are going to monitor and how often you want an interrupt to deliver the numbers into your software.  Likely it will take many runs just to narrow down the stats you want to track and you still have to filter out the noise generated by the operating system and everything else running at the time.  It is a time consuming process, but if you follow it to the end and run many different tests you will come to understand what is going on.  

But is this--shared cache resources in a processor--really your question?  It seems more like you just want to figure out why you have variable run times on one machine and, second, why multi-threaded is slower on both machines than single threaded.  Do I have it right?  If not I will edit my answer and we can talk about processor cache, cache snooping and cache coherency.

So, as to the variability on the i7-2670QM CPU machine, I would start with htop, vmstat 5 and iostat 5 to see if the machine is doing something you didn't realize.  That much variability says the executable is getting stalled because the processor is busy doing something else: going off to the network and not finding a share it expects, unable to connect to a DNS server, getting kerbios failures: it could be a lot of things including hardware failures from a hard disk that is being continually reset.  Oh, and move your program to /dev/shm and cd there before you start it.  That won't help you if there are Python libraries in a bad place on a disk, but at least you won't have issues with your local directory.  Report back what you find and we can make further suggestions.

Your second question as I see it, which is perhaps where you started, is why is your program slower when run multi-threaded than single-threaded.  This is a big subject that will come a lot more in focus if we can see how you multi-threaded it.  But even before we do you have to realize that there are several things that can cause a multi-threaded program to run slower than a single-threaded program, and it can have as much to do with the support infrastructure around your program--libraries and operating system calls you make--as your program.  Just because you do not need mutexes does not mean the libraries and operating system do not need them when they are being called from a multi-threaded application.  Locking a mutex is an expensive operation, especially as different threads are rotated between different cores.  

On top of that, since the vode is not re-entrant, if you called it from multiple threads it is possible that it is having trouble finding convergence and having to recalculate the same values many times before it ""gets lucky"" and has enough processor time to complete an iteration before it is swapped out and intermediate results are overwritten.  Give us the code you are using for your multi-threaded runs and I will add to this answer.
","This is intended as a formatted comment regarding the mathematical background raised in a comment by @Dietrich. As it doesn't address the programming question, I intend to delete this answer in a little while until the bounty blows over.

As @Dietrich noted, you can solve your ODE exactly, since if

x' = A*x,


then the exact solution is

x(t) = exp(A*t)*x0


Already I'd say that an exact solution is always superior than a numerical approximation, but this can indeed be faster than a numerical integration. As you noted in a comment, you're worried about efficiency. So don't compute the matrix exponential for each t: compute the eigensystem of A only once:

A*v_i = L_i*v_i


then

x(t) = sum_i c_i*v_i*exp(L_i*t),


and the coefficients c_i can be determined from the linear equations

x0 = sum_i c_i*v_i.


Now, having an inhomogeneous term doesn't change much, as long as your matrix is not singular:

x' = A*x + b
(x - A^(-1)*b)' = A*(x - A^(-1)*b)


so we can solve the homogeneous equation for y = x - A^(-1)*b and in a final step recover x = y + A^(-1)*b.

This all works nicely while the matrix is regular, but in your specific case it's singular. But it turns out that this is due to your final dimension:

&gt;&gt;&gt; np.linalg.det(A)
0.0
&gt;&gt;&gt; np.linalg.det(A[:-1,:-1])
1920987.0461154305


And also note that the final row of A is all zeros (this is the reason for the singularity of A). So the last dimension of x is constant (or changes linearly due to b).

I suggest eliminating this variable, rewriting your equation for the rest of the variables, and solving the non-singular inhomogeneous linear system of ODEs using the above procedure, exactly. It should be faster and precise.



The following will be a bit speculative, see also the caveat at the end.

In case of user-input A and b, things might get trickier. Finding a zero row/column in your matrix would be easy, but A can be singular even though none of its rows/columns are fully zero. I'm not an expert in the subject, but I think your best bet is using something akin to principal component analysis: transforming your system of equations according to the eigensystem of A. My following thoughts will still assume that A is diagonalizable, but mostly because I'm unfamiliar with singular value decomposition. In realistic cases I'd expect your matrices to be diagonalizable, even if singular.

So I'll assume that the matrix A can be decomposed as

A = V * D * V^(-1),


where D is a diagonal matrix containing the eigenvalues of A, and columns of V are the eigenvectors of A corresponding to each respective eigenvalue. The very same decomposition can be obtained in numpy using

DD,V = np.linalg.eig(A)
D = np.asmatrix(np.diag(DD))


I usually prefer using ndarrays instead of matrices, but this way V*D*np.linalg.inv(V) would really correspond to the matrix product of the three matrices, rather than calling np.dot twice.

Now, rewrite your equation again:

x' = A*x + b
x' = V*D*V^(-1)*x + b
V^(-1)*x' = D*V^(-1)*x + V^(-1)*b


By defining the auxiliary variables

X = V^(-1)*x
B = V^(-1)*b


we obtain

X' = D*X + B


i.e. the usual inhomogeneous form, but now D is a diagonal matrix containing the eigenvalues of A in the diagonal.

Since A is singular, some of the eigenvalues are zero. Look for zero elements in D (well, you can do that already with DD from eig()), and you'll know that they behave trivially during time-evolution. The remaining variables behave well, although at this point we see that the equations for X are decoupled due to D being diagonal, so you could integrate each independently and analytically. For this you need to first go from your initial condition x0 to X0 = np.linalg.inv(V)*x0, then after solving the equations, back to x = V*X.

Caveat: as I said, I'm not an expert in this subject. I can easily imagine that the inversions involved in the diagonalization can be a numerical issue in practical applications. So I'd first test if the matrix is singular, and only carry on with this procedure if it is (or nearly is). It's possible that the above carries a lot of error, in which case numerical integration might be better (I really can't tell).
","on my compiled linux kernel :


times: 8ms, 7ms
times: 5ms, 4ms
times: 4ms, 4ms
times: 8ms, 8ms
times: 4ms, 4ms
times: 5ms, 4ms
times: 4ms, 8ms
times: 8ms, 8ms
times: 8ms, 8ms
times: 4ms, 5ms


Intel(R) Core(TM) i5-4300U CPU @ 1.90GHz

be sure your processor runs at fixed speed, noswap.
/tmp is mounted in RAM.
"
SciPy strange issue,https://stackoverflow.com/questions/28056404,MATLAB and SciPy give different results for &#39;buttord&#39; function,"I'm trying to design an analog Butterworth filter using the buttord function (actually, I'm porting a program where this function is called from MATLAB to Python). 

My parameters are:

Passband frequency (Fp) = 10 Hz, giving Wp = 2*pi*10 Hz

Stopband frequency (Fs) = 100 Hz, giving Ws = 2*pi*100 Hz

The passband and stopband losses/attenuations (Rp, Rs) are 3 and 80 dB respectively.

In MATLAB I use this code:

Wp = 2 * pi * 10
Ws = 2 * pi * 100
Rp = 3
Rs = 80
[N, Wn] = buttord(Wp, Ws, Rp, Rs, 's')


that gives me N = 5, Wn = 99.581776302.

In SciPy I tried to do the same:

from numpy import pi
from scipy import signal
Wp = 2 * pi * 10
Ws = 2 * pi * 100
Rp = 3
Rs = 80
(N, Wn) = signal.buttord(Wp, Ws, Rp, Rs, analog=True)


and I get N = 5 and Wn = 62.861698649592753. Wn is different than the value that MATLAB gives, and is strangely close to Wp. What is wrong here?

Digging into SciPy's sources and issues, I found this pull request which might explain things: turns out MATLAB and SciPy have different design goals (MATLAB tries to optimize for matching the stopband frequency and SciPy tries to optimize for matching the passband frequency).

I'm using MATLAB R2013a, Python 3.4.2 and SciPy 0.15.0 if that matters.
",4,928,"(I also posted the following on the scipy mailing list.)

When you design a Butterworth filter with buttord, there aren't enough
degrees of freedom to meet all the design constraints exactly.  So there
is a choice of which end of the transition region hits the constraints
and which end is ""over-designed"".  A change made in scipy 0.14.0 switched that choice from the stop-band edge to the pass-band edge.

A picture will make it clear.  The script below generates the following plot.  (I changed Rp from 3 to 1.5.  -3 dB coincides with the gain at Wn, that's why your Wn was the same as Wp.)  The filters generated using either the old or new convention both satisfy the design constraints.  With the new convention, the response just bumps against the constraint at the end of the pass-band.



import numpy as np
from scipy.signal import buttord, butter, freqs
import matplotlib.pyplot as plt


# Design results for:
Wp = 2*np.pi*10
Ws = 2*np.pi*100
Rp = 1.5      # instead of 3
Rs = 80

n_old = 5
wn_old = 99.581776302787929

n_new, wn_new = buttord(Wp, Ws, Rp, Rs, analog=True)

b_old, a_old = butter(n_old, wn_old, analog=True)
w_old, h_old = freqs(b_old, a_old)

b_new, a_new = butter(n_new, wn_new, analog=True)
w_new, h_new = freqs(b_new, a_new)


db_old = 20*np.log10(np.abs(h_old))
db_new = 20*np.log10(np.abs(h_new))

plt.semilogx(w_old, db_old, 'b--', label='old')
plt.axvline(wn_old, color='b', alpha=0.25)
plt.semilogx(w_new, db_new, 'g', label='new')
plt.axvline(wn_new, color='g', alpha=0.25)

plt.axhline(-3, color='k', ls=':', alpha=0.5, label='-3 dB')

plt.xlim(40, 1000)
plt.ylim(-100, 5)

xbounds = plt.xlim()
ybounds = plt.ylim()
rect = plt.Rectangle((Wp, ybounds[0]), Ws - Wp, ybounds[1] - ybounds[0],
                     facecolor=""#000000"", edgecolor='none', alpha=0.1, hatch='//')
plt.gca().add_patch(rect)
rect = plt.Rectangle((xbounds[0], -Rp), Wp - xbounds[0], 2*Rp,
                     facecolor=""#FF0000"", edgecolor='none', alpha=0.25)
plt.gca().add_patch(rect)
rect = plt.Rectangle((Ws, ybounds[0]), xbounds[1] - Ws, -Rs - ybounds[0],
                     facecolor=""#FF0000"", edgecolor='none', alpha=0.25)
plt.gca().add_patch(rect)

plt.annotate(""Pass"", (0.5*(xbounds[0] + Wp), Rp+0.5), ha='center')
plt.annotate(""Stop"", (0.5*(Ws + xbounds[1]), -Rs+0.5), ha='center')
plt.annotate(""Don't Care"", (0.1*(8*Wp + 2*Ws), -Rs+10), ha='center')

plt.legend(loc='best')
plt.xlabel('Frequency [rad/s]')
plt.ylabel('Gain [dB]')
plt.show()

",,
SciPy strange issue,https://stackoverflow.com/questions/20600527,Python deadlock related to packaging and mayavi?,"I have a curious problem that I hope someone can shed some light on.

I have a complex piece of code, that started as a directory full of scripts, that I decided to rework into a package. This code change appears to have been the trigger for some strange deadlocks to appear.

Below is an attempt at canonical reproduction of the problem; which fails, in the sense that this code runs as expected. Actually reproducing the issue may require a lot of code; but I cannot for the life of my imagine what is different for the offending code snippet in-context.

import numpy as np
from scipy.sparse import csr_matrix
from threading import Thread

def dummy():
    print 'this is printed'
    I = np.eye(3)
    print 'all is still fine'
    csr_matrix(I)
    print 'this is never printed; csr_matrix appears to be a trigger for deadlock'
    print np.ones(4)
    print 'same problem; somehow, printing ndarrays is no longer cool either'

thr = Thread(target=dummy)
thr.start()


Perhaps this terse comment in the docs is related? I am not sure I fully appreciate what is being said here

http://docs.python.org/2/library/threading#importing-in-threaded-code

Firstly, other than in the main module, an import should not have the side effect of spawning a new thread and then waiting for that thread in any way. Failing to abide by this restriction can lead to a deadlock if the spawned thread directly or indirectly attempts to import a module.

Some context: I am using python 2.7, numpy 1.8, where I try to spawn this new thread from within a mayavi/traitsui thread (which I dont see why it should be relevant, and which worked fine before the package structure, but ok). Also, there is a boatload of numpy/scipy code in my spawned thread that executes perfectly fine; its just printing ndarrays and creating sparse matrices which so far have proven to be triggers for deadlock.

I am suspecting some funky interaction with mayavi, since closing the mayavi window causes all deadlocked threads to start running again. Perhaps these specific statement trigger the python thread to yield back to the mayavi thread, but they somehow fail to gain focus again?

Any hints that lead to further narrowing down on this mystery are much appreciated!
",2,209,"From your comments, it looks like you start up the UI event loop at the top level of one of your subsidiary modules. This is not a good idea because it causes exactly the same problems that the documentation alludes to. import foo should never start a UI event loop. The problem is that the main thread grabs the import lock to handle the import of the module. This module starts up the UI event loop before completing the import. This is essentially the same situation as the waiting for the other threads to finish; you are waiting for the UI loop to finish. If your UI starts up other threads, the code that is running in the other threads will not be able to import anything (both csr_matrix() and ndarray.__repr__() import other modules) because the main thread is still holding onto the import lock.
",,
SciPy strange issue,https://stackoverflow.com/questions/6968219,scipy on os x lion,"I'm am trying to get my python/numpy/scipy working environment running on OS X Lion.

I've already managed to compile numpy and scipy from their latest sourcecode versions with the usual

python setup.py build
python setup.py install


I do run the Python 2.7.2 version downloaded from python.org:

tobi-mbp:~ tobi$ python
Python 2.7.2 (v2.7.2:8527427914a2, Jun 11 2011, 15:22:34) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt;


If i than import numpy it shows:

&gt;&gt;&gt; import numpy
&gt;&gt;&gt; numpy.__version__
'1.5.1'
&gt;&gt;&gt; numpy.__file__
'/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/__init__.pyc'
&gt;&gt;&gt; 


the system instalation of numpy. Whereas

&gt;&gt;&gt; import scipy
&gt;&gt;&gt; scipy.__version__
'0.10.0.dev'
&gt;&gt;&gt; scipy.__file__
'/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/__init__.pyc'
&gt;&gt;&gt;


gives the installed version of scipy. That means the scipy.test() fails and actually gets stuck.

I did try pip and easy_install as well but the fail to compile scipy and with numpy the same strange bahavior occours. Same for the precompiled binarys provided on sourcefourge.

As this seems to be an issue with PYTHONPATH:

&gt;&gt;&gt; import sys, os, pprint
&gt;&gt;&gt; pprint.pprint(sys.path)
['',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/distribute-0.6.19-py2.7.egg',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pip-1.0.2-py2.7.egg',
 '/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python',
 '/Library/Python/2.7/site-packages/nose-1.1.2-py2.7.egg',
 '/Library/Python/2.7/site-packages/pip-1.0.2-py2.7.egg',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python27.zip',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-darwin',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac/lib-scriptpackages',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-old',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload',
 '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages',
 '/Library/Python/2.7/site-packages']
&gt;&gt;&gt; pprint.pprint([p for p in os.environ.items() if p[0].startswith('PYTHON')])
[]
&gt;&gt;&gt; 


And a list of the contents of site-packages

tobi-mbp:site-packages tobi$ ls -l
total 752
-rw-rw-r--   1 root  admin     119 12 Jun 00:25 README
drwxr-xr-x  10 root  admin     340  6 Aug 19:47 distribute-0.6.19-py2.7.egg
-rw-r--r--   1 tobi  admin     237  6 Aug 19:48 easy-install.pth
drwxrwxr-x  48 tobi  admin    1632  6 Aug 18:19 numpy
drwxrwxr-x   6 tobi  admin     204 20 Jul 20:44 numpy-1.6.1-py2.7.egg-info
-rw-r--r--   1 root  admin    1670  6 Aug 18:19 numpy-2.0.0.dev_26aa3cf-py2.7.egg-info
drwxr-xr-x   4 root  admin     136  6 Aug 19:48 pip-1.0.2-py2.7.egg
drwxr-xr-x   8 tobi  admin     272  6 Aug 19:48 readline-6.2.0-py2.7.egg-info
-rwxr-xr-x   1 tobi  admin  357048  6 Aug 19:48 readline.so
drwxrwxr-x  42 tobi  admin    1428  6 Aug 18:21 scipy
-rw-r--r--   1 root  admin    1768  6 Aug 18:21 scipy-0.10.0.dev-py2.7.egg-info
drwxrwxr-x   6 tobi  admin     204 27 Feb 14:00 scipy-0.9.0-py2.7.egg-info
-rw-r--r--   1 tobi  admin     144  6 Aug 19:47 setuptools-0.6c11-py2.7.egg-info
-rw-r--r--   1 tobi  admin      30  6 Aug 19:47 setuptools.pth

tobi-mbp:site-packages tobi$ more easy-install.pth
import sys; sys.__plen = len(sys.path)
./distribute-0.6.19-py2.7.egg
./pip-1.0.2-py2.7.egg
import sys; new=sys.path[sys.__plen:]; del sys.path[sys.__plen:]; p=getattr(sys,'__egginsert',0); sys.path[p:p]=new; sys.__egginsert = p+len(new)

tobi-mbp:site-packages tobi$ more setuptools.pth 
./distribute-0.6.19-py2.7.egg


I do see the System Extras folder in the path, now the question is why and how I can get rid of it.

thanks for your help

cheers Tobi
",2,2260,"There appears to be a path issue somewhere. Perhaps you are setting PYTHONPATH?  When you are running the python.org 2.7.2, the Apple-supplied Python 2.7.1's Extras directory should not be on sys.path.  To help figure out what's going on, launch the Python 2.7.2 and examine the following:

&gt;&gt;&gt; import sys, os, pprint
&gt;&gt;&gt; pprint.pprint(sys.path)
&gt;&gt;&gt; pprint.pprint([p for p in os.environ.items() if p[0].startswith('PYTHON')])


Also the contents of the site-packages directory:

$ cd /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/
$ ls -l
$ more easy-install.pth


and the contents of any other .pth files there.  

If it is still not clear what is going on, edit your answer to show the results of the above.

UPDATE:

Thanks for providing the requested additional info.  It confirms that sys.path does include the Extras directory from the Apple-supplied system Python 2.7.  There is no obvious reason why that should be happening.  Without more information, I can only speculate.  As unlikely as it may seem, my best guess at this point is that you accidentally copied some files from the one Python to the other.  The Apple-supplied Python has a patch in site.py to add the Extras directory:

$ cd /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7
$ ls -l site.py
-rw-r--r--  1 root  wheel  19929 Jun 16 17:03 site.py
$ grep Extras site.py
            sitepackages.append(os.path.join(prefix, ""Extras"", ""lib"", ""python""))


You should not see that in the python.org Python:

$ cd /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7
$ ls -l site.py
-rw-rw-r--  1 root  admin  20288 Jun 11 15:25 site.py
$ grep Extras site.py
$


You could also do a more complete recursive search of all of the directories on sys.path for pth files but that seems unlikely to find anything.

You also appear to have installed two versions each of numpy and scipy in your site-packages, one the latest released version and the other cutting-edge version.   I don't know whether you took steps to clear out the numpy and scipy directories between installs or whether that can cause problems but it might.

At this point, without more information, I would recommend that you completely delete the 2.7.2 installation and start over:

$ sudo rm -rf /Library/Frameworks/Python.framework/Versions/2.7


You might consider using another approach like the binary numpy/scipy installers or install using MacPorts.  A better source of advice might be found on one of the project's mailing lists.  Good luck!
",,
SciPy strange issue,https://stackoverflow.com/questions/41049834,Global Modules are not available in VirtualEnv,"I'm using WinPython to work with libraries such as Scipy. I can not install those inside my virtual environment because blas/lapack is not available. The solution was to use the modules (theano/scipy/numpy) which are already installed on my system inside my virtual environment. 
Here is what I tried:

open console (cmd)

python
&gt;&gt;&gt; import numpy


this works, now creating the virtual environment

set VIRTUALENV_PYTHON=C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\python.exe
set VIRTUALENV_EXTRA_SEARCH_DIR=""C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\libs C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\Lib C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\DLLs C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\ C:\WinPython-64bit-3.5.1.1\python-3.5.1.amd64\Scripts""
virtualenv --system-site-packages -v --always-copy venv


or simply:

virtualenv --system-site-packages venv


but no matter what I try I can't import numpy when entering the venv.

venv\Scripts\activate
python
&gt;&gt;&gt; import numpy
ImportError: No module named 'numpy'


Strange. So I compared the sys.path inside the venv:

&gt;&gt;&gt; sys.path
['', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\Scripts', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\DLLs', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\Lib', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\libs', 'C:\\Users\\user\\venv\\Scripts\\python35.zip', 'C:\\Users\\user\\venv\\DLLs', 'C:\\Users\\user\\venv\\lib', 'C:\\Users\\user\\venv\\Scripts', 'C:\\Users\\user\\venv', 'C:\\Users\\user\\venv\\lib\\site-pack
ages']


with the original one (outside):

&gt;&gt;&gt; sys.path
['', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\Scripts', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\DLLs', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\Lib', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\libs', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\python35.zip', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\lib\\site-packages', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\lib\\site-packages\\win32', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\lib\\site-packages\\win32\\lib', 'C:\\WinPython-64bit-3.5.1.1\\python-3.5.1.amd64\\lib\\site-packages\\Pythonwin']


as you can see, not all the entries got copied over. Why is that and how can I fix it?

And yes I read virtualenv --system-site-packages not using system site packages and deleted my PYTHONPATH variable but the issue remains.
",1,957,"Figured it out eventually. I did not restart after removing my PYTHONPATH variable. So if you have the same issue:


Delete your PYTHONPATH env. variable. 
Reboot!

",,
SciPy strange issue,https://stackoverflow.com/questions/18705141,TypeError when calling scipy griddata,"I'm having an issue with scipy.interpolate's griddata function. I developed a quick interpolator using griddata in a sandbox, then once I had the interpolator how I liked it, a copied the function into a larger model I had developed. Here is the function:

def windGrid(*sensors):
    """"""Creates grids of wind speed components""""""
    xb = [0, 0, num_x, num_x]
    yb = [0, num_y, num_y, 0]

    xs = [s.lng for s in sensors]
    ys = [s.lat for s in sensors]
    us = [s.u for s in sensors]
    vs = [s.v for s in sensors]

    ub, vb = boundaryWeighting(*sensors)

    x = xb+xs
    y = yb+ys
    u = ub+us
    v = vb+vs

    x_grid, y_grid = np.mgrid[0:num_x, 0:num_y]

    zx = griddata((x, y), u, (x_grid, y_grid))
    zy = griddata((x, y), v, (x_grid, y_grid))

   return zx, zy


The boundaryWeighting() function simply returns wind speed components of the corners of the grid so that interpolations can be made within. Now here is the strange part... When I call griddata in the sandbox (i.e. without other independent functions around it), it works fine. However, when called in the python file to which it was copied, it returns this error:

TypeError: griddata() takes at least 5 arguments (3 given)


According to the Scipy documentation, griddata takes 5 arguments, but the last two are optional. I tried inserting the optional arguments (i.e. method and fill_val), but then I got this error:

TypeError: griddata() got an unexpected keyword argument 'method'


So it seems that the python interpreter is referencing a different version/function of griddata in the different python files.

Here is why this doesn't make sense:
1. Both files are in the same directory, and are using the same interpreter
2. Both files reference the same Scipy version, 0.12.0

I've checked all my variable/function assignments and there are no overlaps. Any clues as to why this would be happening?

Thanks
",1,2393,"Do you have something like from matplotlib.pylab import * in one of your files?  You might be getting a namespace collision with the matplotlib version of griddata.
",,
SciPy strange issue,https://stackoverflow.com/questions/77434232,Scipy sosfilt ValueError ndarray is not C-contiguous,"sosfilt from the scipy=1.9.3 library is giving me a strange ValueError when inputting a numpy=1.23.4 array.
MWE:
import numpy as np
from scipy.signal import sosfilt

fs=48000

rng = np.random.default_rng()
signal = rng.normal(size=(fs))

b_0k = [1.01589602025559, 0.958943219304445, 0.961371976333197,
        2.22580350360974, 0.471735128494163, 0.115267139824401,
        0.988029297230954, 1.95223768730136]
b_1k = [-1.92529887777608, -1.80608801184949, -1.76363215433825,
        -1.43465048479216, -0.366091796830044, 0.0, -1.91243380293387,
        0.162319983017519]
b_2k = [0.922118060364679, 0.876438777856084, 0.821787991845146,
        -0.498204282194628, 0.244144703885020, -0.115267139824401,
        0.926131550180785, -0.667994113035186]
a_0k = np.ones(len(b_0k))
a_1k = [-1.92529887777608, -1.80608801184949, -1.76363215433825,
        -1.43465048479216, -0.366091796830044, -1.79600256669201,
        -1.91243380293387, 0.162319983017519]
a_2k = [0.938014080620272, 0.835381997160530, 0.783159968178343,
        0.727599221415107, -0.284120167620817, 0.805837815618546,
        0.914160847411739, 0.284243574266175]

sos = np.array([b_0k, b_1k, b_2k, a_0k, a_1k, a_2k]).T

signalFiltered = sosfilt(sos, signal, axis=0)

gives me
signalFiltered = sosfilt(sos, signal, axis=0)
Traceback (most recent call last):
Cell In[102], line 1
signalFiltered = sosfilt(sos, signal, axis=0)
File C:\ProgramData\Miniconda3\envs\devenv\lib\site-packages\scipy\signal_signaltools.py:4247 in sosfilt
_sosfilt(sos, x, zi)
File _sosfilt.pyx:81 in scipy.signal._sosfilt._sosfilt
File stringsource:660 in View.MemoryView.memoryview_cwrapper
File stringsource:350 in View.MemoryView.memoryview.cinit
ValueError: ndarray is not C-contiguous
However
signal.flags

shows
C_CONTIGUOUS : True
F_CONTIGUOUS : True
OWNDATA : True
WRITEABLE : True
ALIGNED : True
WRITEBACKIFCOPY : False
I have also tried this on other signals imported from wav files, with the same error. Having looked around, there does not seem to be any known issue or other examples of this error using this function (although there are some machine learning package bugs discussed elsewhere)
What is the problem with Scipy?
",0,59,"It's the transpose that's giving problems, making it F contiguous. Add a .copy().
Look at the array flags, before and after transpose and copy.
transpose works by reversing the strides and shape. It's a view, a cheap way of doing the job, but messes with the  continuity.
",,
SciPy strange issue,https://stackoverflow.com/questions/76218384,solve_ivp gets stuck and I can&#39;t figure out why,"I am using SciPy's solve_ivp to simulate a model consisting of 5 equations:
Upsilon = 0.305
Alpha = 0.00000055
r = 0.176
b = 0.0000000000005

def model(t, y0, Beta, Eta, Rmin, p1, p2, p3, A, a, Xi, Epsilon, Lambda, Theta, Mu, Delta, Gamma):

    CD, CT, CM, CE, T, TOTAL = y0

    def k_t(t):
        return Rmin + (p1 / (1 + (p2 * t) ** p3))

    def F_t(T):
        return T / (A + T)

    def f_t(CF, T):
        return (CF / T) / (Upsilon + ((a + CF) / T))

    dCD_dt = -(Beta + Eta) * CD
    dCT_dt = (Eta * CD) + (k_t(t) * F_t(T) * CT) - ((Xi + Epsilon + Lambda) * CT) + (Theta * T * CM) - (Alpha * T * CT)
    dCM_dt = (Epsilon * CT) - (Theta * T * CM) - (Mu * CM)
    dCE_dt = (Lambda * CT) - (Delta * CE)
    dT_dt = ((r * T) * (1 - (b * T))) - (Gamma * f_t((CD + CT), T) * T)

    TOTAL = dCD_dt + dCT_dt + dCM_dt + dCE_dt

    print(t)
    return [dCD_dt, dCT_dt, dCM_dt, dCE_dt, dT_dt, TOTAL]


I am performing sensitivity analysis, so I am running the model about 37,000 times with variations in the input parameters. When I run the model, I do so with the following code:
#p_values is a numpy array

final = 100
t_range = (0, final)
t_eval = np.arange(0, final, 0.1)

# run model for each parameter combination
for i in range(len(p_values)):
    CD = p_values[i][15]
    CT, CM, CE = 0, 0, 0
    T = 10 ** 7
    TOTAL = CD
    y0 = [CD, CT, CM, CE, T, TOTAL]

    solution = solve_ivp(model, t_range, y0, t_eval=t_eval, args=(p_values[i][0], p_values[i][1], p_values[i][2], p_values[i][3], p_values[i][4], p_values[i][5], p_values[i][6], p_values[i][7], p_values[i][8], p_values[i][9], p_values[i][10], p_values[i][11], p_values[i][12], p_values[i][13], p_values[i][14])).y

But at some point while it's running it will just... get stuck. Sometimes it happens early (within the first thousand solutions) and sometimes it can get to 10,000 solutions or more before it gets stuck.
To try and solve the problem, I started printing t for each loop of the model, and I noticed something very strange - when it gets stuck, it seems that t stays stagnant, or at least grows extremely slowly. This doesn't make sense to me, as I assume it should always increase by 0.1 at each step. Whenever this happens t is quite high (above 95), but I don't know if this is a coincidence or not. If not, I suppose it could be an issue with the function k(t) in the model producing a number that is far too small, but I don't see why this should be an issue.
If anyone with more experience than me has a suggestion to try and fix it, that would obviously be much appreciated. In lieu of that, I would also be okay with having some way to skip over that run of the model and move onto the next when the running time gets too long, but I don't know how to implement something like this without making the runs very slow.
",0,82,"You could possibly try using the optional method parameter in your solve_ivp, and set it to method = ""BDF"". It might have a little more accuracy.
According to
https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html :

BDF: Implicit multi-step variable-order (1 to 5) method based on a
backward differentiation formula for the derivative approximation [5].
The implementation follows the one described in [6]. A quasi-constant
step scheme is used and accuracy is enhanced using the NDF
modification. Can be applied in the complex domain.

",,
SciPy strange issue,https://stackoverflow.com/questions/64130416,How to remove hidden utdated scipy file so it won&#39;t import with python in VS Code OSX,"I am currently trying to run a program with Scipy, and I want to use the load_npz module.
Whenever I tried to run it, the compiler would say that that module doesn't exist.
I ran scipy.__version__ and got 0.13.0b1, which makes sense as to why it couldn't find the module as it doesn't exist in that version, but I am confused as I have 1.5.2 installed in both pip and brew yet it keeps defaulting to the oldest version which is very frustrating.
Does anybody know how to get rid of this version? I have tried uninstalling from pip and brew, along with finding the path of the imported scipy with the outdated version yet it still is causing issues.
I do have a lot of packages installed (numpy, matplotlib, etc.) so could it be a dependency that keeps reinstalling an old version?
Strangely, even if I delete scipy from both brew and pip, it will still show the old version but throw an error on a different local file that also uses scipy saying the module does not exist (which is expected as I deleted it).
",0,25,"I figured it out, I just deleted all my possible scipy locations and then just downloaded Anaconda and I'm using that as my python interpreter.
","write this python to find out the location of the imported scipy
import scipy
print(scipy.__file__)

",
SciPy strange issue,https://stackoverflow.com/questions/58558248,Why does gstreamer capture image incorrectly?,"Hello,

I have a Jetson Nano device with a Raspberry Pi v2.1-camera and use gstreamer and OpenCV to capture images with it. Still, I thought this question would be better preserved here because I mainly think it is a software issue.
The point is: I use following Python-script to capture my image:

import cv2
import numpy as np
import matplotlib.pyplot as plt
from scipy.misc import imshow

def gstreamer_pipeline (capture_width=3280, capture_height=2464, display_width=1280, display_height=720, framerate=21, flip_method=0) :   
    return ('nvarguscamerasrc ! ' 
    'video/x-raw(memory:NVMM), '
    'width=(int)%d, height=(int)%d, '
    'format=(string)NV12, framerate=(fraction)%d/1 ! '
    'nvvidconv flip-method=%d ! '
    'video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! '
    'videoconvert ! '
    'video/x-raw, format=(string)BGR ! appsink'  % (capture_width,capture_height,framerate,flip_method,display_width,display_height))


if __name__ == '__main__':
    cap = cv2.VideoCapture(gstreamer_pipeline(flip_method=0), cv2.CAP_GSTREAMER)
    if cap.isOpened():
        ret_val, img = cap.read()
        img = np.flipud(img)
        img = np.fliplr(img)
        imshow(img)
        cv2.imwrite(""test.jpg"", img)
    else:
        print(""Unable to open camera."")


My problem is I am not sure if it captures color values correctly. When I use imshow(img) from SciPy to display the image I just took, it looks like this:
(I do not have enough reputation to insert it, so please see this link)

That is strange because it is very blue-toned. After I use cv2.imwrite(""test.jpg"", img) to save it, it looks normal again: (Here the another image)

I do not know if I defined something wrong in the gstreamer-pipeline or what else could cause the error. It is important for my program to capture colors correctly and since I do not want to save an image and then load it just to get the colors right (bad in terms of speed and CPU-power), I would be very glad if someone could help me out.

Thanks for the answers in advance!
",0,1057,"As mentioned by HansHirse your color space is wrong. 

You could alter your gstreamer pipeline to output RGB

format=(string)RGB    


Or convert from BGR to RGB within oopencv 

im_rgb = cv2.cvtColor(im_cv, cv2.COLOR_BGR2RGB)

",,
tensorflow unexpected behavior,https://stackoverflow.com/questions/38094217,TensorFlow: argmax (-min),"I just noticed an unexpected (at least for me) behavior in TensorFlow. I thought tf.argmax (-argmin) operates on the ranks of a Tensor from outer to inner, but apparently it does not?!

Example:

import numpy as np
import tensorflow as tf

sess = tf.InteractiveSession()

arr = np.array([[31, 23,  4, 24, 27, 34],
                [18,  3, 25,  0,  6, 35],
                [28, 14, 33, 22, 20,  8],
                [13, 30, 21, 19,  7,  9],
                [16,  1, 26, 32,  2, 29],
                [17, 12,  5, 11, 10, 15]])

# arr has rank 2 and shape (6, 6)
tf.rank(arr).eval()
&gt; 2
tf.shape(arr).eval()
&gt; array([6, 6], dtype=int32)


tf.argmax takes two arguments: input and dimension. Since the indices of array arr are arr[rows, columns], I would expect tf.argmax(arr, 0) to return the index of the maximum element per row, while I would have expected tf.argmax(arr, 1) to return the maximum element per column. Likewise for tf.argmin.

However, the opposite is true:

tf.argmax(arr, 0).eval()
&gt; array([0, 3, 2, 4, 0, 1])

# 0 -&gt; 31 (arr[0, 0])
# 3 -&gt; 30 (arr[3, 1])
# 2 -&gt; 33 (arr[2, 2])
# ...
# thus, this is clearly searching for the maximum element
# for every column, and *not* for every row

tf.argmax(arr, 1).eval()
&gt; array([5, 5, 2, 1, 3, 0])

# 5 -&gt; 34 (arr[0, 5])
# 5 -&gt; 35 (arr[1, 5])
# 2 -&gt; 33 (arr[2, 2])
# ...
# this clearly returns the maximum element per row,
# albeit 'dimension' was set to 1


Can someone explain this behavior?

Generalized every n-dimensional Tensor t is indexed by t[i, j, k, ...]. Thus, t has rank n and shape (i, j, k, ...). Since dimension 0 corresponds to i, dimension 1 to j, and so forth. Why does tf.argmax (&amp; -argmin) ignore this scheme?
",16,14542,"Think of the dimension argument of tf.argmax as the axis across which you reduce. tf.argmax(arr, 0) reduces across dimension 0, i.e. the rows. Reducing across rows means that you will get the argmax of each individual column.

This might be counterintuitive, but it falls in line with the conventions used in tf.reduce_max and so on.
","In an n-dimensional Tensor, any given dimension has n-1 dimensions that form a discrete 2 dimensional subspace. Following the same logic, it has n-2 3 dimensional subspaces, all the way down to n - (n-1), n dimensional subspaces. You could express any aggregation as a function within the remaining subspace(s), or across the subspace(s) that are being aggregated. Since the subspace will no longer exist after the aggregation, Tensorflow has chosen to implement it as an operation across that dimension.

Frankly, it's an implementation choice by the creators of Tensorflow, now you know.
",
tensorflow unexpected behavior,https://stackoverflow.com/questions/56156646,Keras + Tensorflow Model Optimization: TypeError: clone_model() got an unexpected keyword argument &#39;clone_function&#39;,"I'm trying Tensorflow Model Optimization in order to prune a simple Neural Network. Here's my code:

from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

train_images = train_images / 255.0
test_images = test_images / 255.0

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation=tf.nn.relu),
    keras.layers.Dense(10, activation=tf.nn.softmax)
])

import tensorflow_model_optimization as tfmot


pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
                        initial_sparsity=0.0, final_sparsity=0.5,
                        begin_step=2000, end_step=4000)

model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_schedule)


model_for_pruning.compile(optimizer='adam',
          loss='sparse_categorical_crossentropy',
          metrics=['accuracy'])

from tensorflow.keras.callbacks import TensorBoard


tensorboard=TensorBoard(log_dir='D:\Python\logs', histogram_freq=0,  
          write_graph=True, write_images=True)

model_for_pruning.fit(train_images, train_labels, epochs=5,callbacks=tensorboard)


#tensorboard --logdir D:\Python\logs 


I'm getting the following error:

File ""&lt;ipython-input-1-8f75575649d2&gt;"", line 52, in &lt;module&gt;
    model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_schedule)

  File ""C:\Users\Rubens\Anaconda3\lib\site-packages\tensorflow_model_optimization\python\core\sparsity\keras\prune.py"", line 152, in prune_low_magnitude
    to_prune, input_tensors=None, clone_function=_add_pruning_wrapper)

TypeError: clone_model() got an unexpected keyword argument 'clone_function'


That is, clone function does not belong to Keras' file models.py. I tried to add **kwargs to it, without success:

def clone_model(model, input_tensors=None,**kwargs):
""""""Clone any `Model` instance.

Model cloning is similar to calling a model on new inputs,
except that it creates new layers (and thus new weights) instead
of sharing the weights of the existing layers.

# Arguments
    model: Instance of `Model`
        (could be a functional model or a Sequential model).
    input_tensors: optional list of input tensors
        to build the model upon. If not provided,
        placeholders will be created.

# Returns
    An instance of `Model` reproducing the behavior
    of the original model, on top of new inputs tensors,
    using newly instantiated weights.

# Raises
    ValueError: in case of invalid `model` argument value.
""""""
if isinstance(model, Sequential):
    return _clone_sequential_model(model, input_tensors=input_tensors)
else:
    return _clone_functional_model(model, input_tensors=input_tensors)


This is the end of file prune.py, belonging to Tensorflow Model Optimization (notice clone_function=_strip_pruning_wrapper):

  def _strip_pruning_wrapper(layer):
    if isinstance(layer, pruning_wrapper.PruneLowMagnitude):
      # The _batch_input_shape attribute in the first layer makes a Sequential
      # model to be built. This makes sure that when we remove the wrapper from
      # the first layer the model's built state preserves.
      if not hasattr(layer.layer, '_batch_input_shape') and hasattr(
          layer, '_batch_input_shape'):
        layer.layer._batch_input_shape = layer._batch_input_shape
      return layer.layer
    return layer

  return keras.models.clone_model(
      model, input_tensors=None, clone_function=_strip_pruning_wrapper)


All libraries included are up-to-date. Any ideas on how to overcome this error ?

Thanks in advance
",3,4105,"I found the answer. There is a tricky workaround: besides fixing the code to:

from tensorflow_model_optimization.sparsity import keras as sparsity

pruning_params = {
      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,
                                                   final_sparsity=0.90,
                                                   begin_step=3,
                                                   end_step=end_step,
                                                   frequency=100)
}

pruned_model = tf.keras.Sequential([
    sparsity.prune_low_magnitude(
        l.Conv2D(32, 5, padding='same', activation='relu'),
        input_shape=input_shape,
        **pruning_params),
    l.MaxPooling2D((2, 2), (2, 2), padding='same'),
    l.BatchNormalization(),
    sparsity.prune_low_magnitude(
        l.Conv2D(64, 5, padding='same', activation='relu'), **pruning_params),
    l.MaxPooling2D((2, 2), (2, 2), padding='same'),
    l.Flatten(),
    sparsity.prune_low_magnitude(l.Dense(1024, activation='relu'),
                                 **pruning_params),
    l.Dropout(0.4),
    sparsity.prune_low_magnitude(l.Dense(num_classes, activation='softmax'),
                                 **pruning_params)
])


... I had to restart Jupyter kernel to get rid of further errors, like Conv2D has no attribute 'kernel', as seen at GitHub:

tf.enable_eager_execution must be called at program startup. #18304
",,
tensorflow unexpected behavior,https://stackoverflow.com/questions/67014270,TypeError: apply_gradients() got an unexpected keyword argument &#39;global_step&#39;,"After days trying to make one RL agent, I finally succeeded in creating its experience, but when I try to train it I get this error. I've tried all I could: different experience, changed step params... I am just out of ideas.
import pyxinput
import time
import cv2
from PIL import ImageGrab
import numpy as np
import keyboard
import tensorflow
import tf_agents
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import torch
#from tf_agents.networks import actor_distribution_networ

from tf_agents.policies import random_py_policy

Tensod_spec = tf_agents.specs.BoundedArraySpec(
   (15,),
   dtype=np.float32,
   name=""XimputSpecs"",
   minimum=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
   maximum=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
)

Tensod_spec2 = tf_agents.specs.TensorSpec(
   [440, 600, 1], dtype=tf.int32, name=""ScreenSpecs""
)

Tensor_reward_spe = tf_agents.specs.TensorSpec(
   [1, 1], dtype=tf.int32, name=""Reward""
)

FromEnv = tf_agents.specs.BoundedTensorSpec(
   shape=(440, 600, 1),
   dtype='uint8',
   name='observation',
   minimum=0,
   maximum=255
)
FromEnv2 = tf_agents.specs.BoundedTensorSpec(
   shape=(1, 440, 600, 1),
   dtype=tf.int32,
   name='observation',
   minimum=0,
   maximum=255
)

fullscreen = [110, 130, 710, 570]

screenpil = ImageGrab.grab(bbox=fullscreen)
showprint = np.array(screenpil)
grayscreen = cv2.cvtColor(showprint, cv2.COLOR_BGR2GRAY)
screenrect = cv2.cvtColor(grayscreen, cv2.COLOR_GRAY2BGR)
grayscreen = grayscreen.reshape(440, 600, 1)

time_step_spec2 = tf_agents.trajectories.time_step.time_step_spec(
   observation_spec=FromEnv,
   #reward_spec = Tensor_reward_spec
)

time_step_spec = tf_agents.trajectories.time_step.time_step_spec(
    observation_spec=FromEnv,
    #reward_spec = Tensor_reward_spec
)

actor_net = tf_agents.networks.actor_distribution_network.ActorDistributionNetwork(
   input_tensor_spec=FromEnv,
   output_tensor_spec=tf_agents.specs.tensor_spec.from_spec(Tensod_spec),
   activation_fn='relu',
   #conv_layer_params=[(25, 40, 2)],
   fc_layer_params=(50, 25, 15),
   #dtype='int32'
)
print(actor_net)

train_step_counter = tf.dtypes.cast(1, tf.int32)

optimizer = tf.keras.optimizers.Adam(learning_rate=0.003)

tf_agent = tf_agents.agents.ReinforceAgent(
   time_step_spec=time_step_spec,
   action_spec=tf_agents.specs.tensor_spec.from_spec(Tensod_spec),
   actor_network=actor_net,
   optimizer=optimizer,
   normalize_returns=True,
   #train_step_counter=tf.Variable(1, name=""global_step"")
   )
tf_agent.initialize()

grayscreen2 = grayscreen
grayscreen2 = grayscreen2.reshape(1, 440, 600, 1)
time_step2 = tf_agents.trajectories.time_step.TimeStep(
   step_type=tf_agents.trajectories.time_step.StepType.FIRST,
   reward=tf.dtypes.cast(1, tf.float32),
   discount=tf.dtypes.cast(1, tf.float32),
   observation=grayscreen2
)

policy_state = tf_agent.policy.get_initial_state(batch_size=1)

policy_step = tf_agent.policy.action(time_step2, policy_state)
print(policy_step)

observe = time_step2.observation
#print(observe.dtype)
#observe = observe.astype(int)
#print(observe.shape)

experience = tf_agents.trajectories.trajectory.Trajectory(
   action=tf.compat.v2.Variable([
           tf.compat.v2.Variable(policy_step.action),
           tf.compat.v2.Variable(policy_step.action),
           tf.compat.v2.Variable(policy_step.action)
   ]),
   reward=tf.compat.v2.Variable([[
       tf.compat.v2.Variable(time_step2.reward),
       tf.compat.v2.Variable(time_step2.reward),
       tf.compat.v2.Variable(time_step2.reward)
   ]]),
   step_type=tf.compat.v2.Variable([[
       tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.FIRST),
       tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.MID),
       tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.LAST)
   ]]),
   observation=tf.compat.v2.Variable([
       tf.compat.v2.Variable(observe),
       tf.compat.v2.Variable(observe),
       tf.compat.v2.Variable(observe)
   ]),
   policy_info=tf_agent.policy.info_spec,
   next_step_type=tf.compat.v2.Variable([[
       tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.MID),
       tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.LAST),
       tf.compat.v2.Variable(tf_agents.trajectories.time_step.StepType.LAST)
   ]]),
   discount=tf.compat.v2.Variable([[
       tf.dtypes.cast(1, tf.float32),
       tf.dtypes.cast(1, tf.float32),
       tf.dtypes.cast(1, tf.float32)
   ]]), 
)

train_loss = tf_agent.train(experience)
print(train_loss)

And I get this error:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-15-4dd3966a32b6&gt; in &lt;module&gt;
      1 #
----&gt; 2 train_loss = tf_agent.train(experience)
      3 print(train_loss)

~\AppData\Local\Programs\Python\Python38\lib\site-packages\tf_agents\agents\tf_agent.py in train(self, experience, weights, **kwargs)
    516 
    517     if self._enable_functions:
--&gt; 518       loss_info = self._train_fn(
    519           experience=experience, weights=weights, **kwargs)
    520     else:

~\AppData\Local\Programs\Python\Python38\lib\site-packages\tf_agents\utils\common.py in with_check_resource_vars(*fn_args, **fn_kwargs)
    183         # We're either in eager mode or in tf.function mode (no in-between); so
    184         # autodep-like behavior is already expected of fn.
--&gt; 185         return fn(*fn_args, **fn_kwargs)
    186       if not resource_variables_enabled():
    187         raise RuntimeError(MISSING_RESOURCE_VARIABLES_ERROR)

~\AppData\Local\Programs\Python\Python38\lib\site-packages\tf_agents\agents\reinforce\reinforce_agent.py in _train(self, experience, weights)
    286                                           self.train_step_counter)
    287 
--&gt; 288     self._optimizer.apply_gradients(
    289         grads_and_vars, global_step=0)
    290 

TypeError: apply_gradients() got an unexpected keyword argument 'global_step'

What is this global step, and where is this error coming from? Why can't I train my agent?
Specs:

Python 3.8
TensorFlow 2.4 (GPU and non-GPU)
Windows 10 / ubuntu

If you need more info, please let me know.
EDIT: Tried other agents they run fine and i posted this ISUE on Tensor
GIT:https://github.com/tensorflow/tensorflow/issues/48424
If anyone has the same problem in the future
",2,1047,"You should try to use a different Optimizer. Those in tf.keras.optimizer don't take global_steps as an argument in apply_gradients function.
Instead, use these from tf.compat.v1.train, e.g.,
optimizer = tf.compat.v1.train.AdamOptimizer(learn_rate=0.003)

Note this passes the runtime check, but it makes the training impossible to complete. global_step is supposed to take a Variable and its value will be +1 when apply_gradients is called. However, here you see global_step=0 is passed in making it no effect at all. The train_step_counter you defined above will remain 0.
Also note there's a fix on the way.
",,
tensorflow unexpected behavior,https://stackoverflow.com/questions/40327541,Tensorflow shuffle batch fraction unexpected behavior,"I am training a convolutional neural network and I got some unexpected behavior with the shuffle_batch fraction summary, or maybe I just do not understand it. Can someone pls explain it? The difference between those two graphs is that I exchanged the loss function. 

With this loss function I get the line at 0.0

loss = tf.nn.l2_loss(expected_labels-labels)


While this one gives me a constant 1.0 (after hitting 1.0 the first time)

loss = tf.reduce_mean(tf.square(expected_labels - labels))


Can the change of loss function really cause that change? I am not sure what this means.



EDIT: Code as requested
The first part is for setting up the batching and the big picture.

filename_queue = tf.train.string_input_producer(filenames,
                                                num_epochs=None)
label, image = read_and_decode_single_example(filename_queue=filename_queue)
image = tf.image.decode_jpeg(image.values[0], channels=3)
jpeg = tf.cast(image, tf.float32) / 255.
jpeg.set_shape([66,200,3])
images_batch, labels_batch = tf.train.shuffle_batch(
    [jpeg, label], batch_size= FLAGS.batch_size,
    num_threads=8,
    capacity=60000,
    min_after_dequeue=10000)
images_placeholder, labels_placeholder = placeholder_inputs(
    FLAGS.batch_size)

label_estimations, W1_conv, h1_conv, current_images = e2e.inference(images_placeholder)

# Add to the Graph the Ops for loss calculation.
loss = e2e.loss(label_estimations, labels_placeholder)


# Decay once per epoch, using an exponential schedule starting at 0.01.


# Add to the Graph the Ops that calculate and apply gradients.
train_op = e2e.training(loss, FLAGS.learning_rate, FLAGS.batch_size)


Here come the methods for inference loss and train

def inference(images):
with tf.name_scope('conv1'):
    W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 3, FEATURE_MAPS_C1], stddev=STDDEV))
    b_conv1 = tf.Variable(tf.constant(BIAS_INIT, shape=[FEATURE_MAPS_C1]))
    h_conv1 = tf.nn.bias_add(
        tf.nn.conv2d(images, W_conv1, strides=[1, 2, 2, 1], padding='VALID'), b_conv1)

with tf.name_scope('conv2'):
    W_conv2 = tf.Variable(tf.truncated_normal([5, 5, FEATURE_MAPS_C1, 36], stddev=STDDEV))
    b_conv2 = tf.Variable(tf.constant(BIAS_INIT, shape=[36]))
    h_conv2 = tf.nn.conv2d(h_conv1, W_conv2, strides=[1, 2, 2, 1], padding='VALID') + b_conv2

with tf.name_scope('conv3'):
    W_conv3 = tf.Variable(tf.truncated_normal([5, 5, 36, 48], stddev=STDDEV))
    b_conv3 = tf.Variable(tf.constant(BIAS_INIT, shape=[48]))
    h_conv3 = tf.nn.conv2d(h_conv2, W_conv3, strides=[1, 2, 2, 1], padding='VALID') + b_conv3

with tf.name_scope('conv4'):
    W_conv4 = tf.Variable(tf.truncated_normal([3, 3, 48, 64], stddev=STDDEV))
    b_conv4 = tf.Variable(tf.constant(BIAS_INIT, shape=[64]))
    h_conv4 = tf.nn.conv2d(h_conv3, W_conv4, strides=[1, 1, 1, 1], padding='VALID') + b_conv4

with tf.name_scope('conv5'):
    W_conv5 = tf.Variable(tf.truncated_normal([3, 3, 64, 64], stddev=STDDEV))
    b_conv5 = tf.Variable(tf.constant(BIAS_INIT, shape=[64]))
    h_conv5 = tf.nn.conv2d(h_conv4, W_conv5, strides=[1, 1, 1, 1], padding='VALID') + b_conv5
    h_conv5_flat = tf.reshape(h_conv5, [-1, 1 * 18 * 64])


with tf.name_scope('fc1'):
    W_fc1 = tf.Variable(tf.truncated_normal([1 * 18 * 64, 100], stddev=STDDEV))
    b_fc1 = tf.Variable(tf.constant(BIAS_INIT, shape=[100]))
    h_fc1 = tf.matmul(h_conv5_flat, W_fc1) + b_fc1

with tf.name_scope('fc2'):
    W_fc2 = tf.Variable(tf.truncated_normal([100, 50], stddev=STDDEV))
    b_fc2 = tf.Variable(tf.constant(BIAS_INIT, shape=[50]))
    h_fc2 = tf.matmul(h_fc1, W_fc2) + b_fc2

with tf.name_scope('fc3'):
    W_fc3 = tf.Variable(tf.truncated_normal([50, 10], stddev=STDDEV))
    b_fc3 = tf.Variable(tf.constant(BIAS_INIT, shape=[10]))
    h_fc3 = tf.matmul(h_fc2, W_fc3) + b_fc3

with tf.name_scope('fc4'):
    W_fc4 = tf.Variable(tf.truncated_normal([10, 1], stddev=STDDEV))
    b_fc4 = tf.Variable(tf.constant(BIAS_INIT, shape=[1]))
    h_fc4 = tf.matmul(h_fc3, W_fc4) + b_fc4


return h_fc4


Here is the loss function, using l2 causes the issue.

def loss(label_estimations, labels):    
    n_labels = tf.reshape(label_estimations, [-1])
    # Here are the two loss functions
    #loss = tf.reduce_mean(tf.square(n_labels - labels))
    loss = tf.nn.l2_loss(n_labels-labels)
    return loss


Train method:

def training(loss, learning_rate, batch_size): 
    global_step = tf.Variable(0, name='global_step', trainable=False)
    tf.scalar_summary('learning_rate',learning_rate)
    tf.scalar_summary('Loss ('+loss.op.name+')', loss)

    optimizer = tf.train.AdamOptimizer(learning_rate)
    train_op = optimizer.minimize(loss, global_step=global_step)
    return train_op


Plot for tf.reduce_sum(tf.square(n_labels - labels)/2)


",1,784,"As mentioned in TensorFlow's original guide https://www.tensorflow.org/programmers_guide/reading_data


  How many threads do you need? the tf.train.shuffle_batch* functions add a summary to the graph that indicates how full the example queue is. If you have enough reading threads, that summary will stay above zero. You can view your summaries as training progresses using TensorBoard.


It seems better if the queue is never empty, i.e. the ""fraction_full"" stays non-zero. If not, you should allocate more threads to queue_runner
","The only difference between your loss and l2 is scaling, thus you might need to play around with your learning rate / other hyperparameters to take this into account. 

l2 loss in TF is defined as:

1/2 SUM_i^N (pred(x_i) - y_i)^2


while your cost is

1/N SUM_i^N (pred(x_i) - y_i)^2


Of course since you are using stochastic gradient approach, efficienty you are using an approximator of form

1/2 SUM_{(x_i, y_i) in batch} (pred(x_i) - y_i)^2 # l2
1/#batch SUM_{(x_i, y_i) in batch} (pred(x_i) - y_i)^2 # you


Thus you would have to multiply your cost by batch_size / 2 to get the original cost. Typically this is not a problem, but sometimes wrong scaling can put you in very degenerated parts of the error surface, and the optimizer will simply fail (especially such aggressive one like Adam).

Side note - you are aware that your model is a deep linear model? You do not have any non-linearities in the model. This is very specific network.
",
tensorflow unexpected behavior,https://stackoverflow.com/questions/59072533,Subclassing Sequential() keras-model,"I wanted to subclass a sequential model in order to be able to write a custom call() and handle named inputs. However, I got, for me, some unexpected behavior already for very minor changes to the __init__ function. If I try to add a new member to my subclass and initialize it after calling super().__init__() the model fails to build automatically.

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Activation, MaxPooling2D, Dense, Flatten
import tensorflow as tf
class Sequential2(Sequential):

    def __init__(self):
        super(Sequential2, self).__init__()
        self.custom_member = []

    def get_my_custom_member(self):
        return self.custom_member

model = Sequential2()

if tf.keras.backend.image_data_format() == 'channels_first':
    input_shape = (1, 28, 28)
else:
    assert tf.keras.backend.image_data_format() == 'channels_last'
    input_shape = (28, 28, 1)

layers = [Conv2D(32, (3, 3), input_shape=input_shape)]

for layer in layers:
    model.add(layer)

model.add(Dense(10))
model.add(Activation('relu'))

model.summary()


fails with output: ValueError: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.

However if self.custom_member = [] is left out it works as expected.

What am I missing here? (tested with Tensorflow 1.14)
",1,1117,"This issue was fixed in TF 2.2. You can refer working code as shown below
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Activation, MaxPooling2D, Dense, Flatten

import tensorflow as tf
print(tf.__version__)

class Sequential2(Sequential):

    def __init__(self):
        super(Sequential2, self).__init__()
        self.custom_member = []

    def get_my_custom_member(self):
        return self.custom_member

model = Sequential2()

if tf.keras.backend.image_data_format() == 'channels_first':
    input_shape = (1, 28, 28)
else:
    assert tf.keras.backend.image_data_format() == 'channels_last'
    input_shape = (28, 28, 1)

layers = [Conv2D(32, (3, 3), input_shape=input_shape)]

for layer in layers:
    model.add(layer)

model.add(Dense(10))
model.add(Activation('relu'))

model.summary()

Output:
2.2.0
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 26, 26, 32)        320       
_________________________________________________________________
dense (Dense)                (None, 26, 26, 10)        330       
_________________________________________________________________
activation (Activation)      (None, 26, 26, 10)        0         
=================================================================
Total params: 650
Trainable params: 650
Non-trainable params: 0
_________________________________________________________________

",,
tensorflow unexpected behavior,https://stackoverflow.com/questions/60173337,How to manipulate and return tf.Variable using a for loop over tf.data.Dataset inside function decorated with @tf.function?,"I am trying to create a function containing a for loop over a TensorFlow Dataset that assigns a new value to a TensorFlow Variable in each iteration. The Variable should also be returned as output of the function. With eager execution enabled, there are no issues, however, in graph mode, some unexpected things seem to happen. Consider the following simple dummy code:

import tensorflow as tf


class Test(object):
    def __init__(self):
        self.var = tf.Variable(0, trainable=False, dtype=tf.float32)
        self.increment = tf.constant(1, dtype=tf.float32)
        self.dataset = tf.data.Dataset.from_tensor_slices([0, 1, 2])

    @tf.function
    def fn1(self):
        self.var.assign(0)
        for _ in tf.range(3):
            self.var.assign(self.var+self.increment)
            tf.print(self.var)
        tf.print(self.var)
        return self.var

    @tf.function
    def fn2(self):
        self.var.assign(0)
        for _ in self.dataset:
            self.var.assign(self.var+self.increment)
            tf.print(self.var)
        tf.print(self.var)
        return self.var

    @tf.function
    def fn3(self):
        self.var.assign(0)
        y = self.var
        for _ in self.dataset:
            self.var.assign(self.var+self.increment)
            y = self.var
            tf.print(y)
        tf.print(y)
        return y

    @tf.function
    def fn4(self):
        var = 0.0
        for _ in self.dataset:
            var += 1.0
            tf.print(var)
        tf.print(var)
        return var



test.fn1(), test.fn3() and test.fn4() all return the following (desired) output:

1
2
3
3
&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0&gt;


However, test.fn2() behaves differently:

1
2
3
0
&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.0&gt;


Interestingly, after execution of test.fn2, test.var does seem to contain the correct value:

&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0&gt;


I am not sure why test.fn2 fails. Clearly, it is doing some things correctly (as test.var contains the correct value after execution of the function), but it does not deliver the correct result. Can you help me understand what causes this code to fail?

The behavior described above occurs when using TensorFlow 2.1.0 for Python 3.6 on CentOS 7.
",1,346,"Running this on TensorFlow 2.1.0 reproduces your scenario.

Which prints 1 2 3 0 for test.fn2(), but you should also consider that when you print self.var in test.fn3() it will also show you self.var = 0 during the function call.

Modified fn3( ):

    @tf.function
    def fn3(self):
        self.var.assign(0)
        y = self.var
        for _ in self.dataset:
            self.var.assign(self.var+self.increment)
            y = self.var
            tf.print(y)
        tf.print(self.var)  # Inspect self.var value
        tf.print(y)
        return y


Output:

# Executed in Tensorflow 2.1.0
# test.fn3()
1
2
3
0  &lt;&lt; self.var
3


This is already fixed If you execute this in Tensorflow 2.2.0-rc2.
The output will be your desired outcome even when printing it during graph execution.

To quickly simulate this you could use Google Colab and use %tensorflow_version 2.x to get the latest available version for Tensorflow.

Output:

# Executed in Tensorflow 2.2.0-rc2
Function 1
1
2
3
3
Function 2
1
2
3
3
Function 3
1
2
3
3 &lt;&lt; Value of self.var in test.fn3()
3
Function 4
1
2
3
3


You could check more about the changes in the latest Tensorflow Updates in this link.  
",,
tensorflow unexpected behavior,https://stackoverflow.com/questions/53331211,Tensorflow bazel quantization build error,"I am trying to build tensorflow tools package with bazel 0.18.0

following steps are ok

git clone https://github.com/tensorflow/tensoflow
bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package
bazel build --config=cuda //tensorflow/examples/label_image:label_image


until trying to run this command line 

bazel build --config=cuda //tensorflow/contrib/quantization:quantize_graph


it show error, so should i give something else then quantize_graph? and what i can use or find?

root@24663fb1018d:/srv/wu/tensorflow-src/tensorflow# bazel build --config=cuda //tensorflow/contrib/quantization:quantize_graph
WARNING: Duplicate rc file: /srv/wu/tensorflow-src/tensorflow/tools/bazel.rc is read multiple times, most recently imported from /srv/wu/tensorflow-src/tensorflow/.bazelrc
WARNING: Processed legacy workspace file /srv/wu/tensorflow-src/tensorflow/tools/bazel.rc. This file will not be processedin the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: Skipping '//tensorflow/contrib/quantization:quantize_graph': no such target '//tensorflow/contrib/quantization:quantize_graph': target 'quantize_graph' not declared in package 'tensorflow/contrib/quantization' defined by /srv/wu/tensorflow-src/tensorflow/tensorflow/contrib/quantization/BUILD
WARNING: Target pattern parsing failed.
ERROR: no such target '//tensorflow/contrib/quantization:quantize_graph': target 'quantize_graph' not declared in package 'tensorflow/contrib/quantization' defined by /srv/wu/tensorflow-src/tensorflow/tensorflow/contrib/quantization/BUILD
INFO: Elapsed time: 1.195s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (1 packages loaded)

and then i tried with tools path, no luck

bazel build --config=cuda //tensorflow/tools/quantization:quantize_graph


WARNING: Duplicate rc file: /srv/wu/tensorflow-src/tensorflow/tools/bazel.rc is read multiple times, most recently imported from /srv/wu/tensorflow-src/tensorflow/.bazelrcWARNING: Processed legacy workspace file /srv/wu/tensorflow-src/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.ERROR: Skipping '//tensorflow/tools/quantization:quantize_graph': no such package'tensorflow/tools/quantization': BUILD file not found on package pathWARNING: Target pattern parsing failed.
ERROR: no such package 'tensorflow/tools/quantization': BUILD file not found on package path
INFO: Elapsed time: 0.506s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)

also the toco is not working

bazel build --config=cuda tensorflow/contrib/lite/toco:toco


WARNING: Duplicate rc file: /srv/wu/tensorflow-src/tensorflow/tools/bazel.rc is read multiple times, most recently imported from /srv/wu/tensorflow-src/tensorflow/.bazelrc
WARNING: Processed legacy workspace file /srv/wu/tensorflow-src/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.ERROR: Skipping 'tensorflow/contrib/lite/toco:toco': no such package 'tensorflow/contrib/lite/toco': BUILD file not found on package path
WARNING: Target pattern parsing failed.ERROR: no such package 'tensorflow/contrib/lite/toco': BUILD file not found on package pathINFO: Elapsed time: 0.500s
INFO: 0 processes.FAILED: Build did NOT complete successfully (0 packages loaded)
",0,1193,"To verify where targets are, look into the BUILD file in the package directory. 

The quantize_graph target has been moved to the //tensorflow/contrib/quantize package. This should work:

$ bazel build --config=cuda //tensorflow/contrib/quantize:quantize_graph


The toco target has been moved from //tensorflow/contrib/lite/toco to //tensorflow/lite/toco. Like quantize_graph, this should work:

$ bazel build --config=cuda //tensorflow/lite/toco:toco

",,
tensorflow unexpected behavior,https://stackoverflow.com/questions/55029279,Tensorflow / keras multi_gpu_model is not splitted to more than one gpu,"I'm encountered the problem, that I can not successfully split my training batches to more than one GPU. If multi_gpu_model from tensorflow.keras.utils is used, tensorflow allocates the full memory on all available (for example 2) gpus, but only the first one (gpu[0]) is utilized to 100% if nvidia-smi is watched.

I'm using tensorflow 1.12 right now.

Test on single device

model = getSimpleCNN(... some parameters)

model .compile()
model .fit()


As expected, data is loaded by cpu and the model runs on gpu[0] with 97% - 100% gpu utilization:


Create a multi_gpu model

As described in the tensorflow api for multi_gpu_model here, the device scope for model definition is not changed.

from tensorflow.keras.utils import multi_gpu_model

model = getSimpleCNN(... some parameters)
parallel_model = multi_gpu_model(model, gpus=2, cpu_merge=False)  # weights merge on GPU (recommended for NV-link)

parallel_model.compile()
parallel_model.fit()


As seen in the timeline, cpu now not only loads the data, but is doing some other calculations. Notice: the second gpu is nearly doing nothing:


The question

The effect even worsens as soon as four gpus are used. Utilization of the first one goes up to 100% but for the rest there are only short peeks. 

Is there any solution to fix this? How to properly train on multiple gpus?

Is there any difference between tensorflow.keras.utils and keras.utils which causes the unexpected behavior? 
",0,976,"I just ran into the same issue. 
In my case, the problem came from the use of a build_model(... parameters) function that returned the model. 
Be careful with your  getSimpleCNN() function, as I don't know what is in it my best advice is to build the model sequentially in your code without using this function.
",,
tensorflow unexpected behavior,https://stackoverflow.com/questions/64650756,"Bazel build error with Cuda on Windows 10, how resolve it?","I'm trying to compile a  Tensorflow core, with CUDA support (I need cc 8.6 for GeForce3090) on Windows 10 64bit via Bazel by this instruction: https://www.tensorflow.org/install/source_windows
I use this build string:
bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
I get error, like bellow:
How I can resolve this?
Thanx.
  &gt; D:\Python\tensorflow&gt;bazel build --config=opt --config=cuda
    &gt; --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package Extracting Bazel
    &gt; installation... WARNING: Ignoring JAVA_HOME, because it must point to
    &gt; a JDK, not a JRE. Starting local Bazel server and connecting to it...
    &gt; WARNING: The following configs were expanded more than once: [cuda,
    &gt; using_cuda]. For repeatable flags, repeats are counted twice and may
    &gt; lead to unexpected behavior. INFO: Options provided by the client:  
    &gt; Inherited 'common' options: --isatty=1 --terminal_columns=237 INFO:
    &gt; Reading rc options for 'build' from d:\python\tensorflow\.bazelrc:  
    &gt; Inherited 'common' options: --experimental_repo_remote_exec INFO:
    &gt; Options provided by the client:   'build' options:
    &gt; --python_path=C:/Program Files/Python37/python.exe INFO: Reading rc options for 'build' from d:\python\tensorflow\.bazelrc:   'build'
    &gt; options: --apple_platform_type=macos --define
    &gt; framework_shared_object=true --define open_source_build=true
    &gt; --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2 INFO: Reading rc options for 'build' from d:\python\tensorflow\.tf_configure.bazelrc:   'build' options:
    &gt; --action_env PYTHON_BIN_PATH=C:/Program Files/Python37/python.exe --action_env PYTHON_LIB_PATH=C:/Program Files/Python37/lib/site-packages --python_path=C:/Program
    &gt; Files/Python37/python.exe --config=xla --action_env
    &gt; CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing
    &gt; Toolkit/CUDA/v11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.2,8.6
    &gt; --config=cuda --action_env TF_CONFIGURE_IOS=0 INFO: Found applicable config definition build:short_logs in file
    &gt; d:\python\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
    &gt; INFO: Found applicable config definition build:v2 in file
    &gt; d:\python\tensorflow\.bazelrc: --define=tf_api_version=2
    &gt; --action_env=TF2_BEHAVIOR=1 INFO: Found applicable config definition build:xla in file d:\python\tensorflow\.bazelrc:
    &gt; --define=with_xla_support=true INFO: Found applicable config definition build:cuda in file d:\python\tensorflow\.bazelrc:
    &gt; --config=using_cuda --define=using_cuda_nvcc=true INFO: Found applicable config definition build:using_cuda in file
    &gt; d:\python\tensorflow\.bazelrc: --define=using_cuda=true --action_env
    &gt; TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
    &gt; --define=tensorflow_enable_mlir_generated_gpu_kernels=1 INFO: Found applicable config definition build:opt in file
    &gt; d:\python\tensorflow\.tf_configure.bazelrc: --copt=/arch:SSE4.2
    &gt; --define with_default_optimizations=true INFO: Found applicable config definition build:cuda in file d:\python\tensorflow\.bazelrc:
    &gt; --config=using_cuda --define=using_cuda_nvcc=true INFO: Found applicable config definition build:using_cuda in file
    &gt; d:\python\tensorflow\.bazelrc: --define=using_cuda=true --action_env
    &gt; TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
    &gt; --define=tensorflow_enable_mlir_generated_gpu_kernels=1 INFO: Found applicable config definition build:windows in file
    &gt; d:\python\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES
    &gt; --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false INFO: Found applicable config definition build:monolithic in file d:\python\tensorflow\.bazelrc:
    &gt; --define framework_shared_object=false DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by
    &gt; modifying arguments shallow_since = ""1557349968 -0400"" DEBUG:
    &gt; Repository io_bazel_rules_go instantiated at:   no stack
    &gt; (--record_rule_instantiation_callstack not enabled) Repository rule
    &gt; git_repository defined at:  
    &gt; C:/users/vladimir/_bazel_vladimir/sd7zocps/external/bazel_tools/tools/build_defs/repo/git.bzl:195:33:
    &gt; in &lt;toplevel&gt; INFO: Repository local_config_cuda instantiated at:   no
    &gt; stack (--record_rule_instantiation_callstack not enabled) Repository
    &gt; rule cuda_configure defined at:  
    &gt; D:/python/tensorflow/third_party/gpus/cuda_configure.bzl:1418:33: in
    &gt; &lt;toplevel&gt; 

------------------------------------------------------------------------
**ERROR: An error occurred during the fetch of repository
    &gt; 'local_config_cuda':**    Traceback (most recent call last):
    &gt;         File ""D:/python/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1388
    &gt;                 _create_local_cuda_repository(&lt;1 more arguments&gt;)
    &gt;         File ""D:/python/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1064,
    &gt; in _create_local_cuda_repository
    &gt;                 _find_libs(repository_ctx, &lt;2 more arguments&gt;)
    &gt;         File ""D:/python/tensorflow/third_party/gpus/cuda_configure.bzl"", line 599,
    &gt; in _find_libs
    &gt;                 _check_cuda_libs(repository_ctx, &lt;2 more arguments&gt;)
    &gt;         File ""D:/python/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501,
    &gt; in _check_cuda_libs
    &gt;                 execute(repository_ctx, &lt;1 more arguments&gt;)***
    &gt;         File ""D:/python/tensorflow/third_party/remote_config/common.bzl"", line 217,
    &gt; in execute
    &gt;                 fail(&lt;1 more arguments&gt;) Repository command failed ""C:/Program"" is not an internal or external command, executable
    &gt; program, or batch file. 

---------------------------------------------------------------------------

ERROR: Skipping
    &gt; '//tensorflow/tools/pip_package:build_pip_package': no such package
    &gt; '@local_config_cuda//cuda': Traceback (most recent call last):
    &gt;         File ""D:/python/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1388
    &gt;                 _create_local_cuda_repository(&lt;1 more arguments&gt;)
    &gt;         File ""D:/python/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1064,
    &gt; in _create_local_cuda_repository
    &gt;                 _find_libs(repository_ctx, &lt;2 more arguments&gt;)
    &gt;         File ""D:/python/tensorflow/third_party/gpus/cuda_configure.bzl"", line 599,
    &gt; in _find_libs
    &gt;                 _check_cuda_libs(repository_ctx, &lt;2 more arguments&gt;)
    &gt;         File ""D:/python/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501,
    &gt; in _check_cuda_libs
    &gt;                 execute(repository_ctx, &lt;1 more arguments&gt;)
    &gt;         File ""D:/python/tensorflow/third_party/remote_config/common.bzl"", line 217,
    &gt; in execute
    &gt;                 fail(&lt;1 more arguments&gt;) Repository command failed ""C:/Program"" is not an internal or external command, executable
    &gt; program, or batch file. WARNING: Target pattern parsing failed. ERROR:
    &gt; no such package '@local_config_cuda//cuda': Traceback (most recent
    &gt; call last):
    &gt;         File ""D:/python/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1388
    &gt;                 _create_local_cuda_repository(&lt;1 more arguments&gt;)
    &gt;         File ""D:/python/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1064,
    &gt; in _create_local_cuda_repository
    &gt;                 _find_libs(repository_ctx, &lt;2 more arguments&gt;)
    &gt;         File ""D:/python/tensorflow/third_party/gpus/cuda_configure.bzl"", line 599,
    &gt; in _find_libs
    &gt;                 _check_cuda_libs(repository_ctx, &lt;2 more arguments&gt;)
    &gt;         File ""D:/python/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501,
    &gt; in _check_cuda_libs
    &gt;                 execute(repository_ctx, &lt;1 more arguments&gt;)
    &gt;         File ""D:/python/tensorflow/third_party/remote_config/common.bzl"", line 217,
    &gt; in execute
    &gt;                 fail(&lt;1 more arguments&gt;) Repository command failed ""C:/Program"" is not an internal or external command, executable
    &gt; program, or batch file. INFO: Elapsed time: 47.934s INFO: 0 processes.
    &gt; FAILED: Build did NOT complete successfully (0 packages loaded)
    &gt;     currently loading: tensorflow/tools/pip_package

",0,2595,"Summary:
I was resolved this problem by to steps:

Move Python from C:\Program Files\Python... to C:\Python...
and after we need change correspondence string in PATH variable
2)Check and delete another bash installation,exept  msys64, check: passway C:\msys64\usr\bin mast be present in your PATH variable

",,
tensorflow unexpected result,https://stackoverflow.com/questions/47125665,simple dask map_partitions example,,10,32888,,,
tensorflow unexpected result,https://stackoverflow.com/questions/70818269,"Tensorflow ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)","I'm trying to make a face detection model with CNN. I used codes that I made for number detection. When I use number images, program work. But, when I use my face images, I get an error that is:
Unexpected result of train_function (Empty logs). Please use Model.compile(..., run_eagerly=True), or tf.config.run_functions_eagerly(True) for more information of where went wrong, or file a issue/bug to tf.keras.
Notebook link: https://github.com/AkifCanSonmez/ImageProccessingCourse/blob/main/CNN/Number%20Classification%20Project/Building%20Model/Building%20Number%20Classification%20Model%20with%20Keras.ipynb
Number image:
Face image:
",8,33969,"Your input images have a shape of (32,32,3) whil you first conv2D layer sets the inputshape to (32,32,1). Most likely your numbers have only 1 channel since they are grayscale, while you face images have 3 color channels.
change:
model.add(tf.keras.layers.Conv2D(input_shape = (32,32,1), filters = 8, kernel_size = (5,5),activation = ""relu"", padding = ""same"" ))

to
model.add(tf.keras.layers.Conv2D(input_shape = (32,32,3), filters = 8, kernel_size = (5,5),activation = ""relu"", padding = ""same"" ))

","To me, the solution of the problem was a bad definition of the function len() of my custom data generator.
","Also ensure that you declared initialised all the parameters in the compile functions , i had the exact error when i forgot to initialise batch_size in the fit function after the compile function.
"
tensorflow unexpected result,https://stackoverflow.com/questions/38688777,TensorFlow: Graph Optimization (GPU vs CPU Performance),,8,3637,,,
tensorflow unexpected result,https://stackoverflow.com/questions/61919774,unexpected keyword argument &#39;sample_weight&#39; when sub-classing tensor-flow loss class (categorical_crossentropy) to created a weighted loss function,"Struggling to get a sub-classed loss function to work in Tensorflow (2.2.0).
Initially tried this code (which I know has worked for others - see https://github.com/keras-team/keras/issues/2115#issuecomment-530762739):



import tensorflow.keras.backend as K
from tensorflow.keras.losses import CategoricalCrossentropy


class WeightedCategoricalCrossentropy(CategoricalCrossentropy):

    def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):
        assert(cost_mat.ndim == 2)
        assert(cost_mat.shape[0] == cost_mat.shape[1])

        super().__init__(name=name, **kwargs)
        self.cost_mat = K.cast_to_floatx(cost_mat)

    def __call__(self, y_true, y_pred):

        return super().__call__(
            y_true=y_true,
            y_pred=y_pred,
            sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),
        )

def get_sample_weights(y_true, y_pred, cost_m):
    num_classes = len(cost_m)

    y_pred.shape.assert_has_rank(2)
    y_pred.shape[1].assert_is_compatible_with(num_classes)
    y_pred.shape.assert_is_compatible_with(y_true.shape)

    y_pred = K.one_hot(K.argmax(y_pred), num_classes)

    y_true_nk1 = K.expand_dims(y_true, 2)
    y_pred_n1k = K.expand_dims(y_pred, 1)
    cost_m_1kk = K.expand_dims(cost_m, 0)

    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k
    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])

    return sample_weights_n


Used as follows:

model.compile(optimizer='adam',
              loss={'simple_Class': 'categorical_crossentropy',
                    'soundClass': 'binary_crossentropy', 
                    'auxiliary_soundClass':'binary_crossentropy',
                    'auxiliary_class_training': WeightedCategoricalCrossentropy(cost_matrix), 
                    'class_training':WeightedCategoricalCrossentropy(cost_matrix)
},

              loss_weights={'simple_Class': 1.0,
                            'soundClass': 1.0, 
                            'auxiliary_soundClass':0.7,
                            'auxiliary_class_training': 0.7,
                            'class_training':0.4})



(where cost_matrix is a 2-dimensional numpy array). Training trough model.fit() with batch_size=512.
However, this results in the following error:

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-21-3428d6d8967a&gt; in &lt;module&gt;()
     82          'class_training': class_lables_test}),
     83 
---&gt; 84     epochs=nb_epoch, batch_size=batch_size, initial_epoch=initial_epoch, verbose=0, shuffle=True, callbacks=[se, tb, cm, mc, es, rs])
     85 
     86 #model.save(save_version_dir,save_format='tf')

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---&gt; 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    846                 batch_size=batch_size):
    847               callbacks.on_train_batch_begin(step)
--&gt; 848               tmp_logs = train_function(iterator)
    849               # Catch OutOfRangeError for Datasets of unknown size.
    850               # This blocks until the batch has finished executing.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    578         xla_context.Exit()
    579     else:
--&gt; 580       result = self._call(*args, **kwds)
    581 
    582     if tracing_count == self._get_tracing_count():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    625       # This is the first call of __call__, so we have to initialize.
    626       initializers = []
--&gt; 627       self._initialize(args, kwds, add_initializers_to=initializers)
    628     finally:
    629       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    504     self._concrete_stateful_fn = (
    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 506             *args, **kwds))
    507 
    508     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2444       args, kwargs = None, None
   2445     with self._lock:
-&gt; 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2447     return graph_function
   2448 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2775 
   2776       self._function_cache.missed.add(call_context_key)
-&gt; 2777       graph_function = self._create_graph_function(args, kwargs)
   2778       self._function_cache.primary[cache_key] = graph_function
   2779       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2665             arg_names=arg_names,
   2666             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2667             capture_by_value=self._capture_by_value),
   2668         self._function_attributes,
   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--&gt; 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    440         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    442     weak_wrapped_fn = weakref.ref(wrapped_fn)
    443 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, ""ag_error_metadata""):
--&gt; 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

TypeError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:533 train_step  **
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)

    TypeError: __call__() got an unexpected keyword argument 'sample_weight'


This problem is resolved when I replace the __call__() magic methods with call() and implement some of the underlying logic manually. This works, with the same usage. The __call__ method is changed to:

def call(self, y_true, y_pred):
    return super().call(y_true, y_pred) * get_sample_weights(y_true, y_pred, self.cost_mat)


i.e. we calculate a categorical cross-entropy loss on y_true and y_pred and then multiply against our weight matrix directly, rather than passing y_true, y_pred and self-cost_mat to the categorical cross-entropy call method and use the inherited method's own logic for multiplying the loss by the weights. This isn't a massive problem, as the code does work - but I can't figure out why I was unable to use the inherited class' own __call__ implementation properly (as per the original code).

Also I changed y_pred.shape[1].assert_is_compatible_with(num_classes) to assert(y_pred.shape[1] == num_classes) - this was because y_pred.shape[1] was returning an int. I have no idea why, as, inspecting y_pred, it is, of course, a tf.Tensor, and so, .shape[1] should return a tf.TesnorShape object, upon which .assert_is_compatible_with() could be called on.

This is the whole class implementation that I've used successfully. 

Note - it includes from_config and get_config methods, alongside an explicit assignment to the Keras loss namespace (last line) to enable whole-model + optimizer state saving through model.save(save_format='tf'). Some of this functionality was challenging to get working: I had to implement an explicit cast to a NumPy array (see the first line of __init__ method).

class WeightedCategoricalCrossentropy(tensorflow.keras.losses.CategoricalCrossentropy):

  def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):

    cost_mat = np.array(cost_mat)   
    ## when loading from config, self.cost_mat returns as a list, rather than an numpy array. 
    ## Adding the above line fixes this issue, enabling .ndim to call sucessfully. 
    ## However, this is probably not the best implementation

    assert(cost_mat.ndim == 2)
    assert(cost_mat.shape[0] == cost_mat.shape[1])
    super().__init__(name=name, **kwargs)
    self.cost_mat = K.cast_to_floatx(cost_mat)

  def call(self, y_true, y_pred):
    return super().call(y_true, y_pred) * get_sample_weights(y_true, y_pred, self.cost_mat)

  def get_config(self):
    config = super().get_config().copy()
    # Calling .update on the line above, during assignment, causes an error with config becoming None-type.
    config.update({'cost_mat': (self.cost_mat)})
    return config

  @classmethod
  def from_config(cls, config):
    # something goes wrong here and changes self.cost_mat to a list variable.
    # See above for temporary fix
    return cls(**config)

def get_sample_weights(y_true, y_pred, cost_m):
    num_classes = len(cost_m)

    y_pred.shape.assert_has_rank(2)
    assert(y_pred.shape[1] == num_classes)
    y_pred.shape.assert_is_compatible_with(y_true.shape)

    y_pred = K.one_hot(K.argmax(y_pred), num_classes)

    y_true_nk1 = K.expand_dims(y_true, 2)
    y_pred_n1k = K.expand_dims(y_pred, 1)
    cost_m_1kk = K.expand_dims(cost_m, 0)

    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k
    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])

    return sample_weights_n


tf.keras.losses.WeightedCategoricalCrossentropy = WeightedCategoricalCrossentropy


Finally, saving the model is implemented like so:

model.save(save_version_dir,save_format='tf')


and loading the model as follows:

model = tf.keras.models.load_model(
          save_version_dir,
          compile=True,
          custom_objects={
             'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_matrix)
              }
           )

",7,4667,,,
tensorflow unexpected result,https://stackoverflow.com/questions/56557587,Why my one-filter convolutional neural network is unable to learn a simple gaussian kernel?,,6,809,,,
tensorflow unexpected result,https://stackoverflow.com/questions/72993109," ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(, run_eagerly=True)","Ive been training an Image Classifier that can detect Oil and Pencil Paintings. While doing so I encountered a value error. I searched the web thoroughly unfortunately, I didnt find any satisfactory answer as there was not much discussion about it.
Code:
    import tensorflow as tf
    import os
    import matplotlib.pyplot as plt
    
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
    
      tf.config.experimental.set_memory_growth(gpu, True)
    len(gpus)
    
    # Data Generator
    IMAGE_SHAPE = (224, 224)
    TRAINING_DATA = '/content/drive/MyDrive/Datasets/train'
    
    
    VALID_DATA = '/content/drive/MyDrive/Datasets/test'
    
    datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    
        rescale=1./225
    
    )
    
    train_generator = datagen.flow_from_directory(
    
        TRAINING_DATA,
    
        shuffle=True,
    
        target_size=IMAGE_SHAPE,
    
    )
    
    valid_generator = datagen.flow_from_directory(
    
        VALID_DATA,
    
        shuffle=False,
    
        target_size = IMAGE_SHAPE,
    
    )
    
    def build_model(num_classes):
    
      model = tf.keras.Sequential([
    
                                   tf.keras.layers.Conv2D(filters=8, kernel_size=(3,3), activation='relu',
    
                                                          input_shape=(224, 224, 3)),
    
                                   tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=2),
    
                                   tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu'),
    
                                   tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),
    
                                   tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),
    
                                   tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),
    
                                   tf.keras.layers.Flatten(),
    
                                   tf.keras.layers.Dense(64, activation='relu'),
    
                                   tf.keras.layers.Dense(num_classes, activation='softmax')
    
                                   
    
      ])
    
      return model
    
    model = build_model(num_classes=2)
    
    model.compile(
    
        optimizer=tf.keras.optimizers.Adam(lr=0.0001),
    
        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
    
        metrics=['accuracy']
    
    )
    
    print(model.summary())

Error Code:
EPOCHS = 20

BATCH_SIZE = 32

history = model.fit(train_generator,

                    steps_per_epoch=train_generator.samples // BATCH_SIZE, 

                    epochs=EPOCHS,

                    validation_data=valid_generator,

                    validation_steps=valid_generator.samples // BATCH_SIZE,

                    verbose=1

                    )

   1393         logs = tf_utils.sync_to_numpy_or_python_type(logs)
   1394         if logs is None:
-&gt; 1395           raise ValueError('Unexpected result of `train_function` '
   1396                            '(Empty logs). Please use '
   1397                            '`Model.compile(..., run_eagerly=True)`, or '

ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

I've seen some other Stack Overflow answers where they mentioned set input_shape=3, which I already had by default in my code still I was having this error. I saw some GitHub error reports about it but there was no solution available. By the way, I'm using the latest version of Tensorflow and TensorGPU, and Keras. If someone could help me to understand what went wrong that would be awesome cuz at the moment I don't know whether it's a bug in Keras or a fault in my code.
",4,3321,"I had a similar issue a lot of times, and every time it occurred because of the wrong dataset directory. You can set a full dataset directory path when importing data, like this if you're using Mac:
'/Users/&lt;path to dataset&gt;

Or just copy path from files and paste it in the code.
",,
tensorflow unexpected result,https://stackoverflow.com/questions/66141235,Jupyter with docker: __init__() got an unexpected keyword argument &#39;column&#39;,"I recently installed TensorFlow with GPU support using docker:
docker pull tensorflow/tensorflow:latest-gpu-jupyter

But sometimes when I start a jupyter notebook server using the command:
docker run --gpus all -it -p 8888:8888 tensorflow/tensorflow:latest-gpu-jupyter jupyter notebook --notebook-dir=/tf --ip 0.0.0.0 --allow-root --NotebookApp.allow_origin='https://colab.research.google.com'

I see the following exception on the terminal:
[IPKernelApp] ERROR | Exception in message handler:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 272, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
  File ""/usr/local/lib/python3.6/dist-packages/tornado/gen.py"", line 762, in run
    value = future.result()
  File ""/usr/local/lib/python3.6/dist-packages/tornado/gen.py"", line 234, in wrapper
    yielded = ctx_run(next, result)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/gen.py"", line 162, in _fake_ctx_run
    return f(*args, **kw)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 580, in complete_request
    matches = yield gen.maybe_future(self.do_complete(code, cursor_pos))
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 348, in do_complete
    return self._experimental_do_complete(code, cursor_pos)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 373, in _experimental_do_complete
    completions = list(_rectify_completions(code, raw_completions))
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 484, in rectify_completions
    completions = list(completions)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 1818, in completions
    for c in self._completions(text, offset, _timeout=self.jedi_compute_type_timeout/1000):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 1862, in _completions
    full_text=full_text, cursor_line=cursor_line, cursor_pos=cursor_column)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 2030, in _complete
    cursor_pos, cursor_line, full_text)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/completer.py"", line 1374, in _jedi_matches
    text[:offset], namespaces, column=cursor_column, line=cursor_line + 1)
  File ""/usr/local/lib/python3.6/dist-packages/jedi/api/__init__.py"", line 726, in __init__
    project=Project(Path.cwd()), **kwds)
TypeError: __init__() got an unexpected keyword argument 'column'

After that, I have to restart the server or reconnect from google colab.
Any ideas where the error might come from and how to fix it?
",4,1687,"This seems to be an incompatibility between jedi and ipython, see this issue.
The fix would be to pin jedi to 0.17.2, so either run:
pip install jedi==0.17.2

Or if you are using poetry add this to your pyproject.toml:
jedi = ""&lt;=0.17.2""

But since you are using a docker image that image will need to be updated. It seems to be gpu-jupyter.Dockerfile.
I would raise an issue on that project and see if they can pin jedi like they did for nbformat, or you could just fork it. They should probably upgrade python as well, 3.6 is getting a bit long in the tooth.
","I will add more details to @daphtdazz's answer. I had to do the following steps to solve this issue:
1. Download TensorFlow from github:
git clone https://github.com/tensorflow/tensorflow.git

2. Edit the file gpu-jupyter.Dockerfile to add jedi==0.17.2 at the end of line 104:
vim tensorflow/tensorflow/tools/dockerfiles/dockerfiles/gpu-jupyter.Dockerfile 

3. Placed myself inside the dockerfiles folder :
cd tensorflow/tensorflow/tools/dockerfiles/

4. Build the image:
docker build -f ./dockerfiles/gpu-jupyter.Dockerfile -t tf .

",
tensorflow unexpected result,https://stackoverflow.com/questions/58156573,AMD plaidml vs CPU Tensorflow - Unexpected results,"I am currently running a simple script to train the mnist dataset.

Running the training through my CPU via Tensorflow is giving me 49us/sample and a 3e epoch using the following code:-

# CPU

import tensorflow as tf
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = tf.keras.utils.normalize(x_train, axis=1)
x_test = tf.keras.utils.normalize(x_test, axis=1)

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))

model.compile(optimizer='adam', loss=""sparse_categorical_crossentropy"", metrics=[""accuracy""])

model.fit(x_train, y_train, epochs=3)


When I run the dataset through my AMD Pro 580 using the opencl_amd_radeon_pro_580_compute_engine via plaidml setup I get the following results 249us/sample with a 15s epoch, using the following code:-

# GPU

import plaidml.keras
plaidml.keras.install_backend()
import keras
from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = keras.utils.normalize(x_train, axis=1)
x_test = keras.utils.normalize(x_test, axis=1)

model = keras.models.Sequential()
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(128, activation='relu'))
model.add(keras.layers.Dense(128, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))

model.compile(optimizer='adam', loss=""sparse_categorical_crossentropy"", metrics=[""accuracy""])

model.fit(x_train, y_train, epochs=3)


I can see my CPU firing up for the CPU test and my GPU maxing out for the GPU test, but I am very confused as to why the CPU is out performing the GPU by a factor of 5.

Should this be the expected results?

Am I doing something wrong in my code?
",4,4128,,,
tensorflow unexpected result,https://stackoverflow.com/questions/50319066,Creating a neural network in keras to multiply two input integers,"I am playing around with Keras v2.0.8 in Python v2.7 (Tensorflow backend) to create small neural networks that calculate simple arithmetic functions (add, subtract, multiply, etc.), and am a bit confused. The below code is my network which generates a random training dataset of integers with the corresponding labels (the two inputs added together):

def create_data(low, high, examples):

    train_data = []
    label_data = []

    a = np.random.randint(low=low, high=high, size=examples, dtype='int')
    b = np.random.randint(low=low, high=high, size=examples, dtype='int')

    for i in range(0, examples):
        train_data.append([a[i], b[i]])
        label_data.append((a[i] + b[i]))

    train_data = np.array(train_data)
    label_data = np.array(label_data)

    return train_data, label_data

X, y = create_data(0, 500, 10000)

model = Sequential()
model.add(Dense(3, input_dim=2))
model.add(Dense(5, activation='relu'))
model.add(Dense(3, activation='relu'))
model.add(Dense(5, activation='relu'))
model.add(Dense(1, activation='relu'))

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])
model.fit(X, y, epochs=10, batch_size=10)

test_data, _ = create_data(0, 500, 10)
results = model.predict(test_data, batch_size=2)

sq_error = []
for i in range(0, len(test_data)):
    print 'test value:', test_data[i], 'result:', results[i][0], 'error:',\
          '%.2f' %(results[i][0] - (test_data[i][0] + test_data[i][1]))
    sq_error.append((results[i][0] - (test_data[i][0] + test_data[i][1])))

print '\n total rmse error: ', sqrt(np.sum(np.array(sq_error)))


This trains perfectly well and produces no unexpected results. However, when I create the training data by multiplying the two inputs together the model's loss for each epoch stays around 7,000,000,000 and the model does not converge at all. The data creation function for this is as follows:

def create_data(low, high, examples):

    train_data = []
    label_data = []

    a = np.random.randint(low=low, high=high, size=examples, dtype='int')
    b = np.random.randint(low=low, high=high, size=examples, dtype='int')

    for i in range(0, examples):
        train_data.append([a[i], b[i]])
        label_data.append((a[i] * b[i]))

    train_data = np.array(train_data)
    label_data = np.array(label_data)

    return train_data, label_data 


I also had the same problem when I had training data of a single input integer and created the label by squaring the input data. However, it worked fine when I only multiplied the single input by a constant value or added/subtracted by a constant.

I have two questions:

1) Why is this the case? I assume it has something to do with the fundamentals of neural networks, but I can't work it out. 

2) How could I adapt this code to train a model that multiplies two input numbers together.

The network architecture (2 - 3 - 5 - 3 - 5 - 1) is fairly random right now. I've tried lots of different ones varying in layers and neurons, this one just happened to be on my screen as I write this and got an accuracy of 100% for adding two inputs.
",4,3119,,,
tensorflow unexpected result,https://stackoverflow.com/questions/68609414,How to calculate correct Cross Entropy between 2 tensors in Pytorch when target is not one-hot?,"I am confused about the calculation of cross entropy in Pytorch. If I want to calculate the cross entropy between 2 tensors and the target tensor is not a one-hot label, which loss should I use? It is quite common to calculate the cross entropy between 2 probability distributions instead of the predicted result and a determined one-hot label.
The basic loss function CrossEntropyLoss forces the target as the index integer and it is not eligible in this case. BCELoss seems to work but it gives an unexpected result. The expected formula to calculate the cross entropy is

But BCELoss calculates the BCE of each dimension, which is expressed as
-yi*log(pi)-(1-yi)*log(1-pi)

Compared with the first equation, the term -(1-yi)*log(1-pi) should not be involved. Here is an example using BCELoss and we can see the second term is involved in each dimension's result. And that make the result different from the correct one.
import torch.nn as nn
import torch
from math import log

a = torch.Tensor([0.1,0.2,0.7])
y = torch.Tensor([0.2,0.2,0.6])
L = nn.BCELoss(reduction='none')
y1 = -0.2 * log(0.1) - 0.8 * log(0.9)
print(L(a, y))
print(y1)

And the result is
tensor([0.5448, 0.5004, 0.6956])
0.5448054311250702

If we sum the results of all the dimensions, the final cross entropy doesn't correspond to the expected one. Because each one of these dimensions involves the -(1-yi)*log(1-pi) term. In constrast, Tensorflow can calculate the correct cross entropy value with CategoricalCrossentropy. Here is the example with the same setting and we can see the cross entropy is calculated in the same way as the first formula.
import tensorflow as tf
from math import log
L = tf.losses.CategoricalCrossentropy()
a = tf.convert_to_tensor([0.1,0.2,0.7])
y = tf.convert_to_tensor([0.2,0.2,0.6])
y_ = -0.2* log(0.1) - 0.2 * log(0.2) - 0.6 * log(0.7)

print(L(y,a), y_)

tf.Tensor(0.9964096, shape=(), dtype=float32) 0.9964095674488687

Is there any function can calculate the correct cross entropy in Pytorch, using the first formula, just like CategoricalCrossentropy in Tensorflow?
",3,4110,"The fundamental problem is that you are incorrectly using the BCELoss function.
Cross-entropy loss is what you want. It is used to compute the loss between two arbitrary probability distributions. Indeed, its definition is exactly the equation that you provided:

where p is the target distribution and q is your predicted distribution. See this StackOverflow post for more information.
In your example where you provide the line
y = tf.convert_to_tensor([0.2, 0.2, 0.6])

you are implicitly modeling a multi-class classification problem where the target class can be one of three classes (the length of that tensor). More specifically, that line is saying that for this one data instance, class 0 has probably 0.2, class 1 has probability 0.2, and class 2 has probability 0.6.
The problem you are having is that PyTorch's BCELoss computes the binary cross-entropy loss, which is formulated differently. Binary cross-entropy loss computes the cross-entropy for classification problems where the target class can be only 0 or 1.
In binary cross-entropy, you only need one probability, e.g. 0.2, meaning that the probability of the instance being class 1 is 0.2. Correspondingly, class 0 has probability 0.8.
If you give the same tensor [0.2, 0.2, 0.6] to BCELoss, you are modeling a situation where there are three data instances, where data instance 0 has probability 0.2 of being class 1, data instance 1 has probability 0.2 of being class 1, and data instance 2 has probability 0.6 of being class 1.
Now, to your original question:

If I want to calculate the cross entropy between 2 tensors and the target tensor is not a one-hot label, which loss should I use?

Unfortunately, PyTorch does not have a cross-entropy function that takes in two probability distributions. See this question:
https://discuss.pytorch.org/t/how-should-i-implement-cross-entropy-loss-with-continuous-target-outputs/10720
The recommendation is to implement your own function using its equation definition. Here is code that works:
def cross_entropy(input, target):
    return torch.mean(-torch.sum(target * torch.log(input), 1))


y = torch.Tensor([[0.2, 0.2, 0.6]])
yhat = torch.Tensor([[0.1, 0.2, 0.7]])
cross_entropy(yhat, y)
# tensor(0.9964)

It provides the answer that you wanted.
","Update: from version 1.10, Pytorch supports class probability targets in CrossEntropyLoss, so you can now simply use:
criterion = torch.nn.CrossEntropyLoss()
loss = criterion(x, y)

where x is the input, y is the target. When y has the same shape as x, it's gonna be treated as class probabilities. Note that x is expected to contain raw, unnormalized scores for each class, while y is expected to contain probabilities for each class (typically the output of the softmax layer). You can find details in the docs.
","Maybe you should try the torch.nn.CrossEntropyLoss function
"
tensorflow unexpected result,https://stackoverflow.com/questions/69997327,"Tensorflow: ValueError: Input 0 is incompatible with layer model: expected shape=(None, 99), found shape=(None, 3)","I am trying to predict with a ANN classification model made in Tensorflow to classify pose keypoints with MediaPipe. The mediapipe pose tracker has 33 keypoints for x y and z coordinates for a total of 99 data points.
I am training for 4 classes.
This is running the pose embedding
import mediapipe as mp
import numpy as np
import tensorflow as tf
from tensorflow import keras
mp_pose = mp.solutions.pose


def get_center_point(landmarks, left_bodypart, right_bodypart):
  """"""Calculates the center point of the two given landmarks.""""""

  left = tf.gather(landmarks, left_bodypart.value, axis=1)
  right = tf.gather(landmarks, right_bodypart.value, axis=1)
  center = left * 0.5 + right * 0.5
  return center


def get_pose_size(landmarks, torso_size_multiplier=2.5):
  """"""Calculates pose size.

  It is the maximum of two values:
    * Torso size multiplied by `torso_size_multiplier`
    * Maximum distance from pose center to any pose landmark
  """"""
  # Hips center
  hips_center = get_center_point(landmarks, mp_pose.PoseLandmark.LEFT_HIP, 
                                 mp_pose.PoseLandmark.RIGHT_HIP)

  # Shoulders center
  shoulders_center = get_center_point(landmarks,mp_pose.PoseLandmark.LEFT_SHOULDER,
                                      mp_pose.PoseLandmark.RIGHT_SHOULDER)

  # Torso size as the minimum body size
  torso_size = tf.linalg.norm(shoulders_center - hips_center)

  # Pose center
  pose_center_new = get_center_point(landmarks,mp_pose.PoseLandmark.LEFT_HIP, 
                                     mp_pose.PoseLandmark.RIGHT_HIP)
  pose_center_new = tf.expand_dims(pose_center_new, axis=1)
  # Broadcast the pose center to the same size as the landmark vector to
  # perform substraction
  pose_center_new = tf.broadcast_to(pose_center_new,
                                    [tf.size(landmarks) // (33*3), 33, 3])

  # Dist to pose center
  d = tf.gather(landmarks - pose_center_new, 0, axis=0,
                name=""dist_to_pose_center"")
  # Max dist to pose center
  max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))

  # Normalize scale
  pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)

  return pose_size


def normalize_pose_landmarks(landmarks):
  """"""Normalizes the landmarks translation by moving the pose center to (0,0) and
  scaling it to a constant pose size.
  """"""
  # Move landmarks so that the pose center becomes (0,0)
  pose_center = get_center_point(landmarks, mp_pose.PoseLandmark.LEFT_HIP, 
                                 mp_pose.PoseLandmark.RIGHT_HIP)
  pose_center = tf.expand_dims(pose_center, axis=1)
  # Broadcast the pose center to the same size as the landmark vector to perform
  # substraction
  pose_center = tf.broadcast_to(pose_center, 
                                [tf.size(landmarks) // (33*3), 33, 3])
  landmarks = landmarks - pose_center

  # Scale the landmarks to a constant pose size
  pose_size = get_pose_size(landmarks)
  landmarks /= pose_size

  return landmarks


def landmarks_to_embedding(landmarks_and_scores):
  """"""Converts the input landmarks into a pose embedding.""""""
  # Reshape the flat input into a matrix with shape=(33, 3)
  reshaped_inputs = keras.layers.Reshape((33, 3))(landmarks_and_scores)

  # Normalize landmarks 3D
  landmarks = normalize_pose_landmarks(reshaped_inputs[:, :, :3])

  # Flatten the normalized landmark coordinates into a vector
  embedding = keras.layers.Flatten()(landmarks)

  return embedding

Then I create the model and feed the embedding inputs to it
import csv
import cv2
import itertools
import numpy as np
import pandas as pd
import os
import sys
import tempfile
import tqdm
import mediapipe as mp
from matplotlib import pyplot as plt
from matplotlib.collections import LineCollection
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from poseEmbedding import get_center_point, get_pose_size, normalize_pose_landmarks, landmarks_to_embedding


def load_pose_landmarks(csv_path):
    #load CSV file
    dataframe = pd.read_csv(csv_path)
    df_to_process = dataframe.copy()
    
    #extract the list of class names
    classes = df_to_process.pop('class_name').unique()
    
    #extract the labels
    y = df_to_process.pop('class_no')
    
    #convert the input features and labels into float64 format for training
    X = df_to_process.astype('float64')
    y = keras.utils.to_categorical(y)
    
    return X,y, classes, dataframe
csvs_out_train_path = 'train_data.csv'
csvs_out_test_path = 'test_data.csv'

#Load training data

X, y, class_names, _ = load_pose_landmarks(csvs_out_train_path)

#split training data(X,y) into (X_train, y_train) and (X_val, y_val)
X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.15)

X_test, y_test, _, df_test = load_pose_landmarks(csvs_out_test_path)

mp_pose = mp.solutions.pose

inputs = tf.keras.Input(shape=(99))
embedding = landmarks_to_embedding(inputs)

layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)
layer = keras.layers.Dropout(0.5)(layer)
layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)
layer = keras.layers.Dropout(0.5)(layer)
outputs = keras.layers.Dense(4, activation=""softmax"")(layer)

model = keras.Model(inputs, outputs)
#model.summary()


model.compile(
    optimizer = 'adam',
    loss = 'categorical_crossentropy',
    metrics=['accuracy']
)




# Start training
history = model.fit(X_train, y_train,
                    epochs=200,
                    batch_size=16,
                    validation_data=(X_val, y_val))
model.save(""complete_epoch_model"")
                    
# Visualize the training history to see whether you're overfitting.
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['TRAIN', 'VAL'], loc='lower right')
plt.show()
loss, accuracy = model.evaluate(X_test, y_test)

The model summary prints this out:
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_18 (InputLayer)          [(None, 99)]         0           []                               
                                                                                                  
 reshape_17 (Reshape)           (None, 33, 3)        0           ['input_18[0][0]']               
                                                                                                  
 tf.__operators__.getitem_10 (S  (None, 33, 3)       0           ['reshape_17[0][0]']             
 licingOpLambda)                                                                                  
                                                                                                  
 tf.compat.v1.gather_69 (TFOpLa  (None, 3)           0           ['tf.__operators__.getitem_10[0][
 mbda)                                                           0]']                             
                                                                                                  
 tf.compat.v1.gather_70 (TFOpLa  (None, 3)           0           ['tf.__operators__.getitem_10[0][
 mbda)                                                           0]']                             
                                                                                                  
 tf.math.multiply_69 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_69[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.math.multiply_70 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_70[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.__operators__.add_31 (TFOpL  (None, 3)           0           ['tf.math.multiply_69[0][0]',    
 ambda)                                                           'tf.math.multiply_70[0][0]']    
                                                                                                  
 tf.compat.v1.size_17 (TFOpLamb  ()                  0           ['tf.__operators__.getitem_10[0][
 da)                                                             0]']                             
                                                                                                  
 tf.expand_dims_17 (TFOpLambda)  (None, 1, 3)        0           ['tf.__operators__.add_31[0][0]']
                                                                                                  
 tf.compat.v1.floor_div_17 (TFO  ()                  0           ['tf.compat.v1.size_17[0][0]']   
 pLambda)                                                                                         
                                                                                                  
 tf.broadcast_to_17 (TFOpLambda  (None, 33, 3)       0           ['tf.expand_dims_17[0][0]',      
 )                                                                'tf.compat.v1.floor_div_17[0][0]
                                                                 ']                               
                                                                                                  
 tf.math.subtract_23 (TFOpLambd  (None, 33, 3)       0           ['tf.__operators__.getitem_10[0][
 a)                                                              0]',                             
                                                                  'tf.broadcast_to_17[0][0]']     
                                                                                                  
 tf.compat.v1.gather_75 (TFOpLa  (None, 3)           0           ['tf.math.subtract_23[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.compat.v1.gather_76 (TFOpLa  (None, 3)           0           ['tf.math.subtract_23[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.math.multiply_75 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_75[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.math.multiply_76 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_76[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.__operators__.add_34 (TFOpL  (None, 3)           0           ['tf.math.multiply_75[0][0]',    
 ambda)                                                           'tf.math.multiply_76[0][0]']    
                                                                                                  
 tf.compat.v1.size_18 (TFOpLamb  ()                  0           ['tf.math.subtract_23[0][0]']    
 da)                                                                                              
                                                                                                  
 tf.compat.v1.gather_73 (TFOpLa  (None, 3)           0           ['tf.math.subtract_23[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.compat.v1.gather_74 (TFOpLa  (None, 3)           0           ['tf.math.subtract_23[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.compat.v1.gather_71 (TFOpLa  (None, 3)           0           ['tf.math.subtract_23[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.compat.v1.gather_72 (TFOpLa  (None, 3)           0           ['tf.math.subtract_23[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.expand_dims_18 (TFOpLambda)  (None, 1, 3)        0           ['tf.__operators__.add_34[0][0]']
                                                                                                  
 tf.compat.v1.floor_div_18 (TFO  ()                  0           ['tf.compat.v1.size_18[0][0]']   
 pLambda)                                                                                         
                                                                                                  
 tf.math.multiply_73 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_73[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.math.multiply_74 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_74[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.math.multiply_71 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_71[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.math.multiply_72 (TFOpLambd  (None, 3)           0           ['tf.compat.v1.gather_72[0][0]'] 
 a)                                                                                               
                                                                                                  
 tf.broadcast_to_18 (TFOpLambda  (None, 33, 3)       0           ['tf.expand_dims_18[0][0]',      
 )                                                                'tf.compat.v1.floor_div_18[0][0]
                                                                 ']                               
                                                                                                  
 tf.__operators__.add_33 (TFOpL  (None, 3)           0           ['tf.math.multiply_73[0][0]',    
 ambda)                                                           'tf.math.multiply_74[0][0]']    
                                                                                                  
 tf.__operators__.add_32 (TFOpL  (None, 3)           0           ['tf.math.multiply_71[0][0]',    
 ambda)                                                           'tf.math.multiply_72[0][0]']    
                                                                                                  
 tf.math.subtract_25 (TFOpLambd  (None, 33, 3)       0           ['tf.math.subtract_23[0][0]',    
 a)                                                               'tf.broadcast_to_18[0][0]']     
                                                                                                  
 tf.math.subtract_24 (TFOpLambd  (None, 3)           0           ['tf.__operators__.add_33[0][0]',
 a)                                                               'tf.__operators__.add_32[0][0]']
                                                                                                  
 tf.compat.v1.gather_77 (TFOpLa  (33, 3)             0           ['tf.math.subtract_25[0][0]']    
 mbda)                                                                                            
                                                                                                  
 tf.compat.v1.norm_14 (TFOpLamb  ()                  0           ['tf.math.subtract_24[0][0]']    
 da)                                                                                              
                                                                                                  
 tf.compat.v1.norm_15 (TFOpLamb  (3,)                0           ['tf.compat.v1.gather_77[0][0]'] 
 da)                                                                                              
                                                                                                  
 tf.math.multiply_77 (TFOpLambd  ()                  0           ['tf.compat.v1.norm_14[0][0]']   
 a)                                                                                               
                                                                                                  
 tf.math.reduce_max_7 (TFOpLamb  ()                  0           ['tf.compat.v1.norm_15[0][0]']   
 da)                                                                                              
                                                                                                  
 tf.math.maximum_7 (TFOpLambda)  ()                  0           ['tf.math.multiply_77[0][0]',    
                                                                  'tf.math.reduce_max_7[0][0]']   
                                                                                                  
 tf.math.truediv_7 (TFOpLambda)  (None, 33, 3)       0           ['tf.math.subtract_23[0][0]',    
                                                                  'tf.math.maximum_7[0][0]']      
                                                                                                  
 flatten_7 (Flatten)            (None, 99)           0           ['tf.math.truediv_7[0][0]']      
                                                                                                  
 dense_21 (Dense)               (None, 128)          12800       ['flatten_7[0][0]']              
                                                                                                  
 dropout_14 (Dropout)           (None, 128)          0           ['dense_21[0][0]']               
                                                                                                  
 dense_22 (Dense)               (None, 64)           8256        ['dropout_14[0][0]']             
                                                                                                  
 dropout_15 (Dropout)           (None, 64)           0           ['dense_22[0][0]']               
                                                                                                  
 dense_23 (Dense)               (None, 4)            260         ['dropout_15[0][0]']             
                                                                                                  
==================================================================================================
Total params: 21,316
Trainable params: 21,316
Non-trainable params: 0
__________________________________________________________________________________________________

Now when I try to run inference on my webcam, I get the following error from mediapipe and Tensorflow:
ValueError: Input 0 is incompatible with layer model: expected shape=(None, 99), found shape=(None, 3)

I am not sure how to fix this error as I could only train with shape of 99 as TF was giving me errors for using a shape of 3 when trying to compile. How do I fix this?
This is my inference code:
import cv2
import os
import tqdm
import numpy as np
import logging
from mediapipe.python.solutions import pose as mp_pose
from mediapipe.python.solutions import drawing_utils as mp_drawing
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import backend as K
from tensorflow.keras.utils import CustomObjectScope


def relu6(x):
  return K.relu(x, max_value=6)

logging.getLogger().setLevel(logging.CRITICAL)



cap = cv2.VideoCapture(0)

model = tf.keras.models.load_model('weights_best.hdf5', compile = True,
        custom_objects = {""relu6"": relu6})


with mp_pose.Pose() as pose_tracker:
  while cap.isOpened():
    # Get next frame of the video.
    ret, frame = cap.read()


    # Run pose tracker.
    imagefirst = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    image = cv2.flip(imagefirst,1)

    result = pose_tracker.process(image)
    pose_landmarks = result.pose_landmarks

    # Draw pose prediction.
    if pose_landmarks is not None:
      mp_drawing.draw_landmarks(
          image,
          landmark_list=pose_landmarks,
          connections=mp_pose.POSE_CONNECTIONS)

    if pose_landmarks is not None:
      # Get landmarks.
      frame_height, frame_width = frame.shape[0], frame.shape[1]
      pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]
                                 for lmk in pose_landmarks.landmark], dtype=np.float32)
      assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)
      prediction = model.predict(pose_landmarks)



    # Save the output frame.
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

    cv2.imshow('Raw Webcam Feed', image)
    if cv2.waitKey(10) &amp; 0xFF == ord('q'):
      break

# Close output video.
cap.release()
cv2.destroyAllWindows()

# Release MediaPipe resources.
pose_tracker.close()

",2,3405,"Maybe try changing the shape of pose_landmarks from (33, 3) to (1, 99) after your assertion and before you make a prediction:
import tensorflow as tf

pose_landmarks = tf.random.normal((33, 3))
assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)

pose_landmarks = tf.expand_dims(pose_landmarks, axis=0)
shape = tf.shape(pose_landmarks)
pose_landmarks = tf.reshape(pose_landmarks, (shape[0], shape[1] * shape[2]))

tf.print(pose_landmarks.shape)

TensorShape([1, 99])

",,
tensorflow unexpected result,https://stackoverflow.com/questions/62518906,Keras ImageDataGenerator : how to use data augmentation with images paths,"I am working on a CNN model and I would like to use some data augmentation, but two problems arise :

My labels are images (my model is some kind of autoencoder, but the expected output images are different from my input images), thus I cannot use functions such as ImageDataGenerator.flow_from_directory(). I was thinking of ImageDataGenerator.flow(train_list, y = labels_list), but there comes my second issue :
Both my input and labels datasets being really huge, I'd prefer working with images paths (which are not handled correctly by the flow() function) rather than loading all my dataset in a single array and making my RAM explode.

How can I properly deal with these two issues? For what I've found, there might be two solutions :

Create my own generator : I've heard of the Keras __getitem__ function in the Sequence class, but can it impact the ImageDataGenerator class?
Work with TF DATA or TFRecords, but they seem pretty difficult to use, and the data augmentation is still to be implemented.

Is there an easiest way to overcome this simple problem? A mere trick would be to force ImageDataGenerator.flow() to work with a nparray of images paths rather than a nparray of images, but I fear that modifying the Keras/tensorflow files will have unexpected consequences (as some functions are called in other classes, a local change can soon result in a global change in all of my notebook library).
",2,2679,,,
tensorflow unexpected result,https://stackoverflow.com/questions/59970309,OpenVino Model Optimizer Error when converting TensorFlow model,"I have created a custom image classification .pb model file using the python scripts in the TensorFlow for Poets 2 repo (https://github.com/googlecodelabs/tensorflow-for-poets-2). 

I tried converting it to Intermediate Representation using the OpenVino Model Optimizer using the below scripts:

python mo_tf.py --input_model retrained_graph.pb

python mo_tf.py --input_model retrained_graph.pb --mean_values [127.5,127.5,127.5] --input Mul

In both cases this is what happened:

Model Optimizer arguments:
Common parameters:
        - Path to the Input Model:      C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\retrained_graph.pb
        - Path for generated IR:        C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\.
        - IR output name:       retrained_graph
        - Log level:    ERROR
        - Batch:        Not specified, inherited from the model
        - Input layers:         Not specified, inherited from the model
        - Output layers:        Not specified, inherited from the model
        - Input shapes:         Not specified, inherited from the model
        - Mean values:  Not specified
        - Scale values:         Not specified
        - Scale factor:         Not specified
        - Precision of IR:      FP32
        - Enable fusing:        True
        - Enable grouped convolutions fusing:   True
        - Move mean values to preprocess section:       False
        - Reverse input channels:       False
TensorFlow specific parameters:
        - Input model in text protobuf format:  False
        - Path to model dump for TensorBoard:   None
        - List of shared libraries with TensorFlow custom layers implementation:        None
        - Update the configuration file with input/output node names:   None
        - Use configuration file used to generate the model with Object Detection API:  None
        - Operations to offload:        None
        - Patterns to offload:  None
        - Use the config file:  None
Model Optimizer version:        2019.3.0-408-gac8584cb7
[ ERROR ]  -------------------------------------------------
[ ERROR ]  ----------------- INTERNAL ERROR ----------------
[ ERROR ]  Unexpected exception happened.
[ ERROR ]  Please contact Model Optimizer developers and forward the following information:
[ ERROR ]  local variable 'new_attrs' referenced before assignment
[ ERROR ]  Traceback (most recent call last):
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\extractor.py"", line 608, in extract_node_attrs
    supported, new_attrs = extractor(Node(graph, node))
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\pipeline\tf.py"", line 132, in &lt;lambda&gt;
    extract_node_attrs(graph, lambda node: tf_op_extractor(node, check_for_duplicates(tf_op_extractors)))
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\tf\extractor.py"", line 109, in tf_op_extractor
    attrs = tf_op_extractors[op](node)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\tf\extractor.py"", line 65, in &lt;lambda&gt;
    return lambda node: pb_extractor(node.pb)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\tf\extractors\const.py"", line 31, in tf_const_ext
    result['value'] = tf_tensor_content(pb_tensor.dtype, result['shape'], pb_tensor)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\tf\extractors\utils.py"", line 76, in tf_tensor_content
    dtype=type_helper[0]),
UnicodeDecodeError: 'ascii' codec can't decode byte 0xff in position 0: ordinal not in range(128)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\main.py"", line 298, in main
    return driver(argv)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\main.py"", line 247, in driver
    is_binary=not argv.input_model_is_text)
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\pipeline\tf.py"", line 132, in tf2nx
    extract_node_attrs(graph, lambda node: tf_op_extractor(node, check_for_duplicates(tf_op_extractors)))
  File ""C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\model_optimizer\mo\front\extractor.py"", line 614, in extract_node_attrs
    new_attrs['name'] if 'name' in new_attrs else '&lt;UNKNOWN&gt;',
UnboundLocalError: local variable 'new_attrs' referenced before assignment

[ ERROR ]  ---------------- END OF BUG REPORT --------------
[ ERROR ]  ------------------------------------------------- 


Does anyone know how to fix it?
",2,1018,,,
tensorflow unexpected result,https://stackoverflow.com/questions/55669722,Operation ParseExample not supported while converting SavedModel to TFLite,,2,530,,,
tensorflow unexpected result,https://stackoverflow.com/questions/55114665,&lt;Tensorflow object detection&gt;TypeError: resize_images() got an unexpected keyword argument &#39;preserve_aspect_ratio&#39;,,2,3406,,,
tensorflow unexpected result,https://stackoverflow.com/questions/51027357,"Prediction failed: Error processing input: Expected string, got dict","I've completed the starting tutorial for TensorFlow (https://www.tensorflow.org/get_started/get_started_for_beginners) and made some minor changes to the code to adapt it to my application. The feature columns for my case are the following:

transaction_column = tf.feature_column.categorical_column_with_vocabulary_list(key='Transaction', vocabulary_list=[""buy"", ""rent""])
localization_column = tf.feature_column.categorical_column_with_vocabulary_list(key='Localization', vocabulary_list=[""barcelona"", ""girona""])
dimensions_feature_column = tf.feature_column.numeric_column(""Dimensions"")
buy_price_feature_column = tf.feature_column.numeric_column(""BuyPrice"")
rent_price_feature_column = tf.feature_column.numeric_column(""RentPrice"")

my_feature_columns = [
    tf.feature_column.indicator_column(transaction_column),
    tf.feature_column.indicator_column(localization_column),
    tf.feature_column.bucketized_column(source_column = dimensions_feature_column,
                                        boundaries = [50, 75, 100]),
    tf.feature_column.numeric_column(key='Rooms'),
    tf.feature_column.numeric_column(key='Toilets'),
    tf.feature_column.bucketized_column(source_column = buy_price_feature_column,
                                        boundaries = [1, 180000, 200000, 225000, 250000, 275000, 300000]),
    tf.feature_column.bucketized_column(source_column = rent_price_feature_column,
                                        boundaries = [1, 700, 1000, 1300])
]


After that, I've saved the model so that it can be used in Cloud ML Engine to do predictions.
To export the model I've added the following code (after evaluating the model):

feature_spec = tf.feature_column.make_parse_example_spec(my_feature_columns)
export_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)
servable_model_dir = ""modeloutput""
servable_model_path = classifier.export_savedmodel(servable_model_dir, export_input_fn)


After running the code I get the proper model files in my ""modeloutput"" directory and I create the model in the Cloud (as explained in https://cloud.google.com/ml-engine/docs/tensorflow/getting-started-training-prediction#deploy_a_model_to_support_prediction, ""Deploy a model to support prediction"")

Once the model version is created I simply try to launch an online prediction using this model with the following command on the Cloud Shell:

gcloud ml-engine predict --model $MODEL_NAME --version v1 --json-instances ../prediction.json


Where $MODEL_NAME is my model's name and prediction.json is a JSON file with the following content:

{""inputs"":[
  {
     ""Transaction"":""rent"",
     ""Localization"":""girona"",
     ""Dimensions"":90,
     ""Rooms"":4,
     ""Toilets"":2,
     ""BuyPrice"":0,
     ""RentPrice"":1100
  }
  ]
}


However, the prediction fails and I get the following error message:


  ""error"": ""Prediction failed: Error processing input: Expected string, got {u'BuyPrice': 0, u'Transaction': u'rent', u'Rooms': 4, u'Localization': u'girona', u'Toilets': 2, u'RentPrice': 1100, u'Dimensions': 90} of type 'dict' instead.""


The error is clear, a string is expected instead of a dictionary. If I check my SavedModel SignatureDef I get the following information:

The given SavedModel SignatureDef contains the following input(s):
inputs['inputs'] tensor_info:
  dtype: DT_STRING
  shape: (-1)
  name: input_example_tensor:0
The given SavedModel SignatureDef contains the following output(s):
outputs['classes'] tensor_info:
  dtype: DT_STRING
  shape: (-1, 12)
  name: dnn/head/Tile:0
outputs['scores'] tensor_info:
  dtype: DT_FLOAT
  shape: (-1, 12)
  name: dnn/head/predictions/probabilities:0
Method name is: tensorflow/serving/classify


It's clear that the dtype expected for the input is a string (DT_STRING) but I don't know how to format my input data so that the prediction succeeds. I've tried to write the input JSON in many different ways but I keep getting errors.
If I look at how the prediction is performed in the tutorial (https://www.tensorflow.org/get_started/get_started_for_beginners), I think it's clear that the predict input is passed as a dictionary (predict_x in the tutorial code).

So, where am I wrong? How can I make the prediction with this input data?

Thank you for your time.

EDIT BASED ON ANSWER ------

Following the second suggestion by @Lak, I've updated the code to export the model so it now looks like this:

export_input_fn = serving_input_fn
servable_model_dir = ""savedmodeloutput""
servable_model_path = classifier.export_savedmodel(servable_model_dir, 
 export_input_fn)
...

def serving_input_fn():
feature_placeholders = {
    'Transaction': tf.placeholder(tf.string, [None]),
    'Localization': tf.placeholder(tf.string, [None]),
    'Dimensions': tf.placeholder(tf.float32, [None]),
    'Rooms': tf.placeholder(tf.int32, [None]),
    'Toilets': tf.placeholder(tf.int32, [None]),
    'BuyPrice': tf.placeholder(tf.float32, [None]),
    'RentPrice': tf.placeholder(tf.float32, [None])
    }
features = {
    key: tf.expand_dims(tensor, -1)
    for key, tensor in feature_placeholders.items()
}
return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)


After that, I've created a new model and fed it the following JSON to get the prediction:

{
   ""Transaction"":""rent"",
   ""Localization"":""girona"",
   ""Dimensions"":90.0,
   ""Rooms"":4,
   ""Toilets"":2,
   ""BuyPrice"":0.0,
   ""RentPrice"":1100.0
}


Note that I removed ""inputs"" from the JSON structure as I received the error ""Unexpected tensor name: inputs"" when doing the prediction. However, now I get a new, and uglier error:


  ""error"": ""Prediction failed: Error during model execution: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\""NodeDef mentions attr 'T' not in Op index:int64&gt;; NodeDef: dnn/input_from_feature_columns/input_layer/Transaction_indicator/to_sparse_input/indices = WhereT=DT_BOOL, _output_shapes=[[?,2]], _device=\""/job:localhost/replica:0/task:0/device:CPU:0\"". (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n\t [[Node: dnn/input_from_feature_columns/input_layer/Transaction_indicator/to_sparse_input/indices = WhereT=DT_BOOL, _output_shapes=[[?,2]], _device=\""/job:localhost/replica:0/task:0/device:CPU:0\""]]\"")""


I've checked again the SignatureDef and I get the following information:

The given SavedModel SignatureDef contains the following input(s):
  inputs['Toilets'] tensor_info:
      dtype: DT_INT32
      shape: (-1)
      name: Placeholder_4:0
  inputs['Rooms'] tensor_info:
      dtype: DT_INT32
      shape: (-1)
      name: Placeholder_3:0
  inputs['Localization'] tensor_info:
      dtype: DT_STRING
      shape: (-1)
      name: Placeholder_1:0
  inputs['RentPrice'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1)
      name: Placeholder_6:0
  inputs['BuyPrice'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1)
      name: Placeholder_5:0
  inputs['Dimensions'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1)
      name: Placeholder_2:0
  inputs['Transaction'] tensor_info:
      dtype: DT_STRING
      shape: (-1)
      name: Placeholder:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['class_ids'] tensor_info:
      dtype: DT_INT64
      shape: (-1, 1)
      name: dnn/head/predictions/ExpandDims:0
  outputs['classes'] tensor_info:
      dtype: DT_STRING
      shape: (-1, 1)
      name: dnn/head/predictions/str_classes:0
  outputs['logits'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 12)
      name: dnn/logits/BiasAdd:0
  outputs['probabilities'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 12)
      name: dnn/head/predictions/probabilities:0
Method name is: tensorflow/serving/predict


Have I gone wrong in some of the steps? Thanks!

NEW UPDATE

I've run a local prediction and it has been succesfully executed, receiving the expected prediction result. Command used:

gcloud ml-engine local predict --model-dir $MODEL_DIR --json-instances=../prediction.json


where MODEL_DIR is the directory containing the files generated on the model training.
So the problem seems to be in exporting the model. Somehow the model exported and used later for prediction is not correct. I've read something about TensorFlow versions possibly being the origin of the problem, but I don't understand. Isn't my whole code executed with the same TF version?
Any ideas regarding this point?

Thanks!
",2,1242,,,
tensorflow unexpected result,https://stackoverflow.com/questions/49555489,tf.py_func() unexpected outputs in loop using lambda function,,2,452,,,
tensorflow unexpected result,https://stackoverflow.com/questions/60173337,How to manipulate and return tf.Variable using a for loop over tf.data.Dataset inside function decorated with @tf.function?,,1,346,,,
tensorflow unexpected result,https://stackoverflow.com/questions/75728844,TypeError: VariableMetaclass._variable_v1_call() got an unexpected keyword argument &#39;experimental_enable_variable_lifting&#39;,"I am getting this TypeError while creating a keras model from TensorFlow inside the __init__ method of a custom class, any ideas?
My code:
self.model = keras.Sequential([
    keras.layers.Dense(1, input_dim=self.degree),
    keras.layers.Dense(1)
    ])
self.model.compile(optimizer=optimizer, loss=loss)
self.model.summary()

Error:
self.model = keras.Sequential([
                 ^^^^^^^^^^^^^^^^^^
  File ""C:\Python311\Lib\site-packages\tensorflow\python\trackable\base.py"", line 205, in _method_wrapper
    result = method(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Python311\Lib\site-packages\keras\utils\traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Python311\Lib\site-packages\tensorflow\python\ops\variables.py"", line 285, in __call__
    return cls._variable_v1_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: VariableMetaclass._variable_v1_call() got an unexpected keyword argument 'experimental_enable_variable_lifting'

Tensorflow version information:
Name: tensorflow
Version: 2.12.0rc1
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: C:\Python311\Lib\site-packages
Requires: tensorflow-intel
Required-by:

",1,1482,"I was facing the same issue. Have you tried installing keras 2.12.0rc1?
I tried it here (pip install keras==2.12.0rc1 --user) and it seems to be working now.
","Seems like the TensorFlow upgrade to version 2.12.0 from 2.12.0-rc1 fixed the issue.
",
tensorflow unexpected result,https://stackoverflow.com/questions/72499842,How would I convert this TensorFlow image classification model to Core ML?,"Im learning TensorFlow and want to convert an image classification model to Core ML for use in an iOS app.
This TensorFlow image classification tutorial is a close match to what I want to do for the training, but I havent been able to figure out how to convert that to Core ML.
Heres what Ive tried, adding the following to the end of the Colab notebook for the tutorial:
# install coremltools
!pip install coremltools

# import coremltools
import coremltools as ct

# define the input type
image_input = ct.ImageType()

# create classifier configuration with the class labels
classifier_config = ct.ClassifierConfig(class_names)

# perform the conversion
coreml_model = ct.convert(
    model, inputs=[image_input], classifier_config=classifier_config,
)

# print info about the converted model
print(coreml_model)

# save the file
coreml_model.save('my_coreml_model')

That successfully creates an mlmodel file, but when I download the file and open it in Xcode to test it (under the Preview tab) it shows results like Roses 900% Confidence and Tulips 1,120% Confidence. For my uses, the confidence percentage needs to be from 0 to 100%, so I think Im missing some parameter for the conversion.
On import coremltools as ct I do get some warnings like WARNING:root:TensorFlow version 2.8.2 has not been tested with coremltools. You may run into unexpected errors. but Im guessing thats not the problem since the conversion doesnt report any errors.
Based on information here, Ive also tried setting a scale on the image input:
image_input = ct.ImageType(scale=1/255.0)

 but that made things worse as it then has around 315% confidence that every image is a dandelion. A few other attempts at setting a scale / bias all resulted in the same thing.
At this point Im not sure what else to try. Any help is appreciated!
",1,436,"The last layer of your model should be something like this:
layers.Dense(num_classes, activation='softmax')

The softmax function transforms your output into the probabilities you need.
",,
tensorflow unexpected result,https://stackoverflow.com/questions/70998085,How to reuse variables in tensorflow 2.0?,"When using tensorflow 2.0, I find something weird about tf.Variable? There are two cases bellow.
The first one
x1 = tf.Variable(12., name='x')
x2 = tf.Variable(12., name='x')
print(x1 is x2)
x1.assign(1.)
print(x1)
print(x2)

The output is
False
&lt;tf.Variable 'x:0' shape=() dtype=float32, numpy=1.0&gt;
&lt;tf.Variable 'x:0' shape=() dtype=float32, numpy=12.0&gt;

which means variables with the same name don't share the same memory.
The second one
x = tf.Variable(12., name='x')
print(x)
y = x.assign(5.)
print(y)
print(x is y)

x.assign(3.)
print(x)
print(y)

The output is
&lt;tf.Variable 'x:0' shape=() dtype=float32, numpy=12.0&gt;
&lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=5.0&gt;
False
&lt;tf.Variable 'x:0' shape=() dtype=float32, numpy=3.0&gt;
&lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=3.0&gt;

The result is unexpected, variables x and y with different names share the same memory, but id(x) is not equal to id(y).
Therefore, the name of variable can't distinguish whether variables are identical(share the same memory). And how can I reuse variables in tensorflow 2.0, like with tf.variable_scope(""scope"", reuse=True) tf.get_variable(...) in tensorflow 1.0?
",1,308,"Quoted from your question:

The result is unexpected, variables x and y with different names share the same memory, but id(x) is not equal to id(y).

No, this is incorrect. From the docs of tf.Variable.assign, where read_value is default to True:

read_value: if True, will return something which evaluates to the new value of the variable; if False will return the assign op.

Here ""something"" should be a new operation, which isn't x, but is evaluated to the value of x.
To reuse x, just access x:
y = x
print(y is x) # True

Finally, regarding:

which means variables with the same name don't share the same memory.


Therefore, the name of variable can't distinguish whether [...]

You have to create different(thus distinguishable) names yourself, regarding your first example. You might want to take a look at the comments of this accepted answer https://stackoverflow.com/a/73024334/5290519
",,
tensorflow unexpected result,https://stackoverflow.com/questions/70915345,Why did I get unexpected field names: [&#39;is_dynamic_op&#39;]?,"I am working on a low light video processing project where I am getting some errors in some areas,
For this code.
params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(
    precision_mode='FP16',
    is_dynamic_op = True)

I am getting this error.
&gt; --------------------------------------------------------------------------- ValueError                                Traceback (most recent call
&gt; last) &lt;ipython-input-8-326230ed5373&gt; in &lt;module&gt;()
&gt;       2 params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(
&gt;       3     precision_mode='FP16',
&gt; ----&gt; 4     is_dynamic_op = True)
&gt;       5 
&gt;       6 # Convert the model
&gt; 
&gt; /usr/lib/python3.7/collections/__init__.py in _replace(_self, **kwds)
&gt;     414         result = _self._make(map(kwds.pop, field_names, _self))
&gt;     415         if kwds:
&gt; --&gt; 416             raise ValueError(f'Got unexpected field names: {list(kwds)!r}')
&gt;     417         return result
&gt;     418 
&gt; 
&gt; ValueError: Got unexpected field names: ['is_dynamic_op']

I have used these libraries,
from glob import glob
from PIL import Image
from matplotlib import pyplot as plt
from mirnet.inference import Inferer
from mirnet.utils import download_dataset, plot_result
from tensorflow.python.compiler.tensorrt import trt_convert as trt

import tensorflow as tf
import numpy as np
import time

I have imported all the libraries but am still stuck.
",1,304,"Firstly, it seems that trt.DEFAULT_TRT_CONVERSION_PARAMS doesn't have the field 'is_dynamic_op'. My guess would be that, some documentation exists for the library, and there you can probably see whether the field is settable or not.
Secondly, it seems you are using TensorFlow and the TensorRT API. (but again, it's a guess...). In the source-code of the library, they use ""is_dynamic_op"" on some ""rewrite_config"". Maybe this helps you reformat your code to work: https://github.com/tensorflow/tensorflow/blob/dd38449b8ac3fd9ffaa98349d39d36ec26e72dfe/tensorflow/python/compiler/tensorrt/trt_convert.py#L1136
",,
tensorflow unexpected result,https://stackoverflow.com/questions/65883675,Keras raises an exception on loading a lambda layer,"Saving and loading a model in TensorFlow 2.3.1...
import tensorflow as tf

model1 = tf.keras.Sequential([
    tf.keras.layers.Input(shape = (81,), dtype = 'uint8'), 
    tf.keras.layers.Lambda(tf.keras.backend.one_hot, arguments={'num_classes': 10}, output_shape=(81, 10)),
])

tf.keras.models.save_model(model1, './model')
model2 = tf.keras.models.load_model('./model')
model2.summary()

causes a rather long exception, quoted in full below. It seems to be the Lambda layer that causes issues; save-loading works fine without it. I've tried adding custom_objects={'one_hot' : tf.keras.backend.one_hot} and suchlike to the load call, but it doesn't fix it. I'd be very grateful if anyone had a workaround for this.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in wrapper(*args, **kwargs)
    200     try:
--&gt; 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):

TypeError: 'str' object is not callable

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-20-77668ee9d9b2&gt; in &lt;module&gt;
      7 
      8 tf.keras.models.save_model(model1, './model')
----&gt; 9 model2 = tf.keras.models.load_model('./model', custom_objects={'tf.keras.backend.one_hot' : tf.keras.backend.one_hot})
     10 model2.summary()

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile, options)
    185     if isinstance(filepath, six.string_types):
    186       loader_impl.parse_saved_model(filepath)
--&gt; 187       return saved_model_load.load(filepath, compile, options)
    188 
    189   raise IOError(

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in load(path, compile, options)
    119 
    120   model = tf_load.load_internal(
--&gt; 121       path, options=options, loader_cls=KerasObjectLoader)
    122 
    123   # pylint: disable=protected-access

/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load_internal(export_dir, tags, options, loader_cls)
    631       try:
    632         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,
--&gt; 633                             ckpt_options)
    634       except errors.NotFoundError as err:
    635         raise FileNotFoundError(

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in __init__(self, *args, **kwargs)
    192     self._models_to_reconstruct = []
    193 
--&gt; 194     super(KerasObjectLoader, self).__init__(*args, **kwargs)
    195 
    196     # Now that the node object has been fully loaded, and the checkpoint has

/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in __init__(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options)
    128       self._concrete_functions[name] = _WrapperFunction(concrete_function)
    129 
--&gt; 130     self._load_all()
    131     self._restore_checkpoint()
    132 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _load_all(self)
    219 
    220     # Finish setting up layers and models. See function docstring for more info.
--&gt; 221     self._finalize_objects()
    222 
    223   @property

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _finalize_objects(self)
    528 
    529     # Initialize graph networks, now that layer dependencies have been resolved.
--&gt; 530     self._reconstruct_all_models()
    531 
    532   def _unblock_model_reconstruction(self, layer_id, layer):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _reconstruct_all_models(self)
    546       all_initialized_models.add(model_id)
    547       model, layers = self.model_layer_dependencies[model_id]
--&gt; 548       self._reconstruct_model(model_id, model, layers)
    549       self._add_object_graph_edges(self._proto.nodes[model_id], model_id)
    550       _finalize_config_layers([model])

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _reconstruct_model(self, model_id, model, layers)
    576               dtype=layers[0].dtype,
    577               name=layers[0].name + '_input'))
--&gt; 578       model.__init__(layers, name=config['name'])
    579       if not model.inputs:
    580         first_layer = self._get_child_layer_node_ids(model_id, model.name)[0]

/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--&gt; 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)
    140         layers = [layers]
    141       for layer in layers:
--&gt; 142         self.add(layer)
    143 
    144   @property

/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--&gt; 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)
    219       # If the model is being built continuously on top of an input layer:
    220       # refresh its output.
--&gt; 221       output_tensor = layer(self.outputs[0])
    222       if len(nest.flatten(output_tensor)) != 1:
    223         raise ValueError(SINGLE_LAYER_OUTPUT_ERROR_MSG)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    924     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):
    925       return self._functional_construction_call(inputs, args, kwargs,
--&gt; 926                                                 input_list)
    927 
    928     # Maintains info about the `Layer.call` stack.

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)
   1115           try:
   1116             with ops.enable_auto_cast_variables(self._compute_dtype_object):
-&gt; 1117               outputs = call_fn(cast_inputs, *args, **kwargs)
   1118 
   1119           except errors.OperatorNotAllowedInGraphError as e:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py in call(self, inputs, mask, training)
    901     with backprop.GradientTape(watch_accessed_variables=True) as tape,\
    902         variable_scope.variable_creator_scope(_variable_creator):
--&gt; 903       result = self.function(inputs, **kwargs)
    904     self._check_variables(created_variables, tape.watched_variables())
    905     return result

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in wrapper(*args, **kwargs)
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a
    204       # TypeError, when given unexpected types.  So we need to catch both.
--&gt; 205       result = dispatch(wrapper, args, kwargs)
    206       if result is not OpDispatcher.NOT_SUPPORTED:
    207         return result

TypeError: 'module' object is not callable

",1,1743,"I had the same problem while using a lambda layer.
A possible workaround is defining a custom lamda function outside the model and using it inside the lambda layer.
Here is the updated code:
import tensorflow as tf

def custom_one_hot(x):
    return tf.keras.backend.one_hot(x, num_classes=10) #or tf.one_hot(x, depth=10)

model1 = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(81,), dtype='uint8'),
    tf.keras.layers.Lambda(custom_one_hot, output_shape=(81, 10)),
])

tf.keras.models.save_model(model1, './model')
model2 = tf.keras.models.load_model('./model')
model2.summary()

Output:
Model: ""sequential_8""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lambda_8 (Lambda)           (None, 81, 10)            0         
                                                                 
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________

Important: each time we want to use the model we will need to define it before loading the model. Otherwise will run into an error:
NameError: Exception encountered when calling layer ""lambda_8"" (type Lambda).

name 'custom_one_hot' is not defined

To resolve this issue, we need to ensure that the custom function is defined before loading the model. The code defining the custom function used inside the lambda layer should always be executed before loading the saved models.
","You could maybe use the tf.keras.layers.experimental.preprocessing.CategoryEncoding layer.
import tensorflow as tf

model1 = tf.keras.Sequential([
    tf.keras.layers.Input(shape = (81,), dtype=tf.int32), 
    tf.keras.layers.experimental.preprocessing.CategoryEncoding(
        max_tokens=(10), output_mode='count'), # or 'binary'
])

tf.keras.models.save_model(model1, './model')
model2 = tf.keras.models.load_model('./model')
model2.summary()

inp = tf.random.uniform((1, 81), 0, 10, dtype=tf.int32)

model2(inp)

&lt;tf.Tensor: shape=(1, 10), dtype=float32, 
    numpy=array([[ 7., 11.,  5.,  6., 10.,  5.,  5., 12., 10., 10.]], dtype=float32)&gt;

",
tensorflow unexpected result,https://stackoverflow.com/questions/65346324,Unable to load .h5 file made in Google Colab to Jupyter Notebook,"I tried a code for Face mask detect and alert system and I am getting an error regarding the same. I trained the model in Google Collaboratory and ran the following code in Jupyter Notebook. The code is as follows:
# Import necessary libraries
from keras.models import load_model
import cv2
import numpy as np
import tkinter
from tkinter import messagebox
import smtplib

# Initialize Tkinter
root = tkinter.Tk()
root.withdraw()

#Load trained deep learning model
model = load_model('face_mask_detection_alert_system.h5')

#Classifier to detect face
face_det_classifier=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

# Capture Video
vid_source=cv2.VideoCapture(0)

# Dictionaries containing details of Wearing Mask and Color of rectangle around face. If wearing mask 
then color would be 
# green and if not wearing mask then color of rectangle around face would be red
text_dict={0:'Mask ON',1:'No Mask'}
rect_color_dict={0:(0,255,0),1:(0,0,255)}

SUBJECT = ""Subject""   
TEXT = ""One Visitor violated Face Mask Policy. See in the camera to recognize user. A Person has been 
detected without a face mask in the Hotel Lobby Area 9. Please Alert the authorities.""


# While Loop to continuously detect camera feed
    while(True):

        ret, img = vid_source.read()
        grayscale_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
        faces = face_det_classifier.detectMultiScale(grayscale_img,1.3,5)  

        for (x,y,w,h) in faces:

            face_img = grayscale_img[y:y+w,x:x+w]
            resized_img = cv2.resize(face_img,(56,56))
            normalized_img = resized_img/255.0
            reshaped_img = np.reshape(normalized_img,(1,56,56,1))
            result=model.predict(reshaped_img)

            label=np.argmax(result,axis=1)[0]
  
            cv2.rectangle(img,(x,y),(x+w,y+h),rect_color_dict[label],2)
            cv2.rectangle(img,(x,y-40),(x+w,y),rect_color_dict[label],-1)
            cv2.putText(img, text_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,0,0),2) 
    
            # If label = 1 then it means wearing No Mask and 0 means wearing Mask
            if (label == 1):
                # Throw a Warning Message to tell user to wear a mask if not wearing one. This will 
                stay
                #open and No Access will be given He/She wears the mask
                messagebox.showwarning(""Warning"",""Access Denied. Please wear a Face Mask"")
        
                # Send an email to the administrator if access denied/user not wearing face mask 
                message = 'Subject: {}\n\n{}'.format(SUBJECT, TEXT)
                mail = smtplib.SMTP('smtp.gmail.com', 587)
                mail.ehlo()
                mail.starttls()
                mail.login('aaaa@gmail.com','bbbb@gmail.com')
                mail.sendmail('aaaa@gmail.com','aaaa@gmail.com',message)
                mail.close
            else:
                pass
                break

    cv2.imshow('LIVE Video Feed',img)
    key=cv2.waitKey(1)

    if(key==27):
        break
    
cv2.destroyAllWindows()
source.release()

Error:
Using TensorFlow backend.
Traceback (most recent call last):
  File ""C:\Users\IZZY\Desktop\Dataset\facemaskalert_2.py"", line 14, in &lt;module&gt;
    model = load_model('face_mask_detection_alert_system.h5')
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\engine\saving.py"", line 492, in load_wrapper
    return load_function(*args, **kwargs)
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\engine\saving.py"", line 584, in load_model
    model = _deserialize_model(h5dict, custom_objects, compile)
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\engine\saving.py"", line 274, in _deserialize_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\engine\saving.py"", line 627, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\layers\__init__.py"", line 168, in deserialize
    printable_module_name='layer')
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\utils\generic_utils.py"", line 147, in deserialize_keras_object
    list(custom_objects.items())))
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\engine\sequential.py"", line 301, in from_config
    custom_objects=custom_objects)
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\layers\__init__.py"", line 168, in deserialize
    printable_module_name='layer')
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\utils\generic_utils.py"", line 149, in deserialize_keras_object
    return cls.from_config(config['config'])
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\engine\base_layer.py"", line 1179, in from_config
    return cls(**config)
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\legacy\interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  TypeError: __init__() got an unexpected keyword argument 'ragged'

Source Code : https://theaiuniversity.com/courses/face-mask-detection-alert-system/
Methods Tried but did not help : Unexpected keyword argument 'ragged' in Keras
EDIT:-h5 FILE
https://drive.google.com/file/d/10oHrqtrYoD2Hx0olLnnf0qYSQLlrt3Kq/view?usp=sharing.
",1,1673,"Seems the issue is similar as mentioned in this stackoverflow question. already.
As the accepted answer here mentions that the exported model may be from tf.keras and not keras directly :

""Don't import keras directly as your model is saved with Tensorflow's
keras high level api. Change all your imports to tensorflow.keras"".

My suggestion:
1.You should try to use tf.keras for all keras imports

And also as Mohammad mentions in the answer , use compile=False in load_model.

Check the version of tf and keras in colab and local environment.Both versions need to be same.


From Keras github issues regarding this.
","I think the version of TensorFlow that you used to train and save your model is different than the one that you are using to load the model.
Try compile=False while loading your model:
#Load trained deep learning model
model = load_model('face_mask_detection_alert_system.h5', compile=False)

",
tensorflow unexpected result,https://stackoverflow.com/questions/64620236,Error &quot;ValueError: Graph disconnected&quot; in Keras/Tensorflow,"I am getting an error in Tensorflow 2. How can I solve it?
Here is my code (assume all the relevant modules/objects of Keras have been imported):
dense1 = 2**7
dense2 = 2**8
dense3 = 2**9
dropout = 0.8
price_loss = 1
cut_loss = 1
activation= LeakyReLU()

#====================================================================

# INPUTS

#====================================================================


#----------------------------------------------------------------

carat = Input(
    shape= (1,),
    batch_size= batch_size,
    name= 'carat'
)

#----------------------------------------------------------------

color = Input(
    shape= (1,),
    batch_size= batch_size,
    name= 'color'
)

#----------------------------------------------------------------

clarity = Input(
    shape= (1,),
    batch_size= batch_size,
    name= 'clarity'
)

#----------------------------------------------------------------

depth = Input(
    shape= (1,),
    batch_size= batch_size,
    name= 'depth'
)

#----------------------------------------------------------------

table = Input(
    shape= (1,),
    batch_size= batch_size,
    name= 'table'
)

#----------------------------------------------------------------

x = Input(
    shape= (1,),
    batch_size= batch_size,
    name= 'x'
)

#----------------------------------------------------------------

y = Input(
    shape= (1,),
    batch_size= batch_size,
    name= 'y'
)

#----------------------------------------------------------------

z = Input(
    shape= (1,),
    batch_size= batch_size,
    name= 'z'
)

#----------------------------------------------------------------

#====================================================================

# CREATE EMBEDDINGS FOR CATEGORICAL FEATURES ""COLOR"" AND ""CLARITY""

#====================================================================


color = Embedding(input_dim = 7, output_dim = 1, name = 'color_emb')(color)

clarity = Embedding(input_dim = 8, output_dim = 1, name = 'clarity_emb')(clarity)

color = Flatten()(color)

clarity = Flatten()(clarity)


#====================================================================

# CONCATENATE FEATURES

#====================================================================


x = Concatenate()([color, clarity, carat, depth, table, x, y, z])


#====================================================================

# DENSE NETWORK

#====================================================================


x = Dense(dense1, activation = activation)(x)

x = BatchNormalization()(x)

x = Dense(dense2, activation = activation)(x)

x = BatchNormalization()(x)

x = Dense(dense3, activation = activation)(x)

x = BatchNormalization()(x)

x = Dropout(dropout)(x)

#====================================================================

# PREDICTIONS

# ====================================================================

cut = Dense(1, activation = 'sigmoid')(x)

price = Dense(1)(x)

#====================================================================

# DEFINE THE MODEL

# ====================================================================

model = Model(inputs = [carat, color, clarity, depth, table, x, y, z] , outputs = [cut , price])

#====================================================================

# COMPILE THE MODEL

# ====================================================================

model.compile(

        optimizer = 'Adam',

        loss = {

            ""price"": ""huber_loss"",


            ""cut"": ""binary_crossentropy"",


        },

        loss_weights = [price_loss, cut_loss],

        metrics = {

            ""price"": [""mean_absolute_percentage_error""],


            ""cut"": [tf.keras.metrics.AUC(), tf.keras.metrics.Precision(thresholds = thresholds)],

        }
    )

Stack trace:
WARNING:tensorflow:Functional inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to ""functional_1"" was not an Input tensor, it was generated by layer flatten_8.
Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.
The tensor that caused the issue was: flatten_8/Reshape:0
WARNING:tensorflow:Functional inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to ""functional_1"" was not an Input tensor, it was generated by layer flatten_9.
Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.
The tensor that caused the issue was: flatten_9/Reshape:0
WARNING:tensorflow:Functional inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to ""functional_1"" was not an Input tensor, it was generated by layer dropout_2.
Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.
The tensor that caused the issue was: dropout_2/cond/Identity:0
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-64-132a2d8458b9&gt; in &lt;module&gt;
    135 # ====================================================================
    136 
--&gt; 137 model = Model(inputs = [carat, color, clarity, depth, table, x, y, z] , outputs = [cut , price])
    138 
    139 #====================================================================

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\training.py in __new__(cls, *args, **kwargs)
    240       # Functional model
    241       from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top
--&gt; 242       return functional.Functional(*args, **kwargs)
    243     else:
    244       return super(Model, cls).__new__(cls, *args, **kwargs)

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\training\tracking\base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--&gt; 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\functional.py in __init__(self, inputs, outputs, name, trainable)
    113     #     'arguments during initialization. Got an unexpected argument:')
    114     super(Functional, self).__init__(name=name, trainable=trainable)
--&gt; 115     self._init_graph_network(inputs, outputs)
    116 
    117   @trackable.no_automatic_dependency_tracking

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\training\tracking\base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--&gt; 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\functional.py in _init_graph_network(self, inputs, outputs)
    189     # Keep track of the network's nodes and layers.
    190     nodes, nodes_by_depth, layers, _ = _map_graph_network(
--&gt; 191         self.inputs, self.outputs)
    192     self._network_nodes = nodes
    193     self._nodes_by_depth = nodes_by_depth

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\keras\engine\functional.py in _map_graph_network(inputs, outputs)
    929                              'The following previous layers '
    930                              'were accessed without issue: ' +
--&gt; 931                              str(layers_with_complete_input))
    932         for x in nest.flatten(node.outputs):
    933           computable_tensors.add(id(x))

ValueError: Graph disconnected: cannot obtain value for tensor Tensor(""clarity_8:0"", shape=(20, 1), dtype=float32) at layer ""clarity_emb"". The following previous layers were accessed without issue: []


",1,847,"pay attention to not override the input variables. you overrode color, clarity and x input inside the network
here a possible solution:
dense1 = 2**7
dense2 = 2**8
dense3 = 2**9
dropout = 0.8
price_loss = 1
cut_loss = 1
activation= LeakyReLU()
batch_size = 32

#====================================================================
# INPUTS
#====================================================================
carat = Input(shape= (1,), batch_size= batch_size, name= 'carat')
Color = Input(shape= (1,), batch_size= batch_size, name= 'color')
Clarity = Input(shape= (1,), batch_size= batch_size, name= 'clarity')
depth = Input(shape= (1,), batch_size= batch_size, name= 'depth')
table = Input(shape= (1,), batch_size= batch_size, name= 'table')
X = Input(shape= (1,), batch_size= batch_size, name= 'x')
y = Input(shape= (1,), batch_size= batch_size, name= 'y')
z = Input(shape= (1,), batch_size= batch_size, name= 'z')
#====================================================================
# CREATE EMBEDDINGS FOR CATEGORICAL FEATURES ""COLOR"" AND ""CLARITY""
#====================================================================
color = Embedding(input_dim = 7, output_dim = 1, name = 'color_emb')(Color)
clarity = Embedding(input_dim = 8, output_dim = 1, name = 'clarity_emb')(Clarity)
color = Flatten()(color)
clarity = Flatten()(clarity)
#====================================================================
# CONCATENATE FEATURES
#====================================================================
x = Concatenate()([color, clarity, carat, depth, table, X, y, z])
#====================================================================
# DENSE NETWORK
#====================================================================
x = Dense(dense1, activation = activation)(x)
x = BatchNormalization()(x)
x = Dense(dense2, activation = activation)(x)
x = BatchNormalization()(x)
x = Dense(dense3, activation = activation)(x)
x = BatchNormalization()(x)
x = Dropout(dropout)(x)
#====================================================================
# PREDICTIONS
# ====================================================================
cut = Dense(1, activation = 'sigmoid')(x)
price = Dense(1)(x)
#====================================================================
# DEFINE THE MODEL
# ====================================================================
model = Model(inputs = [carat, Color, Clarity, depth, table, X, y, z] , 
              outputs = [cut , price])
model.compile('adam', 'mse')
model.summary()

here the running notebook: https://colab.research.google.com/drive/1akpvuGKHXt6Frrec692zvCMAfRzZbIaM?usp=sharing
","Well, like always when we are stuck with some error, the answer lies somewhere in error message. Please note that while you have been able to build the structure of your model overwriting your variables color, clarity and so on... the variables changed the type. At the beginning you initialize the variables to be tf.keras.Input and then you overwrite it with Embedding and Flatten layers. So when it comes to building your model, you ask to build model with inputs that are no longer tf.keras.Input but something else.
The solution is to not overwrite those variables.
",
tensorflow unexpected result,https://stackoverflow.com/questions/63441288,How to run a model.fit properly on GPU? (unexptected behaviour),"Currently, I am doing y Udemy Python course for data science. In there, there is the following example to train a model in Tensorflow:
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout

model = Sequential()

# Choose whatever number of layers/neurons you want.
model.add(Dense(units=78,activation='relu'))
model.add(Dense(units=39,activation='relu'))
model.add(Dense(units=19,activation='relu'))
model.add(Dense(units=1,activation='sigmoid'))

# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw

model.compile(loss='binary_crossentropy', optimizer='adam')

model.fit(x=X_train, 
          y=y_train, 
          epochs=3,
          validation_data=(X_test, y_test), verbose=1
          )

My goal now was to get this to run on my GPU. For that, I altered the last part as follows (the epochs are low on purpose, I just want to see how long it takes per epoch before scaling up):
with tf.device(""/gpu:0""):
    model.fit(x=X_train, 
              y=y_train, 
              epochs=3,
              validation_data=(X_test, y_test), verbose=1
              )

and for comparison, also as follows:
with tf.device(""/cpu:0""):
    model.fit(x=X_train, 
              y=y_train, 
              epochs=3,
              validation_data=(X_test, y_test), verbose=1
              )

However, the result is very unexpected: Either, both versions occupy all memory of the GPU but seemingly don't do any calculations on it, and take the exact same time per epoch. Or, the GPU version simply crashes with the following error:
C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\envs\gpu\lib\site-packages\six.py in raise_from(value, from_value)
 
InternalError:  Blas GEMM launch failed : a.shape=(32, 78), b.shape=(78, 78), m=32, n=78, k=78
     [[node sequential/dense/MatMul (defined at &lt;ipython-input-115-79c9a84ee89a&gt;:8) ]] [Op:__inference_distributed_function_874]
 
Function call stack:
distributed_function

Sometimes it crashes, sometimes it kind of works but takes as long as the CPU. Sometimes even the CPU version takes 20 sec per epoch, other times it takes 40 sec. The code stays the same, all that changes is that I restart the Kernel in between. I really don't understand it.
When I test the GPU and conda environment using the following code, everything seems to work fine, reproducible and the GPU is about 20x as fast as the CPU:
# https://     gist.github.com/ikarus-999/1a845437b454cdfcc1eb5455d373fe63
import sys
import numpy as np
import tensorflow.compat.v1 as tf # compatibility for TF 1 code
from datetime import datetime
 
def test_device (device_name: str):
    shape = (int(10000), int(10000))
    startTime = datetime.now()
    with tf.device(device_name):
        random_matrix = tf.random.uniform(shape=shape, minval=0, maxval=1)
        dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))
        sum_operation = tf.reduce_sum(dot_operation)
 
    result = sum_operation
 
    print(""Shape:"", shape, ""Device:"", device_name)
    print(""""*50)
    print(result)
    print(""Time taken:"", datetime.now() - startTime)
    print(""\n"" * 2)
    
test_device(""/cpu:0"") # 6 sec
test_device(""/gpu:0"") # 0.3 sec

So, I am sure there is something I am doing wrong.
TLTR:
What would be the correct way to call model.fit on the GPU? How can different runs (without changing the code) result in so drastically different outcomes (Crash, vastly different calculation times)?
Any help is greatly appreciated, thx!
",1,9217,,,
tensorflow unexpected result,https://stackoverflow.com/questions/62675004,Keras initializer error unexpected keyword argument &#39;dtype&#39;,"I am trying to code a Siamese networks in Keras and Tensorflow, using this Jupiter Notebook as reference:
https://github.com/hlamba28/One-Shot-Learning-with-Siamese-Networks/blob/master/Siamese%20on%20Omniglot%20Dataset.ipynb
When I create the model:
model = get_siamese_model((105, 105, 1))

I got this error:
 Traceback (most recent call last):
  File ""main.py"", line 164, in &lt;module&gt;
    model = get_siamese_model((105, 105, 1))
  File ""main.py"", line 129, in get_siamese_model
    kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 456, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py"", line 198, in add
    layer(x)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 897, in __call__
    self._maybe_build(inputs)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 2416, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 163, in build
    dtype=self.dtype)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 577, in add_weight
    caching_device=caching_device)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 743, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 141, in make_variable
    shape=variable_shape if variable_shape else None)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 259, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 220, in _variable_v1_call
    shape=shape)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 198, in &lt;lambda&gt;
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 2598, in default_variable_creator
    shape=shape)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 263, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1434, in __init__
    distribute_strategy=distribute_strategy)
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1567, in _init_from_args
    initial_value() if init_from_fn else initial_value,
  File ""/home/fabio/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 121, in &lt;lambda&gt;
    init_val = lambda: initializer(shape, dtype=dtype)
TypeError: initialize_weights() got an unexpected keyword argument 'dtype'

What the error means? And how can I solve that?
",1,7223,,,
tensorflow unexpected result,https://stackoverflow.com/questions/51688885,Building Tensorflow with OpenCL support fails on Ubuntu 18.04,,1,866,,,
tensorflow unexpected result,https://stackoverflow.com/questions/46377189,tensorflow java model inference to convert fetched tensor to string?,,1,1148,,,
tensorflow unexpected result,https://stackoverflow.com/questions/45478790,Optimization flags in Bazel while running TensorFlow Tests,,1,987,,,
tensorflow unexpected result,https://stackoverflow.com/questions/76564629,Cannot import protobuf builder when deploying Azure function,"The Problem
I have an Azure function built using the Python v1 programming model which builds a basic Keras model and trains it using some data from an Azure file share.
Tested locally and everything works.
I'm deploying to Azure using Azure Pipelines. It's invoking correctly, but then failing. Checking the detailed invocation history reveals the following error:
Result: Failure Exception: ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/internal/__init__.py). 
Please check the requirements.txt file for the missing module. For more info, please refer the troubleshooting guide: https://aka.ms/functions-modulenotfound
Stack: File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/dispatcher.py"", line 380, in _handle__function_load_request func = loader.load_function( 
File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/utils/wrappers.py"", line 48, in call raise extend_exception_message(e, message) File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/utils/wrappers.py"", line 44, in call return func(*args, **kwargs)
File ""/azure-functions-host/workers/python/3.9/LINUX/X64/azure_functions_worker/loader.py"", line 132, in load_function mod = importlib.import_module(fullmodname)
File ""/usr/local/lib/python3.9/importlib/__init__.py"", line 127, in import_module return _bootstrap._gcd_import(name[level:], package, level) File ""&lt;frozen importlib._bootstrap&gt;"", line 1030, in _gcd_import
File ""&lt;frozen importlib._bootstrap&gt;"", line 1007, in _find_and_load
File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _find_and_load_unlocked 
File ""&lt;frozen importlib._bootstrap&gt;"", line 680, in _load_unlocked
File ""&lt;frozen importlib._bootstrap_external&gt;"", line 850, in exec_module
File ""&lt;frozen importlib._bootstrap&gt;"", line 228, in _call_with_frames_removed 
File ""/home/site/wwwroot/func-gc-imgorientation-train/__init__.py"", line 20, in &lt;module&gt; from core.data.training_image_filestore import TrainingImageFilestore
File ""/home/site/wwwroot/core/data/training_image_filestore.py"", line 4, in &lt;module&gt; import tensorflow as tf
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/__init__.py"", line 37, in &lt;module&gt; from tensorflow.python.tools import module_util as _module_util
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/python/__init__.py"", line 37, in &lt;module&gt; from tensorflow.python.eager import context
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/python/eager/context.py"", line 28, in &lt;module&gt; from tensorflow.core.framework import function_pb2
File ""/home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/core/framework/function_pb2.py"", line 5, in &lt;module&gt; from google.protobuf.internal import builder as _builder

Well Documented Error with a Solution that Works Outside of Azure
This error is well documented and understood as in the following version:
ImportError: cannot import name 'builder' from 'google.protobuf.internal'
In general, arising from API changes in v3.20:
https://stackoverflow.com/a/71984564/1928761
TF adopted these changes, but there were bugs when using TF with protobuf &gt;= v3.20 which were resolved in the most recent Tensorflow release so protobuf 4.23.3 should work with TF 2.12.0 as here:
https://github.com/tensorflow/tensorflow/issues/59221
Problem is resolved locally
I resolved this issue locally and have tested my function. All works fine. The problem only occurs in Azure.
Suspected Root Cause in Azure
Looking through the error above I noticed that all my custom modules, and Tensorflow, are installed to the same root: /home/site/wwwroot/
For instance, my custom core.data module is installed at /home/site/wwwroot/core/data/
Tensorflow is installed at /home/site/wwwroot/.python_packages/lib/site-packages/tensorflow/
I've confirmed that all modules in my requirements.txt are being installed as expected in /home/site/wwwroot/.python_packages/lib/site-packages/ - including the latest protobuf with the builder module as expected.
However, protobuf is being imported from /azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/internal/init.py
Presumably the implication is that TF is using the version of protobuf that's bundled with Azure's python distribution on the host, rather than the version in my site packages.
To test this theory I added the following code to the top of my init.py for the function:
import sys
print(sys.path)
import google.protobuf
print(google.protobuf.__version__)
print(google.protobuf.__path__)

This confirmed that protobuf version 3.19 was being loaded from the following path and not from site packages: /azure-functions-host/workers/python/3.9/LINUX/X64/google/protobuf/
The Question
The question therefore is can I upgrade the version of protobuf that's included in this python bundle?
Alternatively, can I force TF to use the version of protobuf in my site packages rather than the ones in the Azure python bundle?
Attempted Solutions
ADO is building and installing the correct versions of both protobuf and TF, and the builder file is definitely in the installed site-packages. I've confirmed this by downloading the package from my storage account and unzipping it.
To resolve the issue, I've tried the following:

Confirmed that I can import other modules and that this is specific to importing the protobuf builder
Confirmed that I can import google.protobuf.internal.
Tried clearing the protobuf pycache by adding an rm -rvf command to my bash script immediately after the pip install --target....
Using subprocess to run pip install --upgrade protobuf==4.23.3 from within my function and before importing TF (which runs without error, but does not do the job).

At this point, I'm all out of ideas.
requirements.txt
# DO NOT include azure-functions-worker in this file
# The Python Worker is managed by Azure Functions platform
# Manually managing azure-functions-worker may cause unexpected issues

# Protobuf comes first to force the very latest version
protobuf==4.23.3

# Tensorflow, Dotenv, Pillow
absl-py==1.4.0
astunparse==1.6.3
cachetools==5.3.1
certifi==2023.5.7
charset-normalizer==3.1.0
contourpy==1.1.0
cycler==0.11.0
flatbuffers==23.5.26
fonttools==4.40.0
gast==0.4.0
google-auth==2.20.0
google-auth-oauthlib==1.0.0
google-pasta==0.2.0
graphviz==0.20.1
grpcio==1.54.2
h5py==3.8.0
idna==3.4
jax==0.4.12
keras==2.12.0
kiwisolver==1.4.4
libclang==16.0.0
Markdown==3.4.3
MarkupSafe==2.1.3
matplotlib==3.7.1
ml-dtypes==0.2.0
numpy==1.23.5
oauthlib==3.2.2
opt-einsum==3.3.0
packaging==23.1
Pillow==9.5.0
pyasn1==0.5.0
pyasn1-modules==0.3.0
pydot==1.4.2
pyparsing==3.0.9
python-dateutil==2.8.2
python-dotenv
requests==2.31.0
requests-oauthlib==1.3.1
rsa==4.9
scipy==1.10.1
six==1.16.0
tensorboard==2.12.3
tensorboard-data-server==0.7.1
tensorflow==2.12.0
tensorflow-estimator==2.12.0
termcolor==2.3.0
typing_extensions==4.6.3
urllib3==1.26.16
Werkzeug==2.3.6
wrapt==1.14.1

# Azure libraries
azure-functions
azure-identity
azure-keyvault

Deployment pipeline YAML
# Python Function App to Linux on Azure
# Build a Python function app and deploy it to Azure as a Linux function app.
# Add steps that analyze code, save build artifacts, deploy, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/python

trigger:
- main

variables:
  # Azure Resource Manager connection created during pipeline creation
  azureSubscription: '4ae24131-0b22-421c-8e3e-6d766e891ece'

  # Function app name
  functionAppName: 'func-xxxx-dev'

  # Agent VM image name
  vmImageName: 'ubuntu-latest'

  # Working Directory
  workingDirectory: '$(System.DefaultWorkingDirectory)'

stages:
- stage: Build
  displayName: Build stage

  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName)

    steps:
    - bash: |
        if [ -f extensions.csproj ]
        then
            dotnet build extensions.csproj --runtime ubuntu.16.04-x64 --output ./bin
        fi
      workingDirectory: $(workingDirectory)
      displayName: 'Build extensions'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.9'
      inputs:
        versionSpec: 3.9 # Functions V2 supports Python 3.6 as of today

    - bash: |
        python -m pip install --upgrade pip
        pip install --target=""./.python_packages/lib/site-packages"" -r ./requirements.txt
      workingDirectory: $(workingDirectory)
      displayName: 'Install application dependencies'

    - task: ArchiveFiles@2
      displayName: 'Archive files'
      inputs:
        rootFolderOrFile: '$(workingDirectory)'
        includeRootFolder: false
        archiveType: zip
        archiveFile: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
        replaceExistingArchive: true

    - publish: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
      artifact: drop

- stage: Deploy
  displayName: Deploy stage
  dependsOn: Build
  condition: succeeded()

  jobs:
  - deployment: Deploy
    displayName: Deploy
    environment: 'development'
    pool:
      vmImage: $(vmImageName)

    strategy:
      runOnce:
        deploy:

          steps:
          - task: AzureFunctionApp@1
            displayName: 'Azure functions app deploy'
            inputs:
              azureSubscription: '$(azureSubscription)'
              appType: functionAppLinux
              appName: $(functionAppName)
              package: '$(Pipeline.Workspace)/drop/$(Build.BuildId).zip'

Log stream
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778401,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778404,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:50:43Z   [Information]   Host Status: {
  ""id"": ""func-xxxx-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2778801,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}
2023-06-27T11:51:00Z   [Information]   Executing 'Functions.func-xxxx-train' (Reason='Timer fired at 2023-06-27T11:51:00.0016150+00:00', Id=3795600b-379e-423c-b29f-65fec390289a)
2023-06-27T11:51:00Z   [Verbose]   Sending invocation id: '3795600b-379e-423c-b29f-65fec390289a
2023-06-27T11:51:00Z   [Verbose]   Posting invocation id:3795600b-379e-423c-b29f-65fec390289a on workerId:250ae2e1-4416-41ef-b355-0684a59d0a91
2023-06-27T11:51:00Z   [Error]   Executed 'Functions.func-xxxx-train' (Failed, Id=3795600b-379e-423c-b29f-65fec390289a, Duration=2ms)
2023-06-27T11:51:00Z   [Verbose]   Function 'func-xxx-train' updated status: Last='2023-06-27T11:51:00.0015724+00:00', Next='2023-06-27T11:52:00.0000000+00:00', LastUpdated='2023-06-27T11:51:00.0015724+00:00'
2023-06-27T11:51:00Z   [Verbose]   Timer for 'func-gc-imgorientation-train' started with interval '00:00:59.9674482'.
2023-06-27T11:51:05Z   [Information]   Host Status: {
  ""id"": ""func-gc-imgorientation-dev"",
  ""state"": ""Running"",
  ""version"": ""4.21.3.3"",
  ""versionDetails"": ""4.21.3+2e42e3beb40b89d4f5d3dd962f3a5d420d376d71"",
  ""platformVersion"": """",
  ""instanceId"": ""9A4EF22A-638234606489642041"",
  ""computerName"": """",
  ""processUptime"": 2800545,
  ""functionAppContentEditingState"": ""NotAllowed"",
  ""extensionBundle"": {
    ""id"": ""Microsoft.Azure.Functions.ExtensionBundle"",
    ""version"": ""4.5.0""
  }
}

",0,1454,"I had a similar issue and found a different solution. There is an app setting that isolates the application's dependencies from the azure function runtime dependencies:
https://learn.microsoft.com/en-us/azure/azure-functions/functions-app-settings#python_isolate_worker_dependencies
Adding the PYTHON_ISOLATE_WORKER_DEPENDENCIES setting with a value of 1 to my function in azure fixed the issue for me.
","The root cause of the problem, as highlighted by @SiddheshDesai, turned out to be that the Azure Functions host was loading a version of protobuf (3.19.6) into the cache which was earlier than the version that the latest Tensorflow needed (4.23.3).
Even when placing my protobuf/TF imports at the top of my function app module, the same error arose.
I was, however, able to reload protobuf from my site-packages using importlib.
This cause was proven by adding the following code to my function:
import logging
import importlib
import google.protobuf

logging.info(google.protobuf.__version__)
logging.info(google.protobuf.__file__)

importlib.reload(google.protobuf)

logging.info(google.protobuf.__version__)
logging.info(google.protobuf.__file__)

However, the reload then interfered with the Azure Functions library and host causing a heap of other exceptions.
My conclusion therefore is that this is an issue with Azure Functions rather than an issue with my code.
Rather than trying to work around these issues, I've now containerised my function and re-deployed. This has worked as expected.
The answer, therefore, turns out to be - as SiddheshDesai suggested above - that any application needing to use versions of protobuf &gt;= 3.20.0 need to be containerised rather than deployed as code/run from packages.
","If you want to use Protobuf module in Azure Functions, You need to downgrade it to 3.20.* and add it in your requirements.txt.
I added protobuf==3.20.* in my requirements.txt and the Http Trigger got deployed successfully in Azure Functions via DevOps YAML pipeline, Refer below:-
My requirements.txt:-
azure-functions
protobuf==3.20.*

My init.py:-
import logging

import azure.functions as func


def main(req: func.HttpRequest) -&gt; func.HttpResponse:
    logging.info('Python HTTP trigger function processed a request.')

    name = req.params.get('name')
    if not name:
        try:
            req_body = req.get_json()
        except ValueError:
            pass
        else:
            name = req_body.get('name')

    if name:
        return func.HttpResponse(f""Hello, {name}. This HTTP triggered function executed successfully."")
    else:
        return func.HttpResponse(
             ""This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response."",
             status_code=200
        )


My YAML pipeline:-
trigger:
- master

variables:
  
  azureSubscription: 'xxxxxxxx-xxxxx-xxx9bbd4354dd'

 
  functionAppName: 'valleyfunc541'

 
  vmImageName: 'ubuntu-latest'

 
  workingDirectory: '$(System.DefaultWorkingDirectory)'

stages:
- stage: Build
  displayName: Build stage

  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName)

    steps:
    - bash: |
        if [ -f extensions.csproj ]
        then
            dotnet build extensions.csproj --runtime ubuntu.16.04-x64 --output ./bin
        fi
      workingDirectory: $(workingDirectory)
      displayName: 'Build extensions'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.10'
      inputs:
        versionSpec: 3.10 # Functions V2 supports Python 3.6 as of today

    - bash: |
        pip install --target=""./.python_packages/lib/site-packages"" -r ./requirements.txt
      workingDirectory: $(workingDirectory)
      displayName: 'Install application dependencies'

    - task: ArchiveFiles@2
      displayName: 'Archive files'
      inputs:
        rootFolderOrFile: '$(workingDirectory)'
        includeRootFolder: false
        archiveType: zip
        archiveFile: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
        replaceExistingArchive: true

    - publish: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
      artifact: drop

- stage: Deploy
  displayName: Deploy stage
  dependsOn: Build
  condition: succeeded()

  jobs:
  - deployment: Deploy
    displayName: Deploy
    environment: 'development'
    pool:
      vmImage: $(vmImageName)

    strategy:
      runOnce:
        deploy:

          steps:
          - task: AzureFunctionApp@1
            displayName: 'Azure functions app deploy'
            inputs:
              azureSubscription: '$(azureSubscription)'
              appType: functionAppLinux
              appName: $(functionAppName)
              package: '$(Pipeline.Workspace)/drop/$(Build.BuildId).zip'

Output:-

The HTTP Trigger got deployed successfully:-


Reference:- My SO thread answer
"
tensorflow unexpected result,https://stackoverflow.com/questions/73083041,Text-based Tensorflow unexpected result of train_function (empty logs),"I'm trying to create a NN model with Tensorflow using a few different text files. My code are as follows
DIRECTORY_URL = ""https://raw.githubusercontent.com/ake700/Python/main/Data_Science/Text_NeuralNetwork/text_files/""

FILE_NAMES = ['AK.txt', 'AZ.txt', 'SH.txt', 'DK.txt']

for name in FILE_NAMES:
  text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)
  
parent_dir = pathlib.Path(text_dir).parent

list(parent_dir.iterdir())

def labeler(example, index):
  return example, tf.cast(index, tf.int64)

labeled_data_sets = []

for i, file_name in enumerate(FILE_NAMES):
  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))
  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))
  labeled_data_sets.append(labeled_dataset)

BUFFER_SIZE = 50000
BATCH_SIZE = 64
VALIDATION_SIZE = 5000

all_labeled_data = labeled_data_sets[0]
for labeled_dataset in labeled_data_sets[1:]:
  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)

all_labeled_data = all_labeled_data.shuffle(
    BUFFER_SIZE, reshuffle_each_iteration=False)

for text, label in all_labeled_data.take(10):
  print(""Sentence: "", text.numpy())
  print(""Label:"", label.numpy())

for ex in all_labeled_data.take(5):
  print(ex)

Output so far:
Sentence:  b'Molecules in milk absorb light to various degrees depending on the wavelength of light. Milk has a weak incident of visible light absorption mainly contributed by riboflavin in milk serum and ?carotene in milk fat . The ultraviolet absorption in milk is strongly affected by the aromatic rings of tyrosine and tryptophan and double bonds of the milk fat. In contrast strong water vapor absorption bands and other bands such as aliphatic ester bonds and CH of lipids peptide bonds of proteins OH groups of lactose and undissociated COOH groups contribute to the infrared light absorption in milk. '
Label: 2

(&lt;tf.Tensor: shape=(), dtype=string, numpy=b'Molecules in milk absorb light to various degrees depending on the wavelength of light. Milk has a weak incident of visible light absorption mainly contributed by riboflavin in milk serum and ?carotene in milk fat . The ultraviolet absorption in milk is strongly affected by the aromatic rings of tyrosine and tryptophan and double bonds of the milk fat.'&gt;, &lt;tf.Tensor: shape=(), dtype=int64, numpy=2&gt;)

Preparing dataset for training
tokenizer = tf_text.UnicodeScriptTokenizer()

def tokenize(text, unused_label):
  lower_case = tf_text.case_fold_utf8(text)
  return tokenizer.tokenize(lower_case)

tokenized_ds = all_labeled_data.map(tokenize)

for text_batch in tokenized_ds.take(5):
  print(""Tokens: "", text_batch.numpy())

# Tokens:  [b'molecules' b'in' b'milk' b'absorb' b'light' b'to' b'various' b'degrees'
# b'depending' b'on' b'the' b'wavelength' b'of' b'light' b'.' b'milk'
# b'has' b'a' b'weak' b'incident' b'of' b'visible' b'light' b'absorption'

AUTOTUNE = tf.data.AUTOTUNE

def configure_dataset(dataset):
  return dataset.cache().prefetch(buffer_size=AUTOTUNE)

VOCAB_SIZE = 10000

tokenized_ds = configure_dataset(tokenized_ds)

vocab_dict = collections.defaultdict(lambda: 0)
for toks in tokenized_ds.as_numpy_iterator():
  for tok in toks:
    vocab_dict[tok] += 1

vocab = sorted(vocab_dict.items(), key=lambda x: x[1], reverse=True)
vocab = [token for token, count in vocab]
vocab = vocab[:VOCAB_SIZE]
vocab_size = len(vocab)
print(""Vocab size: "", vocab_size)
print(""First five vocab entries:"", vocab[:5])

# Vocab size:  5678
# First five vocab entries: [b'the', b'.', b'of', b'and', b'in']

keys = vocab
values = range(2, len(vocab) + 2)  # Reserve `0` for padding, `1` for OOV tokens.

init = tf.lookup.KeyValueTensorInitializer(
    keys, values, key_dtype=tf.string, value_dtype=tf.int64)

num_oov_buckets = 1
vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov_buckets)

def preprocess_text(text, label):
  standardized = tf_text.case_fold_utf8(text)
  tokenized = tokenizer.tokenize(standardized)
  vectorized = vocab_table.lookup(tokenized)
  return vectorized, label

example_text, example_label = next(iter(all_labeled_data))
print(""Sentence: "", example_text.numpy())
vectorized_text, example_label = preprocess_text(example_text, example_label)
print(""Vectorized sentence: "", vectorized_text.numpy())

# Sentence:  b'Molecules in milk absorb light to various degrees depending on the wavelength of light. Milk has a weak incident of visible light absorption mainly contributed by riboflavin in milk serum and ?carotene in milk fat ... '
#Vectorized sentence:  [1330    6    9 1478  174    7  182 1127 1028   23    2  853    4  174
#    3    9   67    8 1029  817    4 1128  174  193  695 1331   19 3575
#    6    9 1660    5  287 3576    6    9   99    3    2 1479  193    6
#    9   22  818  455   19    2 2697 2698    4 2699    5 2700    5  636
# 1661    4    2    9   99    3    6  390 1209   76 3577  193 1662    5
#   61 1662   74   17 3578 3579 1661    5 3580    4 1210 3581 1661    4
#  663 3582  484    4  913    5 3583 3584  484  561    7    2 2701  174
#  193    6    9    3]

all_encoded_data = all_labeled_data.map(preprocess_text)

train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE)
validation_data = all_encoded_data.take(VALIDATION_SIZE)

train_data = train_data.padded_batch(BATCH_SIZE)
validation_data = validation_data.padded_batch(BATCH_SIZE)

sample_text, sample_labels = next(iter(validation_data))
print(""Text batch shape: "", sample_text.shape)
print(""Label batch shape: "", sample_labels.shape)
print(""First text example: "", sample_text[0])
print(""First label example: "", sample_labels[0])

# Text batch shape:  (64, 213)
# Label batch shape:  (64,)
# First text example:  tf.Tensor( [array] , shape=(213,), dtype=int64)
# First label example:  tf.Tensor(2, shape=(), dtype=int64)

vocab_size += 2

train_data = configure_dataset(train_data)
validation_data = configure_dataset(validation_data)

def create_model(vocab_size, num_labels):
  model = tf.keras.Sequential([
      layers.Embedding(vocab_size, 64, mask_zero=True),
      layers.Conv1D(64, 5, padding=""valid"", activation=""relu"", strides=2),
      layers.GlobalMaxPooling1D(),
      layers.Dense(num_labels)
  ])
  return model

model = create_model(vocab_size=vocab_size, num_labels=3)

model.compile(
    optimizer='adam',
    loss=losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'],
    run_eagerly=True)

history = model.fit(train_data, validation_data=validation_data, epochs=3)

When compiling the model, I am getting this error
ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

According to a previous post, it seems possible to fix this by using an encoded array? I'm not quite sure what this means and how it's applicable to my dataset/model. I also tried to append the run_eagerly=True in my model.compile statement to see more details, but it did not seem to do anything.
Any other models that might work with this type of text-based ML? I posted this in the Tensorflow forum as well for insight, but no solution yet.
",0,507,"is there any reason you're using:
tokenizer = tf_text.UnicodeScriptTokenizer()
I've never used this but it might be causing some issues.
It looks like you're trying to do text classification.
Have you tried this guide?
https://www.tensorflow.org/tutorials/keras/text_classification
",,
tensorflow unexpected result,https://stackoverflow.com/questions/71512318,"I get this Error while trying to Train a Model, Tensorflow ValueError: Unexpected result of `train_function` (Empty logs)","I'm trying to train my model to read some x-ray Images, I'm using Jupyter Notebook, I imported the Libraries, Defined the image properties, Prepared the dataset, Created the neural net model, Defined callbacks... and Managed the Data, But while trying to train my model this is the error I get :
ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

Here's creation of the neural net model :
model=Sequential()
model.add(Conv2D(32,(3,3),activation='relu',input_shape=(Image_Width,Image_Height,Image_Channels)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(64,(3,3),activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(128,(3,3),activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(2,activation='softmax'))
model.compile(loss='categorical_crossentropy',
  optimizer='rmsprop',metrics=['accuracy'])

And the training and validation of the data generator:
base_dir = ""C:/Users/lenovo/PneumoniaClassification/chest_xray""
train_dir = os.path.join(base_dir, 'train')
test_dir = os.path.join(base_dir, 'test')
validation_dir = os.path.join(base_dir, 'val')


train_datagen = ImageDataGenerator(rotation_range=15,
                                    rescale=1./255,
                                    shear_range=0.1,
                                    zoom_range=0.2,
                                    horizontal_flip=True,
                                    width_shift_range=0.1,
                                    height_shift_range=0.1
                                    )
train_generator = train_datagen.flow_from_directory(
        train_dir,  # This is the source directory for training images
        target_size=Image_Size,  # All images will be resized 
        batch_size= 50,
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode='binary')

test_datagen = ImageDataGenerator(rotation_range=15,
                                    rescale=1./255,
                                    shear_range=0.1,
                                    zoom_range=0.2,
                                    horizontal_flip=True,
                                    width_shift_range=0.1,
                                    height_shift_range=0.1)

test_generator = test_datagen.flow_from_directory(
        test_dir,
        target_size=Image_Size,
        batch_size= 50,
        class_mode='binary')

validation_datagen = ImageDataGenerator(rescale=1./255)

validation_generator = test_datagen.flow_from_directory(
        validation_dir,
        target_size=Image_Size,
        batch_size= 50,
        class_mode='binary')

But here when I try to train the Model I get the error Unexpected result of train_function..... using this Code :
epochs=10
model.fit(
    train_generator, 
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=total_validate//batch_size,
    steps_per_epoch=total_train//batch_size,
    callbacks=callbacks
)

This is the full error :
ValueError                                Traceback (most recent call last)
Input In [33], in &lt;cell line: 2&gt;()
      1 epochs=10
----&gt; 2 model.fit(
      3     train_generator, 
      4     epochs=epochs,
      5     validation_data=validation_generator,
      6     validation_steps=total_validate//batch_size,
      7     steps_per_epoch=total_train//batch_size,
      8     callbacks=callbacks
      9 )

File ~\AppData\Roaming\Python\Python39\site-packages\keras\utils\traceback_utils.py:67, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
     65 except Exception as e:  # pylint: disable=broad-except
     66   filtered_tb = _process_traceback_frames(e.__traceback__)
---&gt; 67   raise e.with_traceback(filtered_tb) from None
     68 finally:
     69   del filtered_tb

File ~\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py:1395, in Model.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1393 logs = tf_utils.sync_to_numpy_or_python_type(logs)
   1394 if logs is None:
-&gt; 1395   raise ValueError('Unexpected result of `train_function` '
   1396                    '(Empty logs). Please use '
   1397                    '`Model.compile(..., run_eagerly=True)`, or '
   1398                    '`tf.config.run_functions_eagerly(True)` for more '
   1399                    'information of where went wrong, or file a '
   1400                    'issue/bug to `tf.keras`.')
   1401 epoch_logs = copy.copy(logs)
   1403 # Run validation.

ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

",0,2050,"Can you try a dense layer with a single neuron with sigmoid activation as output (and binary crossentropy as loss)?
",,
tensorflow unexpected result,https://stackoverflow.com/questions/68899955,"loading keras model, TypeError: &#39;module&#39; object is not callable","I have searched through stackoverflow and read through documentation but somehow still my trained model is not being loaded!
I have checked following links and many more so please do not mark as duplicate straight away. thank you

How to save final model using keras?
https://www.tensorflow.org/guide/keras/save_and_serialize
https://www.tensorflow.org/tutorials/distribute/save_and_load
https://www.tensorflow.org/guide/keras/functional
How to load a model from an HDF5 file in Keras?

here is my model:
def get_model(train=True):
    
    set_seed(33)
    
    pre_process = Lambda(preprocess_input)
    vgg = VGG16(weights = 'imagenet', include_top = True, input_shape = SHAPE)
    vgg = Model(vgg.input, vgg.layers[-3].output)
    vgg.trainable = False
    
    inp = Input(SHAPE)
    vgg_16_process = pre_process(GaussianNoise(0.1)(inp))
    vgg_out = vgg(vgg_16_process)
    
    noise = Lambda(tf.zeros_like)(vgg_out)
    noise = GaussianNoise(0.1)(noise)

    if train:
        x = Lambda(lambda z: tf.concat(z, axis=0))([vgg_out,noise])
        x = Activation('relu')(x)
    else:
        x = vgg_out
        
    x = Dense(512, activation='relu')(x)
    x = Dense(128, activation='relu')(x)
    out = Dense(2, activation='softmax')(x)

    model = Model(inp, out)
    model.compile(Adam(learning_rate=1e-4), loss='binary_crossentropy')
    
    return model

And After that I have some test and train generator as follows,
The dataset is classical dog-vs-cat dataset and I'm trying to achieve one-class classification task.

train_datagen = ImageDataGenerator()
test_datagen = ImageDataGenerator()

train_generator = train_datagen.flow_from_directory(
            base_path + 'training_set/training_set/',
            target_size = (SHAPE[0], SHAPE[1]),
            batch_size = batch_size,
            class_mode = 'categorical',
            shuffle = True,
            seed = 33,
            classes = ['cats']
    )

test_generator = test_datagen.flow_from_directory(
            base_path + 'test_set/test_set/',
            target_size = (SHAPE[0], SHAPE[1]),
            batch_size = batch_size,
            class_mode = 'categorical',
            shuffle = True,
            seed = 33,
            classes = ['dogs','cats']
)

Then finally, I do compile the model
model = get_model()
model.fit(wrap_generator(train_generator), steps_per_epoch=train_generator.samples/train_generator.batch_size, epochs=1)

Then I do save model as follow
model.save('my_custom_model')

Either I do save in that format or 'my_custom_model.h5' it saves perfectly without any error.
If I use just save, then a folder is created with 'assets, variables, .pb files' that is expected as mentioned in posts of stackoverflow.
** Then here comes the problem **
from keras.models import load_model
loaded_model = load_model('my_custom_model.h5')
#or 
loaded_model = tf.keras.models.load_model('my_custom_model.h5')
#or 
loaded_model = load_model('my_custom_model') # as from folder


All throw same error
TypeError: 'module' object is not callable
I know I'm doing some mistake or model is bit different, please guide me to the right direction where I can look for solutions.
Python: 3.7.2
Tensorflow: 2.6.0
Keras: 2.6.0
Full Stack Trace
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\ops\array_ops.py in wrapper(*args, **kwargs)
    205     try:
--&gt; 206       return target(*args, **kwargs)
    207     except (TypeError, ValueError):

TypeError: 'str' object is not callable

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-10-cd6655ece7c5&gt; in &lt;module&gt;
      7 # loaded_model = tf.keras.models.load_model(""my_custom_model2"")
      8 
----&gt; 9 loaded_model = tf.keras.models.load_model('model_notebook.h5')
     10 # print(loaded_model)
     11 # print(""------------"")

~\AppData\Roaming\Python\Python37\site-packages\keras\saving\save.py in load_model(filepath, custom_objects, compile, options)
    199             (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):
    200           return hdf5_format.load_model_from_hdf5(filepath, custom_objects,
--&gt; 201                                                   compile)
    202 
    203         filepath = path_to_string(filepath)

~\AppData\Roaming\Python\Python37\site-packages\keras\saving\hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)
    179     model_config = json_utils.decode(model_config)
    180     model = model_config_lib.model_from_config(model_config,
--&gt; 181                                                custom_objects=custom_objects)
    182 
    183     # set weights

~\AppData\Roaming\Python\Python37\site-packages\keras\saving\model_config.py in model_from_config(config, custom_objects)
     50                     '`Sequential.from_config(config)`?')
     51   from keras.layers import deserialize  # pylint: disable=g-import-not-at-top
---&gt; 52   return deserialize(config, custom_objects=custom_objects)
     53 
     54 

~\AppData\Roaming\Python\Python37\site-packages\keras\layers\serialization.py in deserialize(config, custom_objects)
    210       module_objects=LOCAL.ALL_OBJECTS,
    211       custom_objects=custom_objects,
--&gt; 212       printable_module_name='layer')

~\AppData\Roaming\Python\Python37\site-packages\keras\utils\generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    676             custom_objects=dict(
    677                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +
--&gt; 678                 list(custom_objects.items())))
    679       else:
    680         with CustomObjectScope(custom_objects):

~\AppData\Roaming\Python\Python37\site-packages\keras\engine\functional.py in from_config(cls, config, custom_objects)
    661     with generic_utils.SharedObjectLoadingScope():
    662       input_tensors, output_tensors, created_layers = reconstruct_from_config(
--&gt; 663           config, custom_objects)
    664       model = cls(inputs=input_tensors, outputs=output_tensors,
    665                   name=config.get('name'))

~\AppData\Roaming\Python\Python37\site-packages\keras\engine\functional.py in reconstruct_from_config(config, custom_objects, created_layers)
   1281       if layer in unprocessed_nodes:
   1282         for node_data in unprocessed_nodes.pop(layer):
-&gt; 1283           process_node(layer, node_data)
   1284 
   1285   input_tensors = []

~\AppData\Roaming\Python\Python37\site-packages\keras\engine\functional.py in process_node(layer, node_data)
   1229         input_tensors = (
   1230             base_layer_utils.unnest_if_single_tensor(input_tensors))
-&gt; 1231       output_tensors = layer(input_tensors, **kwargs)
   1232 
   1233       # Update node index map.

~\AppData\Roaming\Python\Python37\site-packages\keras\engine\base_layer.py in __call__(self, *args, **kwargs)
    975     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):
    976       return self._functional_construction_call(inputs, args, kwargs,
--&gt; 977                                                 input_list)
    978 
    979     # Maintains info about the `Layer.call` stack.

~\AppData\Roaming\Python\Python37\site-packages\keras\engine\base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)
   1113       # Check input assumptions set after layer building, e.g. input shape.
   1114       outputs = self._keras_tensor_symbolic_call(
-&gt; 1115           inputs, input_masks, args, kwargs)
   1116 
   1117       if outputs is None:

~\AppData\Roaming\Python\Python37\site-packages\keras\engine\base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)
    846       return tf.nest.map_structure(keras_tensor.KerasTensor, output_signature)
    847     else:
--&gt; 848       return self._infer_output_signature(inputs, args, kwargs, input_masks)
    849 
    850   def _infer_output_signature(self, inputs, args, kwargs, input_masks):

~\AppData\Roaming\Python\Python37\site-packages\keras\engine\base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)
    886           self._maybe_build(inputs)
    887           inputs = self._maybe_cast_inputs(inputs)
--&gt; 888           outputs = call_fn(inputs, *args, **kwargs)
    889 
    890         self._handle_activity_regularization(inputs, outputs)

~\AppData\Roaming\Python\Python37\site-packages\keras\layers\core.py in call(self, inputs, mask, training)
    901     with tf.GradientTape(watch_accessed_variables=True) as tape,\
    902         tf.variable_creator_scope(_variable_creator):
--&gt; 903       result = self.function(inputs, **kwargs)
    904     self._check_variables(created_variables, tape.watched_variables())
    905     return result

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\ops\array_ops.py in wrapper(*args, **kwargs)
    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a
    209       # TypeError, when given unexpected types.  So we need to catch both.
--&gt; 210       result = dispatch(wrapper, args, kwargs)
    211       if result is not OpDispatcher.NOT_SUPPORTED:
    212         return result

TypeError: 'module' object is not callable

",0,1917,"
The dataset is classical dog-vs-cat dataset and I'm trying to achieve
one-class classification task.

Then the model should have only one output node because this is an example of Binary Classification. Your model has the last Dense layer with two output units and the softmax as an activation function, which is a possible solution, but for this kind of problem my suggestion is to specify on the last Dense layer only one output unit and the sigmoid as the activation function.
It seems you forgot to add the string dog in the classes list on the train_generator.
train_generator = train_datagen.flow_from_directory(
            base_path + 'training_set/training_set/',
            target_size = (SHAPE[0], SHAPE[1]),
            batch_size = batch_size,
            class_mode = 'categorical',
            shuffle = True,
            seed = 33,
            classes = ['cats']
    )

Now, regards on the main topic of your question.
from keras.models import load_model
loaded_model = load_model('my_custom_model.h5')
#or 
loaded_model = tf.keras.models.load_model('my_custom_model.h5')
#or 
loaded_model = load_model('my_custom_model') # as from folder

I think, but I'm not 100% sure about it, the problem is from this line of code from keras.models import load_model. Starting from Tensorflow 2.x, which includes also the Keras module, every single function, class etc. must be imported by specifying always tensorflow as the first name for import each Keras module. In short, you should write this line of code: from tensorflow.keras.models import load_model and it should work because it calls the function from the Keras module with the Tensorflow as the back engine.
",,
tensorflow unexpected result,https://stackoverflow.com/questions/67258336,What additional loaders do I need? (Module parse failed: Unexpected token (16:9)),"I am using a library called danfojs and it gives me the following error when I compiled.
14:01:51.417    Failed to compile.
14:01:51.417    ./node_modules/danfojs/src/index.js 16:9
14:01:51.417    Module parse failed: Unexpected token (16:9)
14:01:51.418    File was processed with these loaders:
14:01:51.418     * ./node_modules/babel-loader/lib/index.js
14:01:51.418    You may need an additional loader to handle the result of these loaders.
14:01:51.418    | export { Str } from ""./core/strings"";
14:01:51.418    | export { Utils } from ""./core/utils"";
14:01:51.418    &gt; export * as tf from ""@tensorflow/tfjs"";
14:01:51.418    | export var _version = ""0.2.5"";

Seems like I need to set up some additional loader maybe with webpack settings, could any one point me out the right direction of how to do that?
",0,256,"You should not import /src/index.js from that lib: that is apparently source code with ES6 features, not supported by current Node.
",,
tensorflow unexpected result,https://stackoverflow.com/questions/65472737,TypeError: sample_chain() got an unexpected keyword argument &#39;seed&#39; - Tensorflow 2.0,"Error with Tensorflow 2.0 using MCMC on MacOS 10.13.6
The error on the console:
2020-12-27 22:06:48.253835: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-27 22:06:48.254353: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.
objc[69111]: Class zmAppHelper is implemented in both /Library/ScriptingAdditions/zOLPluginInjection.osax/Contents/MacOS/zOLPluginInjection (0x1a48eaf4f0) and /Library/Application Support/Microsoft/ZoomOutlookPlugin/zOutlookPlugin64.bundle/Contents/MacOS/zOutlookPlugin64 (0x1a490e0518). One of the two will be used. Which one is undefined.
objc[69111]: class `ERCalendarEventEditorWindowController' not linked into application
Traceback (most recent call last):
  File ""dc7.py"", line 131, in &lt;module&gt;
    chains, kernel_results = run_chain(initial_state)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 503, in _call
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 408, in _initialize
    *args, **kwds))
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 905, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in converted code:

    dc7.py:117 run_chain  *
        return tfp.mcmc.sample_chain(

    **TypeError: sample_chain() got an unexpected keyword argument 'seed'**

Versions
MacOS 10.13.6 High Sierra

tensorflow                2.0.0           mkl_py37hda344b4_0  
tensorflow-base           2.0.0           mkl_py37h66b1bf0_0  
tensorflow-estimator      2.0.0              pyh2649769_0  
tensorflow-probability    0.8.0                      py_0    conda-forge
jupyter_client            6.1.7                      py_0  
jupyter_core              4.7.0            py37hecd8cb5_0  
jupyterlab_pygments       0.1.2                      py_0  
ipython                   7.19.0           py37h01d92e1_0  
ipython_genutils          0.2.0              pyhd3eb1b0_1  
python                    3.7.9                h26836e1_0  
python-dateutil           2.8.1                      py_0  
python_abi                3.7                     1_cp37m    conda-forge

The source-code:
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

from pprint import pprint
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

#import tensorflow as tf
#print(tf.__version__)

import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()

import tensorflow_probability as tfp

sns.reset_defaults()
sns.set_context(context = 'talk', font_scale = 0.7)
plt.rcParams['image.cmap'] = 'viridis'

#%matplotlib inline

tfd = tfp.distributions
tfb = tfp.bijectors


#### ============================================

#@title Utils { display-mode: ""form"" }
def print_subclasses_from_module(module, base_class, maxwidth=80):
  import functools, inspect, sys
  subclasses = [name for name, obj in inspect.getmembers(module)
                if inspect.isclass(obj) and issubclass(obj, base_class)]
  def red(acc, x):
    if not acc or len(acc[-1]) + len(x) + 2 &gt; maxwidth:
      acc.append(x)
    else:
      acc[-1] += "", "" + x
    return acc
  print('\n'.join(functools.reduce(red, subclasses, [])))

# Generate some data
def f(x, w):
  # Pad x with 1's so we can add bias via matmul
  x = tf.pad(x, [[1, 0], [0, 0]], constant_values=1)
  linop = tf.linalg.LinearOperatorFullMatrix(w[..., np.newaxis])
  result = linop.matmul(x, adjoint=True)
  return result[..., 0, :]

num_features = 2
num_examples = 50
noise_scale = .5
true_w = np.array([-1., 2., 3.])

xs = np.random.uniform(-1., 1., [num_features, num_examples])
ys = f(xs, true_w) + np.random.normal(0., noise_scale, size=num_examples)

# Visualize the data set
plt.scatter(*xs, c=ys, s=100, linewidths=0)

grid = np.meshgrid(*([np.linspace(-1, 1, 100)] * 2))
xs_grid = np.stack(grid, axis=0)
fs_grid = f(xs_grid.reshape([num_features, -1]), true_w)
fs_grid = np.reshape(fs_grid, [100, 100])
plt.colorbar()
plt.contour(xs_grid[0, ...], xs_grid[1, ...], fs_grid, 20, linewidths=1)
plt.show()

### Sampling the noise scale

# Define the joint_log_prob function, and our unnormalized posterior.
def joint_log_prob(w, sigma, x, y):
  # Our model in maths is
  #   w ~ MVN([0, 0, 0], diag([1, 1, 1]))
  #   y_i ~ Normal(w @ x_i, noise_scale),  i=1..N

  rv_w = tfd.MultivariateNormalDiag(
    loc=np.zeros(num_features + 1),
    scale_diag=np.ones(num_features + 1))
  
  rv_sigma = tfd.LogNormal(np.float64(1.), np.float64(5.))

  rv_y = tfd.Normal(f(x, w), sigma[..., np.newaxis])
  return (rv_w.log_prob(w) +
          rv_sigma.log_prob(sigma) +
          tf.reduce_sum(rv_y.log_prob(y), axis=-1))

# Create our unnormalized target density by currying x and y from the joint.
def unnormalized_posterior(w, sigma):
  return joint_log_prob(w, sigma, xs, ys)


# Create an HMC TransitionKernel
hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(
  target_log_prob_fn=unnormalized_posterior,
  step_size=np.float64(.1),
  num_leapfrog_steps=4)



# Create a TransformedTransitionKernl
transformed_kernel = tfp.mcmc.TransformedTransitionKernel(
    inner_kernel=hmc_kernel,
    bijector=[tfb.Identity(),    # w
              tfb.Invert(tfb.Softplus())])   # sigma


# Apply a simple step size adaptation during burnin
@tf.function
def run_chain(initial_state, num_results=1000, num_burnin_steps=500):
  adaptive_kernel = tfp.mcmc.SimpleStepSizeAdaptation(
      transformed_kernel,
      num_adaptation_steps=int(.8 * num_burnin_steps),
      target_accept_prob=np.float64(.75))

  return tfp.mcmc.sample_chain(
    num_results=num_results,
    num_burnin_steps=num_burnin_steps,
    current_state=initial_state,
    kernel=adaptive_kernel,
    seed=(0, 1),
    trace_fn=lambda cs, kr: kr)


# Instead of a single set of initial w's, we create a batch of 8.
num_chains = 8
initial_state = [np.zeros([num_chains, num_features + 1]),
                 .54 * np.ones([num_chains], dtype=np.float64)]

chains, kernel_results = run_chain(initial_state)

r_hat = tfp.mcmc.potential_scale_reduction(chains)
print(""Acceptance rate:"", kernel_results.inner_results.inner_results.is_accepted.numpy().mean())
print(""R-hat diagnostic (per w variable):"", r_hat[0].numpy())
print(""R-hat diagnostic (sigma):"", r_hat[1].numpy())

w_chains, sigma_chains = chains

",0,1357,"I was using incompatible versions of tensorflow and tensorflow_probability.
With the following versions, the above Typererror went away:
ipython                   7.19.0                   pypi_0    pypi
ipython-genutils          0.2.0                    pypi_0    pypi
python                    3.7.9                h26836e1_0
python-dateutil           2.8.1                    pypi_0    pypi
tensorboard               2.4.0                    pypi_0    pypi
tensorboard-plugin-wit    1.7.0                    pypi_0    pypi
tensorflow                2.3.0                    pypi_0    pypi
tensorflow-estimator      2.3.0                    pypi_0    pypi
tensorflow-probability    0.11.0                   pypi_0    pypi
",,
tensorflow unexpected result,https://stackoverflow.com/questions/62529415,TensorFlow 2.0 - Begineer Implementing simple CNN,"I just finished DL specialization from Coursera and I am trying to implement a CNN with TensorFlow 2.0 and my own collected data. I followed the guide and documentation from tensorflow.org and was able to set up a pipeline to load my image. However, when I ran the model I kept running into memory/resource-related issues.
My model should do a multi-label classification with 30 categories. below is my code:
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D

#Import helper modules
import os
import numpy as np
import matplotlib.pyplot as plt
import pathlib
import os
import pandas as pd
import IPython.display as display
from PIL import Image

AUTOTUNE = tf.data.experimental.AUTOTUNE

#Define path to directories
train_dir = pathlib.Path.cwd() / 'train'
validation_dir = pathlib.Path.cwd() / 'validation'
test_dir = pathlib.Path.cwd() / 'test'

#Read csv file containing filename and label
train_csv = pd.read_csv(pathlib.Path.cwd() / 'train.csv')
validation_csv = pd.read_csv(pathlib.Path.cwd() / 'validation.csv')

#Define total number of training and validation set
total_train = train_csv.shape[0]
total_val = validation_csv.shape[0]
print(f'Total training images: {total_train}')
print(f'Total validation images: {total_val}')

the below code is from https://www.tensorflow.org/tutorials/load_data/images
CLASS_NAMES = np.array([item.name for item in train_dir.glob('*')])

#set up variables
BATCH_SIZE = 128
TRAIN_STEPS_PER_EPOCH = np.ceil(total_train/BATCH_SIZE)
VAL_STEPS_PER_EPOCH = np.ceil(total_val/BATCH_SIZE)
IMG_HEIGHT = 150
IMG_WIDTH = 150

#using td.data.Dataset
train_list_ds = tf.data.Dataset.list_files(str(train_dir/'*/*'))
valid_list_ds = tf.data.Dataset.list_files(str(validation_dir/'*/*'))

def get_label(file_path):
  # convert the path to a list of path components
  parts = tf.strings.split(file_path, os.path.sep)
  # The second to last is the class-directory
  return parts[-2] == CLASS_NAMES


def decode_img(img):
  # convert the compressed string to a 3D uint8 tensor
  img = tf.image.decode_jpeg(img, channels=3)
  # Use `convert_image_dtype` to convert to floats in the [0,1] range.
  img = tf.image.convert_image_dtype(img, tf.float32)
  # resize the image to the desired size.
  return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])

def process_path(file_path):
  label = get_label(file_path)
  # load the raw data from the file as a string
  img = tf.io.read_file(file_path)
  img = decode_img(img)
  return img, label

def show_batch(image_batch, label_batch):
  plt.figure(figsize=(10,10))
  for n in range(25):
      ax = plt.subplot(5,5,n+1)
      plt.imshow(image_batch[n])
      plt.title(CLASS_NAMES[label_batch[n]==1][0].title())
      plt.axis('off')

# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.
train_labeled_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)
valid_labeled_ds = valid_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)

def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):
  # use `.cache(filename)` to cache preprocessing work for datasets that don't
  # fit in memory.
  if cache:
    if isinstance(cache, str):
      ds = ds.cache(cache)
    else:
      ds = ds.cache()

  ds = ds.shuffle(buffer_size=shuffle_buffer_size)

  # Repeat forever
  ds = ds.repeat()

  ds = ds.batch(BATCH_SIZE)

  # `prefetch` lets the dataset fetch batches in the background while the model
  # is training.
  ds = ds.prefetch(buffer_size=AUTOTUNE)

  return ds

train_ds = prepare_for_training(train_labeled_ds)
x_train, y_train = next(iter(train_ds))
valid_ds = prepare_for_training(valid_labeled_ds)

the model is from the tutorial: https://www.tensorflow.org/tutorials/images/classification
model = Sequential([
    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),
    MaxPooling2D(),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(30, activation='softmax')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()

model.fit(train_ds.repeat(), 
          epochs=15, batch_size= BATCH_SIZE, 
          validation_data=valid_ds.repeat(), steps_per_epoch= TRAIN_STEPS_PER_EPOCH,        
          validation_steps=VAL_STEPS_PER_EPOCH)

at first I ran into Input ran out of data, so i changed my input dataset into train_ds.repeat() instead of x=x_train, y=y_train.
next issue I encountered is
100/741 [===&gt;..........................] - ETA: 18:07 - loss: 3.7188 - accuracy: 0.04942020-06-23 14:52:29.232604: E tensorflow/core/lib/jpeg/jpeg_mem.cc:323] Premature end of JPEG data. Stopped at line 910/1000
Traceback (most recent call last):
  File ""product_detection.py"", line 123, in &lt;module&gt;
    model.fit(train_ds.repeat(), epochs=15, batch_size= BATCH_SIZE, validation_data=valid_ds.repeat(), steps_per_epoch= TRAIN_STEPS_PER_EPOCH, validation_steps=VAL_STEPS_PER_EPOCH)
  File ""C:\Users\sl199\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""C:\Users\sl199\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 848, in fit
    tmp_logs = train_function(iterator)
  File ""C:\Users\sl199\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\sl199\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\def_function.py"", line 611, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""C:\Users\sl199\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""C:\Users\sl199\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"", line 1665, in _filtered_call
    self.captured_inputs)
  File ""C:\Users\sl199\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"", line 1746, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""C:\Users\sl199\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"", line 598, in call
    ctx=ctx)
  File ""C:\Users\sl199\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Invalid JPEG data or crop window, data size 101360
         [[{{node DecodeJpeg}}]]
         [[IteratorGetNext]]
  (1) Invalid argument:  Invalid JPEG data or crop window, data size 101360
         [[{{node DecodeJpeg}}]]
         [[IteratorGetNext]]
         [[IteratorGetNext/_2]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_1113]

Function call stack:
train_function -&gt; train_function

2020-06-23 14:52:29.411288: W tensorflow/core/kernels/data/cache_dataset_ops.cc:794] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.

at this point I am completely lost, I suspect the prepare_for_training function I copied is not suitable for my application but I don't understand enough to make changes. It explicitly said that it is for a data set of 1000+ images while I'm working with 90k training and 10k validation data sets. I try to change the batch_size yet the issue persists.
I am using TensorFlow-gpu with GTX 1050 Ti. May I ask for a pointer on how to proceed with this? Thank you in advance.
Edit 1: Changed my batch_size to 10 and this error appeared
3537/9484 [==========&gt;...................] - ETA: 12:31 - loss: 3.7519 - accuracy: 0.02602020-06-23 16:10:13.148245: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[943,943,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
2020-06-23 16:10:13.159005: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[678,678,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
 Resou3538/9484 [==========&gt;...................] - ETA: 12:31 - loss: 3.7519 - accuracy: 0.0260rce exhausted: OOM when allocating tensor with shape[956,956,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu

tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted:  OOM when allocating tensor with shape[943,943,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
         [[{{node convert_image/Cast}}]]
         [[IteratorGetNext]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[IteratorGetNext/_2]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted:  OOM when allocating tensor with shape[943,943,3] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
         [[{{node convert_image/Cast}}]]
         [[IteratorGetNext]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

",0,268,,,
tensorflow unexpected result,https://stackoverflow.com/questions/59851300,Unable to run faster R-CNN with elastic inference and tensorflow serving - how to debug?,"I have found a saved_model from Tensorflow's Model Zoo. I am able to run my Faster R-CNN model locally using the following code:

image_np = np.array(Image.open('my_input.jpg'))
image = np.asarray(image_np)
input_tensor = tf.convert_to_tensor(image)
input_tensor = input_tensor[tf.newaxis,...]

model = tf.saved_model.load(os.path.join('&lt;PATH_TO_SAVED_MODEL&gt;'))
model = model.signatures['serving_default']

output_dict = model(input_tensor)


I wanted to try running this using Elastic Inference, and started out with this guide. I swapped out the faster r-cnn model being run by just changing the path to the saved_model when starting up tensorflow:

EI_VISIBLE_DEVICES=0 amazonei_tensorflow_model_server --model_name=f_r_cnn --model_base_path=/tmp/f_r_cnn --port=9000


Now I'm trying to run a client to talk to tensorflow serving, using the template provided:

from __future__ import print_function

import grpc
import tensorflow as tf
from PIL import Image
import numpy as np
import time
import os
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2_grpc

tf.app.flags.DEFINE_string('server', 'localhost:9000',
                           'PredictionService host:port')
tf.app.flags.DEFINE_string('image', '', 'path to image in JPEG format')
FLAGS = tf.app.flags.FLAGS

coco_classes_txt = ""https://raw.githubusercontent.com/amikelive/coco-labels/master/coco-labels-paper.txt""
local_coco_classes_txt = ""/tmp/coco-labels-paper.txt""
# it's a file like object and works just like a file
os.system(""curl -o %s -O %s""%(local_coco_classes_txt, coco_classes_txt))
NUM_PREDICTIONS = 5
with open(local_coco_classes_txt) as f:
  classes = [""No Class""] + [line.strip() for line in f.readlines()]


def main(_):
  channel = grpc.insecure_channel(FLAGS.server)
  stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)

  # Send request
  with Image.open(FLAGS.image) as f:
    f.load()
    # See prediction_service.proto for gRPC request/response details.
    data = np.asarray(f)
    data = np.expand_dims(data, axis=0)

    request = predict_pb2.PredictRequest()
    request.model_spec.name = 'f_r_cnn'
    request.inputs['inputs'].CopyFrom(
        tf.contrib.util.make_tensor_proto(data, shape=data.shape))
    result = stub.Predict(request, 60.0)  # 10 secs timeout
    outputs = result.outputs
    detection_classes = outputs[""detection_classes""]
    detection_classes = tf.make_ndarray(detection_classes)
    num_detections = int(tf.make_ndarray(outputs[""num_detections""])[0])
    print(""%d detection[s]"" % (num_detections))
    class_label = [classes[int(x)]
                   for x in detection_classes[0][:num_detections]]
    print(""SSD Prediction is "", class_label)


if __name__ == '__main__':
  tf.app.run()


While this client ran just fine with the model from the tutorial (no surprise there), it is failing when I try to get it to talk to my Faster R-CNN model with the following error:

debug_error_string = ""{""created"":""@1579654607.391705065"",""description"":""Error received from peer ipv6:[::1]:9000"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1052,""grpc_message"":""Unexpected error in RPC handling"",""grpc_status"":2}""


I googled this error and was unable to find anything useful. What is grpc_status 2? How might I find useful information to help point me in the right direction?
",0,942,,,
tensorflow unexpected result,https://stackoverflow.com/questions/55573670,Unexpected output for tf.nn.sparse_softmax_cross_entropy_with_logits,,0,299,,,
tensorflow unexpected result,https://stackoverflow.com/questions/51474289,Alexnet tensorflow implementation very low accuracy,,0,885,,,
tensorflow unexpected result,https://stackoverflow.com/questions/48850537,Issue with Imagenet classification with VGG16 pretrained weights,,0,1688,,,
tensorflow unexpected result,https://stackoverflow.com/questions/46958739,Error trying to pass list to hyperas as a parameter,,-1,2112,,,
tensorflow unexpected result,https://stackoverflow.com/questions/68363880,Keras neural network multiple output,"I would like to create a neural network with multiple outputs. With one conclusion, I succeeded in doing this, but with two - it does not work. Can you help me please? Do you know any resource with examples for keras? I attach the code and error below. (Sorry for my English, it was translated by google translator)
Code:
from keras.models import Sequential
from keras.layers import Dense
x = [[1, 1, 1, 1], [0, 1, 1, 0], [1, 0, 0, 1], [0, 0, 0, 0], [1, 1, 0, 0], [0, 1, 1, 1], [1, 1, 1, 0], [1, 0, 0, 0]]
y = [[1, 1], [0, 0], [0, 0], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0]]
model = Sequential()
# model.add(Dense(3, activation='sigmoid'))
model.add(Dense(2, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(1e-1), metrics=['accuracy'])
model.fit(x, y, epochs=20)
model.predict(x=[[0, 0, 1, 1]])

Error:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-77-c805cf1cd17e&gt; in &lt;module&gt;()
      3 x = [[1, 1, 1, 1], [0, 1, 1, 0], [1, 0, 0, 1], [0, 0, 0, 0], [1, 1, 0, 0], [0, 1, 1, 1], [1, 1, 1, 0], [1, 0, 0, 0]]
      4 y = [[1, 1], [0, 0], [0, 0], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0]]
----&gt; 5 model = Sequential(input=x, output=y)
      6 # model.add(Dense(3, activation='sigmoid'))
      7 model.add(Dense(2, activation='sigmoid'))

/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    520     self._self_setattr_tracking = False  # pylint: disable=protected-access
    521     try:
--&gt; 522       result = method(self, *args, **kwargs)
    523     finally:
    524       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

TypeError: __init__() got an unexpected keyword argument 'input'

UPD.
I redid the code, as I was advised in the comments, but now, with each training, it outputs some kind of random result that does not lie in the range of 0 - 1.
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense

x = [[0, 0], [0, 1], [1, 0], [1, 1]]
y = [[1, 1], [1, 0], [0, 1], [0, 0]]

visible = Input(shape=(2,))
hidden = Dense(2)(visible)

# hidden2 = Dense(2)(visible)
model = Model(inputs=visible, outputs=[hidden])
model.compile(loss='binary_crossentropy', metrics=['accuracy'])
model.fit(x, y, epochs=8)
model.predict(x=[[1, 0]])

",-2,1186,"First of all, sequential model does not support multi output. If you mean of multi output, multi neurons, then you can use sequential model, and should simply just change number of neurons in last layer.
Here is the modification of your first model code:
from keras.models import Sequential
from keras.layers import Dense
x = [[1, 1, 1, 1], [0, 1, 1, 0], [1, 0, 0, 1], [0, 0, 0, 0], [1, 1, 0, 0], [0, 1, 1, 1], [1, 1, 1, 0], [1, 0, 0, 0]]
y = [[1, 1], [0, 0], [0, 0], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0]]
model = Sequential()
# model.add(Dense(3, activation='sigmoid'))
model.add(Dense(2, activation='sigmoid'))
model.add(Dense(2, activation='sigmoid')) #change neurons to 2
model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(1e-1), metrics=['accuracy'])
model.fit(x, y, epochs=20)
model.predict(x=[[0, 0, 1, 1]])

But if you want an example of functional api, it is here:
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
import keras
x = [[0, 0], [0, 1], [1, 0], [1, 1]]
y = [[1, 1], [1, 0], [0, 1], [0, 0]]

visible = Input(shape=(2,))
hidden = Dense(64, activation='relu')(visible)
hidden = Dense(64, activation='relu')(hidden)
hidden = Dense(64, activation='relu')(hidden)
hidden = Dense(2, activation='sigmoid')(hidden) #use sigmoid activation for output between 0 and 1


model = Model(inputs=visible, outputs=hidden)
model.compile(optimizer='adam', loss='binary_crossentropy')
model.fit(x, y, epochs=100)
model.predict(x=[[1, 0]])

",,
tensorflow unexpected issue,https://stackoverflow.com/questions/38688777,TensorFlow: Graph Optimization (GPU vs CPU Performance),,8,3637,"Thanks for the excellent post.

I am experiencing a similar issue:  GPU/CPU processing takes more CPU and elapsed time than CPU processing alone for two examples provided by TensorFlow:  The linear regression loss model, and the MNIST for Beginners, while the MNIST Deep script shows significant improvement in CPU and Elapsed when using the GPU Profiling GPU and CPU Performance page 10 starts the discussion.

Here are the numbers:

workload     | win 8.1   win 8.1   win8.1     win 10    win 10    win 10  
workload     | cpu only  cpu       gpu        cpu only  cpu       gpu      
-------------+-----------------------------------------------------------
mnist deep   | 14053     384.26   328.92      12406     289.28   211.79 
mnist deep   | 14044     384.59   328.45      12736     293.71   210.48
mnist10,000  | 24.10      45.85     7.67      26.56      44.42     7.32  
mnist10,000  | 23.94      44.98     7.56      25.80      44.24     7.32  
mnist50,000  | 95.49     198.12    38.26     109.99     197.82    36.15  
mnist50,000  | 96.07     197.86    37.91     109.46     195.39    39.44  
   lr10,000  |  6.23      15.08     1.78       7.38      16.79     1.91  
   lr10,000  |  6.33      15.23     1.78       7.44      16.59     1.91  
  lr100,000  | 48.31     124.37    17.67      62.14     148.81    19.04  
  lr100,000  | 48.97     123.35    17.63      61.40     147.69    18.72  


( Source: Profiling GPU and CPU Performance, Fig. 64 Results )
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/61919774,unexpected keyword argument &#39;sample_weight&#39; when sub-classing tensor-flow loss class (categorical_crossentropy) to created a weighted loss function,,7,4667,"As per the comments; the issue here is that TensorFlow is now enforcing inheriting from the original method signature.

The following has been tested (by comparing equal weighting in the cost_matrix to weighting all but a single category to nothing) on a toy problem and works:

class WeightedCategoricalCrossentropy(tf.keras.losses.CategoricalCrossentropy):

  def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):

    cost_mat = np.array(cost_mat)   
    ## when loading from config, self.cost_mat returns as a list, rather than an numpy array. 
    ## Adding the above line fixes this issue, enabling .ndim to call sucessfully. 
    ## However, this is probably not the best implementation
    assert(cost_mat.ndim == 2)
    assert(cost_mat.shape[0] == cost_mat.shape[1])
    super().__init__(name=name, **kwargs)
    self.cost_mat = K.cast_to_floatx(cost_mat)

  def __call__(self, y_true, y_pred, sample_weight=None):
    assert sample_weight is None, ""should only be derived from the cost matrix""  
    return super().__call__(
        y_true=y_true, 
        y_pred=y_pred, 
        sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),
    )


  def get_config(self):
    config = super().get_config().copy()
    # Calling .update on the line above, during assignment, causes an error with config becoming None-type.
    config.update({'cost_mat': (self.cost_mat)})
    return config

  @classmethod
  def from_config(cls, config):
    # something goes wrong here and changes self.cost_mat to a list variable.
    # See above for temporary fix
    return cls(**config)

def get_sample_weights(y_true, y_pred, cost_m):
    num_classes = len(cost_m)

    y_pred.shape.assert_has_rank(2)
    assert(y_pred.shape[1] == num_classes)
    y_pred.shape.assert_is_compatible_with(y_true.shape)

    y_pred = K.one_hot(K.argmax(y_pred), num_classes)

    y_true_nk1 = K.expand_dims(y_true, 2)
    y_pred_n1k = K.expand_dims(y_pred, 1)
    cost_m_1kk = K.expand_dims(cost_m, 0)

    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k
    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])

    return sample_weights_n


# Register the loss in the Keras namespace to enable loading of the custom object.
tf.keras.losses.WeightedCategoricalCrossentropy = WeightedCategoricalCrossentropy


Usage

Where cost_matrix is a 2D NumPy array, eg:

[
 [ Weight Category 1 predicted as Category 1, 
   Weight Category 1 predicted as Category 2,
   Weight Category 1 predicted as Category 3 ]
 [ Weight Category 2 predicted as Category 1,
   ...,
   ...                                       ]
 [ ...,
   ...,
   Weight Category 3 predicted as Category 3 ]
]




model.compile(
     optimizer='adam',
     loss=WeightedCategoricalCrossentropy(cost_matrix)
     )


Model Saving

model.save(save_version_dir,save_format='tf')


Model Loading

model = tf.keras.models.load_model(
    save_version_dir,
    compile=True,
    custom_objects={
        'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_matrix)
        }
    )

",,
tensorflow unexpected issue,https://stackoverflow.com/questions/46528671,transport_encoding error during installing with pip,"I'm getting unexpected arg: keyword encoding in parse() while trying to install any python package through pip. 

I'm getting this problem since i installed tensorflow for python 3.6, which probably led to some issue with html5lib and setuptools. Have reinstalled html5lib1.0b10 using tar.gz file (admin install) but the issue still remains. 
Please help!!

pip install spacy
Collecting spacy
Exception:
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\commands\install.py"", line 335, in run
    wb.build(autobuilding=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\wheel.py"", line 749, in build
    self.requirement_set.prepare_files(self.finder)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 554, in _prepare_file
    require_hashes
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 278, in populate_link
    self.link = finder.find_requirement(self, upgrade)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 465, in find_requirement
    all_candidates = self.find_all_candidates(req.name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 423, in find_all_candidates
    for page in self._get_pages(url_locations, project_name):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 568, in _get_pages
    page = self._get_page(location)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 683, in _get_page
    return HTMLPage.get_page(link, session=self.session)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 811, in get_page
    inst = cls(resp.content, resp.url, resp.headers)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 731, in __init__
    namespaceHTMLElements=False,
TypeError: parse() got an unexpected keyword argument 'transport_encoding'

",5,6495,"Issue was indeed with html5lib and setuptools, I re-installed html5lib using

conda install -c anaconda html5lib


Although package versions were same, it said 'The following packages will be SUPERSEDED by a higher-priority channel', and installed conda, conda-env, html5lib. I'm unsure about higher-priority channel part. 
But this answered my problem!!
","Problem appears after installing tensorflow 1.3.0.

Reinstall html5lib.

conda uninstall html5lib --force
conda install html5lib


Solves the problem temporarily until next pip install --upgrade tensorflow.
Had the same problem with pip install --upgrade tensorflow-gpu as well.
",
tensorflow unexpected issue,https://stackoverflow.com/questions/43068712,IronPython: Unexpected token &#39;from&#39;,"i was running python script from .net using IronPython, below is my python script

import tensorflow as tf    
print('Tensorflow Imported')


below is C# Code

using System;
using System.Text;
using System.IO;
using IronPython.Hosting;
using System.Collections.Generic;
using Microsoft.Scripting.Hosting;

namespace ConsoleApplication1
{
    class Program
    {
        private static void Main()
        {
            var py = Python.CreateEngine();
            List&lt;string&gt; searchPaths = new List&lt;string&gt;();
            searchPaths.Add(@""C:\Users\Admin\AppData\Local\Programs\Python\Python35\Lib)"");
            searchPaths.Add(@""C:\Users\Admin\AppData\Local\Programs\Python\Python35\Lib\site-packages)"");
            py.SetSearchPaths(searchPaths);
            try
            {
                py.ExecuteFile(""script.py"");
            }
            catch (Exception ex)
            {
                Console.WriteLine(ex.Message);
            }           
        }
    }
}


below is my output


  Unexpected token 'from'


if i remove import statement then python script executes fine. I tried including os,sys all those were imported without any issue. I have installed TensorFlow via pip, when i run above script through python console(v3.5) it works fine. 

Update: in TF doc its written ""TensorFlow only supports version 3.5.x of Python on Windows"". but official release of IronPython is version 2.7
I was happy to find IronPython on GitHub, tried building it (i just typed build in console and got freaked out with the long list of error messages it showed! :D
couldn't find pre-compiled binaries

is there any alternative way to import tensorflow in IronPython 2.7 or run Python in .net? 
",5,8274,"Prakash - as you found in the documentation, TensorFlow requires Python 3.5 or 3.6 when running on Windows.  It won't run in IronPython 2.7.

One user on GitHub successfully (with a lot of work and in a not-easy-to-do) way got TF running on Windows under Python2.7, and you might be able to build on their work, but it's not exactly the solution you were looking for for IronPython.  My best suggestion is to go with 3.5 or 3.6.
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/33644635,Python unable to import tensorflow to Mac OS X 10.8,"I am trying to install tensorflow on my mac air (specs: OSX 10.8.4, 4GB RAM). While tensorflow installs cleanly, as on the tensorflow.org page, and I am able to do $ source bin/activate to get a tensorflow prompt, I am not able to import the package in python. This is what I get: 

    &gt;&gt;&gt; import tensorflow
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 4, in &lt;module&gt;
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 22, in &lt;module&gt;
    from tensorflow.python.client.client_lib import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py"", line 35, in &lt;module&gt;
    from tensorflow.python.client.session import InteractiveSession
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 11, in &lt;module&gt;
    from tensorflow.python import pywrap_tensorflow as tf_session
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in &lt;module&gt;
    _pywrap_tensorflow = swig_import_helper()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: dlopen(/usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 2): Symbol not found: ___sincos_stret
  Referenced from: /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
  Expected in: /usr/lib/libSystem.B.dylib
 in /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so


I looked at the common problems section on tensorflow, as well as searched for the ImportError above but have had no luck so far. I wonder if this is a protobuf issue as referenced in  Error in python after 'import tensorflow': TypeError: __init__() got an unexpected keyword argument 'syntax' .
Thanks in advance.
",5,4024,"The issue appears to be that the binary distribution of TensorFlow is not compatible with Mac OS X 10.8. The relevant part of the error message is this one:

Symbol not found: ___sincos_stret


According to various sources, this is a known issue when using binaries compiled for a newer version of Mac OS X on 10.8 (Mavericks), because the symbol ___sincos_stret is not defined in the standard libraries for that version of the OS.

To address this, you will need to (i) upgrade to Mac OS X 10.9 or later, (ii) follow the instructions to install from source, or (iii) use Docker.
","I had similar issue. I used docker toolbox and was able to run tensorflow. You can follow my question and my solution.
",
tensorflow unexpected issue,https://stackoverflow.com/questions/72993109," ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(, run_eagerly=True)",,4,3321,"I had a similar issue a lot of times, and every time it occurred because of the wrong dataset directory. You can set a full dataset directory path when importing data, like this if you're using Mac:
'/Users/&lt;path to dataset&gt;

Or just copy path from files and paste it in the code.
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/64725275,How to configure dataset pipelines with Tensorflow make_csv_dataset for Keras Model,"I have a structured dataset(csv features files) of around 200 GB. I'm using make_csv_dataset to make the input pipelines. Here is my code
def pack_features_vector(features, labels):
    """"""Pack the features into a single array.""""""
    features = tf.stack(list(features.values()), axis=1)
    return features, labels
def main():    
    defaults=[float()]*len(selected_columns)
    data_set=tf.data.experimental.make_csv_dataset(
        file_pattern = ""./../path-to-dataset/Train_DS/*/*.csv"",
        column_names=all_columns,    # all_columns=[""col1,col2,...""]
        select_columns=selected_columns,   # selected_columns= a subset of all_columns
        column_defaults=defaults,
        label_name=""Target"",
        batch_size=1000, 
        num_epochs=20,
        num_parallel_reads=50,
    #    shuffle_buffer_size=10000,
        ignore_errors=True)

    data_set = data_set.map(pack_features_vector)

    N_VALIDATION = int(1e3)
    N_TRAIN= int(1e4)
    BUFFER_SIZE = int(1e4)
    BATCH_SIZE = 1000
    STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE

    validate_ds = data_set.take(N_VALIDATION).cache().repeat()
    train_ds = data_set.skip(N_VALIDATION).take(N_TRAIN).cache().repeat()

    # validate_ds = validate_ds.batch(BATCH_SIZE)
    # train_ds = train_ds.batch(BATCH_SIZE)

    model = tf.keras.Sequential([
    layers.Flatten(),
    layers.Dense(256, activation='elu'),
    layers.Dense(256, activation='elu'),
    layers.Dense(128, activation='elu'),  
    layers.Dense(64, activation='elu'), 
    layers.Dense(32, activation='elu'), 
    layers.Dense(1,activation='sigmoid') 
    ])
    model.compile(optimizer='adam',
                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                metrics=['accuracy'])    
    model.fit(train_ds,
            validation_data=validate_ds,
            validation_steps=1,
            steps_per_epoch= 1,
            epochs=20,
            verbose=1
            )
if __name__ == ""__main__"":
    main()

print('Training completed!')



Now, when I execute this code , it's completed within few minutes (I think not going through the whole training data) with the following warnings:

W tensorflow/core/kernels/data/cache_dataset_ops.cc:798] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to dataset.cache().take(k).repeat(). You should use dataset.take(k).cache().repeat() instead.

As per this warning and as training is completed in few minutes meaning that... input pipeline is not configured correctly... Can anyone please guide me, how to correct this problem.
GPU of my system is NVIDIA Quadro RTX 6000 (compute capability 7.5).
A solution based on some other function like experimental.CsvDataset would work as well.
Edit
That warning gone by changing the code to avoid any cache as
    validate_ds = data_set.take(N_VALIDATION).repeat()
    train_ds = data_set.skip(N_VALIDATION).take(N_TRAIN).repeat()

But now the problem is I'm getting zero accuracy, even on the training data. Which I think is a problem of input pipelines. Here is the output.

Edit2
After some efforts, I managed to resolve the known issues by using a bit lower level but similar API, CsvDataset. But now, I'm getting the accuracy=1.00 which I think is not OK. At first epoch, it's .95 and then for next 19 epochs, it's 1.00. Here is my final code.
def preprocess(*fields):
    features=tf.stack(fields[:-1])
    # convert Target column values to int to make it work for binary classification
    labels=tf.stack([int(x) for x in fields[-1:]])
    return features,labels  # x, y


def main():
    # selected_columns=[""col1,col2,...""]
    selected_indices=[]
    for selected_column in selected_columns:
        index=all_columns.index(selected_column)
        selected_indices.append(index)
        
    print(""All_columns length""+str(len(all_columns)))
    print(""selected_columns length""+str(len(selected_columns)))
    print(""selected_indices length""+str(len(selected_indices)))
    print(selected_indices)
    defaults=[float()]*(len(selected_columns))
    #defaults.append(int())
    print(""defaults""+str(defaults))
    print(""defaults length""+str(len(defaults)))
    FEATURES = len(selected_columns) - 1
    training_csvs =  sorted(str(p) for p in pathlib.Path('.').glob(""path-to-data/Train_DS/*/*.csv""))
    testing_csvs =  sorted(str(p) for p in pathlib.Path('.').glob(""path-to-data/Test_DS/*/*.csv""))

    training_csvs
    testing_csvs

    training_dataset=tf.data.experimental.CsvDataset(        
        training_csvs,
        record_defaults=defaults, 
        compression_type=None, 
        buffer_size=None,
        header=True, 
        field_delim=',',
        # use_quote_delim=True,
        # na_value="""",
        select_cols=selected_indices
        )
    
    print(type(training_dataset))
    for features in training_dataset.take(1):
        print(""Training samples before mapping"")
        print(features)
    
    validate_ds = training_dataset.map(preprocess).take(10).batch(100).repeat()
    train_ds = training_dataset.map(preprocess).skip(10).take(90).batch(100).repeat()
    validate_ds
    train_ds
    for features,labels in train_ds.take(1):
        print(""Training samples"")
        print(features)
        print(labels)
    
    testing_dataset=tf.data.experimental.CsvDataset(        
        testing_csvs,
        record_defaults=defaults, 
        compression_type=None, 
        buffer_size=None,
        header=True, 
        field_delim=',',
        use_quote_delim=True,
        na_value="""",
        select_cols=selected_indices
        )
    
    print(type(testing_dataset))
    test_ds = testing_dataset.map(preprocess).batch(100).repeat()
    test_ds
    for features,labels in test_ds.take(1):
        print(""Testing samples"")
        print(features)
        print(labels)
    
    model = tf.keras.Sequential([        
        layers.Dense(256,activation='elu'),  
        layers.Dense(128,activation='elu'),  
        layers.Dense(64,activation='elu'),  
        layers.Dense(1,activation='sigmoid') 
        ])
    history = model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),
                            metrics=['accuracy'])
    
    model.fit(train_ds,
        validation_data=validate_ds,
        validation_steps=20,
        steps_per_epoch= 20,
        epochs=20,
        verbose=1
        )
    
    loss, accuracy = model.evaluate(test_ds)
    print(""Test Accuracy"", accuracy)


if __name__ == ""__main__"":
    main()

print('Training completed!')

I tried to feed just the few useless features to the model, but still, it's giving accuracy=1.00 or 100 %. Which is going wrong now? Overfitting etc?
",3,1074,"In the snippets, you wrote
model.fit(train_ds,
          validation_data=validate_ds,
          validation_steps=1,
          steps_per_epoch= 1,
          epochs=20,
          verbose=1)

Is the steps_per_epoch= 1 a typo? If not, that would mean you only use one batch per training, which explains the fast training and the low accuracy. validation_steps=1 is also an issue
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/62518906,Keras ImageDataGenerator : how to use data augmentation with images paths,,2,2679,"Ok so I finally found out how to deal with these issues thanks to this article. My mistake was that I kept using ImageDataGenerator despite its lack of flexibility, the solution is thus simple : use another data augmentation tool.
We can resume the author's method as following :

First, create a personalized batch generator as a subclass of Keras Sequence class (which implies to implement a __getitem__ function that loads the images according to their respective paths).
Use the data augmentation albumentations library. It has the advantages of offering more transformation functions as Imgaug or ImageDataGenerator, while being faster. Moreover, this website allows you to test some of its augmentation methods, even with your own images ! See this one for the exhaustive list.


The drawback of this library is that, as it is relatively new, few documentation can be found online, and I've spent several hours trying to resolve an issue I encountered.
Indeed, when I tried to visualize some augmentation functions, the results were entirely black images (strange fact : these would happen only when I was modifying the intensity of the pixels, with methods like RandomGamma or RandomBrightnessContrast. With transformation functions such as HorizontalFlip or ShiftScaleRotate, it would work normally).
After an entire half day of trying-to-find-what's-wrong, I eventually came up with this solution, that might help you if you were to try this library : the loading of images has to be done with OpenCV (I was using load_img and img_to_array functions from tf.keras.preprocessing.image for the loading and processing). If anyone has an explanation of why this doesn't work, I'd be glad to hear it.
Anyway, here is my final code to display an augmented image :
!pip install -U git+https://github.com/albu/albumentations &gt; /dev/null &amp;&amp; echo ""All libraries are successfully installed!""
from albumentations import Compose, HorizontalFlip, RandomBrightnessContrast, ToFloat, RGBShift
import cv2
import matplotlib.pyplot as plt
import numpy as np
from google.colab.patches import cv2_imshow # I work on a Google Colab, thus I cannot use cv2.imshow()


augmentation = Compose([HorizontalFlip(p = 0.5),
                        RandomBrightnessContrast(p = 1),
                        ToFloat(max_value = 255) # Normalize the pixels values into the [0,1] interval
                        # Feel free to add more !
                        ])

img = cv2.imread('Your_path_here.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # cv2.imread() loads the images in BGR format, thus you have to convert it to RGB before applying any transformation function.
img = augmentation(image = img)['image'] # Apply the augmentation functions to the image.
plt.figure(figsize=(7, 7))
plt.imshow((img*255).astype(np.uint8)) # Put the pixels values back to [0,255]. Replace by plt.imshow(img) if the ToFloat function is not used.
plt.show()


'''
If you want to display using cv2_imshow(), simply replace the last three lines by :

img = cv2.normalize(img, None, 255,0, cv2.NORM_MINMAX, cv2.CV_8UC1) # if the ToFloat argument is set up inside Compose(), you have to put the pixels values back to [0,255] before plotting them with cv2_imshow(). I couldn't try with cv2.imshow(), but according to the documentation it seems this line would be useless with this displaying function.
cv2_imshow(img)

I don't recommend it though, because cv2_imshow() plot the images in BGR format, thus some augmentation methods such as RGBShift will not work properly.
'''


EDIT :
I've encountered several issues with the albumentations library (that I described in this question on Github, but for now I still have had no answers) thus I'd better recommend using Imgaug for your data augmentation : it works just fine and is almost as easy to use as albumentations, even though there is a little bit less available transformation functions.
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/49828011,linear regression by tensorflow gets noticeable mean square error,"I am new to tensorflow and I am trying to implement a simple feed-forward network for regression, just for learning purposes. The complete executable code is as follows.

The regression mean squared error is around 6, which is quite large. It is a little unexpected because the function to regress is linear and simple 2*x+y, and I expect a better performance.


  I am asking for help to check if I did anything wrong in the code. I carefully checked the matrix dimensions so that should be good, but it is possible that I misunderstand something so the network or the session is not properly configured (like, should I run the training session multiple times, instead of just one time (the code below enclosed by #TRAINING#)? I see in some examples they input data piece by piece, and run the training progressively. I run the training just one time and input all data).


If the code is good, maybe this is a modeling issue, but I really don't expect to use a complicated network for such a simple regression.

import tensorflow as tf
import numpy as np
from sklearn.metrics import mean_squared_error

# inputs are points from a 100x100 grid in domain [-2,2]x[-2,2], total 10000 points
lsp = np.linspace(-2,2,100)
gridx,gridy = np.meshgrid(lsp,lsp)
inputs = np.dstack((gridx,gridy))
inputs = inputs.reshape(-1,inputs.shape[-1]) # reshpaes the grid into a 10000x2 matrix
feature_size = inputs.shape[1] # feature_size is 2, features are the 2D coordinates of each point
input_size = inputs.shape[0] # input_size is 10000

# a simple function f(x)=2*x[0]+x[1] to regress
f = lambda x: 2 * x[0] + x[1]
label_size = 1
labels = f(inputs.transpose()).reshape(-1,1) # reshapes labels as a column vector

ph_inputs = tf.placeholder(tf.float32, shape=(None, feature_size), name='inputs')
ph_labels = tf.placeholder(tf.float32, shape=(None, label_size), name='labels')

# just one hidden layer with 16 units
hid1_size = 16
w1 = tf.Variable(tf.random_normal([hid1_size, feature_size], stddev=0.01), name='w1')
b1 = tf.Variable(tf.random_normal([hid1_size, label_size]), name='b1')
y1 = tf.nn.relu(tf.add(tf.matmul(w1, tf.transpose(ph_inputs)), b1))

# the output layer
wo = tf.Variable(tf.random_normal([label_size, hid1_size], stddev=0.01), name='wo')
bo = tf.Variable(tf.random_normal([label_size, label_size]), name='bo')
yo = tf.transpose(tf.add(tf.matmul(wo, y1), bo))

# defines optimizer and predictor
lr = tf.placeholder(tf.float32, shape=(), name='learning_rate')
loss = tf.losses.mean_squared_error(ph_labels,yo)
optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)
predictor = tf.identity(yo)

# TRAINING 
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
_, c = sess.run([optimizer, loss], feed_dict={lr:0.05, ph_inputs: inputs, ph_labels: labels})
# TRAINING 

# gets the regression results
predictions = np.zeros((input_size,1))
for i in range(input_size):
    predictions[i] = sess.run(predictor, feed_dict={ph_inputs: inputs[i, None]}).squeeze()

# prints regression MSE
print(mean_squared_error(predictions, labels))

",2,548,"You're right, you understood the problem by yourself.

The problem is, in fact, that you're running the optimization step only one time. Hence you're doing one single update step of your network parameter and therefore the cost won't decrease.

I just changed the training session of your code in order to make it work as expected (100 training steps):

# TRAINING
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
for i in range(100):
    _, c = sess.run(
        [optimizer, loss],
        feed_dict={
            lr: 0.05,
            ph_inputs: inputs,
            ph_labels: labels
        })
    print(""Train step {} loss value {}"".format(i, c))
# TRAINING


and at the end of the training step I go:


  Train step 99 loss value 0.04462708160281181
  
  0.044106700712455045

",,
tensorflow unexpected issue,https://stackoverflow.com/questions/64620236,Error &quot;ValueError: Graph disconnected&quot; in Keras/Tensorflow,,1,847,"pay attention to not override the input variables. you overrode color, clarity and x input inside the network
here a possible solution:
dense1 = 2**7
dense2 = 2**8
dense3 = 2**9
dropout = 0.8
price_loss = 1
cut_loss = 1
activation= LeakyReLU()
batch_size = 32

#====================================================================
# INPUTS
#====================================================================
carat = Input(shape= (1,), batch_size= batch_size, name= 'carat')
Color = Input(shape= (1,), batch_size= batch_size, name= 'color')
Clarity = Input(shape= (1,), batch_size= batch_size, name= 'clarity')
depth = Input(shape= (1,), batch_size= batch_size, name= 'depth')
table = Input(shape= (1,), batch_size= batch_size, name= 'table')
X = Input(shape= (1,), batch_size= batch_size, name= 'x')
y = Input(shape= (1,), batch_size= batch_size, name= 'y')
z = Input(shape= (1,), batch_size= batch_size, name= 'z')
#====================================================================
# CREATE EMBEDDINGS FOR CATEGORICAL FEATURES ""COLOR"" AND ""CLARITY""
#====================================================================
color = Embedding(input_dim = 7, output_dim = 1, name = 'color_emb')(Color)
clarity = Embedding(input_dim = 8, output_dim = 1, name = 'clarity_emb')(Clarity)
color = Flatten()(color)
clarity = Flatten()(clarity)
#====================================================================
# CONCATENATE FEATURES
#====================================================================
x = Concatenate()([color, clarity, carat, depth, table, X, y, z])
#====================================================================
# DENSE NETWORK
#====================================================================
x = Dense(dense1, activation = activation)(x)
x = BatchNormalization()(x)
x = Dense(dense2, activation = activation)(x)
x = BatchNormalization()(x)
x = Dense(dense3, activation = activation)(x)
x = BatchNormalization()(x)
x = Dropout(dropout)(x)
#====================================================================
# PREDICTIONS
# ====================================================================
cut = Dense(1, activation = 'sigmoid')(x)
price = Dense(1)(x)
#====================================================================
# DEFINE THE MODEL
# ====================================================================
model = Model(inputs = [carat, Color, Clarity, depth, table, X, y, z] , 
              outputs = [cut , price])
model.compile('adam', 'mse')
model.summary()

here the running notebook: https://colab.research.google.com/drive/1akpvuGKHXt6Frrec692zvCMAfRzZbIaM?usp=sharing
","Well, like always when we are stuck with some error, the answer lies somewhere in error message. Please note that while you have been able to build the structure of your model overwriting your variables color, clarity and so on... the variables changed the type. At the beginning you initialize the variables to be tf.keras.Input and then you overwrite it with Embedding and Flatten layers. So when it comes to building your model, you ask to build model with inputs that are no longer tf.keras.Input but something else.
The solution is to not overwrite those variables.
",
tensorflow unexpected issue,https://stackoverflow.com/questions/60173337,How to manipulate and return tf.Variable using a for loop over tf.data.Dataset inside function decorated with @tf.function?,,1,346,"Running this on TensorFlow 2.1.0 reproduces your scenario.

Which prints 1 2 3 0 for test.fn2(), but you should also consider that when you print self.var in test.fn3() it will also show you self.var = 0 during the function call.

Modified fn3( ):

    @tf.function
    def fn3(self):
        self.var.assign(0)
        y = self.var
        for _ in self.dataset:
            self.var.assign(self.var+self.increment)
            y = self.var
            tf.print(y)
        tf.print(self.var)  # Inspect self.var value
        tf.print(y)
        return y


Output:

# Executed in Tensorflow 2.1.0
# test.fn3()
1
2
3
0  &lt;&lt; self.var
3


This is already fixed If you execute this in Tensorflow 2.2.0-rc2.
The output will be your desired outcome even when printing it during graph execution.

To quickly simulate this you could use Google Colab and use %tensorflow_version 2.x to get the latest available version for Tensorflow.

Output:

# Executed in Tensorflow 2.2.0-rc2
Function 1
1
2
3
3
Function 2
1
2
3
3
Function 3
1
2
3
3 &lt;&lt; Value of self.var in test.fn3()
3
Function 4
1
2
3
3


You could check more about the changes in the latest Tensorflow Updates in this link.  
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/69885201,Mamba can&#39;t create env giving &#39;nothing provides&#39; but pip sees such version of opencv-python,"I need to create env with

mamba create --name tf1_12__HDR Tensorflow==1.12.0 Keras==2.2.4 opencv-python==3.4.4.19 anaconda

It provides
mamba (0.7.3) supported by @QuantStack

...

Looking for: ['tensorflow==1.12.0', 'keras==2.2.4', 'opencv-python==3.4.4.19', 'anaconda']

...

Problem: nothing provides requested opencv-python 3.4.4.19

I am calling

pip install opencv-python==

ERROR: Could not find a version that satisfies the requirement opencv-python== (from versions: 3.4.2.17, 3.4.3.18, 3.4.4.19, 3.4.5.20, 3.4.6.27, 3.4.7.28, 3.4.8.29, 3.4.9.31, 3.4.9.33, 3.4.10.35, 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.14.51, 3.4.14.53, 3.4.15.55, 3.4.16.57, 4.0.0.21, 4.0.1.23, 4.0.1.24, 4.1.0.25, 4.1.1.26, 4.1.2.30, 4.2.0.32, 4.2.0.34, 4.3.0.36, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.2.52, 4.5.2.54, 4.5.3.56, 4.5.4.58)
ERROR: No matching distribution found for opencv-python==

So such version exists.
I know a cheat to overcome issue using steps:
1 Create env without opencv-python
2 Install opencv-python with pip
but it can lead to conflicts inside enviroment and potentially break it.
Can you please provide a ""clean"" solution and/or describe the reason of this
unexpected output?
",1,2235,"PyPI and Anaconda Cloud packages don't necessarily go by the same names. The opencv package from Conda Forge will include the OpenCV libs (libopencv) and the Python interface (py-opencv). That is, install opencv with the version you want.
BTW, as mentioned in the comments == is Pip syntax, Conda/Mamba use single =.
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/60606297,Cannot convert tf.keras.layers.ConvLSTM2D layer to open vino intermediate representation,"I am trying to convert a trained model in tensorflow to Open VINO Intermediate Representation.

I have a model of the form given below

class Conv3DModel(tf.keras.Model):
    def __init__(self):
        super(Conv3DModel, self).__init__()
        # Convolutions
        self.conv1 = tf.compat.v2.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=""conv1"", data_format='channels_last')
        self.pool1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')
        self.conv2 = tf.compat.v2.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', name=""conv1"", data_format='channels_last')
        self.pool2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')

        # LSTM &amp; Flatten
        self.convLSTM =tf.keras.layers.ConvLSTM2D(40, (3, 3))
        self.flatten =  tf.keras.layers.Flatten(name=""flatten"")

        # Dense layers
        self.d1 = tf.keras.layers.Dense(128, activation='relu', name=""d1"")
        self.out = tf.keras.layers.Dense(6, activation='softmax', name=""output"")


    def call(self, x):
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.convLSTM(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.out(x)


I tried to convert the model into IR. The model is here .

I have trained this model in tensorflow 1.15. Tensorflow 2.0 is currently not supported.

Now I tried to run the command

python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --saved_model_dir jester_trained_models/3dcnn-basic/  --output_dir /home/deepanshu/open_vino/udacity_project_custom_model/

Now i got the following error

Model Optimizer arguments:

Common parameters: 


Path to the Input Model: None
Path for generated IR: /home/deepanshu/open_vino/udacity_project_custom_model/
IR output name: saved_model
Log level: ERROR
Batch: Not specified, inherited from the model
Input layers: Not specified, inherited from the model
Output layers: Not specified, inherited from the model
Input shapes: Not specified, inherited from the model
Mean values: Not specified
Scale values: Not specified
Scale factor: Not specified
Precision of IR: FP32
Enable fusing: True
Enable grouped convolutions fusing: True
Move mean values to preprocess section: False
Reverse input channels: False


TensorFlow specific parameters:


Input model in text protobuf format: False
Path to model dump for TensorBoard: None
List of shared libraries with TensorFlow custom layers implementation: None
Update the configuration file with input/output node names: None
Use configuration file used to generate the model with Object Detection API: None
Operations to offload: None
Patterns to offload: None
Use the config file: None


Model Optimizer version: 2020.1.0-61-gd349c3ba4a

[ ERROR ] Unexpected exception happened during extracting attributes for node conv3d_model/conv_lst_m2d/bias/Read/ReadVariableOp. Original exception message: 'ascii' codec can't decode byte 0xc9 in position 1: ordinal not in range(128)

As far as  I can see it is the tf.keras.layers.ConvLSTM2D(40, (3, 3)) causing problems . I am kind of stuck here . Can anyone tell me where can I proceed further ?

Thanks

Edit to the question

Now I rejected the above tensorflow implementation and used keras . My h5 model developed was converted into .pb format using this post.

Now I ran the model optimizer on this .pb file. Using the command 

python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_model /home/deepanshu/ml_playground/jester_freezed/tf_model.pb  --output_dir /home/deepanshu/open_vino/udacity_project_custom_model/  --input_shape=[1,30,64,64,1] --data_type FP32


Now i am facing another issue . The issue here is point no. 97 on this post.

So my model contains a cycle and model optimizer does not know a way to convert it. Has anybody faced this issue before ?

Please help.

Here is the model .

Here is the defination of the model in keras


from keras.models import Sequential

from keras.layers import Conv3D , MaxPool3D,Flatten ,Dense

from keras.layers.convolutional_recurrent import ConvLSTM2D

import keras


model = Sequential()

model.add(Conv3D(32, (3, 3, 3), 

         name=""conv1"" , input_shape=(30, 64, 64,1) ,  data_format='channels_last',

        activation='relu') )

model.add(MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last'))

model.add(Conv3D(64, (3, 3, 3), activation='relu', name=""conv2"", data_format='channels_last'))

model.add(MaxPool3D(pool_size=(2, 2,2), data_format='channels_last'))

model.add(ConvLSTM2D(40, (3, 3)))

model.add(Flatten(name=""flatten""))

model.add(Dense(128, activation='relu', name=""d1""))

model.add(Dense(6, activation='softmax', name=""output""))

",1,425,"Actually the script to convert from h5 to .pb suggested by intel was not good enough. Always use the code from here to convert your keras model to .pb. 

Once you obtain your .pb file now convert your model to IR using

python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_model ml_playground/try_directory/tf_model.pb   --output_dir /home/deepanshu/open_vino/udacity_project_custom_model/  --input_shape=[1,30,64,64,1] --data_type FP32


After the execution of this script we can obtain the intermediate representation of the keras model. 
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/52080424,TypeError while Running Tensorflow Deep Dream Tutorial from Python: TypeError: __init__() got an unexpected keyword argument &#39;serialized_options&#39;,"I have taken the tensorflow deepdream example and have converted the Jupyter Notebook to a normal Python module.

When running the module from Eclipse / Python 3.6, I get the following traceback:


Traceback (most recent call last):
  File ""C:\git\tensorflow\tensorflow\examples\tutorials\deepdream\deepdream_python.py"", line 43, in 
    import tensorflow as tf
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in 
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 52, in 
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 15, in 
    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\core\framework\node_def_pb2.py"", line 15, in 
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\core\framework\attr_value_pb2.py"", line 15, in 
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\core\framework\tensor_pb2.py"", line 15, in 
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\core\framework\resource_handle_pb2.py"", line 22, in 
    serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\xf8\x01\x01\x62\x06proto3')
TypeError: __init__() got an unexpected keyword argument 'serialized_options'


There is a similar stackoverflow question where the user was able to workaround the problem by updating tensorboard to 1.9.0.  I have the tensorflow and tensorboard 1.10.0 PyPi packages installed and even tried with version 1.9.0, but this doesn't resolve the problem.

There is another question that looks related to the issue, but no one has answered it.
",1,1686,"Installing tensorflow 1.9.0 was a workaround for this issue.

pip install tensorflow==1.9.0
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/50417129,Installation issues with Tensorflow; PermissionError,"I've been having a bit of difficulty installing tensorflow onto my computer, and would like some advice on where to go from here. To be clear, I am trying to install on Windows using the advice for anaconda in windows from the tensorflow website, and I am stuck at part 4 of installing the version without GPU support.

At first, I was having a TypeError, where parse() got an unexpected keyword (the issue in this thread). So I ran the conda install suggestions which were provided in the thread.

After that, I successfully installed both and tried to run:

pip install --ignore-installed --upgrade tensorflow

Which ran for a bit, but then my command prompt stated that I had thrown a PermissionError and that access is denied (the same error found here). To get past this, I tried opening Windows Powershell as an administrator instead, which still gave me this error. If you have any suggestions on what I could try next, it would be greatly appreciated.

EDIT: Here is the traceback error I get:

Traceback (most recent call last):
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\commands\install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 784, in install
    **kwargs
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 851, in install
    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 1064, in move_wheel_files
    isolated=self.isolated,
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\wheel.py"", line 352, in move_wheel_files
    clobber(source, lib_dir, True)
  File ""C:\Users\awm\Anaconda3\lib\site-packages\pip\wheel.py"", line 323, in clobber
    os.unlink(destfile)
PermissionError: [WinError 5] Access is denied: 'C:\\Users\\awm\\Anaconda3\\Lib\\site-packages\\numpy\\.libs\\libopenblas.BNVRK7633HSX7YVO2TADGR4A5KEKXJAW.gfortran-win_amd64.dll'
",1,592,"After restart of kernal and without loading any library try pip install --ignore-installed --upgrade tensorflow .
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/37111877,"creating a variable within tf.variable_scope(name), initialized from another variable&#39;s initialized_value","Hey tensorflow community,

I am experiencing unexpected naming conventions when using variable_scope in the following setup:

with tf.variable_scope(""my_scope""):
    var = tf.Variable(initial_value=other_var.initialized_value())


In the above, it holds that 

other_var.name = 'outer_scope/my_scope/other_var_name:0'


I am therefore ""reusing"" the same scope at this point in the code. Intuitively I do not see an issue with this, but the following happens:

var.name = 'outer_scope/my_scope_1/var_name:0'


So apparently, tf isn't happy with ""my_scope"" and needs to append the ""_1"". 
The ""outer_scope"" remains the same, though.

If I do not initialize with ""other_var"", this behaviour does not come up.

An explanation would be much appreciated! Thx

Mat
",1,573,"You might want to use tf.get_variable() instead of 'tf.Variable`.

with tf.variable_scope('var_scope', reuse=False) as var_scope:
    var = tf.get_variable('var', [1])
    var2 = tf.Variable([1], name='var2')
    print var.name # var_scope/var:0 
    print var2.name # var_scope/var2:0

with tf.variable_scope('var_scope', reuse=True) as var_scope:
    var = tf.get_variable('var', [1])
    var2 = tf.Variable([1], name='var2')
    print var.name # var_scope/var:0 
    print var2.name # var_scope_1/var2:0


The reason behind this I think is that in your example, although you have successfully ""re-entered"" the variable_scope you want, what really affects your variable name is another scope named name_scope intead of variable_scope as you might guess. From the official document here you can see that:


  when we do with tf.variable_scope(""name""), this implicitly opens a
  tf.name_scope(""name"").


name_scope is originally used for managing operation names(such as add, matmul), because tf.Variable is actually an operation and its operation name will be ""inherited"" by variables created by it, so the name of name_scope rather than variable_scope is used as prefix.

But if you want to use tf.Variable, you can also directly use name_scope in with statement:

with tf.name_scope('n_scope') as n_scope:
    var = tf.Variable([1], name='var')
    print var.name #n_scope/var_1:0

with tf.name_scope(n_scope) as n_scope:
    var = tf.Variable([1], name='var')
    print var.name #n_scope/var_1:0


One thing to pay attention to is that you should pass as argument the scope varible previously captured from a with statement when you want to ""re-enter"" a name scope, rather than using str scope name:

  with tf.name_scope('n_scope') as n_scope:
      var = tf.Variable([1], name='var')
      print var.name #n_scope/var_1:0

  with tf.name_scope('n_scope') as n_scope:
      var = tf.Variable([1], name='var')
      print var.name #n_scope_1/var_1:0


Pay attention to the argument passed to tf.name_scope. This behavior is again described in doc string of name_scope:


  The name argument will be interpreted as follows:
  
  
  A string (not ending with /) will create a new name scope, in which
  name is appended to the prefix of all operations created in the
  context. If name has been used before, it will be made unique by
  calling self.unique_name(name). 
  A scope previously captured from a with g.name_scope(...) as 
  scope: statement will be treated as an absolute name scope, 
  which makes it possible to re-enter existing scopes. 
  A value of None or the empty string will reset the current name 
  scope to the top-level (empty) name scope.
  

",,
tensorflow unexpected issue,https://stackoverflow.com/questions/48850537,Issue with Imagenet classification with VGG16 pretrained weights,,0,1688,"It seems that we can (or need to?) use the session from Keras (which has the associated loaded graph with weights) instead of creating a new session in Tensorflow and using the graph obtained from Keras model like below  

VGG = model.graph  


I think the graph got above has no weights in it (and that is the reason predictions are wrong) and the graph from Keras session as the proper weights (so those two graph instances should be different)  

Below is the full code:  

import tensorflow as tf
import numpy as np
from PIL import Image
from tensorflow.python.keras._impl.keras.applications import imagenet_utils
from tensorflow.python.keras._impl.keras import backend as K


model = tf.keras.applications.VGG16()
sess = K.get_session()
VGG = model.graph #Not needed and also doesnt have weights in it

VGG.get_operations()
input = VGG.get_tensor_by_name(""input_1:0"")
output = VGG.get_tensor_by_name(""predictions/Softmax:0"")
print(input)
print(output)

I = Image.open(""Elephant.jpg"")
new_img = I.resize((224,224))
image_array = np.array(new_img)[:, :, 0:3]
image_array = np.expand_dims(image_array, axis=0)
image_array = image_array.astype(np.float32)
image_array = tf.keras.applications.vgg16.preprocess_input(image_array)

pred = (sess.run(output,{input:image_array}))
print(imagenet_utils.decode_predictions(pred))


And this gives the expected result:


  [[('n02504458', 'African_elephant', 0.8518132), ('n01871265', 'tusker', 0.1398836), ('n02504013', 'Indian_elephant', 0.0082286), ('n01704323', 'triceratops', 6.965483e-05), ('n02397096', 'warthog', 1.8662439e-06)]]


Thanks Idavid for the tip about using preprocess_input() function and Nicolas for tips about unloaded weights.
","I think there is 2 mistakes, the first one is that you must rescale your image by dividing by 255 all pixels.

I = Image.open(""Elephant.jpg"")
new_img = I.resize((224,224))
image_array = np.array(new_img)[:, :, 0:3]
image_array /= 255.
image_array = np.expand_dims(image_array, axis=0)


The second point I got it while looking at prediction values. You have a vector of 1000 element and all of them have 0.1% prediction after rescale. That means you have a non-trained model. I don't know exactly how to have if loaded in tensorflow but on Keras for example you can do :

app = applications.vgg16
model = app.VGG16(
        include_top=False,    # this is to have the classifier Standard from imagenet
        weights='imagenet',   # this load weight, else it's random weight
        pooling=""avg"") 


From what I've read, you have to download another file containing weight from for example github.

I hope it helps,

EDIT1:

I tried the same model usign Keras :

from keras.applications.vgg16 import VGG16, decode_predictions
import numpy as np

model = VGG16(weights='imagenet')

I = Image.open(""Elephant.jpg"")
new_img = I.resize((224,224))
image_array = np.array(new_img)[:, :, 0:3]
image_array = image_array/255.
x = np.expand_dims(image_array, axis=0)

preds = model.predict(x)
print('Predicted:', decode_predictions(preds, top=5)[0])


If I comment the rescaling, I have bad predictions :


  Predicted: [('n03788365', 'mosquito_net', 0.22725257), ('n15075141', 'toilet_tissue', 0.026636025), ('n04209239', 'shower_curtain', 0.019786758), ('n02804414', 'bassinet', 0.01353887), ('n03131574', 'crib', 0.01316699)]


Without the rescale, this is good :


  Predicted: [('n02504458', 'African_elephant', 0.95870858), ('n01871265', 'tusker', 0.040065952), ('n02504013', 'Indian_elephant', 0.0012253703), ('n01704323', 'triceratops', 5.0949382e-08), ('n02454379', 'armadillo', 5.0408511e-10)]


Now if I remove the weight, I have the ""same"" as what I have with Tensorflow:


  Predicted: [('n07717410', 'acorn_squash', 0.0010033853), ('n02980441', 'castle', 0.0010028203), ('n02124075', 'Egyptian_cat', 0.0010028186), ('n04179913', 'sewing_machine', 0.0010027955), ('n02492660', 'howler_monkey', 0.0010027081)]


To me, that means that you have no weight applied. Maybe they are downloaded but not used. 
",
tensorflow unexpected issue,https://stackoverflow.com/questions/53331211,Tensorflow bazel quantization build error,,0,1193,"To verify where targets are, look into the BUILD file in the package directory. 

The quantize_graph target has been moved to the //tensorflow/contrib/quantize package. This should work:

$ bazel build --config=cuda //tensorflow/contrib/quantize:quantize_graph


The toco target has been moved from //tensorflow/contrib/lite/toco to //tensorflow/lite/toco. Like quantize_graph, this should work:

$ bazel build --config=cuda //tensorflow/lite/toco:toco

",,
tensorflow unexpected issue,https://stackoverflow.com/questions/70096286,Cannot convert between a TensorFlowLite tensor with type FLOAT32 and a Java object,"I am trying to build a recommendation system using TFlite on Android . I have created the model successfully and have also run inference on the same , which is running pretty good. But the problem lies within trying to integrate the application with the model . I am trying to integrate the model to the official application provided by the tensorflow team .I have done all the steps asked by them , but the problem I faced was regarding the input/output towards the model . I faced the error saying :

Cannot convert between a TensorFlowLite tensor with type FLOAT32 and
a Java object of type [I (which is compatible with the TensorFlowLite
type INT32).

I am not able to understand what this error means nor there are any documentation regarding the same.
The code present for the purpose of input and output in the official code are as follows :
This is the main code wherein the input and outputs are defined :
/** Given a list of selected items, and returns the recommendation results. */
  @WorkerThread
  public synchronized List&lt;Result&gt; recommend(List&lt;MovieItem&gt; selectedMovies) {
    Object[] inputs = preprocess(selectedMovies);

    // Run inference.
    float[] outputIds = new float[config.outputLength];
    float[] confidences = new float[config.outputLength];
    Map&lt;Integer, Object&gt; outputs = new HashMap&lt;&gt;();
    outputs.put(config.outputIdsIndex, outputIds);
    outputs.put(config.outputScoresIndex, confidences);
    tflite.runForMultipleInputsOutputs(inputs, outputs);

    return postprocess(outputIds, confidences, selectedMovies);
  }

This defines the preprocessing part :

  int[] preprocessIds(List&lt;MovieItem&gt; selectedMovies, int length) {
    int[] inputIds = new int[length];
    Arrays.fill(inputIds, config.pad); // Fill inputIds with the default.
    int i = 0;
    for (MovieItem item : selectedMovies) {
      if (i &gt;= inputIds.length) {
        break;
      }
      inputIds[i] = item.id;
      ++i;
    }
    return inputIds;
  }

  int[] preprocessGenres(List&lt;MovieItem&gt; selectedMovies, int length) {
    // Fill inputGenres.
    int[] inputGenres = new int[length];
    Arrays.fill(inputGenres, config.unknownGenre); // Fill inputGenres with the default.
    int i = 0;
    for (MovieItem item : selectedMovies) {
      if (i &gt;= inputGenres.length) {
        break;
      }
      for (String genre : item.genres) {
        if (i &gt;= inputGenres.length) {
          break;
        }
        inputGenres[i] = genres.containsKey(genre) ? genres.get(genre) : config.unknownGenre;
        ++i;
      }
    }
    return inputGenres;
  }

  /** Given a list of selected items, preprocess to get tflite input. */
  @WorkerThread
  synchronized Object[] preprocess(List&lt;MovieItem&gt; selectedMovies) {
    List&lt;Object&gt; inputs = new ArrayList&lt;&gt;();

    // Sort features.
    List&lt;Feature&gt; sortedFeatures = new ArrayList&lt;&gt;(config.inputs);
    Collections.sort(sortedFeatures, (Feature a, Feature b) -&gt; Integer.compare(a.index, b.index));

    for (Feature feature : sortedFeatures) {
      if (Config.FEATURE_MOVIE.equals(feature.name)) {
        inputs.add(preprocessIds(selectedMovies, feature.inputLength));
      } else if (Config.FEATURE_GENRE.equals(feature.name)) {
        inputs.add(preprocessGenres(selectedMovies, feature.inputLength));
      } else {
        Log.e(TAG, String.format(""Invalid feature: %s"", feature.name));
      }
    }
    return inputs.toArray();
  }

What are the needed changes to get the recommendation's working ?
Edit :
I was able to solve the above issue . I found that the input required for genres were of type float , so a float array of genres was passed as input and the issue was resolved .
However, a new error occurred saying :

java.lang.IllegalStateException: Internal error: Unexpected failure
when preparing tensor allocations:
tensorflow/lite/kernels/reshape.cc:66 num_input_elements !=
num_output_elements (10 != 32)
Node number 0 (RESHAPE) failed to prepare.

The issue is related to the input and the output elements not matching . I am not able to figure out the solution for the same .

The link to the model.tflite can be found here :

https://drive.google.com/file/d/1CZxlJRqLZmwrsmgcA8lBz6XCh2KG3lWa/view?usp=sharing

",0,659,"This question is a result of utter confusion and bit of being mislead by the colab file . The colab file present in the tensorflow/examples/recommendation guides into creating a tensorflow lite model with three inputs which are genre, rating and movie-id , but the android application present in the same repository implements code to consider only two inputs which are movie id and genre . The colab can be found at :

https://colab.research.google.com/github/tensorflow/examples/blob/master/lite/examples/recommendation/ml/ondevice_recommendation.ipynb#scrollTo=og0qkYavz3Nt

As per guidance given by @Farmaker, I visualized my model and the model present inside the android application present in the tensorflow-recommendation repo .Here how it looks like :
My-model :

Google's-model :

I without considering the code used for interpreting the .tflite model inside android blindly followed the Google colab file wherein it just mentions the collaboration of the .tflite with android without any extra coded required .
Solution :
First error :

Cannot convert between a TensorFlowLite tensor with type FLOAT32 and a
Java object of type [I (which is compatible with the TensorFlowLite
type INT32).

Since the input object needs to be in a particular format of how the model requires , the second input should have been in the float type but I was passing it int as the parameter . This lead to the particular error which was resolved after passing the parameter's in a particular order which is the required format for the .tflite model .
Second Error :

java.lang.IllegalStateException: Internal error: Unexpected failure
when preparing tensor allocations:
tensorflow/lite/kernels/reshape.cc:66 num_input_elements !=
num_output_elements (10 != 32) Node number 0 (RESHAPE) failed to
prepare.

This error was due to the genres parameter required the float array to be of a size of 32 , but I was only providing the model with a float array of size 10 which on observing the error can be easily said so . This was resolved by me by passing the genre float array of size 32 .
There were several  required changes to be made in the android repository of recommendation system  which were made by me and the code works fine .
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/66044806,Issue installing zip file created by setup.py for deploying custom prediction to AI platform,"Am following google doc on creating custom prediction(https://cloud.google.com/ai-platform/prediction/docs/custom-prediction-routines). While building a new version for a model AI-platform prediction API threw below error:
Error Create Version failed. Bad model detected with error: ""Failed to load model: User-provided package &lt;&gt;-0.1.tar.gz failed to install: Command '['python-default', '-m', 'pip', 'install', '--target=/tmp/custom_lib', '--no-cache-dir', '-b', '/tmp/pip_builds', '/tmp/custom_code/&lt;&gt;.tar.gz']' returned non-zero exit status 1. (Error code: 0)""
Was testing my zip file locally
pip install --target=/tmp/custom_lib --no-cache-dir -b /tmp/pip_builds dist/&lt;&gt;
Throws below error:
Processing ./dist/&lt;tar&gt;
    ERROR: Command errored out with exit status 1:
     command: bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/mj/b_g4b4yj26q278g9k58w92fck5d9wj/T/pip-req-build-9dmmje_z/setup.py'""'""'; __file__='""'""'/private/var/folders/mj/b_g4b4yj26q278g9k58w92fck5d9wj/T/pip-req-build-9dmmje_z/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/mj/b_g4b4yj26q278g9k58w92fck5d9wj/T/pip-pip-egg-info-ic3qhcpo
         cwd: /private/var/folders/mj/b_g4b4yj26q278g9k58w92fck5d9wj/T/pip-req-build-9dmmje_z/
    Complete output (6 lines):
    Parent directory: /private/var/folders/mj/b_g4b4yj26q278g9k58w92fck5d9wj/T/pip-req-build-9dmmje_z
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""/private/var/folders/mj/b_g4b4yj26q278g9k58w92fck5d9wj/T/pip-req-build-9dmmje_z/setup.py"", line 22, in &lt;module&gt;
        REQUIRED_PACKAGES = [line.strip() for line in open(base/""requirements.txt"")] + \
    FileNotFoundError: [Errno 2] No such file or directory: '/private/var/folders/mj/b_g4b4yj26q278g9k58w92fck5d9wj/T/pip-req-build-9dmmje_z/requirements.txt'
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.

Ran ""python setup.py egg_info"" on the mac terminal, nothing useful popped.
Parent directory: .

Packages: ['numpy==1.17.3', 'pandas==1.2.1', 'setuptools==53.0.0', 'Pillow==6.2.1', 'torchvision==0.5.0', 'torch @ https://download.pytorch.org/whl/cpu/torch-1.4.0%2Bcpu-cp37-cp37m-linux_x86_64.whl']


running egg_info
writing &lt;&gt;.egg-info/PKG-INFO
writing dependency_links to &lt;&gt;.egg-info/dependency_links.txt
writing requirements to &lt;&gt;.egg-info/requires.txt
writing top-level names to &lt;&gt;.egg-info/top_level.txt
reading manifest file '&lt;&gt;.egg-info/SOURCES.txt'
writing manifest file '&lt;&gt;.egg-info/SOURCES.txt'

Python packages:
Packages: ['numpy==1.17.3', 'pandas==1.2.1', 'setuptools==53.0.0', 'Pillow==6.2.1', 'torchvision==0.5.0', 'torch @ https://download.pytorch.org/whl/cpu/torch-1.4.0%2Bcpu-cp37-cp37m-linux_x86_64.whl']
Setup.py
from pathlib import Path
""""""
Purpose: Package your Predictor and its dependencies by packaging  Predictor as a .tar.gz.

For deploying any PyTorch model, the PyTorch package is needed to be provided in the below-mentioned command. In this case, 
we have used the package:
torch-1.3.1+cpu-cp37-cp37m-linux_x86_64.whl
For getting pytorch packages according to your requirements,
Visit this link: Pytorch Packages(google search)
Download the package you require and store it in the GCS bucket.
The package we are using provides version resource with PyTorch 1.3.1 for Python 3.7, built to run on a CPU in Linux.

NOTE: We could have directly added torch in the REQUIRED_PACKAGES list in setup.py file in order to provide pytorch as a dependency to be installed while deployment.
This way, it uses the PyPI installation of pytorch.
But the issue with this approach is that it internally downloads PyPI package for pytorch which is of 720 MB Which causes the failure of our model deployment because the AI platform allows custom models of 500MB or below to be deployed on it.
So it is recommended to provide a pytorch package manually using gsutil command under  package-uris.
Try: pip install --target=/tmp/custom_lib --no-cache-dir -b /tmp/pip_builds &lt;&gt;.4.tar.gz for troubleshooting
""""""
base = Path(__file__).parent
print(f""Parent directory: {base}"")
REQUIRED_PACKAGES = [line.strip() for line in open(base/""requirements.txt"")] + \
['torchvision==0.5.0', 'torch @ https://download.pytorch.org/whl/cpu/torch-1.4.0%2Bcpu-cp37-cp37m-linux_x86_64.whl']

print(f""\nPackages: {REQUIRED_PACKAGES}\n\n"")

setup(description=""Extract features of a image"",
      author=&lt;&gt;,
      author_email=&lt;&gt;,
      name=&lt;&gt;,
      version=&lt;&gt;,
      url='&lt;&gt;',
      install_requires=REQUIRED_PACKAGES,
      project_urls={
          'Documentation': 'https://cloud.google.com/ai-platform/prediction/docs/custom-prediction-routines#tensorflow',
          'Deploy': 'https://cloud.google.com/ai-platform/prediction/docs/deploying-models#gcloud_1',
          'Ai_platform troubleshooting': 'https://cloud.google.com/ai-platform/training/docs/troubleshooting',
          'Say Thanks!': ['https://medium.com/searce/deploy-your-own-custom-model-on-gcps-ai-platform-7e42a5721b43',
                          'https://stackoverflow.com/questions/61933879/unexpected-error-when-loading-the-model-problem-in-predictor-modulenotfounder',
                          'https://stackoverflow.com/questions/56428037/what-is-the-best-way-to-save-pil-image-in-json'],
          'google Torch wheels': ""http://storage.googleapis.com/cloud-ai-pytorch/readme.txt"",
          'Torch &amp; torchvision wheels': ""https://download.pytorch.org/whl/torch_stable.html "",
          'Raised issue with google github branch': ""https://github.com/GoogleCloudPlatform/python-docs-samples/issues/5257"",
          ""Stackoverflow"": ""https://stackoverflow.com/questions/65795374/gcp-ai-platform-error-when-creating-a-custom-predictor-model-version-trained""
      },
    python_requires='~=3.7',
    scripts=[&lt;&gt;])```
Have tried the usual upgrading python setuptools to 52.0.0, but can't seem to shake this error off. 

",0,540,"I was able to make it work by removing the 'requirement.txt' and copy packages directly to setup.py. Don't know the real reason for this weird behaviour.
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/63783513,Keras Functional API issue with Input layer and first LSTM layer,"I am trying to create a Functional API as opposed to a Sequential API. I have built the model previously using the Sequential API, and it worked just fine. It is an LSTM, and I am having trouble with the batch_size going from the Input to the LSTM layer. The Sequential API was built as follows:
new_model = Sequential()
new_model.add(LSTM(n_neurons, batch_input_shape=(batch_size,train_X.shape[1], train_X.shape[2]), activation='tanh', stateful=True, return_sequences=True))
new_model.add(Dropout(0))
new_model.add(LSTM(n_neurons, batch_input_shape=(batch_size,train_X.shape[1], train_X.shape[2]), activation='tanh', stateful=True))
new_model.add(Dropout(0))
new_model.add(Dense(n_neurons1, activation='tanh'))
new_model.add(Dropout(0.1))
new_model.add(Dense(nm))
new_model.compile(loss='mse', optimizer=optimizer)

The above snippet works fine. The Functional API I am trying to get to work is as follows:
inp = Input(shape = (train_X.shape[1], train_X.shape[2]), batch_size = batch_size)
L1 = LSTM(n_neurons, batch_input_shape=(batch_size,train_X.shape[1], train_X.shape[2]), activation='tanh', stateful=True, return_sequences=True)(inp)
D1 = Dropout(0)(L1)
L2 = LSTM(n_neurons, batch_input_shape=(batch_size,train_X.shape[1], train_X.shape[2]), activation='tanh', stateful=True, return_sequences=True)(D1)
D2 = Dropout(0)(L2)
F1 = Dense(n_neurons1, activation='tanh')(D2)
D3 = Dropout(0.1)(F1)
out = Dense(nm)
new_model = Model(inp,out)
new_model.compile(loss='mse', optimizer=optimizer)

I get an error saying ""Input() got an unexpected keyword argument 'batch_size"", even though I know batch_size is an argument for the Input layer. Then, if I get rid of the argument, I get an error with the first LSTM layer saying:
""If a RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors:

If using a Sequential model, specify the batch size by passing a batch_input_shape argument to your first layer.
If using the functional API, specify the batch size by passing a batch_shape argument to your Input layer.""

I have already tried updating tensorflow but that did not fix the Input() issue. Where do I go from here?
",0,806,"You describe passing a batch_size parameter via the functional API and getting an error suggesting ""passing a batch_shape argument to your Input layer.""
If you try changing batch_size = batch_size in your input layer to
batch_shape = (batch_size,train_X.shape[1], train_X.shape[2])

does that solve it?
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/61185335,React Native cameraWithTensors onReady is not being called,"I'm trying to capture camera stream using cameraWithTensors from tensorflow/tfjs-react-native example from https://js.tensorflow.org/api_react_native/0.2.3. I'm using react-navigation button tabs (do not know if this is related). 

import { Camera } from 'expo-camera';
import { cameraWithTensors } from '@tensorflow/tfjs-react-native';

const TensorCamera = cameraWithTensors(Camera);

class MyComponent {

  handleCameraStream(images, updatePreview, gl) {
    const loop = async () =&gt; {
      const nextImageTensor = images.next().value

      console.log(""*** This line is not logged ***"");

      // if autorender is false you need the following two lines.
      // updatePreview();
      // gl.endFrameEXP();

      requestAnimation(loop);
    }
    loop();
  }

  render() {
   // Currently expo does not support automatically determining the
   // resolution of the camera texture used. So it must be determined
   // empirically for the supported devices and preview size.

   let textureDims;
   if (Platform.OS === 'ios') {
    textureDims = {
      height: 1920,
      width: 1080,
    };
   } else {
    textureDims = {
      height: 1200,
      width: 1600,
    };
   }

   return &lt;View&gt;
     &lt;TensorCamera
      // Standard Camera props
      style={styles.camera}
      type={Camera.Constants.Type.front}
      // Tensor related props
      cameraTextureHeight={textureDims.height}
      cameraTextureWidth={textureDims.width}
      resizeHeight={200}
      resizeWidth={152}
      resizeDepth={3}
      onReady={this.handleCameraStream}
      autorender={true}
     /&gt;
   &lt;/View&gt;
  }
}


Using the following modules

react-native-cli: 2.0.1
react-native: 0.61.4

""expo-gl"": ""^8.1.0""
""@tensorflow/tfjs"": ""1.7.2"",
""@tensorflow/tfjs-react-native"": ""^0.2.3"",
""core-js"": ""3"",
""expo"": ""^36.0.2"",
""expo-asset"": ""~8.0.0"",
""expo-camera"": ""^8.2.0"",
""expo-constants"": ""^9.0.0"",
""expo-font"": ""~8.0.0"",
""expo-gl"": ""^8.1.0"",
""expo-image-picker"": ""^8.1.0"",
""expo-permissions"": ""^8.1.0"",
""@react-native-community/async-storage"": ""^1.8.1"",
""@react-native-community/masked-view"": ""0.1.5"",
""@react-navigation/bottom-tabs"": ""^5.0.0"",
""@react-navigation/native"": ""^5.0.0"",
""@react-navigation/stack"": ""^5.0.0"",


Tried on iOS simulator and device as well. ""OnReady"" function is not being called or any console is logged. On app startup I got these errors:



  Initialization of backend rn-webgl failed


  Error: ExponentGLObjectManager.createContextAsync: Unexpected error
  occurred when initializing headless context



I tried to debug the react native app got no clue on the root cause of this. It can be a dependency issue. When I use this simple example it works fine, however, when trying to use the same component as a screen tab it fails. 

I'd appreciate if you explain how to resolve these errors above. 

Thank you 
",0,1337,"Apparently, after a few days of digging this out. The dependencies are not the issue here.
Probably if your app.json app name contains spaces for some reason the bundled js is not loading correctly.
The error 


  Initialization of backend rn-webgl failed


as it said your webgl backend failed to load. On some devices, webgl is not supported. I used iPhone X iOS 13 and it seems that webgl 1 &amp; 2 are supported correctly. This is the graphics engine that allows leveraging GPU for complex computations. 
When using expo need to make sure your react-native version matches your expo client (0.61.X supported with the current expo client), otherwise, you will get an exception on app load. 
Second error (Error: ExponentGLObjectManager.createContextAsync: Unexpected error occurred when initializing headless context) is a result of the error above.
The bottom line, need to make sure your index.js, app.json we well as metro caches are aligned and fresh. 
To clear metro cache use 

rm -fr $TMPDIR/metro* &amp; watchman watch-del-all


Then 

rm -fr node_modules

",,
tensorflow unexpected issue,https://stackoverflow.com/questions/57066263,How to apply trained model on images of shape/size larger than what the model was trained on (in Tensorflow)?,"I trained a model on size 512x512 images. I currently have size 982x982 (or any other size) images that I need to have the model predict on. When I run on these images, I get errors about unexpected input array shape.

Does Tensorflow provide any way to conveniently deploy a model on images of size/shape larger than what the model was trained on?

More Details:

Specifically, this is a model used for image segmentation.

I assume one workaround to the issue is creating my own sliding-window script that, instead, inputs into the model windows of the expected size taken at different intervals from the original image, and then somehow pasting all those output windows back into a single image after they have gone through the model. However, I want to know if there is a more standard approach.

I am brand new to Tensorflow (and image segmentation) so it is very possible I just missed something, or that perhaps my question is unclear.

Thank you for any advice :)
",0,479,"I once have the same question about how to deal with larger or smaller images. According to my experience, a possible method is to resize images to the input size of the network. For example, if your current image size is 982x982 and the network input size is 512x512, then you can just use some libraries like Pillow or OpenCV to resize the original image from 982x982 to 512x512.

Your method is a possible solution. I would say that there are many possible solutions other than the resizing operation, but you should better try this simplest method to see if your network works well. What I have learned from my projects is that we can always try the simplest solution, and in most cases, it works perfectly.

Generally speaking, there is not a general perfect way to do it. You can start with the simplest method, and find a more complicated one if it does not work.
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/55823524,hyperledger-fabric fabcar example : Container ... is not running,"Problem

I'm following this tutorial on official document:

https://hyperledger-fabric.readthedocs.io/en/release-1.4/write_first_app.html

but stuck in 'launch the network' (https://hyperledger-fabric.readthedocs.io/en/release-1.4/write_first_app.html#launch-the-network) 

./startFabric.sh javascript returns following error message

Error response from daemon: Container 8d4a67101bafc10453ab0a6c7d4afda63edc686ca157f8279ed1ebd11145b25a is not running


Environment

Below is my environment:

OS: Ubuntu 18.04.1 LTS 
DOCKER: Docker version 18.06.1-ce, build e68fc7a 
DOCKER-COMPOSE: docker-compose version 1.18.0, build 8dd22a9 
GO: go version go1.12.4 linux/amd64 
NPM: 3.5.2 
NODE: v8.10.0 
Python 2.7.15rc1 

/etc/profile:

...
export PATH=$PATH:/usr/local/go/bin
export PATH=/home/sw/fabric/fabric-samples/bin:$PATH
export GOPATH=$HOME/go


(PATH variable, GOPATH SET) 
(my $HOME/go directory is empty)

also my ubuntu user is registered to sudo group and also docker group

$ groups
... ... ... sudo ... ... docker



(I hid the rest)

below is the output when I try to start fabric

$ ./startFabric.sh javascript 

# don't rewrite paths for Windows Git Bash users
export MSYS_NO_PATHCONV=1

docker-compose -f docker-compose.yml down
Removing peer0.org1.example.com ... done
Removing couchdb                ... done
Removing ca.example.com         ... done
Removing orderer.example.com    ... done
Removing network net_basic

docker-compose -f docker-compose.yml up -d ca.example.com orderer.example.com peer0.org1.example.com couchdb
Creating couchdb ... done
Creating peer0.org1.example.com ... done
Creating orderer.example.com ... 
Creating couchdb ... 
Creating peer0.org1.example.com ... 
docker ps -a
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS                    PORTS                                            NAMES
90b396af1160        hyperledger/fabric-peer      ""peer node start""        1 second ago        Up Less than a second     0.0.0.0:7051-&gt;7051/tcp, 0.0.0.0:7053-&gt;7053/tcp   peer0.org1.example.com
a0038f19943f        hyperledger/fabric-ca        ""sh -c 'fabric-ca-se""   5 seconds ago       Up 2 seconds              0.0.0.0:7054-&gt;7054/tcp                           ca.example.com
77a56465104c        hyperledger/fabric-couchdb   ""tini -- /docker-ent""   5 seconds ago       Up 1 second               4369/tcp, 9100/tcp, 0.0.0.0:5984-&gt;5984/tcp       couchdb
7ed9d7dbf17f        hyperledger/fabric-orderer   ""orderer""                5 seconds ago       Up 3 seconds              0.0.0.0:7050-&gt;7050/tcp                           orderer.example.com
a4397f663fdd        tensorflow/tensorflow        ""/run_jupyter.sh --a""   6 months ago        Exited (0) 6 months ago                                                    jolly_vaughan

# wait for Hyperledger Fabric to start
# incase of errors when running later commands, issue export FABRIC_START_TIMEOUT=&lt;larger number&gt;
export FABRIC_START_TIMEOUT=10
#echo ${FABRIC_START_TIMEOUT}
sleep ${FABRIC_START_TIMEOUT}

# Create the channel
docker exec -e ""CORE_PEER_LOCALMSPID=Org1MSP"" -e ""CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/msp/users/Admin@org1.example.com/msp"" peer0.org1.example.com peer channel create -o orderer.example.com:7050 -c mychannel -f /etc/hyperledger/configtx/channel.tx
Error response from daemon: Container 90b396af1160b6c7e3a35ec41806b428c299f598208dc77c4194ee1fa76a351a is not running



seemed like hyperledger/fabric-peer Image is not starting.
So I check the docker log

$ docker logs 90b396
2019-04-24 05:31:04.584 UTC [nodeCmd] serve -&gt; INFO 001 Starting peer:
 Version: 1.4.1
 Commit SHA: 87074a7
 Go version: go1.11.5
 OS/Arch: linux/amd64
 Chaincode:
  Base Image Version: 0.4.15
  Base Docker Namespace: hyperledger
  Base Docker Label: org.hyperledger.fabric
  Docker Namespace: hyperledger
2019-04-24 05:31:04.585 UTC [ledgermgmt] initialize -&gt; INFO 002 Initializing ledger mgmt
2019-04-24 05:31:04.585 UTC [kvledger] NewProvider -&gt; INFO 003 Initializing ledger provider
2019-04-24 05:31:04.873 UTC [kvledger] NewProvider -&gt; INFO 004 ledger provider Initialized
2019-04-24 05:31:05.002 UTC [couchdb] handleRequest -&gt; WARN 005 Retrying couchdb request in 125ms. Attempt:1  Error:Get http://couchdb:5984/: dial tcp 172.18.0.3:5984: connect: connection refused
fatal error: unexpected signal during runtime execution
[signal SIGSEGV: segmentation violation code=0x1 addr=0x63 pc=0x7f12f457d259]

runtime stack:
runtime.throw(0x1272c18, 0x2a)
    /opt/go/src/runtime/panic.go:608 +0x72
runtime.sigpanic()
    /opt/go/src/runtime/signal_unix.go:374 +0x2f2

goroutine 91 [syscall]:
runtime.cgocall(0xe455e0, 0xc0001a9e00, 0x29)
    /opt/go/src/runtime/cgocall.go:128 +0x5e fp=0xc0001a9dc8 sp=0xc0001a9d90 pc=0x4039ee
net._C2func_getaddrinfo(0xc0004580c0, 0x0, 0xc0001d2240, 0xc00079e140, 0x0, 0x0, 0x0)
    _cgo_gotypes.go:91 +0x55 fp=0xc0001a9e00 sp=0xc0001a9dc8 pc=0x616c85
net.cgoLookupIPCNAME.func1(0xc0004580c0, 0x0, 0xc0001d2240, 0xc00079e140, 0x8, 0x8, 0xc0007b0370)
    /opt/go/src/net/cgo_unix.go:149 +0x131 fp=0xc0001a9e48 sp=0xc0001a9e00 pc=0x61c3b1
net.cgoLookupIPCNAME(0xc0004580b0, 0x7, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)
    /opt/go/src/net/cgo_unix.go:149 +0x153 fp=0xc0001a9f38 sp=0xc0001a9e48 pc=0x618243
net.cgoIPLookup(0xc0005144e0, 0xc0004580b0, 0x7)
    /opt/go/src/net/cgo_unix.go:201 +0x4d fp=0xc0001a9fc8 sp=0xc0001a9f38 pc=0x6188fd
runtime.goexit()
    /opt/go/src/runtime/asm_amd64.s:1333 +0x1 fp=0xc0001a9fd0 sp=0xc0001a9fc8 pc=0x45de51
created by net.cgoLookupIP
    /opt/go/src/net/cgo_unix.go:211 +0xad

goroutine 1 [select]:
net/http.(*Transport).getConn(0xc0004c9680, 0xc0001d2120, 0x0, 0xc000674000, 0x4, 0xc0004580b0, 0xc, 0x0, 0x0, 0x20)
    /opt/go/src/net/http/transport.go:1004 +0x58e
net/http.(*Transport).roundTrip(0xc0004c9680, 0xc000798200, 0xc0001d20f0, 0xc000458098, 0xc0004580a0)
    /opt/go/src/net/http/transport.go:451 +0x690
net/http.(*Transport).RoundTrip(0xc0004c9680, 0xc000798200, 0xc0004c9680, 0xbf281b0f07aea3df, 0x851175c92)
    /opt/go/src/net/http/roundtrip.go:17 +0x35
net/http.send(0xc000798000, 0x139e6e0, 0xc0004c9680, 0xbf281b0f07aea3df, 0x851175c92, 0x1fa1740, 0xc00079e110, 0xbf281b0f07aea3df, 0xc0004aab48, 0x1)
    /opt/go/src/net/http/client.go:250 +0x14b
net/http.(*Client).send(0xc00066f560, 0xc000798000, 0xbf281b0f07aea3df, 0x851175c92, 0x1fa1740, 0xc00079e110, 0x0, 0x1, 0x0)
    /opt/go/src/net/http/client.go:174 +0xfa
net/http.(*Client).do(0xc00066f560, 0xc000798000, 0x0, 0x0, 0x0)
    /opt/go/src/net/http/client.go:641 +0x2a8
net/http.(*Client).Do(0xc00066f560, 0xc000798000, 0x10, 0xc0004aae40, 0x1)
    /opt/go/src/net/http/client.go:509 +0x35
github.com/hyperledger/fabric/core/ledger/util/couchdb.(*CouchInstance).handleRequest(0xc00067f740, 0x13b7a20, 0xc000046090, 0x123cf31, 0x3, 0x0, 0x0, 0x124c88f, 0x11, 0xc000128a80, ...)
    /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/util/couchdb/couchdb.go:1752 +0x64e
github.com/hyperledger/fabric/core/ledger/util/couchdb.(*CouchInstance).VerifyCouchConfig(0xc00067f740, 0x0, 0x0, 0x0, 0x0)
    /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/util/couchdb/couchdb.go:410 +0x345
github.com/hyperledger/fabric/core/ledger/util/couchdb.CreateCouchInstance(0xc0000440af, 0xc, 0x0, 0x0, 0x0, 0x0, 0x3, 0xc, 0x826299e00, 0xc000042000, ...)
    /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/util/couchdb/couchdbutil.go:58 +0x29e
github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb/statecouchdb.NewVersionedDBProvider(0x13b0260, 0x1fc5e60, 0xb972cb, 0x10d80c0, 0xc000670018)
    /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/statedb/statecouchdb/statecouchdb.go:46 +0xe4
github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/privacyenabledstate.NewCommonStorageDBProvider(0x13a2ce0, 0xc000670018, 0x13b0260, 0x1fc5e60, 0x139cac0, 0xc0007b4c00, 0x2, 0x4, 0x0, 0xc000128800)
    /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/privacyenabledstate/common_storage_db.go:48 +0x48
github.com/hyperledger/fabric/core/ledger/kvledger.(*Provider).Initialize(0xc000128800, 0xc00062fda0, 0x0, 0x0)
    /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/kvledger/kv_ledger_provider.go:88 +0x25e
github.com/hyperledger/fabric/core/ledger/ledgermgmt.initialize(0xc00046ed70)
    /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/ledgermgmt/ledger_mgmt.go:73 +0x4b4
github.com/hyperledger/fabric/core/ledger/ledgermgmt.Initialize.func1()
    /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/ledgermgmt/ledger_mgmt.go:53 +0x2a
sync.(*Once).Do(0x1fc5f38, 0xc0004794e0)
    /opt/go/src/sync/once.go:44 +0xb3
github.com/hyperledger/fabric/core/ledger/ledgermgmt.Initialize(0xc00046ed70)
    /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/ledgermgmt/ledger_mgmt.go:52 +0x55
github.com/hyperledger/fabric/peer/node.serve(0x1fc5e60, 0x0, 0x0, 0x0, 0x0)
    /opt/gopath/src/github.com/hyperledger/fabric/peer/node/start.go:176 +0x5bd
github.com/hyperledger/fabric/peer/node.glob..func1(0x1eb3b00, 0x1fc5e60, 0x0, 0x0, 0x0, 0x0)
    /opt/gopath/src/github.com/hyperledger/fabric/peer/node/start.go:121 +0x9c
github.com/hyperledger/fabric/vendor/github.com/spf13/cobra.(*Command).execute(0x1eb3b00, 0x1fc5e60, 0x0, 0x0, 0x1eb3b00, 0x1fc5e60)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/spf13/cobra/command.go:762 +0x473
github.com/hyperledger/fabric/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0x1eb4220, 0x8, 0x0, 0x1eb33e0)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/spf13/cobra/command.go:852 +0x2fd
github.com/hyperledger/fabric/vendor/github.com/spf13/cobra.(*Command).Execute(0x1eb4220, 0xc0004a7f40, 0x1)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/spf13/cobra/command.go:800 +0x2b
main.main()
    /opt/gopath/src/github.com/hyperledger/fabric/peer/main.go:53 +0x2f7

goroutine 8 [syscall]:
os/signal.signal_recv(0x0)
    /opt/go/src/runtime/sigqueue.go:139 +0x9c
os/signal.loop()
    /opt/go/src/os/signal/signal_unix.go:23 +0x22
created by os/signal.init.0
    /opt/go/src/os/signal/signal_unix.go:29 +0x41

goroutine 21 [IO wait]:
internal/poll.runtime_pollWait(0x7f12f5994f00, 0x72, 0x0)
    /opt/go/src/runtime/netpoll.go:173 +0x66
internal/poll.(*pollDesc).wait(0xc00045c198, 0x72, 0xc000082000, 0x0, 0x0)
    /opt/go/src/internal/poll/fd_poll_runtime.go:85 +0x9a
internal/poll.(*pollDesc).waitRead(0xc00045c198, 0xffffffffffffff00, 0x0, 0x0)
    /opt/go/src/internal/poll/fd_poll_runtime.go:90 +0x3d
internal/poll.(*FD).Accept(0xc00045c180, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)
    /opt/go/src/internal/poll/fd_unix.go:384 +0x1a0
net.(*netFD).accept(0xc00045c180, 0x7f12fa405000, 0x0, 0xc000058eb0)
    /opt/go/src/net/fd_unix.go:238 +0x42
net.(*TCPListener).accept(0xc00079e690, 0xc000058eb8, 0x40d1d8, 0x30)
    /opt/go/src/net/tcpsock_posix.go:139 +0x2e
net.(*TCPListener).Accept(0xc00079e690, 0x1174aa0, 0xc0001d60c0, 0x1074180, 0x1ea5270)
    /opt/go/src/net/tcpsock.go:260 +0x47
net/http.(*Server).Serve(0xc000665a00, 0x13b6a20, 0xc00079e690, 0x0, 0x0)
    /opt/go/src/net/http/server.go:2826 +0x22f
created by github.com/hyperledger/fabric/core/operations.(*System).Start
    /opt/gopath/src/github.com/hyperledger/fabric/core/operations/system.go:121 +0x1a3

goroutine 22 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util.(*BufferPool).drain(0xc0001f8a80)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util/buffer_pool.go:206 +0x12a
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util.NewBufferPool
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util/buffer_pool.go:237 +0x177

goroutine 10 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).compactionError(0xc0000b31e0)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:90 +0xd3
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:142 +0x40c

goroutine 11 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).mpoolDrain(0xc0000b31e0)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_state.go:101 +0xe7
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:143 +0x42e

goroutine 12 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).tCompaction(0xc0000b31e0)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:834 +0x331
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:149 +0x58c

goroutine 13 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).mCompaction(0xc0000b31e0)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:762 +0x12e
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:150 +0x5ae

goroutine 14 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util.(*BufferPool).drain(0xc0001f82a0)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util/buffer_pool.go:206 +0x12a
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util.NewBufferPool
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util/buffer_pool.go:237 +0x177

goroutine 15 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).compactionError(0xc0000b3380)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:90 +0xd3
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:142 +0x40c

goroutine 16 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).mpoolDrain(0xc0000b3380)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_state.go:101 +0xe7
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:143 +0x42e

goroutine 66 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).tCompaction(0xc0000b3380)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:834 +0x331
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:149 +0x58c

goroutine 67 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).mCompaction(0xc0000b3380)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:762 +0x12e
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:150 +0x5ae

goroutine 68 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util.(*BufferPool).drain(0xc0007a40e0)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util/buffer_pool.go:206 +0x12a
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util.NewBufferPool
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util/buffer_pool.go:237 +0x177

goroutine 69 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).compactionError(0xc0000b3520)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:90 +0xd3
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:142 +0x40c

goroutine 70 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).mpoolDrain(0xc0000b3520)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_state.go:101 +0xe7
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:143 +0x42e

goroutine 71 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).tCompaction(0xc0000b3520)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:834 +0x331
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:149 +0x58c

goroutine 72 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).mCompaction(0xc0000b3520)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:762 +0x12e
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:150 +0x5ae

goroutine 73 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util.(*BufferPool).drain(0xc0007a42a0)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util/buffer_pool.go:206 +0x12a
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util.NewBufferPool
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util/buffer_pool.go:237 +0x177

goroutine 74 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).compactionError(0xc0000b36c0)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:90 +0xd3
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:142 +0x40c

goroutine 75 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).mpoolDrain(0xc0000b36c0)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_state.go:101 +0xe7
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:143 +0x42e

goroutine 76 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).tCompaction(0xc0000b36c0)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:834 +0x331
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:149 +0x58c

goroutine 77 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).mCompaction(0xc0000b36c0)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:762 +0x12e
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:150 +0x5ae

goroutine 78 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util.(*BufferPool).drain(0xc0007a4460)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util/buffer_pool.go:206 +0x12a
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util.NewBufferPool
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util/buffer_pool.go:237 +0x177

goroutine 79 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).compactionError(0xc0000b3860)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:90 +0xd3
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:142 +0x40c

goroutine 80 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).mpoolDrain(0xc0000b3860)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_state.go:101 +0xe7
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:143 +0x42e

goroutine 81 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).tCompaction(0xc0000b3860)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:834 +0x331
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:149 +0x58c

goroutine 82 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).mCompaction(0xc0000b3860)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:762 +0x12e
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:150 +0x5ae

goroutine 83 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util.(*BufferPool).drain(0xc0007a4620)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util/buffer_pool.go:206 +0x12a
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util.NewBufferPool
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/util/buffer_pool.go:237 +0x177

goroutine 36 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).compactionError(0xc0000b3a00)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:90 +0xd3
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:142 +0x40c

goroutine 37 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).mpoolDrain(0xc0000b3a00)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_state.go:101 +0xe7
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:143 +0x42e

goroutine 38 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).tCompaction(0xc0000b3a00)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:834 +0x331
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:149 +0x58c

goroutine 39 [select]:
github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).mCompaction(0xc0000b3a00)
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:762 +0x12e
created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB
    /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:150 +0x5ae

goroutine 89 [select]:
net.(*Resolver).LookupIPAddr(0x1fa0d00, 0x13b7a20, 0xc000046090, 0xc0004580b0, 0x7, 0xc0004580b8, 0x4, 0x1760, 0x0, 0x0)
    /opt/go/src/net/lookup.go:227 +0x55f
net.(*Resolver).internetAddrList(0x1fa0d00, 0x13b7a20, 0xc000046090, 0x123d330, 0x3, 0xc0004580b0, 0xc, 0x0, 0x0, 0x0, ...)
    /opt/go/src/net/ipsock.go:279 +0x614
net.(*Resolver).resolveAddrList(0x1fa0d00, 0x13b7a20, 0xc000046090, 0x123da6a, 0x4, 0x123d330, 0x3, 0xc0004580b0, 0xc, 0x0, ...)
    /opt/go/src/net/dial.go:202 +0x4fb
net.(*Dialer).DialContext(0x1fa18c0, 0x13b7a20, 0xc000046090, 0x123d330, 0x3, 0xc0004580b0, 0xc, 0x0, 0x0, 0x0, ...)
    /opt/go/src/net/dial.go:384 +0x201
net/http.(*Transport).dial(0xc0004c9680, 0x13b7a20, 0xc000046090, 0x123d330, 0x3, 0xc0004580b0, 0xc, 0xc00062e700, 0xc0005b9db8, 0xc0005b9c00, ...)
    /opt/go/src/net/http/transport.go:925 +0x17f
net/http.(*Transport).dialConn(0xc0004c9680, 0x13b7a20, 0xc000046090, 0x0, 0xc000674000, 0x4, 0xc0004580b0, 0xc, 0x0, 0x0, ...)
    /opt/go/src/net/http/transport.go:1240 +0x313
net/http.(*Transport).getConn.func4(0xc0004c9680, 0x13b7a20, 0xc000046090, 0xc0001d2150, 0xc0004684e0)
    /opt/go/src/net/http/transport.go:999 +0x6e
created by net/http.(*Transport).getConn
    /opt/go/src/net/http/transport.go:998 +0x3d7

goroutine 90 [select]:
net.cgoLookupIP(0x13b79e0, 0xc00049e140, 0xc0004580b0, 0x7, 0x0, 0xc000797bc0, 0x1069d40, 0xc000520030, 0x1010720, 0xc0007b1350)
    /opt/go/src/net/cgo_unix.go:212 +0x17b
net.(*Resolver).lookupIP(0x1fa0d00, 0x13b79e0, 0xc00049e140, 0xc0004580b0, 0x7, 0x0, 0xc000462d80, 0xc0006779c0, 0xc000797fa0, 0x0)
    /opt/go/src/net/lookup_unix.go:95 +0x166
net.(*Resolver).lookupIP-fm(0x13b79e0, 0xc00049e140, 0xc0004580b0, 0x7, 0x42be22, 0xc000000008, 0xc0006779c0, 0xc0007b0370, 0xc0001a9ea0)
    /opt/go/src/net/lookup.go:207 +0x56
net.glob..func1(0x13b79e0, 0xc00049e140, 0xc000796350, 0xc0004580b0, 0x7, 0xc000796a70, 0x1069d40, 0xc00019d740, 0x1069d40, 0xc0007a3560)
    /opt/go/src/net/hook.go:19 +0x52
net.(*Resolver).LookupIPAddr.func1(0x0, 0x0, 0x0, 0x0)
    /opt/go/src/net/lookup.go:221 +0xd8
internal/singleflight.(*Group).doCall(0x1fa0d10, 0xc000012230, 0xc0004580b0, 0x7, 0xc0001d21e0)
    /opt/go/src/internal/singleflight/singleflight.go:95 +0x2e
created by internal/singleflight.(*Group).DoChan
    /opt/go/src/internal/singleflight/singleflight.go:88 +0x2a0

goroutine 88 [select]:
net/http.setRequestCancel.func3(0x0, 0xc0001d20f0, 0xc0000121e0, 0xc000458098, 0xc000468480)
    /opt/go/src/net/http/client.go:321 +0xcf
created by net/http.setRequestCancel
    /opt/go/src/net/http/client.go:320 +0x24e



looked like below is the main cause.

[signal SIGSEGV: segmentation violation code=0x1 addr=0x63 pc=0x7f12f457d259]


I have tried
- deleting and reinstalling
- shutting down pre-exist network (https://hyperledger-fabric.readthedocs.io/en/release-1.4/write_first_app.html#set-up-the-blockchain-network)

but the problem remained the same.

So these are all I've got.
Can someone please let me know what is the problem and how to fix it?
",0,328,"You can try to set the following environment variable in the peer: GODEBUG=netdns=go
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/54027170,CUDA_ERROR_LAUNCH_FAILED with Tensorflow and Keras,"I'm using Keras to train a convolutional neural network using the fit_generator function as the images are stored in .h5 files and don't fit in memory. Most of the times I'm not able to train the model as it gets stuck in the middle of the first epoch, or it crashes saying 'GPU sync failed' or 'CUDA_ERROR_LAUNCH_FAILED' (see the logs below). The training using the CPUs works well but of course it is slower. I'm using two different machines and both have the same issues. My guess is that it is an installation/configuration related problem but I don't know how to fix it. 

On both machines Tensorflow was installed as explained here: https://www.anaconda.com/blog/developer-blog/tensorflow-in-anaconda/

I have used this script https://github.com/tensorflow/tensorflow/blob/master/tools/tf_env_collect.sh to collect the following informations.

Here the tf_env.txt

First machine:

Keras 2.2.4. 

== cat /etc/issue ===============================================
Linux liph02.novalocal 3.10.0-862.9.1.el7.x86_64 #1 SMP Mon Jul 16 16:29:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux liph02.novalocal 3.10.0-862.9.1.el7.x86_64 #1 SMP Mon Jul 16 16:29:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                    1.15.4    
numpydoc                 0.8.0     
protobuf                 3.6.1     
tensorflow               1.12.0    

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.12.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-9.2/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Dec 28 16:13:39 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN Xp            Off  | 00000000:00:06.0 Off |                  N/A |
| 22%   38C    P0    57W / 250W |      0MiB / 12196MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/Wolfram/Mathematica/11.3/SystemFiles/Components/MXNetLink/LibraryResources/Linux-x86-64/libcudart.so.9.1


Second machine:

Keras 2.2.4.


== cat /etc/issue ===============================================
Linux liph01.novalocal 3.10.0-862.14.4.el7.x86_64 #1 SMP Wed Sep 26 15:12:11 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 7.3.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux liph01.novalocal 3.10.0-862.14.4.el7.x86_64 #1 SMP Wed Sep 26 15:12:11 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
msgpack-numpy                      0.4.3.2    
numpy                              1.15.3     
numpydoc                           0.8.0      
protobuf                           3.6.0      
tensorflow                         1.11.0     

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.11.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Thu Jan  3 17:38:44 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN Xp            Off  | 00000000:00:07.0 Off |                  N/A |
| 40%   65C    P2    94W / 250W |  11747MiB / 12196MiB |     90%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     16991      C   python                                     11737MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148
/usr/local/cuda-9.2/doc/man/man7/libcudart.7
/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7


Here the two stacktraces 

(dev) -bash-4.2$ python classifier_training.py --dirs /data/simulations/Paranal_gam/ /data/simulations/Paranal_prot/ --epochs 1 --batch_size 32 --workers 16 --model ClassifierV2 --patience 1
Using TensorFlow backend.
ClassifierV2
Building training generator...
Building validation generator...
2018-12-18 12:15:19.553286: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2018-12-18 12:15:20.043811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-18 12:15:20.047991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:00:06.0
totalMemory: 11.91GiB freeMemory: 11.75GiB
2018-12-18 12:15:20.048093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
Traceback (most recent call last):
  File ""classifier_training.py"", line 122, in &lt;module&gt;
    model = class_v2.get_model()
  File ""/data/ctasoft/cta-lstchain/cnn/classifiers.py"", line 40, in get_model
    self.model.add(Conv2D(16, kernel_size=(3, 3), input_shape=(1, self.img_rows, self.img_cols),  data_format='channels_first', activation='relu'))
  File ""/data/ctasoft/anaconda3/envs/cta-dev/lib/python3.6/site-packages/keras/engine/sequential.py"", line 165, in add
    layer(x)
  File ""/data/ctasoft/anaconda3/envs/cta-dev/lib/python3.6/site-packages/keras/engine/base_layer.py"", line 457, in __call__
    output = self.call(inputs, **kwargs)
  File ""/data/ctasoft/anaconda3/envs/cta-dev/lib/python3.6/site-packages/keras/layers/convolutional.py"", line 171, in call
    dilation_rate=self.dilation_rate)
  File ""/data/ctasoft/anaconda3/envs/cta-dev/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 3641, in conv2d
    x, tf_data_format = _preprocess_conv2d_input(x, data_format)
  File ""/data/ctasoft/anaconda3/envs/cta-dev/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 3521, in _preprocess_conv2d_input
    if not _has_nchw_support() or force_transpose:
  File ""/data/ctasoft/anaconda3/envs/cta-dev/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 292, in _has_nchw_support
    gpus_available = len(_get_available_gpus()) &gt; 0
  File ""/data/ctasoft/anaconda3/envs/cta-dev/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 278, in _get_available_gpus
    _LOCAL_DEVICES = get_session().list_devices()
  File ""/data/ctasoft/anaconda3/envs/cta-dev/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 186, in get_session
    _SESSION = tf.Session(config=config)
  File ""/data/ctasoft/anaconda3/envs/cta-dev/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1551, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/data/ctasoft/anaconda3/envs/cta-dev/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 676, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: unspecified launch failure


(dev) -bash-4.2$ python classifier_training.py --dirs /data/simulations/Paranal_gam /data/simulations/Paranal_prot --workers 1 --epochs 10 --batch_size 16 --model ClassifierV2 --patience 9
Using TensorFlow backend.
ClassifierV2
Building training generator...
Building validation generator...
2018-12-29 19:29:11.142008: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2018-12-29 19:29:11.892617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning
 NUMA node zero
2018-12-29 19:29:11.896828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:00:06.0
totalMemory: 11.91GiB freeMemory: 11.75GiB
2018-12-29 19:29:11.896880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-29 19:29:12.960736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-29 19:29:12.960804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2018-12-29 19:29:12.960819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2018-12-29 19:29:12.961681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11366 MB memory) -&gt; physical GPU (device:
0, name: TITAN Xp, pci bus id: 0000:00:06.0, compute capability: 6.1)
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 16, 98, 98)        160
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 16, 96, 96)        2320
_________________________________________________________________
average_pooling2d_1 (Average (None, 16, 48, 48)        0
_________________________________________________________________
dropout_1 (Dropout)          (None, 16, 48, 48)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 46, 46)        4640
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 32, 44, 44)        9248
_________________________________________________________________
average_pooling2d_2 (Average (None, 32, 22, 22)        0
_________________________________________________________________
dropout_2 (Dropout)          (None, 32, 22, 22)        0
_________________________________________________________________
flatten_1 (Flatten)          (None, 15488)             0
_________________________________________________________________
dense_1 (Dense)              (None, 128)               1982592
_________________________________________________________________
dropout_3 (Dropout)          (None, 128)               0
_________________________________________________________________
dense_2 (Dense)              (None, 256)               33024
_________________________________________________________________
dropout_4 (Dropout)          (None, 256)               0
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 257
=================================================================
Total params: 2,032,241
Trainable params: 2,032,241
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
   4/8065 [..............................] - ETA: 1:52:06 - loss: 0.9940 - acc: 0.4531 - precision: 0.4947 - recall: 0.71882018-12-29 19:29:54.459471: E tensorflow/stream_executor/cuda/cuda_event.cc:48] E
rror polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2018-12-29 19:29:54.459645: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1
Aborted

",0,1765,"Looks like on the First machine, CUDA version mismatch,Make sure use single version of CUDA and on the second machine the variables of CUDA and cuDNN are not set properly. Follow the instructions mentioned on Tensorflow with GPU support.
Also check the NVIDIA driver compute capability and install CUDA accordingly.
",,
tensorflow unexpected issue,https://stackoverflow.com/questions/46943200,How to set the weight of tf.slim with a numpy array,"I would like to show my example below:

x = tf.placeholder(dtype=...)
a = numpy.asarray([784, 10])
z = slim.fully_connected(x, 10, weights_initializer=?)


I have tried weights_initializer = lambda x1:a, it reports the error: TypeError: () got an unexpected keyword argument 'dtype'

I also found another post here:https://github.com/tensorflow/tensorflow/issues/4016

However, I still don't know the answer. Thank you very much.
",0,710,"Sorry, I don't really understand what you're trying to do. 
If your fully connected layer has 10 hidden neurons then your initializer must have the shape (input_shape, 10), what you're giving is a (2,) shape. Secondly, to initialize weights with a constant matrix you should use tf.constant_initializer(..) function. 
Are you trying to do the following: (you can change the init function used with numpy)

import tensorflow as tf
import numpy as np

slim = tf.contrib.slim

input_size = ?

x = tf.placeholder(dtype=tf.float32, shape=[input_size])
a = np.random.normal((input_size, 10))

z = slim.fully_connected(x, 10, 
weights_initializer=tf.constant_initializer(a))

init  = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init)

",,
tensorflow strange behavior,https://stackoverflow.com/questions/52343989,Non-deterministic behavior of TensorFlow while_loop(),"I have implemented an algorithm using TensorFlow while_loop with large matrices and I have recently noticed strange behavior: I am getting different results with different runs, sometimes even nan values. I have spend some time on narrowing down the problem and I now have the following minimal example. I  take a large matrix K of size 15000x15000 filled with ones, and then calculate Ku for the vector u filled with ones. After one iteration, I expect as result the vector filled with 15000. But this is not what happens. 

import numpy as np
import tensorflow as tf

n = 15000
np_kernel_mat = np.ones((n, n), dtype=np.float32)
kernel_mat = tf.constant(np_kernel_mat)

# for debugging
def compare_kernel(kernel_matrix):
    print(""AverageDifference:"" + str(np.average(np.abs(np_kernel_mat - kernel_matrix))))
    print(""AmountDifferent:"" + str(np.count_nonzero(np.abs(np_kernel_mat - kernel_matrix))))
    return True

# body of the loop
def iterate(i, u):
    # for debugging
    with tf.control_dependencies(tf.py_func(compare_kernel, [kernel_mat], [tf.bool])):
        u = tf.identity(u)
    # multiply
    u = tf.matmul(kernel_mat, u)
    # check result and kernel 
    u = tf.Print(u, [tf.count_nonzero(tf.abs(kernel_mat-np_kernel_mat))], ""AmountDifferentKernel: "")
    u = tf.Print(u, [tf.count_nonzero(tf.abs(u-float(n)))], ""AmountDifferentRes: "")
    i = i + 1
    return i, u


def cond(i, u):
    return tf.less(i, 5)

u0 = tf.fill((n, 1), 1.0, name='u0')
iu_0 = (tf.constant(0), u0)
iu_final = tf.while_loop(cond, iterate, iu_0, back_prop=False, parallel_iterations=1)
u_res = iu_final[1]


with tf.Session() as sess:
    kernel_mat_eval, u_res_eval = sess.run([kernel_mat, u_res])
    print(np.array_equal(kernel_mat_eval, np_kernel_mat))


Now running this I get the following output:

I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:00:0f.0
totalMemory: 11.93GiB freeMemory: 11.81GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11435 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:00:0f.0, compute capability: 5.2)
minimal_example.py:25: RuntimeWarning: invalid value encountered in subtr[8/281]
  print(""AverageDifference:"" + str(np.average(np.abs(np_kernel_mat - kernel_matr
ix))))
/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:70: RuntimeWarning
: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims)
AverageDifference:nan
minimal_example.py:26: RuntimeWarning: invalid value encountered in subtract
  print(""AmountDifferent:"" + str(np.count_nonzero(np.abs(np_kernel_mat - kernel_
matrix))))
AmountDifferent:4096
AmountDifferentKernel: [0]
AmountDifferentRes, DifferenceRes: [4][inf]
AverageDifference:nan
AmountDifferent:4096
AmountDifferentKernel: [0]
AmountDifferentRes, DifferenceRes: [15000][nan]
AverageDifference:nan
AmountDifferent:4096
AmountDifferentKernel: [0]
AmountDifferentRes, DifferenceRes: [15000][nan]
AverageDifference:nan
...


It is clear that in the second iteration, the result is not 15000 anymore, but that doesn't explain why the difference is nan. On CPU, everything works fine (the difference is then something like 2e08). 

Now my questions are:
Why is the output of the Print op different to the output of the py_func print? Why is the evaluation of the matrix again equal to the original matrix? Why do I get different results over different runs? Can someone reproduce this? 

I am running this on Ubuntu 16.04, TensorFlow 1.8, numpy 1.14, python3.6.
GPU is GeForceGTX 1080. 

NVRM version: NVIDIA UNIX x86_64 Kernel Module  390.48  Thu Mar 22 00:42:57 PDT 2018
GCC 
version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9) 

",33,1255,"Most likely, your problem is stemming from seeding issues, make sure that you set a seed for both random.seed(), and for numpy.random.seed(). You'll need to seed both, as numpy's random seed is independent from the random random state.
",,
tensorflow strange behavior,https://stackoverflow.com/questions/60267911,Keras inconsistent prediction time,"I tried to get an estimate of the prediction time of my keras model and realised something strange. Apart from being fairly fast normally, every once in a while the model needs quite long to come up with a prediction. And not only that, those times also increase the longer the model runs. I added a minimal working example to reproduce the error.

import time
import numpy as np
from sklearn.datasets import make_classification
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# Make a dummy classification problem
X, y = make_classification()

# Make a dummy model
model = Sequential()
model.add(Dense(10, activation='relu',name='input',input_shape=(X.shape[1],)))
model.add(Dense(2, activation='softmax',name='predictions'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(X, y, verbose=0, batch_size=20, epochs=100)

for i in range(1000):
    # Pick a random sample
    sample = np.expand_dims(X[np.random.randint(99), :], axis=0)
    # Record the prediction time 10x and then take the average
    start = time.time()
    for j in range(10):
        y_pred = model.predict_classes(sample)
    end = time.time()
    print('%d, %0.7f' % (i, (end-start)/10))


The time does  not depend on the sample (it is being picked randomly). If the test is repeated, the indices in the for loop where the prediction takes longer are going to be (nearly) the same again.



I'm using:

tensorflow 2.0.0
python 3.7.4


For my application I need to guarantee the execution in a certain time. This is however impossible considering that behaviour. What is going wrong? Is it a bug in Keras or a bug in the tensorflow backend?

EDIT:
predict_on_batch shows the same behavior, however, more sparse:


y_pred = model(sample, training=False).numpy() shows some heavy outliers as well, however, they are not increasing.


EDIT 2:
I downgraded to the latest tensorflow 1 version (1.15). Not only is the problem not existent anymore, also the ""normal"" prediction time significantly improved! I do not see the two spikes as problematic, as they didn't appear when I repeated the test (at least not at the same indices and linearly increasing) and are percentual not as large as in the first plot.


We can thus conclude that this seems to be a problem inherent to tensorflow 2.0, which shows similar behaviour in other situations as @OverLordGoldDragon mentions.
",19,3332,"TF2 generally exhibits poor and bug-like memory management in several instances I've encountered - brief description here and here. With prediction in particular, the most performant feeding method is via model(x) directly - see here, and its linked discussions.

In a nutshell: model(x) acts via its its __call__ method (which it inherits from base_layer.Layer), whereas predict(), predict_classes(), etc. involve a dedicated loop function via _select_training_loop(); each utilize different data pre- and post-processing methods suited for different use-cases, and model(x) in 2.1 was designed specifically to yield fastest small-model / small-batch (and maybe any-size) performance (and still fastest in 2.0).

Quoting a TensorFlow dev from linked discussions:


  You can predict the output using model call, not model predict, i.e., calling model(x) would make this much faster because there are no ""conversion to dataset"" part, and also it's directly calling a cached tf.function.


Note: this should be less of an issue in 2.1, and especially 2.2 - but test each method anyway. Also I realize this doesn't directly answer your question on the time spikes; I suspect it's related to Eager caching mechanisms, but the surest way to determine is via TF Profiler, which is broken in 2.1.



Update: regarding increasing spikes, possible GPU throttling; you've done ~1000 iters, try 10,000 instead - eventually, the increasing should stop. As you noted in your comments, this doesn't occur with model(x); makes sense as one less GPU step is involved (""conversion to dataset"").

Update2: you could bug the devs here about it if you face this issue; it's mostly me singing there
","While I can't explain the inconsistencies in execution time, I can recommend that you try to convert your model to TensorFlow Lite to speed up predictions on single data records or small batches.

I ran a benchmark on this model:

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(384, activation='elu', input_shape=(256,)),
    tf.keras.layers.Dense(384, activation='elu'),
    tf.keras.layers.Dense(256, activation='elu'),
    tf.keras.layers.Dense(128, activation='elu'),
    tf.keras.layers.Dense(32, activation='tanh')
])


The prediction times for single records were:


model.predict(input): 18ms
model(input): 1.3ms
Model converted to TensorFlow Lite: 43us


The time to convert the model was 2 seconds.

The class below shows how to convert and use the model and provides a predict method like the Keras model. Note that it would need to be modified for use with models that dont just have a single 1-D input and a single 1-D output.

class LiteModel:

    @classmethod
    def from_file(cls, model_path):
        return LiteModel(tf.lite.Interpreter(model_path=model_path))

    @classmethod
    def from_keras_model(cls, kmodel):
        converter = tf.lite.TFLiteConverter.from_keras_model(kmodel)
        tflite_model = converter.convert()
        return LiteModel(tf.lite.Interpreter(model_content=tflite_model))

    def __init__(self, interpreter):
        self.interpreter = interpreter
        self.interpreter.allocate_tensors()
        input_det = self.interpreter.get_input_details()[0]
        output_det = self.interpreter.get_output_details()[0]
        self.input_index = input_det[""index""]
        self.output_index = output_det[""index""]
        self.input_shape = input_det[""shape""]
        self.output_shape = output_det[""shape""]
        self.input_dtype = input_det[""dtype""]
        self.output_dtype = output_det[""dtype""]

    def predict(self, inp):
        inp = inp.astype(self.input_dtype)
        count = inp.shape[0]
        out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype)
        for i in range(count):
            self.interpreter.set_tensor(self.input_index, inp[i:i+1])
            self.interpreter.invoke()
            out[i] = self.interpreter.get_tensor(self.output_index)[0]
        return out

    def predict_single(self, inp):
        """""" Like predict(), but only for a single record. The input data can be a Python list. """"""
        inp = np.array([inp], dtype=self.input_dtype)
        self.interpreter.set_tensor(self.input_index, inp)
        self.interpreter.invoke()
        out = self.interpreter.get_tensor(self.output_index)
        return out[0]


The complete benchmark code and a plot can be found here:
https://medium.com/@micwurm/using-tensorflow-lite-to-speed-up-predictions-a3954886eb98
",
tensorflow strange behavior,https://stackoverflow.com/questions/34783030,Saving image files in Tensorflow,"I'm just starting with Tensorflow and I have a newbie question.

I know that Tensorflow is all about neural nets but I'm starting with just the mechanics of it. I'm trying to get it to load, resize, flip, and save two images. Should be a simple operation, right, and it gets me started with the basics.

Here's my code so far:

import tensorflow as tf
import numpy as np

print(""resizing images"")

filenames = ['img1.png', 'img2.png' ]
filename_queue = tf.train.string_input_producer(filenames, num_epochs=1)

reader = tf.WholeFileReader()
key,value = reader.read(filename_queue)
images = tf.image.decode_png(value)

resized = tf.image.resize_images(images, 180,180, 1)
resized.set_shape([180,180,3])

flipped_images = tf.image.flip_up_down(resized)

resized_encoded = tf.image.encode_jpeg(flipped_images,name=""save_me"")

init = tf.initialize_all_variables()
sess = tf.Session()

with sess.as_default():
  tf.train.start_queue_runners()
  sess.run(init)

  f = open(""/tmp/foo1.jpeg"", ""wb+"")
  f.write(resized_encoded.eval())
  f.close()

  f = open(""/tmp/foo2.jpeg"", ""wb+"")
  f.write(resized_encoded.eval())
  f.close()


It works fine, resizing the two images and saving them. But it always ends with an error:

W tensorflow/core/common_runtime/executor.cc:1076] 0x7f97240e7a40
 Compute status: Out of range: Reached limit of 1


I'm obviously doing something wrong. If I take off the num_epochs=1, then it ends with no error.

I have a few questions:

How do I do this correctly?

Also, if I want to preserve the original file names all the way from the filename_queue through to the end so I can save them with the original names, how do I do that? And how do I know how many files I need to save? Let's say I'm making the list of file names by reading a directory. I tried many different things but I could never find out how I know when I reach the end.

It seems strange to me that I'm calling resized_encoded.eval() twice.

Thank you and I'm sure this is a very basic question but I'm not understanding how this works.

Edit: I created an even simpler demonstration of the behavior:

import tensorflow as tf
import numpy as np

filenames = ['file1.png', 'file2.png' ]

filename_queue = tf.train.string_input_producer(filenames, 
                        num_epochs=1, name=""my_file_q"")

reader = tf.WholeFileReader()
key,value = reader.read(filename_queue)
init = tf.initialize_all_variables()

sess = tf.Session()

with sess.as_default():
  print(""session started"")

  sess.run(init)

  coord = tf.train.Coordinator()
  threads = tf.train.start_queue_runners(coord=coord)

  for i in range (2):
    print(key.eval())

  coord.request_stop()
  coord.join(threads)


This gives the same warning. I can't understand why.
",17,26241,,,
tensorflow strange behavior,https://stackoverflow.com/questions/42364629,Tensorflow - casting from int to float strange behavior,"I am working on tensorflow 0.12 and am having problem with casting.
The following snippet of code does a strange thing:

sess = tf.InteractiveSession()
a = tf.constant(1)
b = tf.cast(a, tf.float32)
print b.eval()


I get a value:
6.86574233e-36

I also tried using tf.to_float() and tf.saturate_cast. Both gave the same result.

Please help.
",17,6203,"Since I see that this is still getting some attention, I should mention that the newer versions of tensorflow do not show this behavior, I suggest working with tensorflow version 1.13 or higher
",,
tensorflow strange behavior,https://stackoverflow.com/questions/50391967,How to visualize a TFRecord?,"I was asked this on another forum but thought I'd post it here for anyone that is having trouble with TFRecords.

TensorFlow's Object Detection API can produce strange behavior if the labels in the TFRecord file do not align with the labels in your labels.pbtxt file. It will run, loss will likely decrease but the network will not produce good detections.

Also, I for one always get confused between X-Y, row-col space, and so I always like to double check to make sure that my annotations are actually annotating the right parts of the image.

The best way I've found to do this is by decoding the TFRecord and plotting it with TF tools.  Here's some code below:

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from object_detection.utils import visualization_utils as vu
from object_detection.protos import string_int_label_map_pb2 as pb
from object_detection.data_decoders.tf_example_decoder import TfExampleDecoder as TfDecoder
from google.protobuf import text_format
def main(tfrecords_filename, label_map=None):
    if label_map is not None:
        label_map_proto = pb.StringIntLabelMap()
        with tf.gfile.GFile(label_map,'r') as f:
            text_format.Merge(f.read(), label_map_proto)
            class_dict = {}
            for entry in label_map_proto.item:
                class_dict[entry.id] = {'name':entry.display_name}
    sess = tf.Session()
    decoder = TfDecoder(label_map_proto_file=label_map, use_display_name=False)
    sess.run(tf.tables_initializer())
    for record in tf.python_io.tf_record_iterator(tfrecords_filename):
        example = decoder.decode(record)
        host_example = sess.run(example)
        scores = np.ones(host_example['groundtruth_boxes'].shape[0])
        vu.visualize_boxes_and_labels_on_image_array( 
            host_example['image'],                                               
            host_example['groundtruth_boxes'],                                                     
            host_example['groundtruth_classes'],
            scores,
            class_dict,
            max_boxes_to_draw=None,
            use_normalized_coordinates=True)
plt.imshow(host_example['image'])
plt.show()

",12,5347,"If you'd like to check bounding boxes/labels visually, you can check this TFRecord Viewer: https://github.com/sulc/tfrecord-viewer


","Thank you for the code, @Steve ! I was looking everywhere on the github repo and couldn't find a way to check the tfrecord.

Just wanted to point out that an import line seems missing:

from google.protobuf import text_format 


It ran ok for me after adding this
","I would suggest trying this: https://www.tensorflow.org/tutorials/load_data/tfrecord#read_the_tfrecord_file
import tensorflow as tf

import numpy as np
import IPython.display as display

raw_image_dataset = tf.data.TFRecordDataset('images.tfrecords')

# Create a dictionary describing the features.
image_feature_description = {
    'height': tf.io.FixedLenFeature([], tf.int64),
    'width': tf.io.FixedLenFeature([], tf.int64),
    'depth': tf.io.FixedLenFeature([], tf.int64),
    'label': tf.io.FixedLenFeature([], tf.int64),
    'image_raw': tf.io.FixedLenFeature([], tf.string),
}

def _parse_image_function(example_proto):
  # Parse the input tf.train.Example proto using the dictionary above.
  return tf.io.parse_single_example(example_proto, image_feature_description)

parsed_image_dataset = raw_image_dataset.map(_parse_image_function)
parsed_image_dataset

"
tensorflow strange behavior,https://stackoverflow.com/questions/47743936,Tensorflow ResourceExhaustedError after first batch,"Summary and Test Cases
The core issue is that Tensorflow throws OOM allocations on a batch that is not the first, as I would expect. Therefore, I believe there is a memory leak since all memory is clearly not being freed after each batch.
num_units: 50, batch_size: 1000; fails OOM (gpu) before 1st batch as expected
num_units: 50, batch_size: 800, fails OOM (gpu) before 1st batch as expected
num_units: 50, batch_size: 750; fails OOM (gpu) after 10th batch (???)
num_units: 50, batch_size: 500; fails OOM (gpu) after 90th batch (???)
num_units: 50, batch_size: 300; fails OOM (gpu) after 540th batch (???)
num_units: 50, batch_size: 200; computer freezes after around 900 batches with 100% ram use
num_units: 50, batch_size: 100; passes 1 epoch -- may fail later (unknown)

Explanation:
Essentially, it runs 144 batch with a batch size of 500 before failing on the 145th batch, which seems strange. If it can't allocate enough memory for the 145th batch, why should it work for the first 144? The behavior can be replicated.
Note that each batch DOES vary in size, since each one has dimensions [BATCH_SIZE, MAX_SEQUENCE_LENGTH], and depending on the sequences sampled, the sequence length varies, but the program does not fail on the largest batch; it fails later on a smaller one. Therefore, I conclude that a single oversized batch is not causing the memory error; it appears to be a memory leak.
With a larger batch size, the program fails earlier; with a smaller batch size, it fails later.
The full error is here:
  Traceback (most recent call last):
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1323, in _do_call
    return fn(*args)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[500,80]
     [[Node: decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](decoder/while/BasicDecoderStep/basic_lstm_cell/concat, decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul/Enter)]]
     [[Node: gradients/Add/_282 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_457_gradients/Add"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopdecoder/while/BasicDecoderStep/TrainingHelperNextInputs/add/y/_181)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/me/IdeaProjects/tf-nmt/main.py"", line 89, in &lt;module&gt;
    _ = sess.run([update_step])
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[500,80]
     [[Node: decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](decoder/while/BasicDecoderStep/basic_lstm_cell/concat, decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul/Enter)]]
     [[Node: gradients/Add/_282 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_457_gradients/Add"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopdecoder/while/BasicDecoderStep/TrainingHelperNextInputs/add/y/_181)]]

Caused by op 'decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul', defined at:
  File ""/home/me/IdeaProjects/tf-nmt/main.py"", line 49, in &lt;module&gt;
    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 309, in dynamic_decode
    swap_memory=swap_memory)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2819, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2643, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2593, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py"", line 254, in body
    decoder_finished) = decoder.step(time, inputs, state)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py"", line 138, in step
    cell_outputs, cell_state = self._cell(inputs, state)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 290, in __call__
    return base_layer.Layer.__call__(self, inputs, state, scope=scope)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 618, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 567, in call
    array_ops.concat([inputs, h], 1), self._kernel)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 1993, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 2532, in _mat_mul
    name=name)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3081, in create_op
    op_def=op_def)
  File ""/home/me/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1528, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[500,80]
     [[Node: decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](decoder/while/BasicDecoderStep/basic_lstm_cell/concat, decoder/while/BasicDecoderStep/basic_lstm_cell/MatMul/Enter)]]
     [[Node: gradients/Add/_282 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_457_gradients/Add"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopdecoder/while/BasicDecoderStep/TrainingHelperNextInputs/add/y/_181)]]

Code snippet (from models.py)
import tensorflow as tf
from tensorflow.python.layers import core as layers_core


class NMTModel:
    def __init__(self, hparams, iterator, mode):
        source, target_in, target_out, source_lengths, target_lengths = iterator.get_next()
        true_batch_size = tf.size(source_lengths)

        # Lookup embeddings
        embedding_encoder = tf.get_variable(""embedding_encoder"", [hparams.src_vsize, hparams.src_emsize])
        encoder_emb_inp = tf.nn.embedding_lookup(embedding_encoder, source)
        embedding_decoder = tf.get_variable(""embedding_decoder"", [hparams.tgt_vsize, hparams.tgt_emsize])
        decoder_emb_inp = tf.nn.embedding_lookup(embedding_decoder, target_in)

        # Build and run Encoder LSTM
        encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)
        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell, encoder_emb_inp, sequence_length=source_lengths, dtype=tf.float32)

        # Build and run Decoder LSTM with Helper and output projection layer
        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)
        projection_layer = layers_core.Dense(hparams.tgt_vsize, use_bias=False)
        # if mode is 'TRAIN' or mode is 'EVAL':  # then decode using TrainingHelper
        #     helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, sequence_length=target_lengths)
        # elif mode is 'INFER':  # then decode using Beam Search
        #     helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding_decoder, tf.fill([true_batch_size], hparams.sos), hparams.eos)
        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding_decoder, tf.fill([true_batch_size], hparams.sos), hparams.eos)
        decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection_layer)
        outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, maximum_iterations=tf.reduce_max(target_lengths))
        logits = outputs.rnn_output

        if mode is 'TRAIN' or mode is 'EVAL':  # then calculate loss
            crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_out, logits=logits)
            target_weights = tf.sequence_mask(target_lengths, maxlen=tf.shape(target_out)[1], dtype=logits.dtype)
            self.loss = tf.reduce_sum((crossent * target_weights)) / tf.cast(true_batch_size, tf.float32)

        if mode is 'TRAIN':  # then calculate/clip gradients, then optimize model
            params = tf.trainable_variables()
            gradients = tf.gradients(self.loss, params)
            clipped_gradients, _ = tf.clip_by_global_norm(gradients, hparams.max_gradient_norm)

            optimizer = tf.train.AdamOptimizer(hparams.l_rate)
            self.update_step = optimizer.apply_gradients(zip(clipped_gradients, params))

        if mode is 'EVAL' or mode is 'INFER':  # then allow access to input/output tensors to printout
            self.src = source
            self.tgt = target_out
            self.preds = tf.argmax(logits, axis=2)

        # Designate a saver operation
        self.saver = tf.train.Saver(tf.global_variables())

    def train(self, sess):
        return sess.run([self.update_step, self.loss])

    def eval(self, sess):
        return sess.run([self.loss, self.src, self.tgt, self.preds])

    def infer(self, sess):
        return sess.run([self.src, self.tgt, self.preds])  # tgt should not exist (temporary debugging only)

",7,3791,,,
tensorflow strange behavior,https://stackoverflow.com/questions/55902068,Fitting a Keras model yields error &quot;constant folding failed: Invalid argument: Unsupported type: 21&quot;,"I am following the example from https://www.tensorflow.org/alpha/tutorials/load_data/text
to load my own dataset and run binary classification on the sentences there (TensorFlow 2.0).
The only change I've made to the example is the dataset being used (which I took from https://github.com/UKPLab/emnlp2017-claim-identification/tree/master/src/main/python), and since the labels can be only 0 or 1 I changed the loss function to binary_crossentropy and the optimizer to RMSprop.
When fitting the Keras model which is identical to the model proposed in the tutorial, I'm constantly receiving the following error:

2019-04-29 13:51:15.609297: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:495] constant folding failed: Invalid argument: Unsupported type: 21
2019-04-29 13:51:15.882000: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:495] constant folding failed: Invalid argument: Unsupported type: 21


The fitting process is still advancing between those prints as evident from:

662/4508 [===&gt;..........................] - ETA: 9:35 - loss: 11.0703 - accuracy: 0.2780


but instead of minimizing the loss, it seems to actually be maximizing it, with the accuracy going down after each iteration.
(In fact, if the accuracy metric is correct, it would be a pretty good classifier if I just take not [prediction]).
Is there anyone here who can explain to me what is the meaning of this error, and whether it's related to the strange behavior of the model (and hopefully how to fix it)? I've been trying to look for similar errors but couldn't find any.
Thanks!
",7,1158,"There is one key difference between the Tutorial mentioned in the link, https://www.tensorflow.org/beta/tutorials/load_data/text and your Dataset.
In the tutorial, Labels are 0, 1 and 2, i.e., all the sentences in cowper.txt are Labelled as 0, all the sentences in derby.txt are Labelled as 1, all the sentences in butler.txt are Labelled as 2. But in your Dataset, Labels are at the end of each sentence of the Text Files.
I have executed the code taking part of your Dataset, as shown below:
FILE_NAMES = ['001.dev', '001.test', '001.train', '002.dev', '002.test', '002.train']

parent_dir = ""Issue_55902068/OC""

parent_dir

In order to handle the difference mentioned above, the function, labeler should be modified as shown below:
def labeler(example, index):
  Label = tf.strings.split(example, sep="""")[-1] #It will give 0 or 1 in Str format
  Label = tf.strings.to_number(Label)
  return example, tf.cast(Label, tf.int64)

After that, I changed the loss function to binary_crossentropy and the optimizer to RMSprop as shown below:
model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])

It is working as expected. Screenshot of the output is shown below.

",,
tensorflow strange behavior,https://stackoverflow.com/questions/58455765,Keras sees my GPU but doesn&#39;t use it when training a neural network,"My GPU is not used by Keras/TensorFlow.

To try to make my GPU working with tensorflow, I installed tensorflow-gpu via pip (I am using Anaconda on Windows)

I have nvidia 1080ti

print(tf.test.is_gpu_available())

True


print(tf.config.experimental.list_physical_devices())

[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), 
 PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]


I tied

physical_devices = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)


but it didnt help

sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))
print(sess)

Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1

&lt;tensorflow.python.client.session.Session object at 0x000001A2A3BBACF8&gt;


only warning from tf:

W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows 


entire log:

2019-10-18 20:06:26.094049: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-10-18 20:06:35.078225: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-10-18 20:06:35.090832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-10-18 20:06:35.180744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
2019-10-18 20:06:35.185505: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-10-18 20:06:35.189328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-10-18 20:06:35.898592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-18 20:06:35.901683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-10-18 20:06:35.904235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-10-18 20:06:35.906687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 8784 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-10-18 20:06:38.694481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
2019-10-18 20:06:38.700482: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-10-18 20:06:38.704020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
[I 20:06:47.324 NotebookApp] Saving file at /Untitled.ipynb
2019-10-18 20:07:22.227110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
2019-10-18 20:07:22.246012: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-10-18 20:07:22.261643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-10-18 20:07:22.272150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-18 20:07:22.275457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-10-18 20:07:22.277980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-10-18 20:07:22.316260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8784 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1
2019-10-18 20:07:32.986802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
2019-10-18 20:07:32.990509: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-10-18 20:07:32.993763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-10-18 20:07:32.995570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-18 20:07:32.997920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-10-18 20:07:32.999435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-10-18 20:07:33.001380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8784 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-10-18 20:07:36.048204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-10-18 20:07:37.971703: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows
Relying on driver to perform ptx compilation. This message will be only logged once.
2019-10-18 20:07:38.576861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll


also tried reinstalling tensorflow-gpu with pip

Why I think GPU doesnt work? - Because my python kernel uses CPU 99%, RAM 99% and sometimes GPU ~7% but most of time its 0
I use custom data generator but now its only selects batches and resizes them(skimage.io.resize)
1 epoch ~ 44s
Also has strange behavior of freezing in random points every ~10 samples and freezes hardly on last sample(37/38)(~10-15 sec)

Edit:

I post my custom datagen here

train_gen = DataGenerator(x = x_train,
                              y = y_train,
                              batch_size = 128,
                              target_shape = (100, 100, 3), 
                              sample_std = False,
                              feature_std = False,
                              proj_parameters = None,
                              blur_parameters = None,
                              nois_parameters = None,
                              flip_parameters = None,
                              gamm_parameters = None)


validation is same

Update:

So its a generator that couses the problem, but how i can fix it?
I used only skimage and numpy operations
",7,7425,"The logs are showing that the GPU does get used. You are almost certainly running into an IO bottleneck: your GPU is processing whatever the CPU is throwing at it way faster than the CPU can load and preprocess it. This is very common in deep learning, and there are ways to address it. 

We cannot provide a lot of help without knowing more about your data pipeline (byte size of a batch, preprocessing steps, ...), and how the data is stored. One typical way to speed things up is to store the data is a binary format, like TFRecords, so that the CPU can load it faster. See the official documentation for this. 



Edit: I quickly went through your input pipeline. The issue is very likely to indeed by IO:


You should run the preprocessing steps on the GPU as well, plenty of the augmentation techniques you use are implemented in tf.image. If you can, you should think about using Tensorflow 2.0, because it includes Keras and there are plenty of helpers in there as well.
Checkout the tf.data.Dataset API, it has plenty of helpers to load all the data in different threads, which can roughly speed up the process by the number of cores you have.
You should store your images as TFRecords. This is likely to speed up the loading by an order of magnitude if your input images are smallish.
You could probably try larger batch sizes as well, I'm thinking your images are probably really small.

",,
tensorflow strange behavior,https://stackoverflow.com/questions/42029046,Tensorflow batch loss spikes when restoring model for training from saved checkpoint?,"I'm encountering a strange issue that I've been trying to debug, without much luck. My model starts training properly with batch loss decreasing consistently (from ~6000 initially to ~120 after 20 epochs). However, when I pause training and resume training later by restoring the model from the checkpoint, the batch loss seems to spike unexpectedly from the previous batch loss (before pausing), and resumes decreasing from that higher loss point. My worry is that when I restore the model for evaluation, I may not be using the trained model that I think I am.

I have combed over my code several times, comparing to the Tensorflow tutorials. I tried to ensure that I was saving and restoring using the tutorial-suggested methods. Here is the code snapshot: https://github.com/KaranKash/DigitSpeak/tree/b7dad3128c88061ee374ae127579ec25cc7f5286 - the train.py file contains the saving and restoring steps, the graph setup and training process; while model.py creates the network layers and computes loss.

Here is an example from my print statements - notice batch loss rises sharply when resuming training from epoch 7's checkpoint:

Epoch 6. Batch 31/38. Loss 171.28
Epoch 6. Batch 32/38. Loss 167.02
Epoch 6. Batch 33/38. Loss 173.29
Epoch 6. Batch 34/38. Loss 159.76
Epoch 6. Batch 35/38. Loss 164.17
Epoch 6. Batch 36/38. Loss 161.57
Epoch 6. Batch 37/38. Loss 165.40
Saving to /Users/user/DigitSpeak/cnn/model/model.ckpt
Epoch 7. Batch 0/38. Loss 169.99
Epoch 7. Batch 1/38. Loss 178.42
KeyboardInterrupt
dhcp-18-189-118-233:cnn user$ python train.py
Starting loss calculation...
Found in-progress model. Will resume from there.
Epoch 7. Batch 0/38. Loss 325.97
Epoch 7. Batch 1/38. Loss 312.10
Epoch 7. Batch 2/38. Loss 295.61
Epoch 7. Batch 3/38. Loss 306.96
Epoch 7. Batch 4/38. Loss 290.58
Epoch 7. Batch 5/38. Loss 275.72
Epoch 7. Batch 6/38. Loss 251.12


I've printed the results of the inspect_checkpoint.py script. I've also experimented with other loss functions (Adam and GradientDescentOptimizer) and noticed the same behavior with respect to spiked loss after resuming training.

dhcp-18-189-118-233:cnn user$ python inspect_checkpoint.py
Optimizer/Variable (DT_INT32) []
conv1-layer/bias (DT_FLOAT) [64]
conv1-layer/bias/Momentum (DT_FLOAT) [64]
conv1-layer/weights (DT_FLOAT) [5,23,1,64]
conv1-layer/weights/Momentum (DT_FLOAT) [5,23,1,64]
conv2-layer/bias (DT_FLOAT) [512]
conv2-layer/bias/Momentum (DT_FLOAT) [512]
conv2-layer/weights (DT_FLOAT) [5,1,64,512]
conv2-layer/weights/Momentum (DT_FLOAT) [5,1,64,512]

",6,1860,"I ran into this issue and found it was the fact that I was initializing the graph variables when restoring the graph -- throwing away all learned parameters, to be replaced with whatever initialization values were originally specified for each respective tensor in the original graph definition.

For example, if you used tf.global_variable_initializer() to initialize variables as part of your model program, whatever your control logic to indicate that a saved graph will be restored, make sure the graph restore flow omits:  sess.run(tf.global_variable_initializer())

This was a simple, but costly mistake for me, so I hope someone else is saved a few grey hairs (or hairs, in general).
","I have also seen this issue in PyTorch, resulting from restoring model weights but not optimizer parameters. For example, the Adam optimizer stores estimates of the first and second moments of the gradient. If you only reload the model and don't save/reload the optimizer, the momentum parameters are reinitialized with the optimizer, which often causes a spike in the loss.
",
tensorflow strange behavior,https://stackoverflow.com/questions/47866455,linux shell copy paste adds ~0 and 1~ symbols to string,"I am running Ubuntu 16.04.
Lately, I experience the following strange behavior when copy pasting via the clipboard, e.g. let's say I want to clone some repository. So, I copy the following string:

""https://github.com/tensorflow/tensorflow""

And then paste it to my shell after a git clone using ctrl+shift+v. I would expect:

$ git clone https://github.com/tensorflow/tensorflow

But instead I see

$ git clone ~0https://github.com/tensorflow/tensorflow1~

This does not happen all the time and I have not yet figured out, what causes this annoying behavior. Anyone else experiencing this and has someone found out how to get rid of it? 
",4,4090,,,
tensorflow strange behavior,https://stackoverflow.com/questions/51266268,Exception thrown when running tf.app.run(),"I am toying around with flags at the moment and came across some weird behavior when using tf.app.run(). The following code snippet should simply print the string given via the command line.

import tensorflow as tf

# command line flags
tf.app.flags.DEFINE_string('mystring', 'Hello World!',
                           '''String to print to console.''')

FLAGS = tf.app.flags.FLAGS


def main():

    print(FLAGS.mystring)

if __name__ == '__main__':
    tf.app.run()


During execution, this error is thrown:


  Traceback (most recent call last):
  
  File """", line 1, in 
      runfile('/path/flags.py', wdir='/path')
  
  File
  ""/home/abc/anaconda3/envs/tensorflow/lib/python3.5/site-packages/spyder/utils/site/sitecustomize.py"",
  line 710, in runfile
      execfile(filename, namespace)
  
  File
  ""/home/abc/anaconda3/envs/tensorflow/lib/python3.5/site-packages/spyder/utils/site/sitecustomize.py"",
  line 101, in execfile
      exec(compile(f.read(), filename, 'exec'), namespace)
  
  File ""/path/flags.py"", line 19, in 
      tf.app.run()
  
  File
  ""/home/abc/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/app.py"",
  line 126, in run
      _sys.exit(main(argv))
  
  TypeError: main() takes 0 positional arguments but 1 was given


...which is strange because I do not give a single argument to main(). However, if I add an underscore def main(_):, it works without any errors.

I couldn't find a doc where this is use of the underscore is described. Does anybody know what happens here? Thank you!
",4,4738,"The error message I see in Pycharm IDE when I execute your code is clearer.

Traceback (most recent call last):
  File ""D:/PycharmProjects/TensorFlow/self.py"", line 30, in &lt;module&gt;
    tf.app.run()
  File ""D:\\Anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", 
line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
TypeError: main() takes 0 positional arguments but 1 was given


_sys.exit(main(_sys.argv[:1] + flags_passthrough)) is trying to call our main method with one argument.

This is the run method in app.py

A stripped down version of the run method can be used to test.

import tensorflow as tf
import sys as _sys
from tensorflow.python.platform import flags


# command line flags
tf.app.flags.DEFINE_string('mystring', 'Hello World!',
                           '''String to print to console.''')

FLAGS = tf.app.flags.FLAGS

def run(main=None, argv=None):
  """"""Runs the program with an optional 'main' function and 'argv' list.""""""
  f = flags.FLAGS

  # Extract the args from the optional `argv` list.
  args = argv[1:] if argv else None

  # Parse the known flags from that list, or from the command
  # line otherwise.
  # pylint: disable=protected-access
  flags_passthrough = f._parse_flags(args=args)
  # pylint: enable=protected-access

  main = main or _sys.modules['__main__'].main

  print (_sys.argv[:1])

  # Call the main function, passing through any arguments
  # to the final program.
  #_sys.exit(main(_sys.argv[:1] + flags_passthrough))

  # Call the main function with no arguments
  #_sys.exit(main())


def main():
    print(FLAGS.mystring)

if __name__ == '__main__':
    #tf.app.run()
    run()


print(_sys.argv[1:]) prints ['D:/PycharmProjects/TensorFlow/self.py'] since
argv[0] is the script name passed to the interpreter.
","I had a similar problem when using cProfile and calling script with 

python -m cProfile train.py


Seems like the problem was that tf.app.run called main inside cProfile which wasn't ready for argument passing. In my case the solution was to specify main in tf.app.run():

tf.app.run(main=main)


Don't forget to add fake argument in main like this def main(_):.
","Maybe You can find the answer from this link for explaining how app.py runshow app.py runs

You can also define your main function with def main(argv=None): ..., or like you do def main(_): ..., so this can make it works for you giving the main function parameters.
"
tensorflow strange behavior,https://stackoverflow.com/questions/53658826,Why operations placed on GPU are also executed on CPU (tensorflow)?,"I use tensorflow profiler to profile my model to see how much time is consumed by each operation. I find some strange behavior, for example, Conv2D operation which is placed on GPU (I set log_device_placement=True to see the placement) also has a large CPU execution time. Here is the code I use to do profiling (tensorflow 1.4.0):

import tensorflow as tf
from tensorflow.python.profiler import option_builder

builder = option_builder.ProfileOptionBuilder
run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
run_metadata = tf.RunMetadata()
# run and collect metadata
my_session.run(fetch_something, feed_dict=feed_dict, 
  options=run_options, run_metadata=run_metadata)
profiler_opts = builder(builder.time_and_memory()).order_by('micros').build()
# this will output the following results
tf.profiler.profile(my_graph, run_meta=run_metadata, cmd='scope', options=profiler_opts)


Here is the output of profiler:

node name | requested bytes | total execution time | accelerator execution time | cpu execution time
MyScope/Conv2D (4511.35MB/4511.35MB, 823.47ms/823.47ms, 445.37ms/445.37ms, 378.11ms/378.11ms)


From the profiling result, the Conv2D operation (tf.nn.conv2d) takes 378.11ms on CPU and 445.37ms on GPU. Why don't tensorflow only use GPU for Conv2D? Is the CPU time for data transfer between memory and GPU because this op takes a lot of memory (4511.35MB)?

======== update ========

Another phenomenon I just found. When the ""requested bytes"" of Conv2D is large (in my case &gt;4GB), the CPU execution time is long (about 400~500ms). When the ""requested bytes"" is small (in my case 1.5GB), the CPU execution time is short (about 15ms). I guess the CPU execution time of Conv2D is related with the memory consumption. However I don't why in different batch (my_session.run), Conv2D uses a different amount of ""requested bytes."" The tensor on which Conv2D is applied has almost the same size across different batches.
",4,1128,"Though I don't see your whole graph, but I assume that you feed data to the feed_dict continuously.
So each time the tensors are evaluated, they take the value of the next element in the underlying dataset. This takes time from CPU as well.
There is possibility to feed data directly from GPU memory if you have enough for holding data in it via tf.Tensor object, see documentation:


  If all of your input data fit in memory, the simplest way to create a
  Dataset from them is to convert them to tf.Tensor objects and use
  Dataset.from_tensor_slices().


An example from the corresponding section of tensorflow documentation:

# Load the training data into two NumPy arrays, for example using `np.load()`.
with np.load(""/var/data/training_data.npy"") as data:
  features = data[""features""]
  labels = data[""labels""]

# Assume that each row of `features` corresponds to the same row as `labels`.
assert features.shape[0] == labels.shape[0]

dataset = tf.data.Dataset.from_tensor_slices((features, labels))



  Note that the above code snippet will embed the features and labels
  arrays in your TensorFlow graph as tf.constant() operations. This
  works well for a small dataset, but wastes memory---because the
  contents of the array will be copied multiple times---and can run into
  the 2GB limit for the tf.GraphDef protocol buffer.


But this is not the case. So based of the information you provided, I think the CPU consumption is mainly due (or fully) to the data feeding operation to to the this graph's next input.
",,
tensorflow strange behavior,https://stackoverflow.com/questions/60797725,"Keras - GRU layer with recurrent dropout - loss: &#39;nan&#39;, accuracy: 0","Problem description

I am going through ""Deep Learning in Python"" by Franois Chollet (publisher webpage, notebooks on github). Replicating examples from Chapter 6 I encountered problems with (I believe) GRU layer with recurrent dropout.

The code in which I had first observed those errors is quite long, so I decided to stick to the simplest problem, which could replicate the error: classifying IMDB reviews into ""positive"" and ""negative"" categories.

When I use a GRU layer with recurrent dropout training loss (after couple of batches of first epoch) takes ""value"" of nan, while training accuracy (from the start of second epoch) takes the value of 0. 

   64/12000 [..............................] - ETA: 3:05 - loss: 0.6930 - accuracy: 0.4844
  128/12000 [..............................] - ETA: 2:09 - loss: 0.6926 - accuracy: 0.4766
  192/12000 [..............................] - ETA: 1:50 - loss: 0.6910 - accuracy: 0.5573
(...) 
 3136/12000 [======&gt;.......................] - ETA: 59s - loss: 0.6870 - accuracy: 0.5635
 3200/12000 [=======&gt;......................] - ETA: 58s - loss: 0.6862 - accuracy: 0.5650
 3264/12000 [=======&gt;......................] - ETA: 58s - loss: 0.6860 - accuracy: 0.5650
 3328/12000 [=======&gt;......................] - ETA: 57s - loss: nan - accuracy: 0.5667   
 3392/12000 [=======&gt;......................] - ETA: 57s - loss: nan - accuracy: 0.5560
 3456/12000 [=======&gt;......................] - ETA: 56s - loss: nan - accuracy: 0.5457
(...)
11840/12000 [============================&gt;.] - ETA: 1s - loss: nan - accuracy: 0.1593
11904/12000 [============================&gt;.] - ETA: 0s - loss: nan - accuracy: 0.1584
11968/12000 [============================&gt;.] - ETA: 0s - loss: nan - accuracy: 0.1576
12000/12000 [==============================] - 83s 7ms/step - loss: nan - accuracy: 0.1572 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 2/20

   64/12000 [..............................] - ETA: 1:16 - loss: nan - accuracy: 0.0000e+00
  128/12000 [..............................] - ETA: 1:15 - loss: nan - accuracy: 0.0000e+00
  192/12000 [..............................] - ETA: 1:16 - loss: nan - accuracy: 0.0000e+00
(...)
11840/12000 [============================&gt;.] - ETA: 1s - loss: nan - accuracy: 0.0000e+00
11904/12000 [============================&gt;.] - ETA: 0s - loss: nan - accuracy: 0.0000e+00
11968/12000 [============================&gt;.] - ETA: 0s - loss: nan - accuracy: 0.0000e+00
12000/12000 [==============================] - 82s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 3/20

   64/12000 [..............................] - ETA: 1:18 - loss: nan - accuracy: 0.0000e+00
  128/12000 [..............................] - ETA: 1:18 - loss: nan - accuracy: 0.0000e+00
  192/12000 [..............................] - ETA: 1:16 - loss: nan - accuracy: 0.0000e+00
(...)


Localizing the problem

To find out the solution I wrote the code presented below, which goes through several models (GRU/LSTM, {no dropout, only ""normal"" dropout, only recurrent dropout, ""normal"" and recurrent dropout, rmsprop/adam}) and presents loss and accuracy of all those models. (It also creates smaller, separate graphs for each model.)

# Based on examples from ""Deep Learning with Python"" by Franois Chollet:
## Constants, modules:
VERSION = 2

import os
from keras import models
from keras import layers
import matplotlib.pyplot as plt
import pylab

## Loading data:
from keras.datasets import imdb
(x_train, y_train), (x_test, y_test) = \
    imdb.load_data(num_words=10000)

from keras.preprocessing import sequence
x_train = sequence.pad_sequences(x_train, maxlen=500)
x_test = sequence.pad_sequences(x_test, maxlen=500)


## Dictionary with models' hyperparameters:
MODELS = [
    # GRU:
    {""no"": 1,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": None},

    {""no"": 2,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": None},

    {""no"": 3,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 4,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},

    {""no"": 5,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": None},

    {""no"": 6,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": None},

    {""no"": 7,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 8,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},

    # LSTM:
    {""no"": 9,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": None},

    {""no"": 10,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": None},

    {""no"": 11,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 12,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},

    {""no"": 13,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": None},

    {""no"": 14,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": None},

    {""no"": 15,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 16,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},
]

## Adding name:
for model_dict in MODELS:
    model_dict[""name""] = f""{model_dict['layer_type']}""
    model_dict[""name""] += f""_d{model_dict['dropout']}"" if model_dict['dropout'] is not None else f""_dN""
    model_dict[""name""] += f""_rd{model_dict['recurrent_dropout']}"" if model_dict['recurrent_dropout'] is not None else f""_rdN""
    model_dict[""name""] += f""_{model_dict['optimizer']}""

## Fucntion - defing and training model:
def train_model(model_dict):
    """"""Defines and trains a model, outputs history.""""""

    ## Defining:
    model = models.Sequential()
    model.add(layers.Embedding(10000, 32))

    recurrent_layer_kwargs = dict()
    if model_dict[""dropout""] is not None:
        recurrent_layer_kwargs[""dropout""] = model_dict[""dropout""]
    if model_dict[""recurrent_dropout""] is not None:
        recurrent_layer_kwargs[""recurrent_dropout""] = model_dict[""recurrent_dropout""]

    if model_dict[""layer_type""] == 'GRU':
        model.add(layers.GRU(32, **recurrent_layer_kwargs))
    elif model_dict[""layer_type""] == 'LSTM':
        model.add(layers.LSTM(32, **recurrent_layer_kwargs))
    else:
        raise ValueError(""Wrong model_dict['layer_type'] value..."")
    model.add(layers.Dense(1, activation='sigmoid'))

    ## Compiling:
    model.compile(
        optimizer=model_dict[""optimizer""],
        loss='binary_crossentropy',
        metrics=['accuracy'])

    ## Training:
    history = model.fit(x_train, y_train,
                        epochs=20,
                        batch_size=64,
                        validation_split=0.2)

    return history

## Multi-model graphs' parameters:
graph_all_nrow = 4
graph_all_ncol = 4
graph_all_figsize = (20, 20)

assert graph_all_nrow * graph_all_nrow &gt;= len(MODELS)

## Figs and axes of multi-model graphs:
graph_all_loss_fig, graph_all_loss_axs = plt.subplots(graph_all_nrow, graph_all_ncol, figsize=graph_all_figsize)
graph_all_acc_fig, graph_all_acc_axs = plt.subplots(graph_all_nrow, graph_all_ncol, figsize=graph_all_figsize)

## Loop trough all models:
for i, model_dict in enumerate(MODELS):
    history = train_model(model_dict)

    ## Metrics extraction:
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    epochs = range(1, len(loss) + 1)

    ## Single-model grph - loss:
    graph_loss_fname = fr""{os.path.basename(__file__).replace('.py', '')}""
    graph_loss_fname += fr""_v{VERSION}_{model_dict['no']}_{model_dict['name']}_loss_graph.png""

    graph_loss_fig, graph_loss_ax = plt.subplots()
    graph_loss_ax.plot(epochs, loss, 'bo', label='Training loss')
    graph_loss_ax.plot(epochs, val_loss, 'b', label='Validation loss')
    graph_loss_ax.legend()
    graph_loss_fig.suptitle(""Training and validation loss"")
    graph_loss_fig.savefig(graph_loss_fname)
    pylab.close(graph_loss_fig)


    ## Single-model grph - accuracy:
    graph_acc_fname = fr""{os.path.basename(__file__).replace('.py', '')}""
    graph_acc_fname += fr""_v{VERSION}_{model_dict['no']}_{model_dict['name']}_acc_graph.png""

    graph_acc_fig, graph_acc_ax = plt.subplots()
    graph_acc_ax.plot(epochs, acc, 'bo', label='Training accuracy')
    graph_acc_ax.plot(epochs, val_acc, 'b', label='Validation accuracy')
    graph_acc_ax.legend()
    graph_acc_fig.suptitle(""Training and validation acc"")
    graph_acc_fig.savefig(graph_acc_fname)
    pylab.close(graph_acc_fig)

    ## Position of axes on multi-model graph:
    i_row = i // graph_all_ncol
    i_col = i % graph_all_ncol

    ## Adding model metrics to multi-model graph - loss:
    graph_all_loss_axs[i_row, i_col].plot(epochs, loss, 'bo', label='Training loss')
    graph_all_loss_axs[i_row, i_col].plot(epochs, val_loss, 'b', label='Validation loss')
    graph_all_loss_axs[i_row, i_col].set_title(fr""{model_dict['no']}. {model_dict['name']}"")

    ## Adding model metrics to multi-model graph - accuracy:
    graph_all_acc_axs[i_row, i_col].plot(epochs, acc, 'bo', label='Training acc')
    graph_all_acc_axs[i_row, i_col].plot(epochs, val_acc, 'b', label='Validation acc')
    graph_all_acc_axs[i_row, i_col].set_title(fr""{model_dict['no']}. {model_dict['name']}"")


## Saving multi-model graphs:
# Output files are quite big (8000x8000 PNG), you may want to decrease DPI.
graph_all_loss_fig.savefig(fr""{os.path.basename(__file__).replace('.py', '')}_ALL_loss_graph.png"", dpi=400)
graph_all_acc_fig.savefig(fr""{os.path.basename(__file__).replace('.py', '')}_ALL_acc_graph.png"", dpi=400)


Please find two main graphs below: Loss - binary crossentropy, Accuracy (I am not allowed te embed images in post due to low reputation).

I have also obtained similarly strange problems in regression model - the MAE was in range of several thousands - in the problem where $y$ range was maybe of several tens. (I decided not to include this model here, because it would make this question even longer.)

Versions of modules and libraries, hardware


Modules:


Keras                    2.3.1
Keras-Applications       1.0.8
Keras-Preprocessing      1.1.0
matplotlib               3.1.3
tensorflow-estimator     1.14.0
tensorflow-gpu           2.1.0
tensorflow-gpu-estimator 2.1.0



keras.json file:


{
    ""floatx"": ""float32"",
    ""epsilon"": 1e-07,
    ""backend"": ""tensorflow"",
    ""image_data_format"": ""channels_last""
}



CUDA - I have CUDA 10.0 and CUDA 10.1 installed on my system.
CUDnn - I have three versions: cudnn-10.0 v7.4.2.24, cudnn-10.0 v7.6.4.38, cudnn-9.0 v7.4.2.24
GPU: Nvidia GTX 1050Ti 4gb
Windows 10 Home


Questions


Do you know what may be the reason of this behavior?
Is it possible that this is caused by multiple CUDA and CUDnn installations? Before observing the problem I have trained several models (both from book and my own ones) and the seemed to behave mor or less as expected, while having 2 CUDA and 2 CUDnn versions (those above without cudnn-10.0 v7.6.4.38) installed.
Is there any official/good source of adequate combinations of keras, tensorflow, CUDA, CUDnn (and  other relevant things e.g. maybe Visual Studio)? I cannot really find any authoritative and up-to-date source.


I hope I've described everything clearly enough. If you have any questions, please ask.
",4,1522,"I finally found the solution (sort of). It's enough to change keras to tensorflow.keras.

Revised Code

# Based on examples from ""Deep Learning with Python"" by Franois Chollet:
## Constants, modules:
VERSION = 2

import os
#U: from keras import models
#U: from keras import layers
from tensorflow.keras import models
from tensorflow.keras import layers

import matplotlib.pyplot as plt
import pylab

## Loading data:
from keras.datasets import imdb

(x_train, y_train), (x_test, y_test) = \
    imdb.load_data(num_words=10000)

from keras.preprocessing import sequence

x_train = sequence.pad_sequences(x_train, maxlen=500)
x_test = sequence.pad_sequences(x_test, maxlen=500)

## Dictionary with models' hyperparameters:
MODELS_ALL = [
    # GRU:
    {""no"": 1,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": None},

    {""no"": 2,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": None},

    {""no"": 3,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 4,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},

    {""no"": 5,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": None},

    {""no"": 6,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": None},

    {""no"": 7,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 8,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},

    # LSTM:
    {""no"": 9,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": None},

    {""no"": 10,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": None},

    {""no"": 11,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 12,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},

    {""no"": 13,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": None},

    {""no"": 14,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": None},

    {""no"": 15,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 16,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},
]

MODELS_GRU_RECCURENT = [
    # GRU:
    {""no"": 3,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 4,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},

    {""no"": 7,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 8,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},
]

MODELS = MODELS_ALL   # ""MODELS = MODELS_ALL"" or ""MODELS = MODELS_GRU_RECCURENT""

## Adding name:
for model_dict in MODELS:
    model_dict[""name""] = f""{model_dict['layer_type']}""
    model_dict[""name""] += f""_d{model_dict['dropout']}"" if model_dict['dropout'] is not None else f""_dN""
    model_dict[""name""] += f""_rd{model_dict['recurrent_dropout']}"" if model_dict['recurrent_dropout'] is not None else f""_rdN""
    model_dict[""name""] += f""_{model_dict['optimizer']}""


## Fucntion - defing and training model:
def train_model(model_dict):
    """"""Defines and trains a model, outputs history.""""""

    ## Defining:
    model = models.Sequential()
    model.add(layers.Embedding(10000, 32))

    recurrent_layer_kwargs = dict()
    if model_dict[""dropout""] is not None:
        recurrent_layer_kwargs[""dropout""] = model_dict[""dropout""]
    if model_dict[""recurrent_dropout""] is not None:
        recurrent_layer_kwargs[""recurrent_dropout""] = model_dict[""recurrent_dropout""]

    if model_dict[""layer_type""] == 'GRU':
        model.add(layers.GRU(32, **recurrent_layer_kwargs))
    elif model_dict[""layer_type""] == 'LSTM':
        model.add(layers.LSTM(32, **recurrent_layer_kwargs))
    else:
        raise ValueError(""Wrong model_dict['layer_type'] value..."")
    model.add(layers.Dense(1, activation='sigmoid'))

    ## Compiling:
    model.compile(
        optimizer=model_dict[""optimizer""],
        loss='binary_crossentropy',
        metrics=['accuracy'])

    ## Training:
    history = model.fit(x_train, y_train,
                        epochs=20,
                        batch_size=64,
                        validation_split=0.2)

    return history


## Multi-model graphs' parameters:
graph_all_nrow = 4
graph_all_ncol = 4
graph_all_figsize = (20, 20)

assert graph_all_nrow * graph_all_nrow &gt;= len(MODELS)

# fig and axes of multi-model graphs:
graph_all_loss_fig, graph_all_loss_axs = plt.subplots(graph_all_nrow, graph_all_ncol, figsize=graph_all_figsize)
graph_all_acc_fig, graph_all_acc_axs = plt.subplots(graph_all_nrow, graph_all_ncol, figsize=graph_all_figsize)

## Loop trough all models:
for i, model_dict in enumerate(MODELS):
    history = train_model(model_dict)

    ## Metrics extraction:
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    epochs = range(1, len(loss) + 1)

    ## Single-model graph - loss:
    graph_loss_fname = fr""{os.path.basename(__file__).replace('.py', '')}""
    graph_loss_fname += fr""_v{VERSION}_{model_dict['no']}_{model_dict['name']}_loss_graph.png""

    graph_loss_fig, graph_loss_ax = plt.subplots()
    graph_loss_ax.plot(epochs, loss, 'bo', label='Training loss')
    graph_loss_ax.plot(epochs, val_loss, 'b', label='Validation loss')
    graph_loss_ax.legend()
    graph_loss_fig.suptitle(""Training and validation loss"")
    graph_loss_fig.savefig(graph_loss_fname)
    pylab.close(graph_loss_fig)

    ## Single-model graph - accuracy:
    graph_acc_fname = fr""{os.path.basename(__file__).replace('.py', '')}""
    graph_acc_fname += fr""_v{VERSION}_{model_dict['no']}_{model_dict['name']}_acc_graph.png""

    graph_acc_fig, graph_acc_ax = plt.subplots()
    graph_acc_ax.plot(epochs, acc, 'bo', label='Training accuracy')
    graph_acc_ax.plot(epochs, val_acc, 'b', label='Validation accuracy')
    graph_acc_ax.legend()
    graph_acc_fig.suptitle(""Training and validation acc"")
    graph_acc_fig.savefig(graph_acc_fname)
    pylab.close(graph_acc_fig)

    ## Position of axes on multi-model graph:
    i_row = i // graph_all_ncol
    i_col = i % graph_all_ncol

    ## Adding model metrics to multi-model graph - loss:
    graph_all_loss_axs[i_row, i_col].plot(epochs, loss, 'bo', label='Training loss')
    graph_all_loss_axs[i_row, i_col].plot(epochs, val_loss, 'b', label='Validation loss')
    graph_all_loss_axs[i_row, i_col].set_title(fr""{model_dict['no']}. {model_dict['name']}"")

    ## Adding model metrics to multi-model graph - accuracy:
    graph_all_acc_axs[i_row, i_col].plot(epochs, acc, 'bo', label='Training acc')
    graph_all_acc_axs[i_row, i_col].plot(epochs, val_acc, 'b', label='Validation acc')
    graph_all_acc_axs[i_row, i_col].set_title(fr""{model_dict['no']}. {model_dict['name']}"")

graph_all_loss_fig.suptitle(f""Loss - binary crossentropy [v{VERSION}]"")
graph_all_acc_fig.suptitle(f""Accuracy [v{VERSION}]"")

## Saving multi-model graphs:
graph_all_loss_fig.savefig(fr""{os.path.basename(__file__).replace('.py', '')}_ALL_v{VERSION}_loss_graph.png"", dpi=400)
graph_all_acc_fig.savefig(fr""{os.path.basename(__file__).replace('.py', '')}_ALL_v{VERSION}_acc_graph.png"", dpi=400)

## Saving multi-model graphs (SMALL):
graph_all_loss_fig.savefig(fr""{os.path.basename(__file__).replace('.py', '')}_ALL_v{VERSION}_loss_graph_SMALL.png"", dpi=150)
graph_all_acc_fig.savefig(fr""{os.path.basename(__file__).replace('.py', '')}_ALL_v{VERSION}_acc_graph_SMALL.png"", dpi=150)


Results

Graphs analogous to those in question: Loss - binary crossentropy, Accuracy

More on keras vs tensorflow.keras

As written in Franois Chollet's tweets (found here: https://stackoverflow.com/a/54117754) instead of stand-alone keras there will be tensorflow.keras (that is Keras as official API to TensorFlow) from now on. (I'm not completely sure if I'm 100% correct, feel free to correct me.)

I think it's better just to use tensorflow.keras instead of keras in future projects.
","Same for me while training using R interface to Keras. The issue seems related to recurrent dropout and the length of the ""time"" dimension. It happens using GRU only (lstm has no problem).



# remotes::install_github(""rstudio/keras#1032"")
library(keras)


reticulate::py_config()
#&gt; python:         /home/clanera/anaconda3/envs/r-tensorflow/bin/python
#&gt; libpython:      /home/clanera/anaconda3/envs/r-tensorflow/lib/libpython3.6m.so
#&gt; pythonhome:     /home/clanera/anaconda3/envs/r-tensorflow:/home/clanera/anaconda3/envs/r-tensorflow
#&gt; version:        3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 21:14:29)  [GCC 7.3.0]
#&gt; numpy:          /home/clanera/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/numpy
#&gt; numpy_version:  1.18.1
#&gt; tensorflow:     /home/clanera/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow
#&gt; 
#&gt; NOTE: Python version was forced by RETICULATE_PYTHON
tensorflow::tf_config()
#&gt; TensorFlow v2.0.0 (~/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow)
#&gt; Python v3.6 (~/anaconda3/envs/r-tensorflow/bin/python)
tensorflow::tf_gpu_configured()
#&gt; TensorFlow built with CUDA:  FALSE 
#&gt; GPU device name:
#&gt; [1] FALSE


n &lt;- 100
t &lt;- 80 # with 72- seams have no problem
q &lt;- 10

x &lt;- array(sample(n*t*q), c(n, t, q))
y &lt;- sample(0:1, n, replace = TRUE)


input &lt;- layer_input(c(t, q))
output &lt;- input %&gt;% 
#  ## no problem using LSTM
#  layer_lstm(units = 2, recurrent_dropout = 0.5) %&gt;%
  layer_gru(units = 2, recurrent_dropout = 0.5) %&gt;%
  layer_dense(units = 1, activation = ""sigmoid"")

model &lt;- keras_model(input, output)

summary(model)
#&gt; Model: ""model""
#&gt; ________________________________________________________________________________
#&gt; Layer (type)                        Output Shape                    Param #     
#&gt; ================================================================================
#&gt; input_1 (InputLayer)                [(None, 80, 10)]                0           
#&gt; ________________________________________________________________________________
#&gt; gru (GRU)                           (None, 2)                       78          
#&gt; ________________________________________________________________________________
#&gt; dense (Dense)                       (None, 1)                       3           
#&gt; ================================================================================
#&gt; Total params: 81
#&gt; Trainable params: 81
#&gt; Non-trainable params: 0
#&gt; ________________________________________________________________________________

history &lt;- model %&gt;%
  compile(optimizer = ""adam"", loss = ""binary_crossentropy"") %&gt;% 
  fit(x, y, 2, 3)

history
#&gt; Trained on 100 samples (batch_size=2, epochs=3)
#&gt; Final epoch (plot to see history):
#&gt; loss: NaN


Created on 2020-05-10 by the reprex package (v0.3.0)

sessionInfo()
#&gt; R version 4.0.0 (2020-04-24)
#&gt; Platform: x86_64-pc-linux-gnu (64-bit)
#&gt; Running under: Ubuntu 18.04.4 LTS
#&gt; 
#&gt; Matrix products: default
#&gt; BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so
#&gt; 
#&gt; locale:
#&gt;  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
#&gt;  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
#&gt;  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
#&gt;  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
#&gt;  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
#&gt; [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       
#&gt; 
#&gt; attached base packages:
#&gt; [1] stats     graphics  grDevices datasets  utils     methods   base     
#&gt; 
#&gt; other attached packages:
#&gt; [1] keras_2.2.5.0
#&gt; 
#&gt; loaded via a namespace (and not attached):
#&gt;  [1] Rcpp_1.0.4.6         whisker_0.4          knitr_1.28          
#&gt;  [4] magrittr_1.5         lattice_0.20-41      R6_2.4.1            
#&gt;  [7] rlang_0.4.6          stringr_1.4.0        highr_0.8           
#&gt; [10] tools_4.0.0          grid_4.0.0           xfun_0.13           
#&gt; [13] htmltools_0.4.0      tfruns_1.4           yaml_2.2.1          
#&gt; [16] digest_0.6.25        tensorflow_2.0.0     Matrix_1.2-18       
#&gt; [19] base64enc_0.1-3      zeallot_0.1.0        evaluate_0.14       
#&gt; [22] rmarkdown_2.1        stringi_1.4.6        compiler_4.0.0      
#&gt; [25] generics_0.0.2       reticulate_1.15-9000 jsonlite_1.6.1      
#&gt; [28] renv_0.10.0

",
tensorflow strange behavior,https://stackoverflow.com/questions/47636980,multiple GPUs keras weird speedup,"I did implement a similar code like the multi GPU code from keras 
(multiGPU tutorial). When running this on a Server with 2 GPUs I have the following training times per epoch:


showing Keras only one GPU and setting variable gpus = 1 (only use one GPU), one epoch = 32s
showing Keras two GPUs, and gpus = 1, one epoch = 31 s
showing Keras two GPUs, and gpus = 2, one epoch = 37 s


the output looks a bit strange, while initializing the code seems to create multiple Tensorflow devices per GPU, I'm not sure if this is the correct behavior. But the most other examples I saw had just one such line per GPU.

first test (one GPU shown, gpus = 1):

2017-12-04 14:54:04.071549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.3285
pciBusID 0000:82:00.0
Total memory: 15.93GiB
Free memory: 15.64GiB
2017-12-04 14:54:04.071597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-12-04 14:54:04.071605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-12-04 14:54:04.071619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
2017-12-04 14:54:21.531654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)


second test (2 GPU shown, gpus = 1):

2017-12-04 14:48:24.881733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
...(same as earlier)
2017-12-04 14:48:24.882924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
2017-12-04 14:48:24.882931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:83:00.0)
2017-12-04 14:48:42.353807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
2017-12-04 14:48:42.353851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:83:00.0)


and weirdly for example 3 (gpus = 2):

2017-12-04 14:41:35.906828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties:
...(same as earlier)
2017-12-04 14:41:35.907996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
2017-12-04 14:41:35.908002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:83:00.0)
2017-12-04 14:41:52.944335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
2017-12-04 14:41:52.944377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:83:00.0)
2017-12-04 14:41:53.709812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
2017-12-04 14:41:53.709838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:83:00.0)


the code:

LSTM = keras.layers.CuDNNLSTM
model.add(LSTM(knots, input_shape=(timesteps, X_train.shape[-1]), return_sequences=True))
model.add(LSTM(knots))
model.add(Dense(3, activation='softmax'))

if gpus&gt;=2:
    model_basic = model
    with tf.device(""/cpu:0""):
        model = model_basic
    parallel_model = multi_gpu_model(model, gpus=gpus)
    model = parallel_model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
hist = model.fit(myParameter)


Is this a typical behavior? What is wrong with my code that the multiple devices per GPU are created. Thanks in advance. 
",2,508,,,
tensorflow strange behavior,https://stackoverflow.com/questions/70335600,Why this model can&#39;t overfit one example?,"I am practicing conv1D on TensorFlow 2.7, and I am checking a decoder I developed by checking if it will overfit one example. The model doesn't learn when trained on only one example and can't overfit this one example. I want to understand this strange behavior, please. This is the link to the notebook on colab Notebook.
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv1D, Dense, BatchNormalization 
from tensorflow.keras.layers import ReLU, MaxPool1D, GlobalMaxPool1D
from tensorflow.keras import Model
import numpy as np

def Decoder():
    inputs = Input(shape=(68, 3), name='Input_Tensor')

    # First hidden layer
    conv1 = Conv1D(filters=64, kernel_size=1, name='Conv1D_1')(inputs)
    bn1 = BatchNormalization(name='BN_1')(conv1)
    relu1 = ReLU(name='ReLU_1')(bn1)
      
    # Second hidden layer
    conv2 = Conv1D(filters=64, kernel_size=1, name='Conv1D_2')(relu1)
    bn2 = BatchNormalization(name='BN_2')(conv2)
    relu2 = ReLU(name='ReLU_2')(bn2)

    # Third hidden layer
    conv3 = Conv1D(filters=64, kernel_size=1, name='Conv1D_3')(relu2)
    bn3 = BatchNormalization(name='BN_3')(conv3)
    relu3 = ReLU(name='ReLU_3')(bn3)

    # Fourth hidden layer
    conv4 = Conv1D(filters=128, kernel_size=1, name='Conv1D_4')(relu3)
    bn4 = BatchNormalization(name='BN_4')(conv4)
    relu4 = ReLU(name='ReLU_4')(bn4)

    # Fifth hidden layer
    conv5 = Conv1D(filters=1024, kernel_size=1, name='Conv1D_5')(relu4)
    bn5 = BatchNormalization(name='BN_5')(conv5)
    relu5 = ReLU(name='ReLU_5')(bn5)

    global_features = GlobalMaxPool1D(name='GlobalMaxPool1D')(relu5)
    global_features = tf.keras.layers.Reshape((1, -1))(global_features)

    conv6 = Conv1D(filters=12, kernel_size=1, name='Conv1D_6')(global_features)
    bn6 = BatchNormalization(name='BN_6')(conv6)
    outputs = ReLU(name='ReLU_6')(bn6)
    model = Model(inputs=[inputs], outputs=[outputs], name='Decoder')
    return model

model = Decoder()
model.summary()

optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)
losses = tf.keras.losses.MeanSquaredError()
model.compile(optimizer=optimizer, loss=losses)

n = 1
X = np.random.rand(n, 68, 3)
y = np.random.rand(n, 1, 12)

model.fit(x=X,y=y, verbose=1, epochs=30)

",2,565,"I think the problem here is, that you have no basis to learn anything, so you can't overfit. In every epoch you have just one example which is used to adapt the weights of the network. So there is not enough time to adapt the weights for overfitting here.
So to get the result of overfitting you want to have the same data multiple times inside your training dataset so the weights can change enought to overfitt because you only change them just one small step per epoch.
A deeper look into the back propagation might help you to get a better understanding of the concept. Click
I took th liberty to adapt your notebook and enhanced the dataset as following:
n = 1
X = np.random.rand(n, 68, 3)
y = np.random.rand(n, 1, 12)

for i in range(0,10):
  X=np.append(X,X,axis = 0)
  y=np.append(y,y,axis = 0)
 

And the output would be:
",,
tensorflow strange behavior,https://stackoverflow.com/questions/38384531,TensorFlow - nn.max_pooling increases memory usage enormously,"I try to create a simple convolution neural network in TensorFlow. Everything seems fine when I run my code below. I run it in Spyder IDE and monitor memory usage - it grows to 64-65% on my laptop and not goes any further.

batch_size = 16
patch_size = 5
depth = 16
num_hidden = 64

graph = tf.Graph()

with graph.as_default():

  # Input data.
  tf_train_dataset = tf.placeholder(
    tf.float32, shape=(batch_size, image_size, image_size, num_channels))
  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
  tf_valid_dataset = tf.constant(valid_dataset)
  tf_test_dataset = tf.constant(test_dataset)

  # Variables.
  layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
  layer1_biases = tf.Variable(tf.zeros([depth]))
  layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
  layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
  layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))

  # Model.
  #Now instead of using strides = 2 for convolutions we will use maxpooling with
  #same convolution sizes
  def model(data):
    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')
    hidden = tf.nn.relu(conv + layer1_biases)
    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')
    hidden = tf.nn.relu(conv + layer2_biases)
    shape = hidden.get_shape().as_list()
    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)
    return tf.matmul(hidden, layer4_weights) + layer4_biases

  # Training computation.
  logits = model(tf_train_dataset)
  loss = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))

  # Optimizer.
  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)

  # Predictions for the training, validation, and test data.
  train_prediction = tf.nn.softmax(logits)
  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
  test_prediction = tf.nn.softmax(model(tf_test_dataset))

num_steps = 1001

with tf.Session(graph=graph) as session:
  tf.initialize_all_variables().run()
  print('Initialized')
  for step in range(num_steps):
    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]
    batch_labels = train_labels[offset:(offset + batch_size), :]
    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
    _, l, predictions = session.run(
      [optimizer, loss, train_prediction], feed_dict=feed_dict)
    if (step % 50 == 0):
      print('Minibatch loss at step %d: %f' % (step, l))
      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))
      print('Validation accuracy: %.1f%%' % accuracy(
        valid_prediction.eval(), valid_labels))
  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))


Ok, after that I try to introduce a maxpooling with kernel of 2, and reduce the size of data with maxpooling instead of conv layer. It looks like below:

conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
    maxpool = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')
    hidden = tf.nn.relu(maxpool + layer1_biases)
    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')


Everything else remain same. But when I do this (notice, I introduced only one maxpooling layer) the memory usage grows to 100% and my iPython kernel just dies. Any ideas of such strange behavior, why the memory usage gets so big? Am I doing something wrong? Any suggestions on how to decrease memory usage?
",1,419,,,
tensorflow strange behavior,https://stackoverflow.com/questions/42138462,Accuracy drops after changing training data with one iteration,"I have been working on using LeNet in tensorflow to train and classify the german traffic signs. I have modified the LeNet first and final layers to accept both the 1 and 3 channel color images (layer 1) and the number of classes to 43
(layer 6). 

from tensorflow.contrib.layers import flatten

def LeNet(x, inputdepth):
    # Hyperparameters
    mu = 0
    sigma = 0.1

# Solution: Layer 1: Convolutional input 32x32x3. Output = 28x28x6
conv1_W = tf.Variable(tf.truncated_normal(shape=(5,5,inputdepth,6), mean=mu, stddev=sigma))
conv1_b = tf.Variable(tf.zeros(6))
conv1 = tf.nn.conv2d(x, conv1_W, strides = [1,1,1,1], padding='VALID') + conv1_b

# Solution: Activation
conv1 = tf.nn.relu(conv1)

# Solution: Pooling. INput = 28x28x6. Output = 14x14x6
conv1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')

# Solution: Layer 2: Convolutional Output = 10x10x16
conv2_W = tf.Variable(tf.truncated_normal(shape = (5,5,6,16), mean=mu, stddev=sigma))
conv2_b = tf.Variable(tf.zeros(16))
conv2 = tf.nn.conv2d(conv1, conv2_W, strides = [1,1,1,1], padding='VALID') + conv2_b

# Solution: Activation
conv2 = tf.nn.relu(conv2)

# Solution: Pooling. Input = 10x10x16. Output = 5x5x16
conv2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides = [1,2,2,1], padding='VALID')

# Solution: Flatten. Input = 5x5x16. Output = 400
fc0 = flatten(conv2)

# Solution: Layer 3: Full Connected. Input = 400, Output = 120
fc1_W = tf.Variable(tf.truncated_normal(shape=(400,120), mean = mu, stddev=sigma))
fc1_b = tf.Variable(tf.zeros(120))
fc1 = tf.matmul(fc0, fc1_W) + fc1_b

# Solution: Activation
fc1 = tf.nn.relu(fc1)

# Solution: Layer 4: Fully Connected. Input = 120, Output = 84
fc2_W = tf.Variable(tf.truncated_normal(shape = (120,84), mean=mu, stddev=sigma))
fc2_b = tf.Variable(tf.zeros(84))
fc2 = tf.matmul(fc1,fc2_W) + fc2_b

# Solution: Activation
fc2 = tf.nn.relu(fc2)

# Solution: Layer 5: Fully Connected. Input = 84, Output = 43
fc3_W = tf.Variable(tf.truncated_normal(shape=(84,43), mean=mu, stddev=sigma))
fc3_b = tf.Variable(tf.zeros(43))
logits = tf.matmul(fc2, fc3_W) + fc3_b

return logits


Since the network is configured to accept both 1 and 3 channel images, (with the depth argument, I am trying to use various preprocessing methods (grayscale conversion, normalization between (0,1) and scaling with [-0.5,0.5]) on the input training images and trying to evaluate the accuracy in each step. I have 6 kinds of processed data


Original RGB image
Converting to grayscale
Normalization of grayscale image between [0,1]
Scaling of grayscale image with zero mean and unit variance [-0.5,0.5]
Normalization on RGB image between [0,1]
Scaling on RGB with zero mean and unit variance [-0.5,0.5] 


I wanted to create a pipeline in a loop that takes one type of preprocessed data in one iteration and performs training and validation. My code as follows

inputData = [
             ('RGB',X_train, X_valid),
             ('RGBNormalized', normalizedRGB_train, normalizedRGB_valid),
             ('ScaledRGB', scaledRGB_train, scaledRGB_valid),
             ('Grayscale',grayimage_train, grayimage_valid),
             ('GrayScaleNormalized',normalizedGray_train, normalizedGray_valid), 
             ('GrayScaleScaled',scaledGray_train, scaledGray_valid) 
            ]


the input data is a list of tuples where  in each tuple, elem[0] represents the name, elem[1] represents the training set and elem[2] represents the validation set. Now my pipeline is as follows

def evaluate(X_data, y_data):
    num_examples = len(X_data)
    total_accuracy = 0
    sess = tf.get_default_session()
    for offset in range(0,num_examples, BATCH_SIZE):
        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]
        accuracy = sess.run(accuracy_operation, feed_dict = {x:batch_x, y:batch_y})
        total_accuracy += (accuracy * len(batch_x))
    return total_accuracy / num_examples

import tensorflow as tf
from sklearn.utils import shuffle


# Simulation Control Parameters
EPOCHS = 10
BATCH_SIZE = 128
rate = 0.0001

# Variable to store the accuracy of the model
model_performance = np.zeros((len(inputData),EPOCHS))
modelIndex = 0


for name,trainingData, validationData in inputData:
    if np.shape(trainingData)[-1] == 3:
        depth = 3
    else:
        depth = 1

    # Create tensors for input data
    x = tf.placeholder(tf.float32, (None, 32, 32,depth))
    y = tf.placeholder(tf.int32, (None))
    one_hot_y = tf.one_hot(y,43)

    # Tensor Operations
    logits = LeNet(x,depth)
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,one_hot_y)
    loss_operation = tf.reduce_mean(cross_entropy)
    optimizer = tf.train.AdamOptimizer(learning_rate=rate)
    training_operation = optimizer.minimize(loss_operation)
    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(one_hot_y,1))
    accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))


    # Pipeline for training and evaluation
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        num_examples = len(X_train)

        print(""Training on..."",name,'data', 'with input size of',np.shape(trainingData))
        print()
        for i in range(EPOCHS):
            X_train, y_train = shuffle(trainingData, y_train)
            for offset in range(0, num_examples, BATCH_SIZE):
                end = offset + BATCH_SIZE
                batch_x, batch_y = X_train[offset:end], y_train[offset:end]
                sess.run(training_operation, feed_dict = {x: batch_x, y: batch_y})

            validation_accuracy = evaluate(validationData, y_valid)
            print(""EPOCH {} ..."".format(i+1))
            print(""Validation Accuracy = {:.3f}"".format(validation_accuracy))
            print()
            model_performance[modelIndex][i] = validation_accuracy

        modelIndex = modelIndex + 1

    sess.close()


if I try to train the network with the input data without any preprocessing, the accuracy ranges between 80-90%. However keeping the network under a loop shows strange behavior of dropper accuracy as follows 

    Training on... RGB data with input size of (34799, 32, 32, 3)

EPOCH 1 ...
Training Accuracy = 0.038
Validation Accuracy = 0.598

EPOCH 2 ...
Training Accuracy = 0.057
Validation Accuracy = 0.055

EPOCH 3 ...
Training Accuracy = 0.057
Validation Accuracy = 0.055

EPOCH 4 ...
Training Accuracy = 0.058
Validation Accuracy = 0.054

EPOCH 5 ...
Training Accuracy = 0.057
Validation Accuracy = 0.054

Training on... RGBNormalized data with input size of (34799, 32, 32, 3)

EPOCH 1 ...
Training Accuracy = 0.054
Validation Accuracy = 0.042

EPOCH 2 ...
Training Accuracy = 0.047
Validation Accuracy = 0.049

EPOCH 3 ...
Training Accuracy = 0.054
Validation Accuracy = 0.048

EPOCH 4 ...
Training Accuracy = 0.057
Validation Accuracy = 0.054

EPOCH 5 ...
Training Accuracy = 0.054
Validation Accuracy = 0.048

Training on... ScaledRGB data with input size of (34799, 32, 32, 3)

EPOCH 1 ...
Training Accuracy = 0.056
Validation Accuracy = 0.054

EPOCH 2 ...
Training Accuracy = 0.057
Validation Accuracy = 0.054

EPOCH 3 ...
Training Accuracy = 0.057
Validation Accuracy = 0.054

EPOCH 4 ...
Training Accuracy = 0.057
Validation Accuracy = 0.055

EPOCH 5 ...
Training Accuracy = 0.057
Validation Accuracy = 0.055
Training on... Grayscale data with input size of (34799, 32, 32, 1)

EPOCH 1 ...
Training Accuracy = 0.056
Validation Accuracy = 0.051

EPOCH 2 ...
Training Accuracy = 0.058
Validation Accuracy = 0.049

EPOCH 3 ...
Training Accuracy = 0.056
Validation Accuracy = 0.049

EPOCH 4 ...
Training Accuracy = 0.055
Validation Accuracy = 0.050

EPOCH 5 ...
Training Accuracy = 0.056
Validation Accuracy = 0.050
Training on... GrayScaleNormalized data with input size of (34799, 32, 32, 1)

EPOCH 1 ...
Training Accuracy = 0.055
Validation Accuracy = 0.074

EPOCH 2 ...
Training Accuracy = 0.057
Validation Accuracy = 0.054

EPOCH 3 ...
Training Accuracy = 0.056
Validation Accuracy = 0.061

EPOCH 4 ...
Training Accuracy = 0.057
Validation Accuracy = 0.055

EPOCH 5 ...
Training Accuracy = 0.057
Validation Accuracy = 0.054

Training on... GrayScaleScaled data with input size of (34799, 32, 32, 1)

EPOCH 1 ...
Training Accuracy = 0.055
Validation Accuracy = 0.049

EPOCH 2 ...
Training Accuracy = 0.056
Validation Accuracy = 0.060

EPOCH 3 ...
Training Accuracy = 0.058
Validation Accuracy = 0.054

EPOCH 4 ...
Training Accuracy = 0.056
Validation Accuracy = 0.062

EPOCH 5 ...
Training Accuracy = 0.056
Validation Accuracy = 0.061


Any idea where I am making a mistake?
",1,427,,,
tensorflow strange behavior,https://stackoverflow.com/questions/45083474,How to update vaiables in tensorflow scan,"I would like to make a variable in tensorflow, and then update it in tf.scan. First I tried something like this:

import tensorflow as tf

with tf.variable_scope('foo'):
    tf.get_variable('bar', initializer=tf.zeros([1.0]))

def repeat_me(last, current):
    with tf.variable_scope('foo', reuse=True):
        bar = tf.get_variable('bar')
        bar.assign_add(tf.constant([1.0]))
    return bar
output = tf.scan(repeat_me, tf.range(5), initializer=tf.constant([1.0]))

with tf.Session() as sess: 
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    out = sess.run(output)
    print(out)
    with tf.variable_scope('foo', reuse=True):
        print(tf.get_variable('bar').eval())


This does not seem to update the variable with the name of 'bar'.

[[ 0.]
 [ 0.]
 [ 0.]
 [ 0.]
 [ 0.]]
[ 0.]


What is strange to me is the following modification of the ""repeat_me"" function changes the behavior.

def repeat_me(last, current):
    with tf.variable_scope('foo', reuse=True):
        bar = tf.get_variable('bar')
        b = bar.assign_add(tf.constant([1.0]))
    return b


Then, the script spits out this:

[[ 5.]
 [ 5.]
 [ 5.]
 [ 5.]
 [ 5.]]
[ 5.]


Could anyone explain the difference?
",1,227,,,
tensorflow strange behavior,https://stackoverflow.com/questions/56695299,How should I use mode.predict_generator to evaluate model performance in a Confusion Matrix?,"I am trying to evaluate a transfer learning model in the common dogs and cats filtered dataset using confusion matrix. I have based the code in the transfer learning tutorial of tensorflow. The accuracy graphs for training show an accuracy above 90%. 

However, using generators to get the true labes and model.predict_generator to get the prediction array throws inconsistent results. First, accuracy is not stable, if you run a second time the prediction it changes values. Second, the prediction that I get by using  model.predict_generator seems to be wrong compared to model.predict on individual instance.

In order to test quickly the confusion matrix based on the ImageDataGenerator I downloaded 5 images of cats and 5 images of dogs. Then I created another generator from the folder and checked that labels and classes would be the same as training.

Two Strange Behaviors
After that I just used sklearn metrics confusion matrix to evaluate a prediction using model.predict_generator and the labels that I get from the generator as true labels.

At first run I got a 0.9 accuracy and say cheers!. however, if I try a second time the model.predict_generator and it throws other values for array output and accuracy dropps to 0.5. After that it does not change anymore.... What result is correct? Why does it change?

I have been noticing that you have to run twice to get a final result, but the result obtained is wrong. I wrote some code to test each image individually and I got no wrong in prediction. So what am I doing wrong? or are generators not appliable to this situation. This is a bit confusing

Code can be chacked at my github repository and can be used in google colaboratory to be run if you have no gpu. In fact in my little toshiba satellite runs well with a nvidia gpu of just 2 gb and 300 cuda

complete code at my git

The code is organized as jupyter notebook however here I add the code
Transfer Learning is based on https://www.tensorflow.org/tutorials/images/transfer_learning

To create the generator:

test_base_dir = '.'
test_dir = os.path.join( test_base_dir, 'test')
test_datagen_2 = ImageDataGenerator( rescale = 1.0/255. )
test_generator = test_datagen_2.flow_from_directory( test_dir,
                                                     batch_size  = 1,
                                                     class_mode  = binary', 
                                                     target_size = (image_size, image_size))


And for prediction:

   filenames = test_generator.filenames
   nb_samples = len(filenames)
   y_predict = model.predict_generator(test_generator,steps = 
   nb_samples)
   y_predict


Which I round using numpy to finally use confusion matrix metric


from sklearn.metrics  import confusion_matrix
cm = confusion_matrix(y_true=test_generator.labels, y_pred=y_predict_rounded)
cm


The manual verification is instead:

def prediction(path_img):
img = image.load_img(path_img, target_size=(150,150))
x = image.img_to_array(img)
x = x/255.
x = np.expand_dims(x, axis=0)
classes = model.predict(x)
plt.imshow(img)
if classes &gt; 0.5:
    print(path_img.split('/')[-1]+' is a dog')
else:
     print(path_img.split('/')[-1]+' is a cat')   
return classes


Which I use in the following way:

y_pred_m = []
files=[]
for filename in os.listdir(test_dir):
    file = test_dir+'/'+filename
    for item in os.listdir(file):
        file2 = file+'/'+item
        if file2.split('.')[-1]=='jpg':
            files.append(file2)


And prediction goes:

prediction_array = [prediction(img) for img in files]

np.round(prediction_array, decimals=0)


Expected resutls should be to have a confusion matrix with an accuracy level similar to training. Since the verification of each example individually seems to have no error in prediction, however model.predict_generate seems to go wrong.
",1,1423,"The problem was that as default _flow_from_directory_ uses shuffle = True. Predictions are correct if shuffle goes to False. However, using validation dataset to evaluate training seems to do right even though shuffle is True. I have updated git for these changes to be populated

# Flow validation images in batches of 20 using test_datagen generator
test_generator =  test_datagen_2.flow_from_directory( test_dir,
                                                  batch_size  = 1,
                                                  class_mode  = 'binary', 
                                                  target_size = (image_size, 
image_size),
                                                  shuffle = False)

",,
tensorflow strange behavior,https://stackoverflow.com/questions/57498998,Strange behavior of keras v1.2.2 vs. keras v2+ (HUGE differences in accuracy),"Today I've ran into some very strange behavior of Keras. When I try to do a classification run on the iris-dataset with a simple model, keras version 1.2.2 gives me +- 95% accuracy, whereas a keras version of 2.0+ predicts the same class for every training example (leading to an accuracy of +- 35%, as there are three types of iris).  The only thing that makes my model predict +-95% accuracy is downgrading keras to a version below 2.0:

I think it is a problem with Keras, as I have tried the following things, all do not make a difference;


Switching activation function in the last layer (from Sigmoid to softmax).
Switching backend (Theano and Tensorflow both give roughly same performance).
Using a random seed.
Varying the number of neurons in the hidden layer (I only have 1 hidden layer in this simple model).
Switching loss-functions.


As the model is very simple and it runs on it's own (You just need the easy-to-obtain iris.csv dataset) I decided to include the entire code; 

import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder

#Load data
data_frame = pd.read_csv(""iris.csv"", header=None)
data_set = data_frame.values
X = data_set[:, 0:4].astype(float)
Y = data_set[:, 4]

#Encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)

# convert integers to dummy variables (i.e. one hot encoded)
dummy_y = np_utils.to_categorical(encoded_Y)

def baseline_model():
    #Create &amp; Compile model
    model = Sequential()
    model.add(Dense(8, input_dim=4, init='normal', activation='relu'))
    model.add(Dense(3, init='normal', activation='sigmoid'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

#Create Wrapper For Neural Network Model For Use in scikit-learn
estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, batch_size=5, verbose=0)

#Create kfolds-cross validation
kfold = KFold(n_splits=10, shuffle=True)

#Evaluate our model (Estimator) on dataset (X and dummy_y) using a 10-fold cross-validation procedure (kfold).
results = cross_val_score(estimator, X, dummy_y, cv=kfold)
print(""Accuracy: {:2f}% ({:2f}%)"".format(results.mean()*100, results.std()*100))


if anyone wants to replicate the error here are the dependencies I used to observe the problem:

numpy=1.16.4
pandas=0.25.0
sk-learn=0.21.2
theano=1.0.4
tensorflow=1.14.0

",1,235,"In Keras 2.0, many parameters changed names, there is compatibility layer to keep things working, but somehow it did not apply when using KerasClassifier.

In this part of the code:

estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, batch_size=5, verbose=0)


You are using the old name nb_epoch instead of the modern name of epochs. The default value is epochs=1, meaning that your model was only being trained for one epoch, producing very low quality predictions.

Also note that here:

model.add(Dense(3, init='normal', activation='sigmoid'))


You should be using a softmax activation instead of sigmoid, as you are using the categorical cross-entropy loss:

model.add(Dense(3, init='normal', activation='softmax'))

","I've managed to isolate the issue, if you change nb_epoch to epochs, (All else being exactly equal) the model predicts very good again, in keras 2 as well. I don't know if this is intended behavior or a bug.
",
tensorflow strange behavior,https://stackoverflow.com/questions/61260673,tf.keras convolutional architecture isn&#39;t working,"The following problem is a reduction of a real problem I had, while I have been designing an elementary autoencoder arch.
The following example is enough to reproduce exactly the error I had.
I have been trying around two days now, but I can't find any way out of it.

import tensorflow as tf
import random
import os

RES = [256, 256]
def generator_data(n):
    for i in range(n):
        for j in range(6):
            yield tf.zeros((1, 256, 256, 3)), tf.zeros((1, 256, 256, 3))

def mymodel():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same'))
    # 256 x 256 x 8
    model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same'))
    # 128 x 128 x 8
    model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
    # 128 x 128 x 16
    model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same'))
    # 64 x 64 x 16
    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))
    # 64 x 64 x 32
    model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same'))
    # 32 x 32 x 32

    # 32 x 32 x 32
    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))
    # 32 x 32 x 32
    model.add(tf.keras.layers.UpSampling2D((2, 2)))
    # 64 x 64 x 32
    model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
    # 64 x 64 x 16
    model.add(tf.keras.layers.UpSampling2D((2, 2)))
    # 128 x 128 x 16
    model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same'))
    # 128 x 128 x 8
    model.add(tf.keras.layers.UpSampling2D((2, 2)))
    # 256 x 256 x 8
    model.add(tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))
    return model


if __name__ == ""__main__"":
    # import some data to play with
    x_val, y_val = zip(*generator_data(20))

    model = mymodel()
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss=tf.keras.losses.MeanSquaredError())
    model(tf.zeros((1, 256, 256, 3)))
    model.summary()

    # generator_data(train_list)
    model.fit(x=generator_data(1000),
        validation_data=(list(x_val), list(y_val)),
        verbose=1, epochs=1000)


Firstly I have the strange behavior of a model.summary() that contains:

Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              multiple                  224
_________________________________________________________________
max_pooling2d (MaxPooling2D) multiple                  0
_________________________________________________________________
conv2d_1 (Conv2D)            multiple                  1168
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 multiple                  0
_________________________________________________________________
conv2d_2 (Conv2D)            multiple                  4640
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 multiple                  0
_________________________________________________________________
conv2d_3 (Conv2D)            multiple                  9248
_________________________________________________________________
up_sampling2d (UpSampling2D) multiple                  0
_________________________________________________________________
conv2d_4 (Conv2D)            multiple                  4624
_________________________________________________________________
up_sampling2d_1 (UpSampling2 multiple                  0
_________________________________________________________________
conv2d_5 (Conv2D)            multiple                  1160
_________________________________________________________________
up_sampling2d_2 (UpSampling2 multiple                  0
_________________________________________________________________
conv2d_6 (Conv2D)            multiple                  73
=================================================================
Total params: 21,137
Trainable params: 21,137
Non-trainable params: 0


only multiple on output shape.
 I have looked it up here but the workaround doesn't seem to work.
 But secondly and more importantly I get the error:

ValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), for inputs ['input_1'] but instead got the following list of 120 arrays: [&lt;tf.Tensor: shape=(1, 256, 256, 3), dtype=float32, numpy=
array([[[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0....


which for my unserstanding makes no sense at all. My generator returns [batch, x-dim, y-dim, channel] (I have tried also with [batch, channel, x-dim, y-dim] but also no luck). And in this case the batch equals 1 and not 120.
As I said I cannot tackle/debug these problems no matter what, so I would really appreciate your help.
I am quite new to DL but not in python and I am using Tensorflow-2.1.0, in python-3.7

Thanks a lot.
",1,496,"Here's the working code.

import tensorflow as tf
import random
import os
import numpy as np

RES = [256, 256]
def generator_data(n):
    for i in range(n):
        for j in range(1):
            yield tf.zeros((1, 256, 256, 3)), tf.zeros((1, 256, 256, 3))

def mymodel():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same'))
    # 256 x 256 x 8
    model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same'))
    # 128 x 128 x 8
    model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
    # 128 x 128 x 16
    model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same'))
    # 64 x 64 x 16
    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))
    # 64 x 64 x 32
    model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same'))
    # 32 x 32 x 32

    # 32 x 32 x 32
    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))
    # 32 x 32 x 32
    model.add(tf.keras.layers.UpSampling2D((2, 2)))
    # 64 x 64 x 32
    model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
    # 64 x 64 x 16
    model.add(tf.keras.layers.UpSampling2D((2, 2)))
    # 128 x 128 x 16
    model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same'))
    # 128 x 128 x 8
    model.add(tf.keras.layers.UpSampling2D((2, 2)))
    # 256 x 256 x 8
    model.add(tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))
    return model


if __name__ == ""__main__"":
    # import some data to play with
    z = list(zip(*generator_data(2)))

    x_val = z[0][0]
    y_val = z[0][1]

    model = mymodel()
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss=tf.keras.losses.MeanSquaredError())
    model(tf.zeros((1, 256, 256, 3)))
    model.summary()


    print(x_val.numpy().shape)
    print(y_val.numpy().shape)
    model.fit(x=generator_data(10),
        validation_data=(x_val, y_val),
        verbose=1, epochs=1)


You were using unzip with the generator in the wrong way. I type-casted the output to a list so that it's subscribable. One of the useful trick is to print the shapes and len of the X, y in each step to find out where the bug is.

Update:

Yes, exactly but you need to pass a tensor of shape [batch, 256, 256, 3]. But if a is a list, and a[0] has shape [1, 256, 256, 3], then you need to pass a[0] to the model and that's what I did. But, you were passing a. But a is a list, not a numpy array/tensor and even if we typecast it to a numpy array we'll get shape = (1, 1, 256, 256, 3) - which is not valid.

Also, in your generator_data why you're using an unnecessary second loop?

def generator_data(n):
    for i in range(n):
        for j in range(1): # ??????? Why?
            yield tf.zeros((1, 256, 256, 3)), tf.zeros((1, 256, 256, 3))

","I managed to solve those problems by updating to tf-2.2 nightly and using the tf.data module.

In case you have the same problem, have a look here:

import tensorflow as tf
import random
import os
from functools import partial

RES = [256, 256]
def generator_data(n):
    for i in range(n):
        for j in range(6):
            yield tf.zeros((1, 256, 256, 3)), tf.zeros((1, 256, 256, 3))

def generator_data_val(n):
    for i in range(n):
        for j in range(6):
            yield tf.zeros((256, 256, 3)), tf.zeros((256, 256, 3))


def model():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same', input_shape=(256, 256, 3)))
    # 256 x 256 x 8
    model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same'))
    # 128 x 128 x 8
    model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
    # 128 x 128 x 16
    model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same'))
    # 64 x 64 x 16
    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))
    # 64 x 64 x 32
    model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same'))
    # 32 x 32 x 32

    # 32 x 32 x 32
    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))
    # 32 x 32 x 32
    model.add(tf.keras.layers.UpSampling2D((2, 2)))
    # 64 x 64 x 32
    model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
    # 64 x 64 x 16
    model.add(tf.keras.layers.UpSampling2D((2, 2)))
    # 128 x 128 x 16
    model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same'))
    # 128 x 128 x 8
    model.add(tf.keras.layers.UpSampling2D((2, 2)))
    # 256 x 256 x 8
    model.add(tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))
    return model


if __name__ == ""__main__"":
    # import some data to play with
    x_val, y_val = zip(*generator_data_val(5))
    x_val, y_val = list(x_val), list(y_val)


    model = model()
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss=tf.keras.losses.MeanSquaredError())
    model(tf.zeros((1, 256, 256, 3)))
    model.summary()
    train_dataset = generator_data(5)

    gen = partial(generator_data, n=5)
    train_dataset = tf.data.Dataset.from_generator(
        gen, output_types=(tf.float32, tf.float32),
        output_shapes=(tf.TensorShape([1, 256, 256, 3]), tf.TensorShape([1, 256, 256, 3]))).repeat()
    val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(2)

    # generator_data(train_list)
    model.fit(x=train_dataset,
        steps_per_epoch=40,
        validation_data=val_dataset,
        verbose=1, epochs=1000)

",
tensorflow strange behavior,https://stackoverflow.com/questions/70428593,Vertex AI seems to think a deployed model input shape is different then when predicting locally,"I'm getting what I see as strange behavior out of a deployed model in vertex ai.  I have a CNN model built with tensorflow/keras version 2.7.  My input data is a 3 dimensional array with the follow shape (1, 570, 33).  When I pass the input data to the model locally I have a correct response.
model = keras.models.load_model('model')
x = model.predict(input_data) # input_data is a numpy array of shape (1, 570, 33)   
print(x)
[[0.1259355  0.9124526  0.65782744 0.2628207 ]]

This is a correct prediction and the model does what it is trained to do.  No problems
When I upload the model to Vertex AI using the prebuilt Tensorflow 2.7 docker container with no extra settings (no acceleration for example) and deploy that model to an endpoint this is what I get when I call predict with the same input_data formatted for Vertex AI.
resp = client.predict(
    endpoint=endpoint_path,
    instances=input_data.toList(),
    parameters=parameters,
)

input must be 4-dimensional[1,570,33]\n\t [[{{function_node __inference__wrapped_model_28143}}{{node sequential/conv2d/BiasAdd}}]]

Here is the summary of of the model
Model: ""sequential""
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
conv2d (Conv2D)             (None, 570, 33, 32)       320       
                                                                
batch_normalization (BatchN  (None, 570, 33, 32)      128       
ormalization)                                                   
                                                                
activation (Activation)     (None, 570, 33, 32)       0         
                                                                
conv2d_1 (Conv2D)           (None, 570, 33, 32)       9248      
                                                                
batch_normalization_1 (Batc  (None, 570, 33, 32)      128       
hNormalization)                                                 
                                                                
activation_1 (Activation)   (None, 570, 33, 32)       0         
                                                                
conv2d_2 (Conv2D)           (None, 570, 33, 32)       9248      
                                                                
batch_normalization_2 (Batc  (None, 570, 33, 32)      128       
hNormalization)                                                 
                                                                
activation_2 (Activation)   (None, 570, 33, 32)       0         
                                                                
conv2d_3 (Conv2D)           (None, 285, 17, 64)       18496     
                                                                
batch_normalization_3 (Batc  (None, 285, 17, 64)      256       
hNormalization)                                                 
                                                                
activation_3 (Activation)   (None, 285, 17, 64)       0         
                                                                
conv2d_4 (Conv2D)           (None, 285, 17, 64)       36928     
                                                                
batch_normalization_4 (Batc  (None, 285, 17, 64)      256       
hNormalization)                                                 
                                                                
activation_4 (Activation)   (None, 285, 17, 64)       0         
                                                                
conv2d_5 (Conv2D)           (None, 285, 17, 64)       36928     
                                                                
batch_normalization_5 (Batc  (None, 285, 17, 64)      256       
hNormalization)                                                 
                                                                
activation_5 (Activation)   (None, 285, 17, 64)       0         
                                                                
conv2d_6 (Conv2D)           (None, 285, 17, 64)       36928     
                                                                
batch_normalization_6 (Batc  (None, 285, 17, 64)      256       
hNormalization)                                                 
                                                                
activation_6 (Activation)   (None, 285, 17, 64)       0         
                                                                
conv2d_7 (Conv2D)           (None, 143, 9, 96)        55392     
                                                                
batch_normalization_7 (Batc  (None, 143, 9, 96)       384       
hNormalization)                                                 
                                                                
activation_7 (Activation)   (None, 143, 9, 96)        0         
                                                                
conv2d_8 (Conv2D)           (None, 143, 9, 96)        83040     
                                                                
batch_normalization_8 (Batc  (None, 143, 9, 96)       384       
hNormalization)                                                 
                                                                
activation_8 (Activation)   (None, 143, 9, 96)        0         
                                                                
conv2d_9 (Conv2D)           (None, 143, 9, 96)        83040     
                                                                
batch_normalization_9 (Batc  (None, 143, 9, 96)       384       
hNormalization)                                                 
                                                                
activation_9 (Activation)   (None, 143, 9, 96)        0         
                                                                
conv2d_10 (Conv2D)          (None, 143, 9, 96)        83040     
                                                                
batch_normalization_10 (Bat  (None, 143, 9, 96)       384       
chNormalization)                                                
                                                                
activation_10 (Activation)  (None, 143, 9, 96)        0         
                                                                
conv2d_11 (Conv2D)          (None, 72, 5, 128)        110720    
                                                                
batch_normalization_11 (Bat  (None, 72, 5, 128)       512       
chNormalization)                                                
                                                                
activation_11 (Activation)  (None, 72, 5, 128)        0         
                                                                
conv2d_12 (Conv2D)          (None, 72, 5, 128)        147584    
                                                                
batch_normalization_12 (Bat  (None, 72, 5, 128)       512       
chNormalization)                                                
                                                                
activation_12 (Activation)  (None, 72, 5, 128)        0         
                                                                
conv2d_13 (Conv2D)          (None, 72, 5, 128)        147584    
                                                                
batch_normalization_13 (Bat  (None, 72, 5, 128)       512       
chNormalization)                                                
                                                                
activation_13 (Activation)  (None, 72, 5, 128)        0         
                                                                
conv2d_14 (Conv2D)          (None, 72, 5, 128)        147584    
                                                                
batch_normalization_14 (Bat  (None, 72, 5, 128)       512       
chNormalization)                                                
                                                                
activation_14 (Activation)  (None, 72, 5, 128)        0         
                                                                
conv2d_15 (Conv2D)          (None, 36, 3, 160)        184480    
                                                                
batch_normalization_15 (Bat  (None, 36, 3, 160)       640       
chNormalization)                                                
                                                                
activation_15 (Activation)  (None, 36, 3, 160)        0         
                                                                
conv2d_16 (Conv2D)          (None, 36, 3, 160)        230560    
                                                                
batch_normalization_16 (Bat  (None, 36, 3, 160)       640       
chNormalization)                                                
                                                                
activation_16 (Activation)  (None, 36, 3, 160)        0         
                                                                
conv2d_17 (Conv2D)          (None, 36, 3, 160)        230560    
                                                                
batch_normalization_17 (Bat  (None, 36, 3, 160)       640       
chNormalization)                                                
                                                                
activation_17 (Activation)  (None, 36, 3, 160)        0         
                                                                
conv2d_18 (Conv2D)          (None, 36, 3, 160)        230560    
                                                                
batch_normalization_18 (Bat  (None, 36, 3, 160)       640       
chNormalization)                                                
                                                                
activation_18 (Activation)  (None, 36, 3, 160)        0         
                                                                
conv2d_19 (Conv2D)          (None, 18, 2, 192)        276672    
                                                                
batch_normalization_19 (Bat  (None, 18, 2, 192)       768       
chNormalization)                                                
                                                                
activation_19 (Activation)  (None, 18, 2, 192)        0         
                                                                
conv2d_20 (Conv2D)          (None, 18, 2, 192)        331968    
                                                                
batch_normalization_20 (Bat  (None, 18, 2, 192)       768       
chNormalization)                                                
                                                                
activation_20 (Activation)  (None, 18, 2, 192)        0         
                                                                
conv2d_21 (Conv2D)          (None, 18, 2, 192)        331968    
                                                                
batch_normalization_21 (Bat  (None, 18, 2, 192)       768       
chNormalization)                                                
                                                                
activation_21 (Activation)  (None, 18, 2, 192)        0         
                                                                
conv2d_22 (Conv2D)          (None, 18, 2, 192)        331968    
                                                                
batch_normalization_22 (Bat  (None, 18, 2, 192)       768       
chNormalization)                                                
                                                                
activation_22 (Activation)  (None, 18, 2, 192)        0         
                                                                
conv2d_23 (Conv2D)          (None, 9, 1, 224)         387296    
                                                                
batch_normalization_23 (Bat  (None, 9, 1, 224)        896       
chNormalization)                                                
                                                                
activation_23 (Activation)  (None, 9, 1, 224)         0         
                                                                
reshape (Reshape)           (None, 9, 224)            0         
                                                                
masking (Masking)           (None, 9, 224)            0         
                                                                
lambda (Lambda)             (None, 224)               0         
                                                                
dense (Dense)               (None, 4)                 900       
                                                                
=================================================================
Total params: 3,554,532
Trainable params: 3,548,772
Non-trainable params: 5,760

I've got a classic case of 'It works on my machine' here and could use any input or help :)
",1,442,"When using Vertex Prediction service with Tensorflow models, following is the expected format of input request as per the docs
{
  ""instances"": [
    &lt;value&gt;|&lt;simple/nested list&gt;|&lt;object&gt;,
    ...
  ]
}

For your scenario, try with
{
  ""instances"": [
    [
      [
        [138, 30, 66, ...],
        [130, 20, 56, ...],
        ...
      ],
      [
        [126, 38, 61, ...],
        [122, 24, 57, ...],
        ...
      ],
      ...
    ],
    ...
  ]
}

",,
tensorflow strange behavior,https://stackoverflow.com/questions/70593426,Library import in Python does not work as expected,"I am having a strange error executing this Python code.
import tensorflow as tf
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(2,)),
  tf.keras.layers.Dense(10, activation='sigmoid'),
])
model.compile(optimizer='adam',
              loss='mae',
              metrics=['mse'])
model.summary()

resulting in the following error:
ModuleNotFoundError: No module named 'keras'

( I already tried the same code on google Colab and it works ).
The strange thing for me is that this code instead works without errors on the same machine ( this means that all libraries are there ):
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense

model = Sequential([
  Flatten(input_shape=(28, 28)),
  Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

and, if in a jupyter notebook I execute first the second version and than the first one, I do not encounter errors.
What can be the cause of such behavior and how to fix it ? I am using Python 3.9.7 with miniconda.
UPDATE 1: this behavior seems to go away when downgrading from tensorflow 2.6.0 to tensorflow 2.4.1 .
UPDATE 2: complete traceback as requested:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
/tmp/ipykernel_19355/718649126.py in &lt;module&gt;
----&gt; 1 model = tf.keras.models.Sequential([
      2   tf.keras.layers.Flatten(input_shape=(28, 28)),
      3   tf.keras.layers.Dense(10, activation='softmax')
      4 ])
      5 

~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py in __getattr__(self, item)
     60 
     61   def __getattr__(self, item):
---&gt; 62     module = self._load()
     63     return getattr(module, item)
     64 

~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py in _load(self)
     43     """"""Load the module and insert it into the parent's globals.""""""
     44     # Import the target module and insert it into the parent's namespace
---&gt; 45     module = importlib.import_module(self.__name__)
     46     self._parent_module_globals[self._local_name] = module
     47 

~/miniconda3/envs/tf/lib/python3.9/importlib/__init__.py in import_module(name, package)
    125                 break
    126             level += 1
--&gt; 127     return _bootstrap._gcd_import(name[level:], package, level)
    128 
    129 

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _gcd_import(name, package, level)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _find_and_load(name, import_)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _gcd_import(name, package, level)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _find_and_load(name, import_)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _gcd_import(name, package, level)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _find_and_load(name, import_)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _gcd_import(name, package, level)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _find_and_load(name, import_)

~/miniconda3/envs/tf/lib/python3.9/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)

ModuleNotFoundError: No module named 'keras'

",1,710,"It seems that there's a bug in the current version of tensorflow, particularly with the _load() method in the LazyLoader class defined in the module tensorflow.python.util.lazy_loader. I think we could just leave it until being fixed. I'd like to dig into another question raised here: how come your second segment of codes worked yet the first one failed?
The reason comes from the import system as well as the dot operator in Python.
In the statement import m1.m2.m3 or from m1.m2.m3 import n1, the dot notation in m1.m2.m3 is used to separate names of modules(packages) and submodules(subpackages). It's not a real dot operator. When processing this statement, Python will first import the module(package) m1, then m1.m2, then m1.m2.m3. In each step of loading the modules, two notable side-effects occur(among lots of other actions):

the to-be-imported package's __init__.py file is executed.
a binding is placed in the parent modules namespace to the submodule object. For this example, m1.__dict__['m2'] is set to be the module object m1.m2, and m1.m2.__dict__['m3'] is set to be the module object m1.m2.m3.

And for the real dot operator, for example an expression obj1.attr1, the evaluation order will be:

evaluate obj1.__getattribute__('attr1') and return the value
if no such an attribute found in the first step, then evaluate odj1.__getattr__('attr1')

where the search of obj1.__dict__['attr1'] is launched in the first step.
Back to tensorflow, there's such a statement in __init__.py under the package tensorflow:
keras = _LazyLoader(""keras"", globals(), _keras_module)

Here's a simplified ""second version"" of your sample as below:
from tensorflow.keras.models import Sequential 
# tensorflow.__dict__['keras'] is set to be the module tensorflow.keras
import tensorflow as tf # make the module name tensorflow useable

print(type(tf.keras)) # tf.keras is evaluated to the submodule tensorflow.keras
# output: &lt;class 'module'&gt;

It turns out the name binding to the submodule object overwrites the binding(assignment) in __init__.py.
But if we don't import the submodule tensorflow.keras, then the binding in __init__.py works:
import tensorflow as tf

print(type(tf.keras))
# output: &lt;class 'tensorflow.python.util.lazy_loader.LazyLoader'&gt;

That explains why your second method worked but the first one failed.

Reference:
For import mechanism:

7.11. The import statement
5. The import system

For dot operator:

Descriptor HowTo Guide - Invocation from an instance

","You can use from tensorflow import keras and try again executing the same code.
if it shows the same error again then run below code in jupyter notebook:
import keras
print(keras.__version__)
import tensorflow
print(tensorflow.__version__)

if you find some differences in versions, then execute below code in jupyter notebook which will automatically install upgraded and compatible version of tensorflow and keras along with its other required libraries.
!pip insatll tensorflow
import tensorflow
print(tensorflow.__version__)

!pip install keras
import keras
print(keras.__version__)

Let us know if issue still persists.
",
tensorflow strange behavior,https://stackoverflow.com/questions/47897199,Bug when using TensorFlow-GPU + Python multiprocessing?,"I have noticed a strange behavior when I use TensorFlow-GPU + Python multiprocessing.

I have implemented a DCGAN with some customizations and my own dataset. Since I am conditioning the DCGAN to certain features, I have training data and also test data for evaluation.

Due to the size of my datasets, I have written data loaders that run concurrently and preload into a queue using Python's multiprocessing.

The structure of the code roughly looks like this:

class ConcurrentLoader:
    def __init__(self, dataset):
        ...

class DCGAN
     ...

net = DCGAN()
training_data = ConcurrentLoader(path_to_training_data)
test_data = ConcurrentLoader(path_to_test_data)


This code runs fine on TensorFlow-CPU and on TensorFlow-GPU &lt;= 1.3.0 using CUDA 8.0, but when I run the exact same code with TensorFlow-GPU 1.4.1 and CUDA 9 (latest releases of TF &amp; CUDA as of Dec 2017) it crashes:

2017-12-20 01:15:39.524761: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2017-12-20 01:15:39.527795: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2017-12-20 01:15:39.529548: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2017-12-20 01:15:39.535341: E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2017-12-20 01:15:39.535383: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
2017-12-20 01:15:39.535397: F tensorflow/core/kernels/conv_ops.cc:667] Check failed: stream-&gt;parent()-&gt;GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo&lt;T&gt;(), &amp;algorithms) 
[1]    32299 abort (core dumped)  python dcgan.py --mode train --save_path ~/tf_run_dir/test --epochs 1


What really confuses me is that if I just remove test_data the error does not occur. Thus, for some strange reason, TensorFlow-GPU 1.4.1 &amp; CUDA 9 work with just a single ConcurrentLoader, but crash when multiple loaders are initialized.

Even more interesting is that (after the exception) I have to manually shut down the python processes, because the GPU's VRAM, the system's RAM and even the python processes stay alive after the script crashes.

Furthermore, it has to have some weird connection to Python's multiprocessing module, because when I implement the same model in Keras (using TF backend!) the code also runs just fine, with 2 concurrent loaders. I guess Keras is somehow creating an layer of abstraction in between that keeps TF from crashing.

Where could I possibly have screwed up with the multiprocessing module that it causes crashes like this one?

These are the parts of the code that use multiprocessing inside the ConcurrentLoader:

def __init__(self, dataset):
    ...
    self._q = mp.Queue(64)
    self._file_cycler = cycle(img_files)
    self._worker = mp.Process(target=self._worker_func, daemon=True)
    self._worker.start()

def _worker_func(self):
    while True:
        ... # gets next filepaths from self._file_cycler
        buffer = list()
        for im_path in paths:
            ... # uses OpenCV to load each image &amp; puts it into the buffer
        self._q.put(np.array(buffer).astype(np.float32))


...and this is it.

Where have I written ""unstable"" or ""non-pythonic"" multiprocessing code? I thought daemon=True should ensure that every process gets killed as soon as the main process dies? Unfortunately, this is not the case for this specific error.

Did I misuse the default multiprocessing.Process or multiprocessing.Queue here? I thought simply writing a class where I store batches of images inside a Queue and make it accessible through methods / instance variables should be just fine.
",0,2676,,,
tensorflow strange behavior,https://stackoverflow.com/questions/48458509,Strange behavior of tensorflow when dividing by 0 in tf.metrics.mean_absolute_error,"So I do not know if is a bug or the problem is in my code, but I am trying to understand what is happening. 
when I run the model and got to estimate the accuracy using Mean Relative Error. I know in my validation data I have 0s so I was expecting to get some error or some inf. However this is not the case.
This is my code:

X_test_norm = preprocessing.scale(X_test)
predictions = sess.run(pred, feed_dict={x: X_test_norm})
prediction = tf.convert_to_tensor(predictions)
expectation = tf.cast(Y_test, tf.float32)

MANUAL_MRE = tf.reduce_mean(tf.abs((Y_test - tf.transpose(predictions)) / Y_test))
MAE_op, MAE = tf.metrics.mean_absolute_error(expectation, prediction)
MRE_op, MRE = tf.metrics.mean_relative_error(expectation, prediction, expectation)
tf.local_variables_initializer().run()

print(""MANUAL_MRE: %4f"" % sess.run(MANUAL_MRE))
print(""MRE: %4f"" % sess.run(MRE))
print(""MAE: %4f"" % sess.run(MAE))


This is the output:

MANUAL_MRE:  inf

MRE: 1.603528

MAE: 76.489990


When I run it on a data that has values bigger than 0, my MANUAL_MRE and MRE values are the same like it should be. I checked the documentation of TF and the first case does not make sense. 

Can someone tell me where I am wrong or I just found a bug/ new feature. 
",0,321,,,
tensorflow strange behavior,https://stackoverflow.com/questions/48997844,Tensorflow Estimator.predict_scores not yielding the correct number of predictions when using the Dataset API in the input function,"I am using tensorflow 1.5 and I am puzzled by this strange behavior that I can't explain.
I produced a minimal example:

import tensorflow as tf
import numpy as np


def input_function(x, y, batch_size=128, shuffle=True, n_epochs=None):
    data_set = tf.data.Dataset.from_tensor_slices({""x"": x, ""y"": y})
    if shuffle:
        data_set = data_set.shuffle(buffer_size=1024, seed=None, reshuffle_each_iteration=True)
    data_set = data_set.batch(batch_size)
    data_set = data_set.repeat(n_epochs)
    iterator = data_set.make_one_shot_iterator()
    example = iterator.get_next()
    return {""features"": example[""x""]}, example[""y""]


def main():
    n_samples = 256
    n_features = 16
    n_labels = 1

    x = np.random.rand(n_samples, n_features).astype(np.float32)
    y = np.random.rand(n_samples, n_labels).astype(np.float32)

    feature_column = tf.contrib.layers.real_valued_column(column_name='features', dimension=n_features)
    estimator = tf.contrib.learn.DNNRegressor([10], [feature_column], optimizer=tf.train.AdamOptimizer())

    estimator.fit(input_fn=lambda: input_function(x, y, batch_size=128, shuffle=True, n_epochs=32))
    pred = estimator.predict_scores(input_fn=lambda: input_function(x, y, batch_size=16, shuffle=False, n_epochs=1))
    print(""len(pred) = {} (should be {})"".format(len(list(pred)), n_samples))


if __name__ == '__main__':
    main()


In this example, the call to 'fit' seems to be working fine (I'm not sure though) but the call to 'predict_scores' only produces batch_size (=16) predictions instead of n_samples (=256). What I am doing wrong ?
This problem disappears if I use the tf.esimator.inputs.numpy_input_fn although eventually I'll have to use an input function that uses a TFRecordDataset to read a large amount of training data from tfrecord files, similarly to what is showed here:
https://www.tensorflow.org/programmers_guide/datasets#using_high-level_apis
Any help would be really appreciated.
",0,319,,,
tensorflow strange behavior,https://stackoverflow.com/questions/53363060,keras unable to call model.predict_classes for multiple times,"def predictOne(imgPath):

    model = load_model(""withImageMagic.h5"")
    image = read_image(imgPath)
    test_sample = preprocess(image)
    predicted_class = model.predict_classes(([test_sample]))
    return predicted_class


I have already trained a model. In this function, I load my model, read a new image, do some preprocessing and finally predict its label. 

When I run my main.py file, this function is called and everything goes smoothly. However, after a couple of seconds, this function will be called again with another image and I get this error:


'Cannot interpret feed_dict key as Tensor: ' + e.args[0])

  
  TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(""Placeholder:0"", shape=(5, 5, 1, 32), dtype=float32) is not an element of this graph.


It's very strange that the function only works the first time. I tested multiple images and got the same behavior.

Windows 10 - tensorflow-gpu with keras
",0,1226,"Try loading model from file outside the function, and give the model object as argument to the function def predictOne(imgPath, model). This will also be much faster, since the weights don't need to be loaded from disk every time a prediction is needed. 

If you want to keep loading model inside the function, import the backend:

from keras import backend as K


and then 

K.clear_session() 


before loading the model.
","class one_model:
    session = None
    graph = None 
    loadModel = None
    __instance = None
    @staticmethod
    def getInstance(modelPath):
        """""" Static access method. """"""
        if one_model.__instance == None:
            one_model.__instance = one_model(modelPath)
        return one_model.__instance
        
    def __init__(self, modelPath):
        self.modelPath = modelPath
        self.session = tf.Session(graph=tf.Graph())
        self.loadOneModel()
            
    def loadOneModel(self):
        try:
            with self.session.graph.as_default():
                K.set_session(self.session)
                self.loadModel = keras.models.load_model(self.modelPath)               
        except Exception as e:
            logging.error(str(e))
            print(str(e))
                        
    def getPredictionOne(self, input_file_path): 
        #Predict the data once the model is loaded
        if self.loadModel is not None and self.session is not None: 
            try:
                image = load_img(input_file_path, target_size=inputShape)
                image = img_to_array(image)
                image = np.expand_dims(image, axis=0)
                image = preprocess(image)
                with self.session.graph.as_default():
                    K.set_session(self.session)
                    preds = self.loadModel.predict(image)
                    return preds
            except Exception as e:
                logging.error(str(e))
        
        return -1


if __name__== ""__main__"": 
    #First Model 
    data = web.input()
        fileapth = data.imagefilepath  
        modelfilepath = data.modelfilepath
        one_modelObj = one_model.getInstance(modelfilepath)        
        value = one_modelObj.getPredictionOne(fileapth) 

",
tensorflow strange behavior,https://stackoverflow.com/questions/54152947,Tensorflow Object detection: Trained model does not predict ALL instances of the same object in the image,"I am using Tensorflow Object Detection API to train my custom dataset that contains 2 classes. The network that I use is Faster RCNN + ResNet101 and I utilize the pre-trained model to initialize weights. The training set consists of total 1200 images with class 1 containing 20,000 instances and class 2 containing 6,200 instances. The training seems to work fine - the loss converges well and the mAP on the validation set is ~88%.
However, the prediction results are weird. For example: in an image, if an instance of one class appears multiple times (looking exactly same), the trained model does not predict all the instances in that image. It predicts only a few of them. What can be the reason for such strange behavior?

Due to the imbalance of class instances in the dataset, I augmented class 2 instances so that the number of instances is equal. Even when training with the balanced data, the prediction issue still remains.

I have also tried several combinations of hyperparameters. That doesn't solve the issue either.
",0,523,"The visualization module draws at most 20 boxes on one image by default, you can increase that number by passing max_boxes_to_draw to the visualization function.

If that is not the issue, have you tried increasing the number of proposals? It appears that you have ~20 objects per image on average which is more that the average number of objects in images of, say, COCO dataset.
",,
tensorflow strange behavior,https://stackoverflow.com/questions/56700022,tensorflow-hub: Python import error when file is in a specific folder,"So I ran into this super strange issue. I am trying to run the example code of tensorflow-hub given on this webpage: https://www.tensorflow.org/hub
(I removed both !pip statements)

I want to use a small piece of python code for the machine learning stuff within my application which is otherwise written in go. However when I execute the example code in the project directory of my application it gives an import error.

So when I create a file tf-hub.py in ~/ and execute it, it runs, prints some warnings and the result:

$python3 tf-hub.py
WARNING: Logging before flag parsing goes to stderr.
W0621 10:35:21.367268 140170246772224 deprecation_wrapper.py:118] From tf-hub.py:4: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

2019-06-21 10:35:21.407732: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-06-21 10:35:21.427007: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2793545000 Hz
2019-06-21 10:35:21.428085: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563567fc7a90 executing computations on platform Host. Devices:
2019-06-21 10:35:21.428143: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-06-21 10:35:21.505990: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1541] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-06-21 10:35:21.513612: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 498570752 exceeds 10% of system memory.
2019-06-21 10:35:22.109499: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 498570752 exceeds 10% of system memory.
(3, 128)



However when I copy the same file to the project folder of the go project I get the following import error:

$ python3 go/src/MyProject/tagger/imageClassifier/tf-hub.py 
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/tf_v1.py"", line 29, in &lt;module&gt;
    from tensorflow.compat.v1 import *  # pylint: disable=wildcard-import
ModuleNotFoundError: No module named 'tensorflow.compat'; 'tensorflow' is not a package

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""go/src/EmbeddingImageTagger/tagger/imageClassifier/tf-hub.py"", line 1, in &lt;module&gt;
    import tensorflow as tf
  File ""/home/***/go/src/EmbeddingImageTagger/tagger/imageClassifier/tensorflow.py"", line 2, in &lt;module&gt;
    import tensorflow_hub as hub
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/__init__.py"", line 30, in &lt;module&gt;
    from tensorflow_hub.estimator import LatestModuleExporter
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/estimator.py"", line 25, in &lt;module&gt;
    from tensorflow_hub import tf_utils
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/tf_utils.py"", line 28, in &lt;module&gt;
    from tensorflow_hub import tf_v1
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_hub/tf_v1.py"", line 33, in &lt;module&gt;
    from tensorflow import add_to_collection
ImportError: cannot import name 'add_to_collection' from 'tensorflow' (/home/***/go/src/MyProject/tagger/imageClassifier/tensorflow.py)


I have to use tf-nightly, otherwise I run into this issue: https://github.com/tensorflow/hub/issues/289

I am using:


Fedora 29 64-bit
Python 3.7.2
tf-nightly 1.14.1.dev20190621
tensorflow-hub 0.4.0


Any ideas what could cause this weird behavior that the file runs in one directory, but not in another? I also tried to run both from the same terminal. Still it does not work.
",0,1675,"Oh my. So I found the culprit after I cleaned up the debug stuff and moved my development to my home folder.

The problem was that I had a python file called tensorflow.py because it was supposed to contain a classifier solution with tensorflow. Well it seems that this confuses python and python now imports this file when importing tensorflow.

I had to just rename the file and now it works. 
",,
tensorflow strange result,https://stackoverflow.com/questions/52343989,Non-deterministic behavior of TensorFlow while_loop(),,33,1255,,,
tensorflow strange result,https://stackoverflow.com/questions/40879967,How to use Batch Normalization correctly in tensorflow?,,19,28109,,,
tensorflow strange result,https://stackoverflow.com/questions/42364629,Tensorflow - casting from int to float strange behavior,,17,6203,"Since I see that this is still getting some attention, I should mention that the newer versions of tensorflow do not show this behavior, I suggest working with tensorflow version 1.13 or higher
",,
tensorflow strange result,https://stackoverflow.com/questions/65023353,Difference between keras.metrics.Accuracy() and &quot;accuracy&quot;,"I have been testing different approaches in building nn models (tensorflow, keras) and I saw that there was something strange with metric during compile model.
I checked two ways:
    model.compile(
        loss=keras.losses.CategoricalCrossentropy(),
        optimizer=keras.optimizers.Adam(),
        metrics=keras.metrics.Accuracy()
    )

and
    model.compile(
        loss=keras.losses.CategoricalCrossentropy(),
        optimizer=keras.optimizers.Adam(),
        metrics=[""accuracy""]
    )


Result of first approach:
    Epoch 1/2
    1875/1875 - 2s - loss: 0.0494 - accuracy: 0.0020
    Epoch 2/2
    1875/1875 - 2s - loss: 0.0401 - accuracy: 0.0030

    &lt;tensorflow.python.keras.callbacks.History at 0x7f9c00bc06d8&gt;

Result of second approach:
    Epoch 1/2
    1875/1875 - 2s - loss: 0.0368 - accuracy: 0.9884
    Epoch 2/2
    1875/1875 - 2s - loss: 0.0303 - accuracy: 0.9913

    &lt;tensorflow.python.keras.callbacks.History at 0x7f9bfd7d35c0&gt;

This is quite strange, I thought that ""accuracy"" is exactly the same as keras.metrics.Accuracy().
At least this is the case in arguments ""loss"" and ""optimizer"", e.g. ""adam"" is the same as keras.optimizers.Adam().
Does anybody know why is this so weird or I missed something?
Edit:
Approach with metric in [] gives strange results too:
    model.compile(
        loss=keras.losses.CategoricalCrossentropy(),
        optimizer=keras.optimizers.Adam(),
        metrics=[keras.metrics.Accuracy()]
    )

    Epoch 1/2
    1875/1875 - 2s - loss: 0.2996 - accuracy: 0.0000e+00
    Epoch 2/2
    1875/1875 - 2s - loss: 0.1431 - accuracy: 1.8333e-05

    &lt;tensorflow.python.keras.callbacks.History at 0x7f9bfd1045f8&gt;


",16,7318,"When you are mentioning keras.metrics.Accuracy() you are explicitly asking the library to calculate the metric Accuracy which is simple comparison between how many target values matches the predicted values.
However, when you mention the string accuracy then depending on the type of loss you have chosen a different Metric gets selected. This is what is mentioned in the documentation of Keras,
When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy, tf.keras.metrics.CategoricalAccuracy, tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well.
Hence, as CategoricalCrossEntropy is the loss so CategoricalAccuracy gets calculated in case 2. This is based on finding argmax and then comparing the one-hot encoding. As a result, you are seeing better accuracy values in case 2 and very bad in case 1.
So the string accuracy will not always mean the metric function Accuracy().
The explanation of different metrics for reference, https://keras.io/api/metrics/accuracy_metrics/
The explanation of argument metrics for reference,
https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile
","It is because you have to specify your metrics into a list. Try this :
model.compile(
        loss=keras.losses.CategoricalCrossentropy(),
        optimizer=keras.optimizers.Adam(),
        metrics=[keras.metrics.Accuracy()]
    )

and you should get the same results.
Edit :
keras.metrics.Accuracy() calculates the accuracy between the equality of the predition and the ground truth (see doc). In your case, you want to calculate the accuracy of the match in the correct class. So you should use keras.metrics.BinaryAccuracy()or keras.metrics.CategroicalAccuracy() according to your problem.
",
tensorflow strange result,https://stackoverflow.com/questions/45193744,Keras + Tensorflow strange results,,8,965,,,
tensorflow strange result,https://stackoverflow.com/questions/34500052,tensorflow: saving and restoring session,,8,12999,,,
tensorflow strange result,https://stackoverflow.com/questions/34289131,Basic neural network in TensorFlow,,8,3631,,,
tensorflow strange result,https://stackoverflow.com/questions/59486265,Multi GPU training slower than single GPU on Tensorflow,"I have created 3 virtual GPU's (have 1 GPU) and try to speedup vectorization on images. However, using provided below code with manual placement from off docs (here) I got strange results: training on all GPU two times slower than on a single one. Also check this code(and remove virtual device initialization) on machine with 3 physical GPU's - work the same.

Environment: Python 3.6, Ubuntu 18.04.3, tensorflow-gpu 1.14.0. 

Code(this example create 3 virtual devices and you could test it on a PC with one GPU):

import os
import time
import numpy as np
import tensorflow as tf

start = time.time()

def load_graph(frozen_graph_filename):
    # We load the protobuf file from the disk and parse it to retrieve the
    # unserialized graph_def
    with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    # Then, we import the graph_def into a new Graph and returns it
    with tf.Graph().as_default() as graph:
        # The name var will prefix every op/nodes in your graph
        # Since we load everything in a new graph, this is not needed
        tf.import_graph_def(graph_def, name="""")
    return graph

path_to_graph = '/imagenet/'  # Path to imagenet folder where graph file is placed
GRAPH = load_graph(os.path.join(path_to_graph, 'classify_image_graph_def.pb'))

# Create Session
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.9
config.gpu_options.allow_growth = True
session = tf.Session(graph=GRAPH, config=config)

output_dir = '/vectors/'  # where to saved vectors from images

# Single GPU vectorization
for image_index, image in enumerate(selected_list):
    with Image.open(image) as f:
        image_data = f.convert('RGB')
        feature_tensor = session.graph.get_tensor_by_name('pool_3:0')
        feature_vector = session.run(feature_tensor, {'DecodeJpeg:0': image_data})
        feature_vector = np.squeeze(feature_vector)
        outfile_name = os.path.basename(image) + "".vc""
        out_path = os.path.join(output_dir, outfile_name)
        # Save vector
        np.savetxt(out_path, feature_vector, delimiter=',')

print(f""Single GPU: {time.time() - start}"")
start = time.time()

print(""Start calculation on multiple GPU"")
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  # Create 3 virtual GPUs with 1GB memory each
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],
        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),
         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),
         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    print(len(gpus), ""Physical GPU,"", len(logical_gpus), ""Logical GPUs"")
  except RuntimeError as e:
    # Virtual devices must be set before GPUs have been initialized
    print(e)

print(""Create prepared ops"")
start1 = time.time()
gpus = logical_gpus  # comment this line to use physical GPU devices for calculations

image_list = ['1.jpg', '2.jpg', '3.jpg']  # list with images to vectorize (tested on 100 and 1000 examples)
# Assign chunk of list to each GPU
# image_list1, image_list2, image_list3 = image_list[:len(image_list)],\
#                                         image_list[len(image_list):2*len(image_list)],\
#                                         image_list[2*len(image_list):]
selected_list = image_list # commit this line if you want to try to assign chunk of list manually to each GPU
output_vectors = []
if gpus:
  # Replicate your computation on multiple GPUs
  feature_vectors = []
  for gpu in gpus:  # iterating on a virtual GPU devices, not physical
    with tf.device(gpu.name):
      print(f""Assign list of images to {gpu.name.split(':', 4)[-1]}"")
      # Try to assign chunk of list with images to each GPU - work the same time as single GPU
      # if gpu.name.split(':', 4)[-1] == ""GPU:0"":
      #     selected_list = image_list1
      # if gpu.name.split(':', 4)[-1] == ""GPU:1"":
      #     selected_list = image_list2
      # if gpu.name.split(':', 4)[-1] == ""GPU:2"":
      #     selected_list = image_list3
      for image_index, image in enumerate(selected_list):
          with Image.open(image) as f:
            image_data = f.convert('RGB')
            feature_tensor = session.graph.get_tensor_by_name('pool_3:0')
            feature_vector = session.run(feature_tensor, {'DecodeJpeg:0': image_data})
            feature_vectors.append(feature_vector)

print(""All images has been assigned to GPU's"")
print(f""Time spend on prep ops: {time.time() - start1}"")
print(""Start calculation on multiple GPU"")
start1 = time.time()
for image_index, image in enumerate(image_list):
  feature_vector = np.squeeze(feature_vectors[image_index])
  outfile_name = os.path.basename(image) + "".vc""
  out_path = os.path.join(output_dir, outfile_name)
  # Save vector
  np.savetxt(out_path, feature_vector, delimiter=',')

# Close session
session.close()
print(f""Calc on GPU's spend: {time.time() - start1}"")
print(f""All time, spend on multiple GPU: {time.time() - start}"")


Provide view of output(from list with 100 images):

1 Physical GPU, 3 Logical GPUs
Single GPU: 18.76301646232605
Start calculation on multiple GPU
Create prepared ops
Assign list of images to GPU:0
Assign list of images to GPU:1
Assign list of images to GPU:2
All images has been assigned to GPU's
Time spend on prep ops: 18.263537883758545
Start calculation on multiple GPU
Calc on GPU's spend: 11.697082042694092
All time, spend on multiple GPU: 29.960679531097412


What I tried: split list with images into 3 chunks and assign each chunk to GPU(see commited lines of code). This reduce multiGPU time to 17 seconds, which a little bit faster than single GPU run 18 seconds (~5%).

Expected results: MultiGPU version is faster than singleGPU version (at least 1.5x speedup).

Ideas, why it maybe happens: I wrote calculation in a wrong way
",7,2724,"There are two basic misunderstandings that are causing your trouble:

with tf.device(...): applies to the graph nodes created within the scope, not Session.run calls.

Session.run is a blocking call. They don't run in parallel. TensorFlow can only parallelize the contents of a single Session.run.


Modern TF (&gt;= 2.0) can make this much easier.
Mainly you can stop using tf.Session and tf.Graph. Use @tf.function instead, I believe this basic structure will work:
@tf.function
def my_function(inputs, gpus, model):
  results = []
  for input, gpu in zip(inputs, gpus):
    with tf.device(gpu):
      results.append(model(input))    
  return results

But you will want to try a more realistic test. With just 3 images you're not at all measuring real performance.
Also note:

The tf.distribute.Strategy class may help simplify some of this, by separating the device specification from the @tf.function that's being run. strategy.experimental_run_v2(my_function, args=(dataset_inputs,))
Using tf.data.Dataset input pipelines will help you overlap loading/preprocessing with model execution.

But if you're really intent on doing this using tf.Graph and tf.Session I think you basically need to reorganize your code from this:
# Your code:
# Builds a graph
graph = build_graph()

for gpu in gpus():
  with tf.device(gpu):
    # Calls `gpu` in each device scope.
    session.run(...)

To this:
g = tf.Graph()
with g.as_default():
  results = []
  for gpu in gpus:
    # Build the graph, on each device
    input = iterator.get_next()
    with tf.device(gpu):    
      results.append(my_function(input))       

# Use a single `Session.run` call
np_result = session.run(results, feed_dict={inputs: my_inputs})

",,
tensorflow strange result,https://stackoverflow.com/questions/42029046,Tensorflow batch loss spikes when restoring model for training from saved checkpoint?,,6,1860,"I have also seen this issue in PyTorch, resulting from restoring model weights but not optimizer parameters. For example, the Adam optimizer stores estimates of the first and second moments of the gradient. If you only reload the model and don't save/reload the optimizer, the momentum parameters are reinitialized with the optimizer, which often causes a spike in the loss.
",,
tensorflow strange result,https://stackoverflow.com/questions/64324685,Why my PCA is not invariant to rotation and axis swap?,"I have a voxel (np.array) with size 3x3x3, filled with some values, this setup is essential for me. I want to have rotation-invariant representation of it. For this case, I decided to try PCA representation which is believed to be invariant to orthogonal transformations. another
For simplicity, I took some axes swap, but in case I'm mistaken there can be np.rot90.
I have interpereted my 3d voxels as a set of weighted 3d cube point vectors which I incorrectly called ""basis"", total 27 (so that is some set of 3d point in space, represented by the vectors, obtained from cube points, scaled by voxel values).
import numpy as np

voxel1 = np.random.normal(size=(3,3,3))
voxel2 =  np.transpose(voxel1, (1,0,2)) #np.rot90(voxel1) #


basis = []
for i in range(3):
    for j in range(3):
        for k in range(3):
            basis.append([i+1, j+1, k+1]) # avoid 0
basis = np.array(basis)


voxel1 = voxel1.reshape((27,1))
voxel2 = voxel2.reshape((27,1))

voxel1 = voxel1*basis # weighted basis vectors
voxel2 = voxel2*basis

print(voxel1.shape)
(27, 3)

Then I did PCA to those 27 3-dimensional vectors:
def pca(x):
    center = np.mean(x, 0)
    x = x - center

    cov = np.cov(x.T) / x.shape[0]

    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)

    v = e_vectors[:, order].transpose()

    return x.dot(v)

vp1 = pca(voxel1)
vp2 = pca(voxel2)

But the results in vp1 and vp2 are different. Perhaps, I have a mistake (though I beleive this is the right formula), and the proper code must be
x.dot(v.T)
But in this case the results are very strange. The upper and bottom blocks of the transofrmed data are the same up to the sign:
&gt;&gt;&gt; np.abs(np.abs(vp1)-np.abs(vp2)) &gt; 0.01
array([[False, False, False],
       [False, False, False],
       [False, False, False],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True, False,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [False, False, False],
       [False, False, False],
       [False, False, False],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True, False,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True],
       [False, False, False],
       [False, False, False],
       [False, False, False]])

What I'm doing wrong?
What I want to do is to find some invariant representation of my weighted voxel, something like positioning according to the axes of inertia or principal axes. I would really appreciate if someone helps me.
UPD: Found the question similar to mine, but code is unavailable
EDIT2: Found the code InertiaRotate and managed to monkey-do the following:
import numpy as np

# https://github.com/smparker/orient-molecule/blob/master/orient.py

voxel1 = np.random.normal(size=(3,3,3))
voxel2 =  np.transpose(voxel1, (1,0,2))

voxel1 = voxel1.reshape((27,))
voxel2 = voxel2.reshape((27,))


basis = []
for i in range(3):
    for j in range(3):
        for k in range(3):
            basis.append([i+1, j+1, k+1]) # avoid 0
basis = np.array(basis)
basis = basis - np.mean(basis, axis=0)



def rotate_func(data, mass):

    #mass = [ masses[n.lower()] for n in geom.names ]

    inertial_tensor = -np.einsum(""ax,a,ay-&gt;xy"", data, mass, data)
    # negate sign to reverse the sorting of the tensor
    eig, axes = np.linalg.eigh(-inertial_tensor)
    axes = axes.T

    # adjust sign of axes so third moment moment is positive new in X, and Y axes
    testcoords = np.dot(data, axes.T) # a little wasteful, but fine for now
    thirdmoment = np.einsum(""ax,a-&gt;x"", testcoords**3, mass)

    for i in range(2):
        if thirdmoment[i] &lt; 1.0e-6:
            axes[i,:] *= -1.0

    # rotation matrix must have determinant of 1
    if np.linalg.det(axes) &lt; 0.0:
        axes[2,:] *= -1.0

    return axes

axes1 = rotate_func(basis, voxel1)
v1 = np.dot(basis, axes1.T)
axes2 = rotate_func(basis, voxel2)
v2 = np.dot(basis, axes2.T)


print(v1)
print(v2)

It seems to use basis (coordinates) and mass separately. The results are quite similar to my problem above: some parts of the transformed data match up to the sign, I believe those are some cube sides
print(np.abs(np.abs(v1)-np.abs(v2)) &gt; 0.01)
[[False False False]
 [False False False]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [False False False]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [ True  True  True]
 [False False False]
 [False False False]
 [False False False]]


Looking for some explanation. This code is designed for molecules, and must work...
UPD: Tried to choose 3 vectors as a new basis from those 24 - the one with biggest norm, the one with the smallest and their cross product. Combined them into the matrix V, then used the formula V^(-1)*X to transform coordinates, and got the same problem - the resulting sets of vectors are not equal for rotated voxels.

UPD2: I agree with meTchaikovsky that my idea of multiplying voxel vectors by weights and thus creating some non-cubic point cloud was incorrect. Probably, we indeed need to take the solution for rotated ""basis""(yes, this is not a basis, but rather a way to determine point cloud) which will work later when ""basis"" is the same, but the weights are rotated according to the 3D rotation.
Based on the answer and the reference provided by meTchaikovsky, and finding other answers we together with my friend came to conclusion that rotate_func from molecular package mentioned above tries to invent some convention for computing the signs of the components. Their solution tries to use 3rd moment for the first 2 axes and determinant for the last axis (?). We tried a bit another approach and succeeded to have half of the representations matching:
# -*- coding: utf-8 -*-
""""""
Created on Fri Oct 16 11:40:30 2020

@author: Dima
""""""


import numpy as np
from numpy.random import randn
from numpy import linalg as la
from scipy.spatial.transform import Rotation as R
np.random.seed(10)

rotate_mat = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])

def pca(feat, x):
    # pca with attemt to create convention on sign changes
    
    x_c =x- np.mean(x,axis=0)
    x_f= feat*x
    x_f-= np.mean(x_f, axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)[::-1]
    #print(order)
    v = e_vectors[:,order]
    v= v/np.sign(v[0,:])
    if(la.det(v)&lt;0):
        v= -v
    return x_c @ v

def standardize(x):
    # take vector with biggest norm, with smallest and thir cross product as basis
    x -= np.mean(x,axis=0)
    nrms= la.norm(x, axis=1)
    imin= argmin(nrms)
    imax= argmax(nrms)
    vec1= x[imin, :]
    vec2= x[imax, :]
    vec3= np.cross(vec1, vec2)
    Smat= np.stack([vec1, vec2, vec3], axis=0)
    if(la.det(Smat)&lt;0):
        Smat= -Smat
    return(la.inv(Smat)@x.T)

    

angles = np.linspace(0.0,90.0,91)
voxel1 = np.random.normal(size=(3,3,3))    
res = []
for angle in angles:

    
    voxel2 = voxel1.copy()
    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)
    
    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    basis1 = basis1+1e-4*randn(27,3) # perturbation
    basis2 = basis1 @rotate_mat(np.deg2rad(angle))
    #voxel1 = voxel1*basis1
    #voxel2 = voxel2*basis2

    #print(angle,(np.abs(pca(voxel1) - pca(voxel2) )))
    #gg= np.abs(standardize(basis1) - standardize(basis2) )
    gg= np.abs(pca(voxel1, basis1) - pca(voxel1, basis2) )
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4) 
           
    print(angle,ss,  bl)
    #res.append(np.all(np.abs(pca(voxel1) - pca(voxel2) &lt; 1e-6)))
    del basis1, basis2

The results are good up to 58 degree angle (yet we're still experimenting with rotation of x, y axes). After that we have constant difference which indicates some uncounted sign reverse. This is better than the less consistent result of rotate_func:
0.0 0.0 True
1.0 1.1103280567106161e-13 True
2.0 5.150139890290964e-14 True
3.0 8.977126225544196e-14 True
4.0 5.57341699240722e-14 True
5.0 4.205149954378956e-14 True
6.0 3.7435437643664957e-14 True
7.0 1.2943967187158123e-13 True
8.0 5.400185371573149e-14 True
9.0 8.006410204958181e-14 True
10.0 7.777189536904011e-14 True
11.0 5.992073021576436e-14 True
12.0 6.3716122222085e-14 True
13.0 1.0120048110065158e-13 True
14.0 1.4193029076233626e-13 True
15.0 5.32774440341853e-14 True
16.0 4.056702432878251e-14 True
17.0 6.52062429116855e-14 True
18.0 1.3237663595853556e-13 True
19.0 8.950259695710006e-14 True
20.0 1.3795067925438317e-13 True
21.0 7.498727794307339e-14 True
22.0 8.570866862371226e-14 True
23.0 8.961510590826412e-14 True
24.0 1.1839169916779899e-13 True
25.0 1.422193407555868e-13 True
26.0 6.578778015788652e-14 True
27.0 1.0042963537887101e-13 True
28.0 8.438153062569065e-14 True
29.0 1.1299103064863272e-13 True
30.0 8.192453876745831e-14 True
31.0 1.2618492405483406e-13 True
32.0 4.9237819394886296e-14 True
33.0 1.0971028569666842e-13 True
34.0 1.332138304559801e-13 True
35.0 5.280024600049296e-14 True

From the code above, you can see that we tried to use another basis: vector with the biggest norm, vector with the smallest and their cross product. Here we should have only two variants (direction of the cross product) which could be later fixed, but I couldn't manage this alternative solution to work.
I hope that someone can help me finish this and obtain rotation-invariant representation for voxels.

EDIT 3. Thank you very much meTchaikovsky, but the situation is still unclear. My problem initially lies in processing 3d voxels which are (3,3,3) numpy arrays. We reached the conclusion that for finding invariant representation, we just need to fix 3d voxel as weights used for calculating cov matrix, and apply rotations on the centered ""basis"" (some vectors used for describing point cloud).
Therefore, when we achieved invariance to ""basis"" rotations, the problem should have been solved: now, when we fix ""basis"" and use rotated voxel, the result must be invariant. Surprisingly, this is not so. Here I check 24 rotations of the cube with basis2=basis1 (except small perturbation):
import scipy.ndimage

def pca(feat, x):

    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]

    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_c @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign

    return proj

def rotate_3d(image1, alpha, beta, gamma):
    # z
    # The rotation angle in degrees.
    image2 = scipy.ndimage.rotate(image1, alpha, mode='nearest', axes=(0, 1), reshape=False)

    # rotate along y-axis
    image3 = scipy.ndimage.rotate(image2, beta, mode='nearest', axes=(0, 2), reshape=False)

    # rotate along x-axis
    image4 = scipy.ndimage.rotate(image3, gamma, mode='nearest', axes=(1, 2), reshape=False)
    return image4



voxel10 = np.random.normal(size=(3,3,3))

angles = [[x,y,z] for x in [-90,0,90] for y in [-90,0,90] for z in [-90,0,90]]
res = []
for angle in angles:

    voxel2 = rotate_3d(voxel10, angle[0], angle[1], angle[2])
    voxel1 = voxel10.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)

    basis1 += 1e-4*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1
    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel2, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))


difference before pca 0.000, difference after pca 45.738 False
difference before pca 0.000, difference after pca 12.157 False
difference before pca 0.000, difference after pca 26.257 False
difference before pca 0.000, difference after pca 37.128 False
difference before pca 0.000, difference after pca 52.131 False
difference before pca 0.000, difference after pca 45.436 False
difference before pca 0.000, difference after pca 42.226 False
difference before pca 0.000, difference after pca 18.959 False
difference before pca 0.000, difference after pca 38.888 False
difference before pca 0.000, difference after pca 12.157 False
difference before pca 0.000, difference after pca 26.257 False
difference before pca 0.000, difference after pca 50.613 False
difference before pca 0.000, difference after pca 52.132 False
difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 52.299 False

Here basis1=basis2 (hence basis difference before pca=0), and you can see 0 for (0,0,0) rotation. But rotated voxels give different result. In case scipy does something wrong, I've checked the approach with numpy.rot90 with the same result:
rot90 = np.rot90

def rotations24(polycube):
    # imagine shape is pointing in axis 0 (up)

    # 4 rotations about axis 0
    yield from rotations4(polycube, 0)

    # rotate 180 about axis 1, now shape is pointing down in axis 0
    # 4 rotations about axis 0
    yield from rotations4(rot90(polycube, 2, axis=1), 0)

    # rotate 90 or 270 about axis 1, now shape is pointing in axis 2
    # 8 rotations about axis 2
    yield from rotations4(rot90(polycube, axis=1), 2)
    yield from rotations4(rot90(polycube, -1, axis=1), 2)

    # rotate about axis 2, now shape is pointing in axis 1
    # 8 rotations about axis 1
    yield from rotations4(rot90(polycube, axis=2), 1)
    yield from rotations4(rot90(polycube, -1, axis=2), 1)

def rotations4(polycube, axis):
    """"""List the four rotations of the given cube about the given axis.""""""
    for i in range(4):
        yield rot90(polycube, i, axis)



def rot90(m, k=1, axis=2):
    """"""Rotate an array k*90 degrees in the counter-clockwise direction around the given axis""""""
    m = np.swapaxes(m, 2, axis)
    m = np.rot90(m, k)
    m = np.swapaxes(m, 2, axis)
    return m


voxel10 = np.random.normal(size=(3,3,3))

gen = rotations24(voxel10)

res = []
for voxel2 in gen:

    #voxel2 = rotate_3d(voxel10, angle[0], angle[1], angle[2])
    voxel1 = voxel10.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)

    basis1 += 1e-4*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1

    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel2, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

I tried to investigate this case, and the only perhaps irrelevant thing I found the following:
voxel1 = np.ones((3,3,3))
voxel1[0,0,0] = 0 # if I change 0 to 0.5 it stops working at all

# mirrored around diagonal
voxel2 = np.ones((3,3,3))
voxel2[2,2,2] = 0

for angle in range(1):

    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1) 

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)

    basis1 = basis1 + 1e-4 * randn(27,3) # perturbation
    basis2 = basis1

# If perturbation is used we have 

# difference before pca 0.000, difference after pca 0.000 True
# correct for 100.0 percent of time

# eigenvalues for both voxels
# [1.03417495 0.69231107 0.69235402]
# [0.99995368 0.69231107 0.69235402]


# If no perturbation applied for basis, difference is present

# difference before pca 0.000, difference after pca 55.218 False
# correct for 0.0 percent of time

# eignevalues for both voxels (always have 1.):
# [0.69230769 1.03418803 0.69230769]
# [1.         0.69230769 0.69230769]




Currently don't know how to proceed from there.

EDIT4:
I'm currently thinking that there is some problem with voxel rotations transformed into basis coefficients via voxel.reshape()
Simple experiment with creating array of indices
indices = np.arange(27)
indices3d = indices.reshape((3,3,3))
voxel10 = np.random.normal(size=(3,3,3))
assert voxel10[0,1,2] == voxel10.ravel()[indices3d[0,1,2]]

And then using it for rotations
gen = rotations24(indices3d)

res = []
for ind2 in gen:

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    voxel1 = voxel10.copy().reshape(27,1) #np.array([voxel10[i,j,k] for k in range(3) for j in range(3) for i in range(3)])[...,np.newaxis]

    voxel2 = voxel1[ind2.reshape(27,)]

    basis1 += 1e-4*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1[ind2.reshape(27,)]

    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel2, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

Shows that those rotations are not correct, because on my opinion rotated voxel and basis should match:
difference before pca 0.000, difference after pca 0.000 True
difference before pca 48.006, difference after pca 87.459 False
difference before pca 72.004, difference after pca 70.644 False
difference before pca 48.003, difference after pca 71.930 False
difference before pca 72.004, difference after pca 79.409 False
difference before pca 84.005, difference after pca 36.177 False


EDIT 5: Okaaay, so here we go at least for 24 rotations. At first, we had a slight change of logic lurked into our pca function. Here we center x_c (basis) and forget about it, further centering x_f (features*basis) and transforming it with pca. This does not work perhaps because our basis is not centered and multiplication by features further increased the bias. If we center x_c first, and multiply it by features, everything will be Ok. Also, previously we had proj = x_c @ v with v computed from x_f which was totally wrong in this case, as x_f and x_c were centered around different centers.
def pca(feat, x):
    
    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    
    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    
    
    return proj

Secondly, as we already found, we need to sort vectors obtained by pca, for example by the first column:
    basis2 = basis1

    original_diff = np.sum(np.abs(basis1-basis2))

    a = pca(voxel1, basis1)
    t1 = a[a[:,0].argsort()]

    a = pca(voxel2, basis2)
    t2 = a[a[:,0].argsort()]

    gg= np.abs(t1-t2)

And the last thing we also discovered already, is that simple reshape is wrong for voxel, it must correspond to rotation:
voxel2 = voxel1[ind2.reshape(27,)] #np.take(voxel10, ind2).reshape(27,1).
One more important comment to understand the solution. When we perform PCA on the 3d vectors (point cloud, defined by our basis) with weights assigned (analogously to the inertia of the rigid body), the actual assignment of the weights to the points is sort of external information, which becomes hard-defined for the algorithm. When we rotated basis by applying rotation matrices, we did not change the order of the vectors in the array, hence the order of the mass assignments wasn't changed too. When we start to rotate voxel, we change the order of the masses, so in general PCA algorithm will not work without the same transformation applied to the basis. So, only if we have some array of 3d vectors, transformed by some rotation AND the list of masses re-arranged accordingly, we can detect the rotation of the rigid body using PCA. Otherwise, if we detach masses from points, that would be another body in general.
So how does it work for us then? It works because our points are fully symmetric around the center after centering basis. In this case reassignment of the masses does not change ""the body"" because vector norms are the same. In this case we can use the same (numerically) basis2=basis1 for testing 24 rotations and rotated voxel2 (rotated point cloud cubes match, just masses migrate). This correspond to the rotation of the point cloud with mass points around the center of the cube. PCA will transform vectors with the same lengths and different masses in the same way according to the body's ""inertia"" then (after we reached convention on the signs of the components). The only thing left is to sort the pca transformed vectors in the end, because they have different position in the array (because our body was rotated, mass points changed their positions). This makes us lose some information related to the order of the vectors but it looks inevitable.
Here is the code which checks the solution for 24 rotations. If should theoretically work in the general case as well, giving some closer values for more complicated objects rotated inside a bigger voxel:
import numpy as np
from numpy.random import randn

#np.random.seed(20)

def pca(feat, x):
    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x_c
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign
    return proj


# must be correct https://stackoverflow.com/questions/15230179/how-to-get-the-linear-index-for-a-numpy-array-sub2ind
indices = np.arange(27)
indices3d = indices.reshape((3,3,3))
voxel10 = np.random.normal(size=(3,3,3))
assert voxel10[0,1,2] == voxel10.ravel()[indices3d[0,1,2]]

rot90 = np.rot90

def rotations24(polycube):
    # imagine shape is pointing in axis 0 (up)

    # 4 rotations about axis 0
    yield from rotations4(polycube, 0)

    # rotate 180 about axis 1, now shape is pointing down in axis 0
    # 4 rotations about axis 0
    yield from rotations4(rot90(polycube, 2, axis=1), 0)

    # rotate 90 or 270 about axis 1, now shape is pointing in axis 2
    # 8 rotations about axis 2
    yield from rotations4(rot90(polycube, axis=1), 2)
    yield from rotations4(rot90(polycube, -1, axis=1), 2)

    # rotate about axis 2, now shape is pointing in axis 1
    # 8 rotations about axis 1
    yield from rotations4(rot90(polycube, axis=2), 1)
    yield from rotations4(rot90(polycube, -1, axis=2), 1)

def rotations4(polycube, axis):
    """"""List the four rotations of the given cube about the given axis.""""""
    for i in range(4):
        yield rot90(polycube, i, axis)



def rot90(m, k=1, axis=2):
    """"""Rotate an array k*90 degrees in the counter-clockwise direction around the given axis""""""
    m = np.swapaxes(m, 2, axis)
    m = np.rot90(m, k)
    m = np.swapaxes(m, 2, axis)
    return m


gen = rotations24(indices3d)

res = []

for ind2 in gen:

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    voxel1 = voxel10.copy().reshape(27,1)

    voxel2 = voxel1[ind2.reshape(27,)] #np.take(voxel10, ind2).reshape(27,1)

    basis1 += 1e-6*np.random.normal(size=(27, 1)) # perturbation
    basis2 = basis1

    original_diff = np.sum(np.abs(basis1-basis2))
    a = pca(voxel1, basis1)
    t1 = a[a[:,0].argsort()]
    a = pca(voxel2, basis2)
    t2 = a[a[:,0].argsort()]
    gg= np.abs(t1-t2)
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 0.000 True
difference before pca 0.000, difference after pca 0.000 True


PS. I want to propose better ordering theme to take into account zero values in the voxel which might confuse previous approach when entire first column of PCA vectors is zero, etc. I propose to sort by vector norms, multiplied by the sign of the sum of elements. Here is tensorflow 2 code:

def infer_shape(x):
    x = tf.convert_to_tensor(x)

    # If unknown rank, return dynamic shape
    if x.shape.dims is None:
        return tf.shape(x)

    static_shape = x.shape.as_list()
    dynamic_shape = tf.shape(x)

    ret = []
    for i in range(len(static_shape)):
        dim = static_shape[i]
        if dim is None:
            dim = dynamic_shape[i]
        ret.append(dim)

    return ret

def merge_last_two_dims(tensor):
    shape = infer_shape(tensor)
    shape[-2] *= shape[-1]
    #shape.pop(1)
    shape = shape[:-1]
    return tf.reshape(tensor, shape)


def pca(inpt_voxel):
        patches = tf.extract_volume_patches(inpt_voxel, ksizes=[1,3,3,3,1], strides=[1, 1,1,1, 1], padding=""VALID"")
        features0 = patches[...,tf.newaxis]*basis
        # centered basises
        basis1_ = tf.ones(shape=tf.shape(patches[...,tf.newaxis]), dtype=tf.float32)*basis
        basis1 = basis1_ - tf.math.divide_no_nan(tf.reduce_sum(features0, axis=-2), tf.reduce_sum(patches, axis=-1)[...,None])[:,:,:,:,None,:]
        features = patches[...,tf.newaxis]*basis1
        features_centered_basis = features - tf.reduce_mean(features, axis=-2)[:,:,:,:,None,:]
        x = features_centered_basis
        m = tf.cast(x.get_shape()[-2], tf.float32)
        cov = tf.matmul(x,x,transpose_a=True)/(m - 1)
        e,v = tf.linalg.eigh(cov,name=""eigh"")
        proj = tf.matmul(x,v,transpose_b=False)
        asign = tf.sign(proj)
        max_ind = tf.argmax(tf.abs(proj),axis=-2)[:,:,:,:,None,:]
        sign = tf.gather(asign,indices=max_ind, batch_dims=4, axis=-2)
        sign = tf.linalg.diag_part(sign)
        proj = proj * sign
        # But we can have 1st coordinate zero. In this case,
        # other coordinates become ambiguous
        #s = tf.argsort(proj[...,0], axis=-1)
        # sort by l2 vector norms, multiplied by signs of sums
        sum_signs = tf.sign(tf.reduce_sum(proj, axis=-1))
        norms = tf.norm(proj, axis=-1)
        s = tf.argsort(sum_signs*norms, axis=-1)
        proj = tf.gather(proj, s, batch_dims=4, axis=-2)
        return merge_last_two_dims(proj)

",6,931,"Firstly, your pca function is not correct, it should be
def pca(x):
    
    x -= np.mean(x,axis=0)
    cov = np.cov(x.T)
    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    
    return x @ v

You shouldn't transpose the e_vectors[:,order] because we want each column of the v array is an eigenvector, therefore, x @ v will be projections of x on those eigenvectors.
Secondly, I think you misunderstand the meaning of rotation. It is not voxel1 that should be rotated, but the basis1. If you rotate (by taking transposition) voxel1, what you really do is to rearrange the indices of grid points, while the coordinates of the points basis1 are not changed.
In order to rotate the points (around the z axis for example), you can first define a function to calculate the rotation matrix given an angle
rotate_mat = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])

with the rotation matrix generated by this function, you can rotate the array basis1 to create another array basis2
basis2 = basis1 @ rotate_mat(np.deg2rad(angle))

Now it comes to the title of your question ""Why my PCA is not invariant to rotation and axis swap?"", from this post, the PCA result is not unique, you can actually run a test to see this
import numpy as np

np.random.seed(10)

rotate_mat = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])

def pca(x):
    
    x -= np.mean(x,axis=0)
    cov = np.cov(x.T)
    e_values, e_vectors = np.linalg.eig(cov)

    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    return x @ v


angles = np.linspace(0,90,91)
    
res = []
for angle in angles:

    voxel1 = np.random.normal(size=(3,3,3))
    voxel2 = voxel1.copy()
    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)
    
    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)])
    # basis2 = np.hstack((-basis1[:,1][:,None],basis1[:,0][:,None],-basis1[:,2][:,None]))
    basis2 = basis1 @ rotate_mat(np.deg2rad(angle))
    voxel1 = voxel1*basis1
    voxel2 = voxel2*basis2

    print(angle,np.all(np.abs(pca(voxel1) - pca(voxel2) &lt; 1e-6)))
    res.append(np.all(np.abs(pca(voxel1) - pca(voxel2) &lt; 1e-6)))
    
print()
print(np.sum(res) / len(angles))

After you run this script, you will see that in only 21% of times the two PCA results are the same.

UPDATE
I think instead of focusing on the eigenvectors of the principal components, you can instead focus on the projections. For two clouds of points, even though they are essentially the same, the eigenvectors can be drastically different. Therefore, hardcoding in order to somehow let the two sets of eigenvectors to be the same is a very difficult task.
However, based on this post, for the same cloud of points, two sets of eigenvectors can be different only up to a minus sign. Therefore, the projections upon the two sets of eigenvectors are also different only up to a minus sign. This actually offers us an elegant solution, for the projections along an eigenvector (principal axis), all we need to do is to switch the sign of the projections so that the projection with the largest absolute value along that principal axis is positive.
import numpy as np
from numpy.random import randn

#np.random.seed(20)

rotmat_z = lambda theta: np.array([[np.cos(theta),-np.sin(theta),0.],[np.sin(theta),np.cos(theta),0.],[0.,0.,1.]])
rotmat_y = lambda theta: np.array([[np.cos(theta),0.,np.sin(theta)],[0.,1.,0.],[-np.sin(theta),0.,np.cos(theta)]])
rotmat_x = lambda theta: np.array([[1.,0.,0.],[0.,np.cos(theta),-np.sin(theta)],[0.,np.sin(theta),np.cos(theta)]])
# based on https://en.wikipedia.org/wiki/Rotation_matrix
rot_mat = lambda alpha,beta,gamma: rotmat_z(alpha) @ rotmat_y(beta) @ rotmat_x(gamma)

deg2rad = lambda alpha,beta,gamma: [np.deg2rad(alpha),np.deg2rad(beta),np.deg2rad(gamma)]

def pca(feat, x):
    
    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]
    
    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign
    
    return proj

ref_angles = np.linspace(0.0,90.0,10)
angles = [[alpha,beta,gamma] for alpha in ref_angles for beta in ref_angles for gamma in ref_angles]


voxel1 = np.random.normal(size=(3,3,3))
res = []
for angle in angles:

    voxel2 = voxel1.copy()
    voxel1 = voxel1.reshape(27,1)
    voxel2 = voxel2.reshape(27,1)

    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    basis1 = basis1 + 1e-4 * randn(27,3) # perturbation
    basis2 = basis1 @ rot_mat(*deg2rad(*angle))
   
    original_diff = np.sum(np.abs(basis1-basis2))
    gg= np.abs(pca(voxel1, basis1) - pca(voxel1, basis2))
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference before pca %.3f,' % original_diff, 'difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

As you can see by running this script, the projections on the principal axis are the same, this means we have resolved the issue of PCA results being not unique.

Reply to EDIT 3
As for the new issue you raised, I think you missed an important point, it is the projections of the cloud of points onto the principal axes that are invariant, not anything else. Therefore, if you rotate voxel1 and obtain voxel2, they are the same in the sense that their own respective projections onto the principal axes of the cloud of points are the same, it actually does not make too much sense to compare pca(voxel1,basis1) with pca(voxel2,basis1).
Furthermore, the method rotate of scipy.ndimage actually changes information, as you can see by running this script
image1 = np.linspace(1,100,100).reshape(10,10)
image2 = scipy.ndimage.rotate(image1, 45, mode='nearest', axes=(0, 1), reshape=False)
image3 = scipy.ndimage.rotate(image2, -45, mode='nearest', axes=(0, 1), reshape=False)

fig,ax = plt.subplots(nrows=1,ncols=3,figsize=(12,4))
ax[0].imshow(image1)
ax[1].imshow(image2)
ax[2].imshow(image3)

The output image is
As you can see the matrix after rotation is not the same as the original one, some information of the original matrix is changed.


Reply to EDIT 4
Actually, we are almost there, the two pca results are different because we are comparing pca components for different points.
indices = np.arange(27)
indices3d = indices.reshape((3,3,3))
# apply rotations to the indices, it is not weights yet
gen = rotations24(indices3d)

# construct the weights
voxel10 = np.random.normal(size=(3,3,3))

res = []
count = 0
for ind2 in gen:
    count += 1
    # ind2 is the array of indices after rotation
    # reindex the weights with the indices after rotation 
    voxel1 = voxel10.copy().reshape(27,1) 
    voxel2 = voxel1[ind2.reshape(27,)]

    # basis1 is the array of coordinates where the points are
    basis1 = np.array([[i+1,j+1,k+1] for k in range(3) for j in range(3) for i in range(3)]).astype(np.double)
    basis1 += 1e-4*np.random.normal(size=(27, 1))
    # reindex the coordinates with the indices after rotation
    basis2 = basis1[ind2.reshape(27,)]

    # add a slight modification to pca, return the axes also 
    pca1,v1 = pca(voxel1,basis1)
    pca2,v2 = pca(voxel2,basis2)
    # sort the principal components before comparing them 
    pca1 = np.sort(pca1,axis=0)
    pca2 = np.sort(pca2,axis=0)
    
    gg= np.abs(pca1 - pca2)
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference after pca %.3f' % ss,bl)
    res.append(bl)
    
    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

Running this script, you will find, for each rotation, the two sets of principal axes are different only up to a minus sign. The two sets of pca results are different because the indices of the cloud of points before and after rotation are different (since you apply rotation to the indices). If you sort the pca results before comparing them, you will find the two pca results are exactly the same.

Summary
The answer to this question can be divided into two parts. In the first part, the rotation is applied to the basis (the coordinates of points), while the indices and the corresponding weights are unchanged. In the second part, the rotation is applied to the indices, then the weights and the basis are rearranged with the new indices. For both of the two parts, the solution pca function is the same
def pca(feat, x):

    # pca with attemt to create convention on sign changes
    x_c = x - np.mean(x,axis=0)
    x_f = feat * x
    x_f -= np.mean(x_f,axis=0)
    cov = np.cov(x_f.T)
    e_values, e_vectors = np.linalg.eig(cov)
    order = np.argsort(e_values)[::-1]
    v = e_vectors[:,order]

    # here is the solution, we switch the sign of the projections
    # so that the projection with the largest absolute value along a principal axis is positive
    proj = x_f @ v
    asign = np.sign(proj)
    max_ind = np.argmax(np.abs(proj),axis=0)[None,:]
    sign = np.take_along_axis(asign,max_ind,axis=0)
    proj = proj * sign

    return proj

The idea of this function is, instead of matching the principal axes, we can match the principal components since it is the principal components that are rotationally invariant after all.
Based on this function pca, the first part of this answer is easy to understand, since the indices of the points are unchanged while we only rotate the basis. In order to understand the second part of this answer (Reply to EDIT 5), we must first understand the function rotations24. This function rotates the indices rather than the coordinates of the points, therefore, if we stay at the same position observing the points, we will feel that the positions of the points are changed.

With this in mind, it is not hard to understand Reply to EDIT 5.
Actually, the function pca in this answer can be applied to more general cases, for example (we rotate the indices)
num_of_points_per_dim = 10
num_of_points = num_of_points_per_dim ** 3

indices = np.arange(num_of_points)
indices3d = indices.reshape((num_of_points_per_dim,num_of_points_per_dim,num_of_points_per_dim))
voxel10 = 100*np.random.normal(size=(num_of_points_per_dim,num_of_points_per_dim,num_of_points_per_dim))

gen = rotations24(indices3d)

res = []
for ind2 in gen:

    voxel1 = voxel10.copy().reshape(num_of_points,1)
    voxel2 = voxel1[ind2.reshape(num_of_points,)]

    basis1 = 100*np.random.rand(num_of_points,3)
    basis2 = basis1[ind2.reshape(num_of_points,)]

    pc1 = np.sort(pca(voxel1, basis1),axis=0)
    pc2 = np.sort(pca(voxel2, basis2),axis=0)
    
    gg= np.abs(pc1-pc2)
    ss= np.sum(np.ravel(gg))
    bl= np.all(gg&lt;1e-4)
    print('difference after pca %.3f' % ss,bl)
    res.append(bl)

    del basis1, basis2

print('correct for %.1f percent of time' % (100*(np.sum(res) / len(res))))

",,
tensorflow strange result,https://stackoverflow.com/questions/51676447,Python: Use PIL to load png file gives strange results,,6,4798,,,
tensorflow strange result,https://stackoverflow.com/questions/40350839,Tensorflow unhashable type &#39;list&#39; in sess.run,,6,8202,,,
tensorflow strange result,https://stackoverflow.com/questions/37956242,Saving a collection of variable length tensors to a TFRecords file in TensorFlow,,6,2649,,,
tensorflow strange result,https://stackoverflow.com/questions/50938992,TensorFlow lite: High loss in accuracy after converting model to tflite,"I have been trying TFLite to increase detection speed on Android but strangely my .tflite model now almost only detects 1 category.

I have done testing on the .pb model that I got after retraining a mobilenet and the results are good but for some reason, when I convert it to .tflite the detection is way off...

For the retraining I used the retrain.py file from Tensorflow for poets 2

I am using the following commands to retrain, optimize for inference and convert the model to tflite:

python retrain.py \
--image_dir ~/tf_files/tw/ \
--tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/feature_vector/1 \
--output_graph ~/new_training_dir/retrainedGraph.pb \
-saved_model_dir ~/new_training_dir/model/ \
--how_many_training_steps 500 

sudo toco \
--input_file=retrainedGraph.pb \
--output_file=optimized_retrainedGraph.pb \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TENSORFLOW_GRAPHDEF \
--input_shape=1,224,224,3 \
--input_array=Placeholder \
--output_array=final_result \

sudo toco \
--input_file=optimized_retrainedGraph.pb \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--output_file=retrainedGraph.tflite \
--inference_type=FLOAT \
--inference_input_type=FLOAT \
--input_arrays=Placeholder \
--output_array=final_result \
--input_shapes=1,224,224,3


Am I doing anything wrong here? Where could the loss in accuracy come from?
",5,3565,"I faced the same issue while I was trying to convert a .pb model into .lite.

In fact, my accuracy would come down from 95 to 30!

Turns out the mistake I was committing was not during the conversion of .pb to .lite or in the command involved to do so. But it was actually while loading the image and pre-processing it before it is passed into the lite model and inferred using 

interpreter.invoke()


command.

The below code you see is what I meant by pre-processing:

test_image=cv2.imread(file_name)
test_image=cv2.resize(test_image,(299,299),cv2.INTER_AREA)
test_image = np.expand_dims((test_image)/255, axis=0).astype(np.float32)
interpreter.set_tensor(input_tensor_index, test_image)
interpreter.invoke()
digit = np.argmax(output()[0])
#print(digit)
prediction=result[digit]


As you can see there are two crucial commands/pre-processing done on the image once it is read using ""imread()"":

i) The image should be resized to the size that is the ""input_height"" and ""input_width"" values of the input image/tensor that was used during the training. In my case (inception-v3) this was 299 for both ""input_height"" and ""input_width"". (Read the documentation of the model for this value or look for this variable in the file that you used to train or retrain the model)

ii) The next command in the above code is:

test_image = np.expand_dims((test_image)/255, axis=0).astype(np.float32)


I got this from the ""formulae""/model code:

test_image = np.expand_dims((test_image-input_mean)/input_std, axis=0).astype(np.float32)


Reading the documentation revealed that for my architecture input_mean = 0 and input_std = 255.

When I did the said changes to my code, I got the accuracy that was expected (90%).

Hope this helps.
",,
tensorflow strange result,https://stackoverflow.com/questions/50047653,Set &quot;training=False&quot; of &quot;tf.layers.batch_normalization&quot; when training will get a better validation result,"I use TensorFlow to train DNN. I learned that Batch Normalization is very helpful for DNN , so I  used it in DNN.

I use ""tf.layers.batch_normalization"" and follow the instructions of the API document to build the network: when training, set its parameter ""training=True"", and when validate, set ""training=False"". And add tf.get_collection(tf.GraphKeys.UPDATE_OPS).

Here is my code:

# -*- coding: utf-8 -*-
import tensorflow as tf
import numpy as np

input_node_num=257*7
output_node_num=257

tf_X = tf.placeholder(tf.float32,[None,input_node_num])
tf_Y = tf.placeholder(tf.float32,[None,output_node_num])
dropout_rate=tf.placeholder(tf.float32)
flag_training=tf.placeholder(tf.bool)
hid_node_num=2048

h1=tf.contrib.layers.fully_connected(tf_X, hid_node_num, activation_fn=None)
h1_2=tf.nn.relu(tf.layers.batch_normalization(h1,training=flag_training))
h1_3=tf.nn.dropout(h1_2,dropout_rate)

h2=tf.contrib.layers.fully_connected(h1_3, hid_node_num, activation_fn=None)
h2_2=tf.nn.relu(tf.layers.batch_normalization(h2,training=flag_training))
h2_3=tf.nn.dropout(h2_2,dropout_rate)

h3=tf.contrib.layers.fully_connected(h2_3, hid_node_num, activation_fn=None)
h3_2=tf.nn.relu(tf.layers.batch_normalization(h3,training=flag_training))
h3_3=tf.nn.dropout(h3_2,dropout_rate)

tf_Y_pre=tf.contrib.layers.fully_connected(h3_3, output_node_num, activation_fn=None)

loss=tf.reduce_mean(tf.square(tf_Y-tf_Y_pre))

update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for i1 in range(3000*num_batch):
        train_feature=... # Some processing
        train_label=...  # Some processing
        sess.run(train_step,feed_dict={tf_X:train_feature,tf_Y:train_label,flag_training:True,dropout_rate:1}) # when train , set ""training=True"" , when validate ,set ""training=False"" , get a bad result . However when train , set ""training=False"" ,when validate ,set ""training=False"" , get a better result .

        if((i1+1)%277200==0):# print validate loss every 0.1 epoch
            validate_feature=... # Some processing
            validate_label=... # Some processing

            validate_loss = sess.run(loss,feed_dict={tf_X:validate_feature,tf_Y:validate_label,flag_training:False,dropout_rate:1})
            print(validate_loss)


Is there any error in my code ?
if my code is right , I think I get a strange result:

when training, I set ""training = True"", when validate, set ""training = False"", the result is not good . I print validate loss every 0.1 epoch , the validate loss in 1st to 3st epoch is 

 0.929624
 0.992692
 0.814033
 0.858562
 1.042705
 0.665418
 0.753507
 0.700503
 0.508338
 0.761886
 0.787044
 0.817034
 0.726586
 0.901634
 0.633383
 0.783920
 0.528140
 0.847496
 0.804937
 0.828761
 0.802314
 0.855557
 0.702335
 0.764318
 0.776465
 0.719034
 0.678497
 0.596230
 0.739280
 0.970555


However , when I change the code ""sess.run(train_step,feed_dict={tf_X:train_feature,tf_Y:train_label,flag_training:True,dropout_rate:1})"" , that : set ""training=False"" when training, set ""training=False"" when validate . The result is good . The validate loss in 1st epoch is

 0.474313
 0.391002
 0.369357
 0.366732
 0.383477
 0.346027
 0.336518
 0.368153
 0.330749
 0.322070
 0.335551


Why does this result appear ? Is it necessary to set ""training=True"" when training, set ""training=False"" when validate ?
",5,15004,,,
tensorflow strange result,https://stackoverflow.com/questions/42473052,TensorFlow 1.0 does not see GPU on Windows (but Theano does),,5,1457,,,
tensorflow strange result,https://stackoverflow.com/questions/39032277,Number recognition from Image with python and tensorflow,,5,5934,,,
tensorflow strange result,https://stackoverflow.com/questions/53658826,Why operations placed on GPU are also executed on CPU (tensorflow)?,,4,1128,,,
tensorflow strange result,https://stackoverflow.com/questions/64645579,Keras: custom data validation callback on training data always returns validation data results,"I am working on an autoencoder in Keras that contains some dropout layers. To evaluate bias and variance, I'd like to compare the losses of training and test data. However, since dropout is used during training, the losses cannot be compared. (See here for an explanation of why the training data results can be worse than test data results.)
In order to get training data losses that are not influenced by the dropout, I wrote a callback to validate some additional data set (in this case, it would be the training data again).
The strange thing is that I ALWAYS get the same results as for the validation data. Here's a minimal example:
from pprint import pprint

import keras
import numpy as np
import pandas as pd
from numpy.random import seed as np_seed
from tensorflow.random import set_seed as tf_seed

np_seed(1)
tf_seed(2)

# Generation of data sets for training and testing. Random data is only used to showcase the problem. 
df_train = pd.DataFrame(data=np.random.random((1000, 10))) # This will be used for training
df_test_1 = pd.DataFrame(data=np.random.random((1000, 10))) # This will be used as validation data set directly
df_test_2 = pd.DataFrame(data=np.random.random((1000, 10))) # This will be used within the callback

np_seed(1)
tf_seed(2)

model = keras.models.Sequential(
    [
        keras.Input(shape=(10, )),
        keras.layers.Dropout(rate=0.01),
        keras.layers.Dense(5, activation='relu'),
        keras.layers.Dropout(rate=0.01),
        keras.layers.Dense(10, activation='linear'),
    ]
)

model.compile(
    loss='mean_squared_error',
    optimizer=keras.optimizers.Adam(),
)


class CustomDataValidation(keras.callbacks.Callback):
    def __init__(self, x=None, y=None):
        self.x = x
        self.y = y

    def on_epoch_end(self, epoch, logs=None):
        result = self.model.evaluate(x=self.x, y=self.y, return_dict=True)

        for loss_name, loss_value in result.items():
            logs[""custom_"" + loss_name] = loss_value


cdv = CustomDataValidation(df_test_2, df_test_2)

hist = model.fit(df_train, df_train, validation_data=(df_test_1, df_test_1), epochs=2, validation_split=0.1, callbacks=[cdv])
pprint(hist.history)

The output is
Epoch 1/2
4/4 [==============================] - 0s 1ms/step - loss: 0.7625
29/29 [==============================] - 0s 5ms/step - loss: 0.9666 - val_loss: 0.7625
Epoch 2/2
4/4 [==============================] - 0s 1ms/step - loss: 0.5331
29/29 [==============================] - 0s 2ms/step - loss: 0.6638 - val_loss: 0.5331
{'custom_loss': [0.7624925374984741, 0.5331208109855652],
 'loss': [0.9665887951850891, 0.6637843251228333],
 'val_loss': [0.7624925374984741, 0.5331208109855652]}

'custom_loss' and 'val_loss' are equal although they should be based on totally different data sets.
The question is therefore: How can I evaluate the model performance on custom data within a callback?
Edit: Since I did not yet got an answer on stackoverflow, I created an issue at tensorflow's git repo. Also, there's now a notebook available that shows the problem.
",4,1136,"It seems that this is a bug in tensorflow versions 2.3.x (tested with 2.3.0 and 2.3.1). In versions 2.4.0-rc0 and 2.2.1, the loss outputs of loss and custom_loss differ, which is the expected behavior:
{'custom_loss': [0.7694963216781616, 0.541864812374115],
'loss': [0.9665887951850891, 0.6637843251228333],
'val_loss': [0.7624925374984741, 0.5331208109855652]}

",,
tensorflow strange result,https://stackoverflow.com/questions/53968334,Is there any similar funtion as list.append() in tensorflow?,,4,3345,,,
tensorflow strange result,https://stackoverflow.com/questions/44676248,Is this a bug in tensorflow?,,4,319,,,
tensorflow strange result,https://stackoverflow.com/questions/39817949,Setting up a MLP for binary classification with tensorflow,,4,2013,,,
tensorflow strange result,https://stackoverflow.com/questions/61510865,Tensorflow has no Attribute &quot;sort&quot; in GPT 2 Git Release?,"I downloaded the git repo (https://github.com/openai/gpt-2) and followed the python3 instructions (in DEVELOPERS.MD) for installation on my Kubuntu 18.04LTS box, but I cannot run it and instead get an error.

Here is what I've done so far:

pip3 install tensorflow==1.12.0
pip3 install -r requirements.txt
python3 download_model.py 124M
python3 download_model.py 355M
python3 download_model.py 774M
python3 download_model.py 1558M
export PYTHONIOENCODING=UTF-8


I then ran:

sarah@LesserArk:~/Custom Programs/gpt-2$ python3 src/interactive_conditional_samples.py 
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2020-04-29 16:08:30.016586: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""src/interactive_conditional_samples.py"", line 91, in &lt;module&gt;
    fire.Fire(interact_model)
  File ""/home/sarah/.local/lib/python3.6/site-packages/fire/core.py"", line 138, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/sarah/.local/lib/python3.6/site-packages/fire/core.py"", line 468, in _Fire
    target=component.__name__)
  File ""/home/sarah/.local/lib/python3.6/site-packages/fire/core.py"", line 672, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""src/interactive_conditional_samples.py"", line 65, in interact_model
    temperature=temperature, top_k=top_k, top_p=top_p
  File ""/home/sarah/Custom Programs/gpt-2/src/sample.py"", line 74, in sample_sequence
    past, prev, output = body(None, context, context)
  File ""/home/sarah/Custom Programs/gpt-2/src/sample.py"", line 66, in body
    logits = top_p_logits(logits, p=top_p)
  File ""/home/sarah/Custom Programs/gpt-2/src/sample.py"", line 28, in top_p_logits
    sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)
AttributeError: module 'tensorflow' has no attribute 'sort'


Which culminates in the error: AttributeError: module 'tensorflow' has no attribute 'sort'.

This is strange, and I'm not sure how to proceed. I would have thought that the instructions would lead to successful installation, but it appears that they don't.

Uninstalling and reinstalling has no effect on the final result? How can I get tensorflow to execute GPT-II?
",3,1701,"I was able to get the sample to work using:
pip3 install tensorflow==1.13.1
Additionally, there is a response to the GitHub issue with a commit showing changing Dockerfile.cpu to building FROM tensorflow/tensorflow:1.15.0-py3 fixes the docker image issue.
","pip install tesorflow==1.14.0
after installing this my problem get solve. try it
",
tensorflow strange result,https://stackoverflow.com/questions/60251715,Difference between Keras and TensorFlow Hub Version of MobileNetV2,"I am working on a transfer learning approach and got very different results when using the MobileNetV2 from keras.applications and the one available on TensorFlow Hub. This seems strange to me as both versions claim here and here to extract their weights from the same checkpoint mobilenet_v2_1.0_224.
This is how the differences can be reproduced, you can find the Colab Notebook here:

!pip install tensorflow-gpu==2.1.0
import tensorflow as tf
import numpy as np
import tensorflow_hub as hub
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2

def create_model_keras():
  image_input = tf.keras.Input(shape=(224, 224, 3))
  out = MobileNetV2(input_shape=(224, 224, 3),
                  include_top=True)(image_input)
  model = tf.keras.models.Model(inputs=image_input, outputs=out)
  model.compile(optimizer='adam', loss=[""categorical_crossentropy""])
  return model

def create_model_tf():
  image_input = tf.keras.Input(shape=(224, 224 ,3))
  out = hub.KerasLayer(""https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4"",
                      input_shape=(224, 224, 3))(image_input)
  model = tf.keras.models.Model(inputs=image_input, outputs=out)
  model.compile(optimizer='adam', loss=[""categorical_crossentropy""])
  return model


When I try to predict on a random batch, the results are not equal:

keras_model = create_model_keras()
tf_model = create_model_tf()
np.random.seed(42)
data = np.random.rand(32,224,224,3)
out_keras = keras_model.predict_on_batch(data)
out_tf = tf_model.predict_on_batch(data)
np.array_equal(out_keras, out_tf)


The output of the version from keras.applications sums up to 1 but the version from TensorFlow Hub does not. Also the shape of the two versions is different: TensorFlow Hub has 1001 labels, keras.applications has 1000. 

np.sum(out_keras[0]), np.sum(out_tf[0])


prints (1.0000001, -14.166359)

What is the reason for these differences? Am I missing something?

Edit 18.02.2020

As Szymon Maszke pointed out, the TFHub version returns logits. That's why i added a Softmax layer to the create_model_tf as follows:
out = tf.keras.layers.Softmax()(x)

arnoegw mentioned that the TfHub version requires an image normalized to [0,1], whereas the keras version requires normalization to [-1,1]. When I use the following preprocessing on a test image:

from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
img = tf.keras.preprocessing.image.load_img(""/content/panda.jpeg"", target_size=(224,224))
img = tf.keras.preprocessing.image.img_to_array(img)
img = preprocess_input(img)


img = tf.io.read_file(""/content/panda.jpeg"")
img = tf.image.decode_jpeg(img)
img = tf.image.convert_image_dtype(img, tf.float32)
img = tf.image.resize(img, (224,224))


Both correctly predict the same label and the following condition is true: np.allclose(out_keras, out_tf[:,1:], rtol=0.8)

Edit 2 18.02.2020
Before I wrote that it is not possible to convert the formats into each other. This was caused by a bug.
",3,1538,"There are several documented differences:


Like Szymon said, the TF Hub version returns logits (before the softmax function that turns them into probabilities), which is a common practice, because the cross-entropy loss can be computed with greater numerical stability from the logits.
The TF Hub model assumes float32 inputs in the range of [0,1], which is what you get from tf.image.decode_jpeg(...) followed by tf.image.convert_image_dtype(..., tf.float32). The Keras code uses a model-specific range (likely [-1,+1]).
The TF Hub model reflects the original SLIM checkpoint more completely in returning all its 1001 output classes. As stated in the ImageNetLabels.txt linked from the documentation, the added class 0 is ""background"" (aka. ""stuff""). That is what object detection uses to indicate image background as opposed to an object of any known class.

",,
tensorflow strange result,https://stackoverflow.com/questions/55772880,TF 2.0 - Method estimator.model_to_estimator( ) fails but model.fit works for tf.keras created model,"While using the TF 2.0 Alpha and converting a working TF Keras model to TF estimator I encountered a strange error.

The below code has been taken from the Tensorflow without PHD series - RNN time series prediction without any modification.

When I run model.fit( ) on result from model_fn_keras(), it runs absolutely fine but on converting it to TF estimator it fails.

This is also raised on Tensorflow issues board but they seem not to be considering this as a bug - https://github.com/tensorflow/tensorflow/issues/27750

def compile_keras_sequential_model(list_of_layers, msg):

    # a tf.keras.Sequential model is a sequence of layers
    model = tf.keras.Sequential(list_of_layers)

    # keras does not have a pre-defined metric for Root Mean Square Error. Let's define one.
    def rmse(y_true, y_pred): # Root Mean Squared Error
      return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))

    print('\nModel ', msg)

    #Optimizer
    sgd = tf.keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)

    # to finalize the model, specify the loss, the optimizer and metrics
    model.compile(
       loss = 'mean_squared_error',
       optimizer = sgd,
#         optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),
       metrics = [rmse])

    # this prints a description of the model
    model.summary()

    return model
#Create Keras model
def model_fn_keras():

    # RNN model (RMSE: 0.164 after 10 epochs)
    model_layers_RNN = [
        l.Reshape([SEQLEN, 1], input_shape=[SEQLEN,]), # [BATCHSIZE, SEQLEN, 1] is necessary for RNN model
        l.GRU(RNN_CELLSIZE, return_sequences=True),  # output shape [BATCHSIZE, SEQLEN, RNN_CELLSIZE]
        l.GRU(RNN_CELLSIZE), # keep only last output in sequence: output shape [BATCHSIZE, RNN_CELLSIZE]
        l.Dense(1) # output shape [BATCHSIZE, 1]
    ]

    model_RNN = compile_keras_sequential_model(model_layers_RNN, ""RNN"")

    return(model_RNN)

#Convert
estimator = tf.keras.estimator.model_to_estimator(keras_model=model_fn_keras())


Errors out as:

Model  RNN
Model: ""sequential_27""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
reshape_27 (Reshape)         (None, 16, 1)             0         
_________________________________________________________________
unified_gru_57 (UnifiedGRU)  (None, 16, 32)            3360      
_________________________________________________________________
unified_gru_58 (UnifiedGRU)  (None, 32)                6336      
_________________________________________________________________
dense_27 (Dense)             (None, 1)                 33        
=================================================================
Total params: 9,729
Trainable params: 9,729
Non-trainable params: 0
_________________________________________________________________
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-52-05ea50da2f1c&gt; in &lt;module&gt;()
      5 #Convert Keras model to Estimator
      6 # tf.disable_eager_execution()
----&gt; 7 estimator = tf.keras.estimator.model_to_estimator(keras_model=model_fn_keras())
      8 # estimator = model_fn_keras()
      9 

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\keras\estimator\__init__.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config)
     71       custom_objects=custom_objects,
     72       model_dir=model_dir,
---&gt; 73       config=config)
     74 
     75 # LINT.ThenChange(//tensorflow_estimator/python/estimator/keras.py)

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow_estimator\python\estimator\keras.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config)
    488   if keras_model._is_graph_network:
    489     warm_start_path = _save_first_checkpoint(keras_model, custom_objects,
--&gt; 490                                              config)
    491   elif keras_model.built:
    492     logging.warning('You are creating an Estimator from a Keras model manually '

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow_estimator\python\estimator\keras.py in _save_first_checkpoint(keras_model, custom_objects, config)
    365           # pylint: disable=protected-access
    366           model._make_train_function()
--&gt; 367           K._initialize_variables(sess)
    368           # pylint: enable=protected-access
    369         saver = saver_lib.Saver()

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\keras\backend.py in _initialize_variables(session)
    760     # marked as initialized.
    761     is_initialized = session.run(
--&gt; 762         [variables_module.is_variable_initialized(v) for v in candidate_vars])
    763     uninitialized_vars = []
    764     for flag, v in zip(is_initialized, candidate_vars):

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    928     try:
    929       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 930                          run_metadata_ptr)
    931       if run_metadata:
    932         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1151     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1152       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1153                              feed_dict_tensor, options, run_metadata)
   1154     else:
   1155       results = []

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1327     if handle is None:
   1328       return self._do_call(_run_fn, feeds, fetches, targets, options,
-&gt; 1329                            run_metadata)
   1330     else:
   1331       return self._do_call(_prun_fn, handle, feeds, fetches)

c:\users\hrafiq\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1347           pass
   1348       message = error_interpolation.interpolate(message, self._graph)
-&gt; 1349       raise type(e)(node_def, op, message)
   1350 
   1351   def _extend_graph(self):

InvalidArgumentError: Node 'training/SGD/gradients/unified_gru_58/StatefulPartitionedCall_grad/StatefulPartitionedCall': Connecting to invalid output 4 of source node unified_gru_58/StatefulPartitionedCall which has 4 outputs


",3,408,"As discussed here , this is indeed a bug that was caused by the combination of tf.function in keras layer and v1 tf.session which is used by model_to_estimator

Also you can refer to the following link, where the bug has been fixed. Thanks!

If you feel like the answer was helpful, please upvote it. Thanks!
",,
tensorflow strange result,https://stackoverflow.com/questions/52507748,Using multiple CPU cores in TensorFlow,,3,8518,,,
tensorflow strange result,https://stackoverflow.com/questions/49370940,One hot encoding characters,"Is there a possibilty to one-hot encode characters of a text in Tensorflow or Keras?


tf.one_hot seem to take only integers.
tf.keras.preprocessing.text.one_hot seems to one-hot encode sentences
to words, but not to characters.


Beside that, tf.keras.preprocessing.text.one_hot works really strange, since the response does not really seem one-hot encoded, since the following code:

text = ""ab bba bbd""
res = tf.keras.preprocessing.text.one_hot(text=text,n=3)
print(res)


Lead to this result:

[1,2,2]


Every time I run this program, the output is a different 3d vector, sometimes it is [1,1,1] or [2,1,1]. The documentation says, that unicity is not guaranteed, but this seems really senseless to me.
",3,3629,,,
tensorflow strange result,https://stackoverflow.com/questions/46580112,R-squared results of test and validation differ by a huge margin,"I am working on a regression problem with keras and tensorflow using a neural network. The data is split, so that 282774 datasets are for training, 70694 are for validation and 88367 are for testing. To evaluate my models I am printing out the mean squared error (MSE), the mean absolute error (MAE) and the R-squared score. These are some examples from the results I get:

               MSE           MAE             R-squared
Training       1.562072899   0.958128839     0.849787137
Validation     0.687871457   0.62066941      0.935365564
Test           0.683918759   0.618674863   -16.22829222


I do not understand the value for R-squared on test data. I know that R-squared can be negative, but how can it be that there is such a big difference between validation and test if both fall into the category of unseen data. Can someone give me a hint?

Some background information:

Since keras does not have the R-squared metric built in, I implemented it with some code I found on the web and which seems logical for me:

def r2_keras(y_true, y_pred):
    SS_res =  K.sum(K.square(y_true - y_pred)) 
    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) 
    return ( 1 - SS_res/(SS_tot + K.epsilon()) )


And if it helps: this is my model:

model = Sequential()
model.add(Dense(75, input_shape=(7,)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Dense(1, activation='linear'))

adam = optimizers.Adam(lr=0.001)
model.compile(loss='mse',
              optimizer=adam,
              metrics=['mse', 'mae', r2_keras])

history = model.fit(x_train, y_train,
                    epochs=50,
                    batch_size=32,
                    validation_split=0.2)


score = model.evaluate(x_test, y_test, batch_size=32)


One strange thing I noticed is, that not all testing data seems to be considered. The console prints out the following:

86304/88367 [============================&gt;.] - ETA: 0s-----


Maybe this leads to a miscalculation for R-squared?

I am thankful for any help/hint I can get on understanding this issue.

Update:
I checked for outliers, but could not find any significant one. Min and max-values for test and train are close by, considering the standard deviation. Also the histograms look very much alike.

So in the next step I let my model predict the values for test data again and used pandas + numpy to calculate the r2_score. This time I got a value which is approximately equal to the r2_score for validation.

Below is how I did it. Do you see any flaws in the way I performed the calculation? (I just want to be sure that the old r2_score for ""test"" was indeed a calculation error)

# ""test"" is a dataframe with input data and the real outputs
# ""inputs"" is a list of the input column names
# The real/true outputs are contained in the column ""output""
test['output_pred'] = model.predict(x=np.array(test[inputs]))
output_mean = test['output'].mean()    # Is this the correct mean value for r2 here?
test['SSres'] = np.square(test['output']-test['output_pred'])
test['SStot'] = np.square(test['output']-output_mean)
r2 = 1-(test['SSres'].sum()/(test['SStot'].sum()))

",3,4042,"Tensorflow's built-in evaluate method evaluates your test set batch by batch and hence calculates r2 at each batch. The metrics produced from model.evaluate() is then simple average of all r2 from each batch. While in model.fit(), r2 (and all metrics on validation set) are calculated per epoch (instead of per batch and then take avg.)
You may slice your output and output_pred into batches of the same batch size you used in model.evaluate() and calculate r2 on each batch. I guess the model produces high r2 on batches with high total sum of squares (SS_tot) and bad r2 on lower ones. So when taken average, result would be poor (however when calculate r2 on entire dataset, samples with higher ss_tot usually dominate the result).
",,
tensorflow strange result,https://stackoverflow.com/questions/36666358,TensorFlow implementing Seq2seq Sentiment analysis,,3,1668,,,
tensorflow strange result,https://stackoverflow.com/questions/72881142,Measuring incertainty in Bayesian Neural Network,"Hy everybody,
I'm beginning with tensorflow probability and I have some difficulties to interpret my Bayesian neural network outputs.
I'm working on a regression case, and started with the example provided by tensorflow notebook here: https://blog.tensorflow.org/2019/03/regression-with-probabilistic-layers-in.html?hl=fr
As I seek to know the uncertainty of my network predictions, I dived directly into example 4 with Aleatoric &amp; Epistemic Uncertainty. You can find my code bellow:
def negative_loglikelihood(targets, estimated_distribution):
    return -estimated_distribution.log_prob(targets)


def posterior_mean_field(kernel_size, bias_size, dtype=None):
    n = kernel_size + bias_size #number of total paramaeters (Weights and Bias)
    c = np.log(np.expm1(1.)) 
    return tf.keras.Sequential([
        tfp.layers.VariableLayer(2 * n, dtype=dtype, initializer=lambda shape, dtype: random_gaussian_initializer(shape, dtype), trainable=True), 
        tfp.layers.DistributionLambda(lambda t: tfd.Independent(
            # The Normal distribution with location loc and scale parameters.
            tfd.Normal(loc=t[..., :n],
                       scale=1e-5 +0.01*tf.nn.softplus(c + t[..., n:])),
            reinterpreted_batch_ndims=1)),
    ])



def prior(kernel_size, bias_size, dtype=None):
    n = kernel_size + bias_size
    return tf.keras.Sequential([
        tfp.layers.VariableLayer(n, dtype=dtype),
        tfp.layers.DistributionLambda(lambda t: tfd.Independent(
            tfd.Normal(loc=t, scale=1),
            reinterpreted_batch_ndims=1)),
    ])




def build_model(param):
    model = keras.Sequential()
    for i in range(param[""n_layers""] ):
        name=""n_units_l""+str(i)
        num_hidden = param[name]
        model.add(tfp.layers.DenseVariational(units=num_hidden, make_prior_fn=prior,make_posterior_fn=posterior_mean_field,kl_weight=1/len(X_train),activation=""relu""))
    model.add(tfp.layers.DenseVariational(units=2, make_prior_fn=prior,make_posterior_fn=posterior_mean_field,activation=""relu"",kl_weight=1/len(X_train))) 
    model.add(tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[..., :1],scale=1e-3 + tf.math.softplus(0.01 * t[...,1:]))))
    
    lr = param[""learning_rate""]
    optimizer=optimizers.Adam(learning_rate=lr)
        
    model.compile(
        loss=negative_loglikelihood,  #negative_loglikelihood, 
        optimizer=optimizer,
        metrics=[keras.metrics.RootMeanSquaredError()],
    )

    return model


I think I have the same network than in tfp example, I just added few hidden layers with differents units. Also I added 0.01 in front of the Softplus in the posterior as suggested here, which allows the network to come up to good performances.
Not able to get reasonable results from DenseVariational
The performances of the model are very good (less than 1% of error) but I have some questions:

As Bayesian neural networks ""promise"" to mesure the uncertainty of the predictions, I was expecting bigger errors on high variance predictions. I ploted the absolute error versus variance and the results are not good enough on my mind. Of course, the model is better at low variance but I can have really bad predicitions at low variance, and therefore cannot really use standard deviation to filter bad predictions. Why is my Bayesian neural netowrk struggling to give me the uncertainty ?



The previous network was train 2000 epochs and we can notice a strange phenome with a vertical bar on lowest stdv. If I increase the number of epoch up to 25000, my results get better either on training and validation set.


But the phenomene of vertical bar that we may notice on the figure 1 is much more obvious. It seems that as much as I increase the number or EPOCH, all output variance converge to 0.68. Is that a case of overfitting ? Why this value of 0.6931571960449219 and why I can't get lower stdv ? As the phenome start appearing at 2000 EPOCH, am i already overfitting at 2000 epochs ?

At this point stdv is totaly useless. So is there a kind of trade off ? With few epochs my model is less performant but gives me some insigh about uncertainty (even if I think they're not sufficient), where with lot of epochs I have better performances but no more uncertainty informations as all outputs have the same stdv.
Sorry for the long post and the language mistakes.
Thank you in advance for you help and any feed back.
",2,457,"I solved the problem of why my uncertainty could not get lower than 0.6931571960449219.
Actually this value is converging to log(2). This is due to my relu activation function on my last Dense Variational layer.
Indeed, the scale of tfd.Normal is a softplus (tf.math.softplus).
And softplus is implement like that : softplus(x) = log(exp(x) + 1). As my x doesn't go in negative values, my minumum incertainty il log(2).
A basic linear activation function solved the problem and my uncertainty has a normal behavior now.
",,
tensorflow strange result,https://stackoverflow.com/questions/60604007,Tensorflow SDK vs MLKit SDK,"We have a strange problem that we need help solving. 
Here's the scenario: 
We've got an native android app using the Tensorflow SDK and a tflite model that recognises different fish species. This solution has varying results when capturing input from the camera, but when using pictures from the camera roll it works very well and serves us stable probabilities for the different fish species. 

We also have an native iOS app that uses Google's MLKit with the same tensorflow lite model and this performs very well, but on some pictures it totally misses the target. 

If we load the same picture from the camera roll and run it through the interpreter on the android app and the iOS app we get totally different results, but only on SOME species. 

Does anybody have an explanation as to why we can get such different results using the same tflite model in both apps and the same picture? 

Are there differences between the MLKit SDK and the Tensorflow SDK that can cause this kind of behaviour?

Best regards, 
Christian
",2,1421,"There could be a couple of reasons. 


TFLite runtime versions. ML Kit custom model also uses TFLite under the hood, and you could find the TFLite runtime version in your Pod dependencies. Make sure you are using the same TFLite versions in the test.
Image processing. The images you send to ML Kit SDK or TFLite SDK are always much larger than the model inputs, so resizing and even image rotations will be performed before running the model. Different platform may use different libraries for the image processing, and the results may have slightly difference. But the slight different may cause the model output different results. It could due to the model is a bit unstable for SOME species. You may try to send the exact same image with the model expected image format/size/orientation, so that image processing is not triggered in both platforms and see whether it helps. 

",,
tensorflow strange result,https://stackoverflow.com/questions/58569584,Val loss behaves strange while using custom training loop in tensorflow 2.0,"I'm using a VGG16 model written in tf2.0 to train on my own datasets. Some BatchNormalization layers were included in the model and the ""training"" argument were set to True during training time and False during validation time as described in many tutorials.
The train_loss decreased to a certain level during training as expected.  However, the val_loss behaves really strange. I checked out the output of the model after training and found out that, if I set the training argument to True, the output is quite correct, but if I set it to False, the result is incorrect at all.
According to the tutorials in tensorflow website, when training is set to False , the model will normalize its inputs using the mean and variance of its moving statistics learned during training but it doesn't seem so. Am I missing something?

I've provided the trainning and validation code in the below.

def train():
    logging.basicConfig(level=logging.INFO)
    tdataset = tf.data.Dataset.from_tensor_slices((train_img_list[:200], train_label_list[:200]))
    tdataset = tdataset.map(parse_function, 3).shuffle(buffer_size=200).batch(batch_size).repeat(repeat_times)
    vdataset = tf.data.Dataset.from_tensor_slices((val_img_list[:100], val_label_list[:100]))
    vdataset = vdataset.map(parse_function, 3).batch(batch_size)

    ### Vgg model
    model = VGG_PR(num_classes=num_label)

    logging.info('Model loaded')

    start_epoch = 0
    latest_ckpt = tf.train.latest_checkpoint(os.path.dirname(ckpt_path))
    if latest_ckpt:
        start_epoch = int(latest_ckpt.split('-')[1].split('.')[0])
        model.load_weights(latest_ckpt)
        logging.info('model resumed from: {}, start at epoch: {}'.format(latest_ckpt, start_epoch))
    else:
        logging.info('training from scratch since weights no there')

    ######## training loop ########
    loss_object = tf.keras.losses.MeanSquaredError()
    val_loss_object = tf.keras.losses.MeanSquaredError()
    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)
    train_loss = tf.metrics.Mean(name='train_loss') 
    val_loss = tf.metrics.Mean(name='val_loss')
    writer = tf.summary.create_file_writer(log_path.format(case_num))

    with writer.as_default():
        for epoch in range(start_epoch, total_epoch):
            print('start training')
            try:
                for batch, data in enumerate(tdataset):
                    images, labels = data
                    with tf.GradientTape() as tape:
                        pred = model(images, training=True)
                        if len(pred.shape) == 2:
                            pred = tf.reshape(pred,[-1, 1, 1, num_label])
                        loss = loss_object(pred, labels)
                    gradients = tape.gradient(loss, model.trainable_variables)
                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
                    if batch % 20 ==0:
                        logging.info('Epoch: {}, iter: {}, loss:{}'.format(epoch, batch, loss.numpy()))
                    tf.summary.scalar('train_loss', loss.numpy(), step=epoch*1250*repeat_times+batch)      # the tdataset has been repeated 5 times..
                    tf.summary.text('Zernike_coe_pred', tf.as_string(tf.squeeze(pred)), step=epoch*1250*repeat_times+batch)
                    tf.summary.text('Zernike_coe_gt', tf.as_string(tf.squeeze(labels)), step=epoch*1250*repeat_times+batch)

                    writer.flush()
                    train_loss(loss)
                model.save_weights(ckpt_path.format(epoch=epoch))
            except KeyboardInterrupt:
                logging.info('interrupted.')
                model.save_weights(ckpt_path.format(epoch=epoch))
                logging.info('model saved into {}'.format(ckpt_path.format(epoch=epoch)))
                exit(0)
            # validation step
            for batch, data in enumerate(vdataset):
                images, labels = data
                val_pred = model(images, training=False)
                if len(val_pred.shape) == 2:
                    val_pred = tf.reshape(val_pred,[-1, 1, 1, num_label])
                v_loss = val_loss_object(val_pred, labels)
                val_loss(v_loss)
            logging.info('Epoch: {}, average train_loss:{}, val_loss: {}'.format(epoch, train_loss.result(), val_loss.result()))
            tf.summary.scalar('val_loss', val_loss.result(), step = epoch)
            writer.flush()
            train_loss.reset_states()
            val_loss.reset_states()
        model.save_weights(ckpt_path.format(epoch=epoch))


The train losss reduced to a very small value like the groundtruth label are in the range of [0, 1] and the average train loss can be 0.007, but the val loss is much higher than this. The output of the model tends to be close to 0 if I set training to False.

updated on Nov. 6th:
I have found an interesting thing that if I use tf.function to decorate my model in its call method, the val loss will turn to be correct, but I'm not sure what has happened?
",2,656,"Mentioning the Answer for the benefit of the community.

Issue is resolved, i.e., val loss will turn to be correct if tf.function is used to decorate the model in its call method.
",,
tensorflow strange result,https://stackoverflow.com/questions/56721498,How can I use the Keras.applications&#39; ResNeXt in TensorFlow&#39;s eager execution?,"I am trying to get ResNet101 or ResNeXt, which are only available in Keras' repository for some reason, from Keras applications in TensorFlow 1.10:

import tensorflow as tf
from keras import applications

tf.enable_eager_execution()

resnext = applications.resnext.ResNeXt101(include_top=False, weights='imagenet', input_shape=(SCALED_HEIGHT, SCALED_WIDTH, 3), pooling=None)



However, this results in:

Traceback (most recent call last):
  File ""myscript.py"", line 519, in get_fpn
    resnet = applications.resnet50.ResNet50(include_top=False, weights='imagenet', input_shape=(SCALED_HEIGHT, SCALED_WIDTH, 3), pooling=None)
  File ""Keras-2.2.4-py3.5.egg/keras/applications/__init__.py"", line 28, in wrapper
    return base_fun(*args, **kwargs)
  File ""Keras-2.2.4-py3.5.egg/keras/applications/resnet50.py"", line 11, in ResNet50
    return resnet50.ResNet50(*args, **kwargs)
  File ""Keras_Applications-1.0.8-py3.5.egg/keras_applications/resnet50.py"", line 214, in ResNet50
    img_input = layers.Input(shape=input_shape)
  File ""Keras-2.2.4-py3.5.egg/keras/engine/input_layer.py"", line 178, in Input
    input_tensor=tensor)
  File ""Keras-2.2.4-py3.5.egg/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  File ""Keras-2.2.4-py3.5.egg/keras/engine/input_layer.py"", line 87, in __init__
    name=self.name)
  File ""Keras-2.2.4-py3.5.egg/keras/backend/tensorflow_backend.py"", line 529, in placeholder
    x = tf.placeholder(dtype, shape=shape, name=name)
  File ""tensorflow/python/ops/array_ops.py"", line 1732, in placeholder
    raise RuntimeError(""tf.placeholder() is not compatible with ""
RuntimeError: tf.placeholder() is not compatible with eager execution.


I installed Keras from its GitHub master branch, since the pip installs of Keras and TensorFlow's Keras API for some strange reason do not include ResNet101, ResNetv2, ResNeXt, etc. Does anyone know how I can run such models (preferably ResNeXt) in TensorFlow's eager execution?
",2,4123,,,
tensorflow strange result,https://stackoverflow.com/questions/55992559,"Training loss value is increasing after some training time, but the model detects objects pretty good",,2,2595,,,
tensorflow strange result,https://stackoverflow.com/questions/55974198,Strange results while training with keras,"I am trying to train a unet model on braTS18 dataset (medical data with nifiti images) using keras with tensorflow. However I am getting very strange results:





as you can see, accuracy starts with 96% and gets to 99% at the third epoch. Also the validation loss doesn't go lower ever. Also there is nothing predicted by the trained model.

I have split the data in different ways (20% train 60% validation, or 60% train 20% validation) but didn't work. I think the problem might be with my model or with the data generator. Here are the codes:

unet model

def unet_model(filters=16, dropout=0.1, batch_normalize=True):

    # Build U-Net model
    inputs = Input((img_height, img_width, img_channels), name='main_input')
    s = Lambda(lambda x: x / 255) (inputs)

    c1 = Conv2D(filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c1') (s)
    c1 = Dropout(0.1) (c1)
    c1 = Conv2D(filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c1_d') (c1)
    p1 = MaxPooling2D((2, 2)) (c1)

    c2 = Conv2D(2*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c2') (p1)
    c2 = Dropout(0.1) (c2)
    c2 = Conv2D(2*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c2_d') (c2)
    p2 = MaxPooling2D((2, 2)) (c2)

    c3 = Conv2D(4*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c3') (p2)
    c3 = Dropout(0.2) (c3)
    c3 = Conv2D(4*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c3_d') (c3)
    p3 = MaxPooling2D((2, 2)) (c3)

    c4 = Conv2D(8*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c4') (p3)
    c4 = Dropout(0.2) (c4)
    c4 = Conv2D(8*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c4_d') (c4)
    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)

    c5 = Conv2D(16*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c5') (p4)
    c5 = Dropout(0.3) (c5)
    c5 = Conv2D(16*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c5_d') (c5)

    u6 = Conv2DTranspose(8*filters, (2, 2), strides=(2, 2), padding='same', name = 'u6') (c5)
    u6 = concatenate([u6, c4])
    c6 = Conv2D(8*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c6') (u6)
    c6 = Dropout(0.2) (c6)
    c6 = Conv2D(8*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c6_d') (c6)

    u7 = Conv2DTranspose(4*filters, (2, 2), strides=(2, 2), padding='same', name = 'u7') (c6)
    u7 = concatenate([u7, c3])
    c7 = Conv2D(4*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c7') (u7)
    c7 = Dropout(0.2) (c7)
    c7 = Conv2D(4*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c7_d') (c7)

    u8 = Conv2DTranspose(2*filters, (2, 2), strides=(2, 2), padding='same', name = 'u8') (c7)
    u8 = concatenate([u8, c2])
    c8 = Conv2D(2*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c8') (u8)
    c8 = Dropout(0.1) (c8)
    c8 = Conv2D(2*filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c8_d') (c8)

    u9 = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same', name = 'u9') (c8)
    u9 = concatenate([u9, c1], axis=3)
    c9 = Conv2D(filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c9') (u9)
    c9 = Dropout(0.1) (c9)
    c9 = Conv2D(filters, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name = 'c9_d') (c9)

    outputs = Conv2D(1, (1, 1), activation='sigmoid', name = 'output') (c9)

    adam = optimizers.Adam(lr=lr, beta_1=beta1, decay=lr_decay, amsgrad=False)

    model = Model(inputs=[inputs], outputs=[outputs])
    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy',dice,jaccard])

    plot_model(model, to_file=os.path.join(save_dir +""model.png""))
    if os.path.exists(os.path.join(save_dir +""model.txt"")):
        os.remove(os.path.join(save_dir +""model.txt""))
    with open(os.path.join(save_dir +""model.txt""),'w') as fh:
        model.summary(positions=[.3, .55, .67, 1.], print_fn=lambda x: fh.write(x + '\n'))

    model.summary()

    return model


and here is the code for data generator:

def generate_data(X_data, Y_data, batch_size):

    samples_per_epoch = total_folders
    number_of_batches = samples_per_epoch/batch_size
    counter=0

    while True:

        X_batch = X_data[batch_size*counter:batch_size*(counter+1)]
        Y_batch = Y_data[batch_size*counter:batch_size*(counter+1)]

        counter += 1

        yield X_batch, Y_batch

        if counter &gt;= number_of_batches:
            counter = 0
...
in the main function
...

if __name__ == ""__main__"":

    callbacks = [
    EarlyStopping(patience=1000, verbose=1),
    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),
    ModelCheckpoint(save_dir + 'model.{epoch:02d}-{val_loss:.2f}.h5', verbose=1, save_best_only=True, save_weights_only=True)
    ]

    model = unet_model(filters=16, dropout=0.05, batch_normalize=True)


    H = model.fit_generator(generate_data(X_train,Y_train,batch_size), 
                        epochs= epochs,
                        steps_per_epoch = total_folders/batch_size, 
                        validation_data=generate_data(X_test,Y_test,batch_size*2),
                        callbacks=callbacks,
                        validation_steps= total_folders/batch_size*2)


what am I doing wrong here?
",2,459,,,
tensorflow strange result,https://stackoverflow.com/questions/51421371,Tensorflow in Android: java.lang.illegalArgumentsException,,2,454,,,
tensorflow strange result,https://stackoverflow.com/questions/46522622,Confusing results with tensorflow&#39;s tensordot,,2,247,,,
tensorflow strange result,https://stackoverflow.com/questions/44669869,Tensorflow&#39;s computation time gradually slowing down in very simple &quot;for loop&quot;,,2,836,,,
tensorflow strange result,https://stackoverflow.com/questions/44333516,"Tensorflow, placement of shared variable definitions",,2,224,,,
tensorflow strange result,https://stackoverflow.com/questions/44220225,Tensorflow build from source not faster for retraining?,,2,1089,,,
tensorflow strange result,https://stackoverflow.com/questions/39550258,keras errors with theano/tensowflow backend,,2,1037,,,
tensorflow strange result,https://stackoverflow.com/questions/36807107,Strange error when taking gradient of TensorArray,,2,1058,,,
tensorflow strange result,https://stackoverflow.com/questions/56695299,How should I use mode.predict_generator to evaluate model performance in a Confusion Matrix?,,1,1423,"The problem was that as default _flow_from_directory_ uses shuffle = True. Predictions are correct if shuffle goes to False. However, using validation dataset to evaluate training seems to do right even though shuffle is True. I have updated git for these changes to be populated

# Flow validation images in batches of 20 using test_datagen generator
test_generator =  test_datagen_2.flow_from_directory( test_dir,
                                                  batch_size  = 1,
                                                  class_mode  = 'binary', 
                                                  target_size = (image_size, 
image_size),
                                                  shuffle = False)

",,
tensorflow strange result,https://stackoverflow.com/questions/70593426,Library import in Python does not work as expected,,1,710,"It seems that there's a bug in the current version of tensorflow, particularly with the _load() method in the LazyLoader class defined in the module tensorflow.python.util.lazy_loader. I think we could just leave it until being fixed. I'd like to dig into another question raised here: how come your second segment of codes worked yet the first one failed?
The reason comes from the import system as well as the dot operator in Python.
In the statement import m1.m2.m3 or from m1.m2.m3 import n1, the dot notation in m1.m2.m3 is used to separate names of modules(packages) and submodules(subpackages). It's not a real dot operator. When processing this statement, Python will first import the module(package) m1, then m1.m2, then m1.m2.m3. In each step of loading the modules, two notable side-effects occur(among lots of other actions):

the to-be-imported package's __init__.py file is executed.
a binding is placed in the parent modules namespace to the submodule object. For this example, m1.__dict__['m2'] is set to be the module object m1.m2, and m1.m2.__dict__['m3'] is set to be the module object m1.m2.m3.

And for the real dot operator, for example an expression obj1.attr1, the evaluation order will be:

evaluate obj1.__getattribute__('attr1') and return the value
if no such an attribute found in the first step, then evaluate odj1.__getattr__('attr1')

where the search of obj1.__dict__['attr1'] is launched in the first step.
Back to tensorflow, there's such a statement in __init__.py under the package tensorflow:
keras = _LazyLoader(""keras"", globals(), _keras_module)

Here's a simplified ""second version"" of your sample as below:
from tensorflow.keras.models import Sequential 
# tensorflow.__dict__['keras'] is set to be the module tensorflow.keras
import tensorflow as tf # make the module name tensorflow useable

print(type(tf.keras)) # tf.keras is evaluated to the submodule tensorflow.keras
# output: &lt;class 'module'&gt;

It turns out the name binding to the submodule object overwrites the binding(assignment) in __init__.py.
But if we don't import the submodule tensorflow.keras, then the binding in __init__.py works:
import tensorflow as tf

print(type(tf.keras))
# output: &lt;class 'tensorflow.python.util.lazy_loader.LazyLoader'&gt;

That explains why your second method worked but the first one failed.

Reference:
For import mechanism:

7.11. The import statement
5. The import system

For dot operator:

Descriptor HowTo Guide - Invocation from an instance

","You can use from tensorflow import keras and try again executing the same code.
if it shows the same error again then run below code in jupyter notebook:
import keras
print(keras.__version__)
import tensorflow
print(tensorflow.__version__)

if you find some differences in versions, then execute below code in jupyter notebook which will automatically install upgraded and compatible version of tensorflow and keras along with its other required libraries.
!pip insatll tensorflow
import tensorflow
print(tensorflow.__version__)

!pip install keras
import keras
print(keras.__version__)

Let us know if issue still persists.
",
tensorflow strange result,https://stackoverflow.com/questions/73125716,Tensorflow Lite Android: Both GPU delegate and NNAPI delegate are slower than CPU,"I am currently evaluating and comparing the performance of some tensorflow models on different smartphones. I am testing the MNIST and CIFAR10 databases. The strange thing is, when i try to speed up the inference times with hardware acceleration, they always perform worse than before.
For example, these are the results I got on a Galaxy Note 20 Ultra, which definitely has got some powerful GPUs and NPUs (all results are milliseconds per inference):
MNIST CPU:      0.040
MNIST GPU:      2.322
MNIST NNAPI:    2.839
CIFAR10 CPU:    0.810
CIFAR10 GPU:    8.120
CIFAR10 NNAPI:  6.608
I warmed up the processing unit before the benchmark and executed the inferences multiple times, so these are averages and should not be random. Below you can see the code I used to configure the NNAPI or GPU-API of TensorFlow-Lite:
val model = loadModelFile(assetManager, modelPath)
val compatList = CompatibilityList()
var nnApiDelegate: NnApiDelegate? = null

val options = Interpreter.Options().apply{
    if (USE_NNAPI &amp;&amp; Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.P) {
        nnApiDelegate = NnApiDelegate()
        this.addDelegate(nnApiDelegate)
    }
    else if(USE_GPU &amp;&amp; compatList.isDelegateSupportedOnThisDevice){
        val delegateOptions = compatList.bestOptionsForThisDevice
        this.addDelegate(GpuDelegate(delegateOptions))
    } else {
        // if the GPU is not supported, run on 4 threads
        this.setNumThreads(4)
    }
}

val interpreters = Interpreter(model, options)

Does anybody know what could be the reason for this or how to fix that?
Thanks in advance for any tips or clues!
EDIT:
Input size MNIST: 24 x 24 x 255
Input size CIFAR10: 32 x 32 x 3 x 255
I measure the inference times by measuring the time of performing an inference a few thousand times on the device and then I calculate the average afterwards.
",1,1106,"It seems that both models are already performing well on CPU with the inference latency &lt; 1ms.
Accelerators are not always faster than CPU. Often, there is some overhead when accessing the accelerators. Also, accelerators could run certain models / operators really well, but they may not support all the operators that the CPU supports. Additionally, CPU might simply be as-fast or even faster if the performance of a model is memory-bound.
It might worth trying with some larger vision models, e.g. mobilenet_v1_1.0_224 and see if there is a speedup with GPU or other accelerators.
",,
tensorflow strange result,https://stackoverflow.com/questions/69293455,Why does Keras BatchNorm produce different output than PyTorch?,"Torch'1.9.0+cu111'
Tensorflow-gpu'2.5.0'
I came across a strange thing, when using the Batch Normal layer of tensorflow 2.5 and the BatchNorm2d layer of Pytorch 1.9 to calculate the same Tensor , and the results were quite different (TensorFlow is close to 1, Pytorch is close to 0).I thought at first it was the difference between the momentum and epsilon , but after changing them to the same, the result was the same.
from torch import nn
import torch
x = torch.ones((20, 100, 35, 45))
a = nn.Sequential(
            # nn.Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), padding=0, bias=True),
            nn.BatchNorm2d(100)
        )
b = a(x)

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.layers import *
x = tf.ones((20, 35, 45, 100))
a = keras.models.Sequential([
            # Conv2D(128, (1, 1), (1, 1), padding='same', use_bias=True),
            BatchNormalization()
        ])
b = a(x)



",1,1632,"Batchnormalization works differently in training and inference,
During training (i.e. when using fit() or when calling the layer/model with the argument training=True), the layer normalizes its output using the mean and standard deviation of the current batch of inputs. That is to say, for each channel being normalized, the layer returns
gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta

where:

epsilon is small constant (configurable as part of the constructor arguments)
gamma is a learned scaling factor (initialized as 1), which can be disabled by passing scale=False to the constructor.
beta is a learned offset factor (initialized as 0), which can be disabled by passing center=False to the constructor.

During inference (i.e. when using evaluate() or predict() or when calling the layer/model with the argument training=False (which is the default), the layer normalizes its output using a moving average of the mean and standard deviation of the batches it has seen during training. That is to say, it returns
gamma * (batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta.

self.moving_mean and self.moving_var are non-trainable variables that are updated each time the layer in called in training mode, as such:
    moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)
    moving_var = moving_var * momentum + var(batch) * (1 - momentum)

ref: https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization
If you run the pytorch batchnorm in eval mode, you get close results (the rest of the discrepancy comes from the different internal implementation, parameter choices, etc.),
from torch import nn
import torch
x = torch.ones((1, 2, 2, 2))
a = nn.Sequential(
            # nn.Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), padding=0, bias=True),
            nn.BatchNorm2d(2)
        )
a.eval()
b = a(x)
print(b)
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.layers import *
x = tf.ones((1, 2, 2, 2))
a = keras.models.Sequential([
            # Conv2D(128, (1, 1), (1, 1), padding='same', use_bias=True),
            BatchNormalization()
        ])
b = a(x)
print(b)

out:
tensor([[[[1.0000, 1.0000],
          [1.0000, 1.0000]],

         [[1.0000, 1.0000],
          [1.0000, 1.0000]]]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tf.Tensor(
[[[[0.9995004 0.9995004]
   [0.9995004 0.9995004]]

  [[0.9995004 0.9995004]
   [0.9995004 0.9995004]]]], shape=(1, 2, 2, 2), dtype=float32)

",,
tensorflow strange result,https://stackoverflow.com/questions/69105473,tflite quantized mobilenet v2 classifier not working,"My goal is to convert a PyTorch Model into a quantized tflite model that can be used for inference on the Edge TPU.
I was able to convert a fairly complex depth estimation model from PyTorch to tflite and I successfully ran it on the Edge TPU. But because not all operations were supported, inference was pretty slow (&gt;800ms).
Number of operations that will run on Edge TPU: 87
Number of operations that will run on CPU: 47

Depth Estimation
Because I want a model that runs fully on the TPU, I tried converting the simplest model I could think of, a MobilenetV2 classification model. But when running the quantized model, I get strangely inaccurate results.




PyTorch
TFLite




Samoyed:0.8303
missile: 0.184565


Pomeranian: 0.06989
kuvasz: 0.184565


keeshond: 0.01296
stupa: 0.184565


collie: 0.0108
Samoyed: 0.184565


Great Pyrenees: 0.00989
Arctic fox: 0.184565




Is this caused by quantizing the model from float32 to uint8 or am I doing something wrong? And if it is caused by quantization, how can I mitigate that? The classification example from corral works fine and, as far as I know, is uses the same model.
Conversion Process
PyTorch -&gt; ONNX -&gt; OpenVINO -&gt; TensorFlow -&gt; TensorFlowLite
I wrote my own code to convert the model from PyTorch to ONNX and from TensorFlow(pd) into TFlite. For the other conversion steps, I used the OpenVINO mo.py script and the openvino2tensorflow toll because of the nchw nhwc mismatch between PyTorch and TensorFlow.
Downloads
Depth Estimation Model: https://github.com/AaronZettler/miscellaneous/blob/master/mobilenet_v2_depth_est.pth?raw=true
Classification Model: https://github.com/AaronZettler/miscellaneous/blob/master/mobilenetv2.tflite?raw=true
Labels: https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt
Image: https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg
Code
This code does not require the Edge TPU to be run, but it does require the google coral libraries.
If I use different parameters for mean and std, like (2.0, 76.0), I get a solid result for the dog.jpg image but if I try to classify something else I have the same problem.

import numpy as np
from PIL import Image
from pycoral.adapters import classify
from pycoral.adapters import common
from pycoral.utils.dataset import read_label_file
from torchvision import transforms

from tensorflow.lite.python.interpreter import Interpreter


def cropPIL(image, new_width, new_height):
    width, height = image.size

    left = (width - new_width)/2
    top = (height - new_height)/2
    right = (width + new_width)/2
    bottom = (height + new_height)/2

    return image.crop((left, top, right, bottom))

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

def classify_img(image_dir, lables_dir, model_dir, mean, std):
    #loading lables and model
    labels = read_label_file(lables_dir)
    interpreter = Interpreter(model_path=model_dir)
    interpreter.allocate_tensors()
    
    #load an resize image
    size = (256, 256)
    image = Image.open(image_dir).convert('RGB')
    image = image.resize(((int)(size[0]*image.width/image.height), size[1]), Image.ANTIALIAS)
    image = cropPIL(image, 224, 224)
    image = np.asarray(image)

    #normalizing the input image
    params = common.input_details(interpreter, 'quantization_parameters')
    scale = params['scales']
    zero_point = params['zero_points']

    normalized_input = (image - mean) / (std * scale) + zero_point
    np.clip(normalized_input, 0, 255, out=normalized_input)

    #setting the image as input
    common.set_input(interpreter, normalized_input.astype(np.uint8))
    
    #run inference
    interpreter.invoke()

    #get output tensor and run softmax
    output_details = interpreter.get_output_details()[0]
    output_data = interpreter.tensor(output_details['index'])().flatten()
    scores = softmax(output_data.astype(float))

    #get the top 10 classes
    classes = classify.get_classes_from_scores(scores, 5, 0.0)

    print('-------RESULTS--------')
    for c in classes:
       print('%s: %f' % (labels.get(c.id, c.id), c.score))


image_dir  = 'data/dog.jpg'
lables_dir = 'data/imagenet_classes.txt'
model_dir  = 'models/mobilenetv2.tflite'

classify_img(image_dir, lables_dir, model_dir, 114.0, 57.0)

To run the PyTorch model on google colab I had to replace
model = torch.hub.load('pytorch/vision:v0.9.0', 'mobilenet_v2', pretrained=True)
with
model = torchvision.models.mobilenet_v2(pretrained=True)
to make it work.
This is the code I used to test The PyTorch model on my machine.
import torch
from PIL import Image
from torchvision import transforms
import torchvision

import numpy as np
import matplotlib.pyplot as plt
    
def inference(model, input_image, lables_dir):
    preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    input_tensor = preprocess(input_image)
    input_batch = input_tensor.unsqueeze(0)

    # move the input and model to GPU for speed if available
    if torch.cuda.is_available():
        input_batch = input_batch.to('cuda')
        model.to('cuda')

    with torch.no_grad():
        output = model(input_batch)

    probabilities = torch.nn.functional.softmax(output[0], dim=0)

    # Read the categories
    with open(lables_dir, ""r"") as f:
        categories = [s.strip() for s in f.readlines()]

    # Show top categories per image
    top5_prob, top5_catid = torch.topk(probabilities, 5)
    result = {}
    for i in range(top5_prob.size(0)):
        result[categories[top5_catid[i]]] = top5_prob[i].item()
    return result

def classify(image_dir, lables_dir):
    model = torchvision.models.mobilenet_v2(pretrained=True)
    model.eval()

    im = Image.open(image_dir)
    results = inference(model, im, lables_dir)
    for result in results:
        print(f'{result}: {round(results[result], 5)}')


classify('data/dog.jpg', 'data/imagenet_classes.txt')

",1,981,"EdgeTPU mapping of PReLU (LeakyReLU) is now supported in openvino2tensorflow v1.20.4.
However, due to the large size of the model, it is not possible to map all operations to the EdgeTPU. Therefore, the part of the EdgeTPU that does not fit in RAM is offloaded to the CPU for inference, which is very slow. In this case, inference by the CPU alone is 4 to 5 times faster. EdgeTPU does not support PReLU (LeakyReLU), so the operations must be replaced. However, openvino2tensorflow v1.20.4 automatically replaces the operations in the conversion process.

Converted model
https://github.com/PINTO0309/PINTO_model_zoo/tree/main/149_depth_estimation

Convert sample


docker run --gpus all -it --rm \
-v `pwd`:/home/user/workdir \
pinto0309/openvino2tensorflow:latest

cd workdir

MODEL=depth_estimation_mbnv2

H=180
W=320
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model ${MODEL}_${H}x${W}.onnx \
--data_type FP32 \
--output_dir ${H}x${W}/openvino/FP32
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model ${MODEL}_${H}x${W}.onnx \
--data_type FP16 \
--output_dir ${H}x${W}/openvino/FP16
mkdir -p ${H}x${W}/openvino/myriad
${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/lib/intel64/myriad_compile \
-m ${H}x${W}/openvino/FP16/${MODEL}_${H}x${W}.xml \
-ip U8 \
-VPU_NUMBER_OF_SHAVES 4 \
-VPU_NUMBER_OF_CMX_SLICES 4 \
-o ${H}x${W}/openvino/myriad/${MODEL}_${H}x${W}.blob

openvino2tensorflow \
--model_path ${H}x${W}/openvino/FP32/${MODEL}_${H}x${W}.xml \
--output_saved_model \
--output_pb \
--output_no_quant_float32_tflite \
--output_weight_quant_tflite \
--output_float16_quant_tflite \
--output_integer_quant_tflite \
--string_formulas_for_normalization 'data / 255' \
--output_integer_quant_type 'uint8' \
--output_tfjs \
--output_coreml \
--output_tftrt
mv saved_model saved_model_${H}x${W}

openvino2tensorflow \
--model_path ${H}x${W}/openvino/FP32/${MODEL}_${H}x${W}.xml \
--output_saved_model \
--output_pb \
--output_edgetpu \
--string_formulas_for_normalization 'data / 255' \
--output_integer_quant_type 'uint8'
mv saved_model/model_full_integer_quant.tflite saved_model_${H}x${W}/model_full_integer_quant.tflite
mv saved_model/model_full_integer_quant_edgetpu.tflite saved_model_${H}x${W}/model_full_integer_quant_edgetpu.tflite

mv ${H}x${W}/openvino saved_model_${H}x${W}/openvino
mv ${MODEL}_${H}x${W}.onnx saved_model_${H}x${W}/${MODEL}_${H}x${W}.onnx


H=240
W=320
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model ${MODEL}_${H}x${W}.onnx \
--data_type FP32 \
--output_dir ${H}x${W}/openvino/FP32
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model ${MODEL}_${H}x${W}.onnx \
--data_type FP16 \
--output_dir ${H}x${W}/openvino/FP16
mkdir -p ${H}x${W}/openvino/myriad
${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/lib/intel64/myriad_compile \
-m ${H}x${W}/openvino/FP16/${MODEL}_${H}x${W}.xml \
-ip U8 \
-VPU_NUMBER_OF_SHAVES 4 \
-VPU_NUMBER_OF_CMX_SLICES 4 \
-o ${H}x${W}/openvino/myriad/${MODEL}_${H}x${W}.blob

openvino2tensorflow \
--model_path ${H}x${W}/openvino/FP32/${MODEL}_${H}x${W}.xml \
--output_saved_model \
--output_pb \
--output_no_quant_float32_tflite \
--output_weight_quant_tflite \
--output_float16_quant_tflite \
--output_integer_quant_tflite \
--string_formulas_for_normalization 'data / 255' \
--output_integer_quant_type 'uint8' \
--output_tfjs \
--output_coreml \
--output_tftrt
mv saved_model saved_model_${H}x${W}

openvino2tensorflow \
--model_path ${H}x${W}/openvino/FP32/${MODEL}_${H}x${W}.xml \
--output_saved_model \
--output_pb \
--output_edgetpu \
--string_formulas_for_normalization 'data / 255' \
--output_integer_quant_type 'uint8'
mv saved_model/model_full_integer_quant.tflite saved_model_${H}x${W}/model_full_integer_quant.tflite
mv saved_model/model_full_integer_quant_edgetpu.tflite saved_model_${H}x${W}/model_full_integer_quant_edgetpu.tflite

mv ${H}x${W}/openvino saved_model_${H}x${W}/openvino
mv ${MODEL}_${H}x${W}.onnx saved_model_${H}x${W}/${MODEL}_${H}x${W}.onnx


PReLU (LeakyReLU) to Maximum (ReLU), Minimum, Mul, Add
From:

To:

EdgeTPU model



",,
tensorflow strange result,https://stackoverflow.com/questions/62251893,Protobuf version mismatch in GPflow/TensorFlow optimization when used together with MATLAB engine for Python,"I have some simulation code which is written in MATLAB and I have to run bayesian optimization on that using GPflowOpt. I use the MATLAB Engine for Python to retrieve the results of the MATLAB objective function and pass them to my Python optimization code. I installed GPflowOpt (here) and the MATLAB Engine (here) according to the instructions and my stripped down version (see below) of this example code from the GPflowOpt docs works as expected.

import gpflow
import gpflowopt
import matlab.engine
import numpy as np

def vlmop2(x):
    transl = 1 / np.sqrt(2)
    part1 = (x[:, [0]] - transl) ** 2 + (x[:, [1]] - transl) ** 2
    part2 = (x[:, [0]] + transl) ** 2 + (x[:, [1]] + transl) ** 2
    y1 = 1 - np.exp(-1 * part1)
    y2 = 1 - np.exp(-1 * part2)
    return np.hstack((y1, y2))

if __name__ == '__main__':
#    engine = matlab.engine.start_matlab()
#    print('Working dir: ', engine.pwd())

    domain = gpflowopt.domain.ContinuousParameter('x1', -2, 2) + \
             gpflowopt.domain.ContinuousParameter('x2', -2, 2)

    design = gpflowopt.design.LatinHyperCube(11, domain)
    X = design.generate()
    Y = vlmop2(X)

    # One model for each objective
    objective_models = [gpflow.gpr.GPR(X.copy(), Y[:,[i]].copy(), gpflow.kernels.Matern52(2, ARD=True)) for i in range(Y.shape[1])]
    for model in objective_models:
        model.likelihood.variance = 0.01

    hvpoi = gpflowopt.acquisition.HVProbabilityOfImprovement(objective_models)

    # First setup the optimization strategy for the acquisition function
    # Combining MC step followed by L-BFGS-B
    acquisition_opt = gpflowopt.optim.StagedOptimizer([gpflowopt.optim.MCOptimizer(domain, 1000),
                                                       gpflowopt.optim.SciPyOptimizer(domain)])

    # Then run the BayesianOptimizer for 20 iterations
    optimizer = gpflowopt.BayesianOptimizer(domain, hvpoi, optimizer=acquisition_opt, verbose=True)
    result = optimizer.optimize([vlmop2], n_iter=5)

    print(result)
    print(optimizer.acquisition.pareto.front.value)


But as soon as I make only one single call to the MATLAB engine somewhere in the code (just uncomment the two lines in my above snippet) execution breaks on line

result = optimizer.optimize([vlmop2], n_iter=5)


and I am confronted with some strange error complaining about some Protobuf version number mismatch:

[libprotobuf FATAL /tmp/B3p3/glnxa64/protobuf3/src/google/protobuf/stubs/common.cc:68] This program requires version 3.8.0 of the Protocol Buffer runtime library, but the installed version is 3.6.1.  Please update your library.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in ""external/com_google_protobuf/src/google/protobuf/any.pb.cc"".)
terminate called after throwing an instance of 'google::protobuf::FatalException'


I read a bit about what Protobuf is, what it does and that it obviously is part of TensorFlow, but I don't have any clue why I could possibly have some version mismatch here. I installed everything inside a virtualenv and reinstalled the tensorflow and protobuf packages back and forth several times in different versions to no avail. This is an excerpt of the currently installed relevant packages:

Package               Version
--------------------- ----------
gpflow                0.5
gpflowopt             0.1.1
matlabengineforpython R2019b
numpy                 1.18.5
protobuf              3.12.2
tensorflow            1.15.3
tensorflow-estimator  1.15.1


Strange enough I don't even know why the error message states I have Protobuf version 3.6.1 installed, though the protobuf package clearly is a later version and I don't know where this 3.6.1 version could hide on my system. What's more interesting is that the code works like a charm as long as the MATLAB engine is not involved. But as soon as I just make one single call to engine.eval() it's over and the code goes south. Interestingly the code still works as long as I just initialize the engine and don't exchange any data with it

I already have used the MATLAB engine on another optimization task using scikit-opt earlier, where I basically did the same (fetch a function result from a MATLAB script and process it in Python) and it worked flawlessly. But somehow it refuses to co-exist with TensorFlow and I don't know where to look into.

I'm happy to provide more information if needed and maybe there is someone out there who has experience with TensorFlow+Protobuf+MATLAB and knows what it's all about.
",1,240,"Unfortunatelly I forgot about my own question but since apparently I'm not the only one with that, I'll post the my personal solution here:
Basically, it all came down to first execute some arbitrary code which boostraps Tensorflow which in turn loads the protobuf library (presumably for some internal data exchange) before exchanging data with the MATLAB engine.
It seems like data exchange between the Python world and MATLAB is done via protobuf messages. So, if you first execute some dummy Tensorflow code which internally needs the protobuf library, it will happily load the one that you have installed in your local environment and everything is fine. If you instead first exchange some data with the MATLAB engine (like calling engine.pwd() as I did in the commented part above), the engine will load the protobuf library which comes built-in with your MATLAB installation first and that one will most likely be older and incompatible with the one that came with your Tensorflow installation.
So what I did in the end was to just perform a dummy optimization over, say, 3 iterations of some dull function like x^2 to make sure that all parts of Tensorflow were executed before initializing the MATLAB engine and interacting with it.
Maybe there is an even simpler way to force loading the correct protobuf version through Tensorflow but I didn't have the time nor the willingness to dig down that deep. Honestly, back then I stumbled across protobuf for the first time and didn't exactly know what it was for and how to use it. Now, I'd probably just look into Python's protobuf library and see if there's some kind of init or load function that could do the job.
I hope this description is of any help for everyone who reads it and comes across the same issue.
",,
tensorflow strange result,https://stackoverflow.com/questions/61076672,Resnet-50 adversarial training with cleverhans FGSM accuracy stuck at 5%,"I am facing a strange problem when adversarially training a resnet-50, and I am not sure whether is's a logical error, or a bug somewhere in the code/libraries.
I am adversarially training a resnet-50 thats loaded from Keras, using the FastGradientMethod from cleverhans, and expecting the adversarial accuracy to rise at least above 90% (probably 99.x%). The training algorithm, training- and attack-params should be visible in the code.
The problem, as already stated in the title is, that the accuracy is stuck at 5% after training ~3000 of 39002 training inputs in the first epoch. (GermanTrafficSignRecognitionBenchmark, GTSRB).

When training without and adversariy loss function, the accuracy does not get stuck after 3000 samples, but continues to rise &gt; 0.95 in the first epoch.

When substituting the network with a lenet-5, alexnet and vgg19, the code works as expected, and an accuracy absolutely comparabele to the non-adversarial, categorical_corssentropy lossfunction is achieved. I've also tried running the procedure using solely tf-cpu and different versions of tensorflow, the result is always the same.

Code for obtaining ResNet-50:

def build_resnet50(num_classes, img_size):
    from tensorflow.keras.applications import ResNet50
    from tensorflow.keras import Model
    from tensorflow.keras.layers import Dense, Flatten
    resnet = ResNet50(weights='imagenet', include_top=False, input_shape=img_size)
    x = Flatten(input_shape=resnet.output.shape)(resnet.output)
    x = Dense(1024, activation='sigmoid')(x)
    predictions = Dense(num_classes, activation='softmax', name='pred')(x)
    model = Model(inputs=[resnet.input], outputs=[predictions])
    return model


Training:

def lr_schedule(epoch):
    # decreasing learning rate depending on epoch
    return 0.001 * (0.1 ** int(epoch / 10))


def train_model(model, xtrain, ytrain, xtest, ytest, lr=0.001, batch_size=32, 
epochs=10, result_folder=""""):
    from cleverhans.attacks import FastGradientMethod
    from cleverhans.utils_keras import KerasModelWrapper
    import tensorflow as tf

    from tensorflow.keras.optimizers import SGD
    from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint
    sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)

    model(model.input)

    wrap = KerasModelWrapper(model)
    sess = tf.compat.v1.keras.backend.get_session()
    fgsm = FastGradientMethod(wrap, sess=sess)
    fgsm_params = {'eps': 0.01,
                   'clip_min': 0.,
                   'clip_max': 1.}

    loss = get_adversarial_loss(model, fgsm, fgsm_params)

    model.compile(loss=loss, optimizer=sgd, metrics=['accuracy'])

    model.fit(xtrain, ytrain,
                    batch_size=batch_size,
                    validation_data=(xtest, ytest),
                    epochs=epochs,
                    callbacks=[LearningRateScheduler(lr_schedule)])


Loss-function:

def get_adversarial_loss(model, fgsm, fgsm_params):
    def adv_loss(y, preds):
         import tensorflow as tf

        tf.keras.backend.set_learning_phase(False) #turn off dropout during input gradient calculation, to avoid unconnected gradients

        # Cross-entropy on the legitimate examples
        cross_ent = tf.keras.losses.categorical_crossentropy(y, preds)

        # Generate adversarial examples
        x_adv = fgsm.generate(model.input, **fgsm_params)
        # Consider the attack to be constant
        x_adv = tf.stop_gradient(x_adv)

        # Cross-entropy on the adversarial examples
        preds_adv = model(x_adv)
        cross_ent_adv = tf.keras.losses.categorical_crossentropy(y, preds_adv)

        tf.keras.backend.set_learning_phase(True) #turn back on

        return 0.5 * cross_ent + 0.5 * cross_ent_adv
    return adv_loss


Versions used:
tf+tf-gpu: 1.14.0
keras: 2.3.1
cleverhans: &gt; 3.0.1 - latest version pulled from github
",1,561,"It is a side-effect of the way we estimate the moving averages on BatchNormalization.

The mean and variance of the training data that you used are different from the ones of the dataset used to train the ResNet50. Because the momentum on the BatchNormalization has a default value of 0.99, with only 10 iterations it does not converge quickly enough to the correct values for the moving mean and variance. This is not obvious during training when the learning_phase is 1 because BN uses the mean/variance of the batch. Nevertheless when we set learning_phase to 0, the incorrect mean/variance values which are learned during training significantly affect the accuracy. 

You can fix this problem by below approachs:


More iterations


Reduce the size of the batch from 32 to 16(to perform more updates per epoch) and increase the number of epochs from 10 to 250. This way the moving average and variance will converge to the correct values.


Change the momentum of BatchNormalization


Keep the number of iterations fixed but change the momentum of the BatchNormalization layer to update more aggressively the rolling mean and variance (not recommended for production models). 

On the original snippet, add the following code between reading the base_model and defining the new layers:

# ....
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)

# PATCH MOMENTUM - START
import json
conf = json.loads(base_model.to_json())
for l in conf['config']['layers']:
    if l['class_name'] == 'BatchNormalization':
        l['config']['momentum'] = 0.5


m = Model.from_config(conf['config'])
for l in base_model.layers:
    m.get_layer(l.name).set_weights(l.get_weights())

base_model = m
# PATCH MOMENTUM - END

x = base_model.output
# ....


Would also recommend you to try another hack provided bu us here.
",,
tensorflow strange result,https://stackoverflow.com/questions/59073146,Better / Faster Result with smaller batch Size linear classifier,"I'm currently training multiple linear classifier with tensorflow and I found something which is strange.

If the batch_size is small, my results are better ( the model learn faster)
I'm working on FashionMNIST

epochs = 300
batch_size = 5000

# Create and fit model
model = tf.keras.Sequential()
model.add(Dense(1, activation=""linear"", input_dim=28*28))
model.add(Dense(10, activation=""softmax"", input_dim=1))
model.compile(optimizer=Adam(), loss=[categorical_crossentropy], metrics=[categorical_accuracy])
model.fit(x_train, y_one_hot_train, validation_data=(x_val, y_one_hot_val), epochs=epochs, batch_size=batch_size)


Results

Batch-size : 20000 and 200 epochs

loss: 2.7494 - categorical_accuracy: 0.2201 - val_loss: 2.8695 - val_categorical_accuracy: 0.2281


Batch-size : 10000 and 200 epochs

loss: 1.7487 - categorical_accuracy: 0.3336 - val_loss: 1.8268 - val_categorical_accuracy: 0.3331


Batch-size : 2000 and 200 epochs

loss: 1.2906 - categorical_accuracy: 0.5123 - val_loss: 1.3247 - val_categorical_accuracy: 0.5113


Batch-size : 1000 and 200 epochs

loss: 1.1080 - categorical_accuracy: 0.5246 - val_loss: 1.1261 - val_categorical_accuracy: 0.5273


Do you know why I got these kind of results ?
",1,351,"Batch size impacts learning significantly. What happens when you put a batch through your network is that you average the gradients. The concept is that if your batch size is big enough, this will provide a stable enough estimate of what the gradient of the full dataset would be. By taking samples from your dataset, you estimate the gradient while reducing computational cost significantly. The lower you go, the less accurate your esttimate will be, however in some cases these noisy gradients can actually help escape local minima. When it is too low, your network weights can just jump around if your data is noisy and it might be unable to learn or it converges very slowly, thus negatively impacting total computation time.
","In addition to the other answer, it should be noted that increasing the batch size while keeping the number of epochs the same will result in fewer training steps overall. This is because one epoch is the number of steps it takes to iterate over the data once, which is smaller if the batches are bigger.
E.g., increasing batch size by 10 will reduce the number of training steps by 10. So it's not really a fair comparison. Your model with batch size 20000 only gets 600 training steps (3 steps per epoch) whereas the model with batch size 1000 gets 12000 (60 steps per epoch).
",
tensorflow strange result,https://stackoverflow.com/questions/52611575,tf.while_loop gives wrong result when it runs in parallel,"I want to update a two dimensional tf.variable inside a tf.while_loop in tensorflow by row. For this reason, I use the tf.assign method. The problem is that with my implementation and parallel_iterations&gt;1 the result is wrong. With parallel_iterations=1 the result is correct. The code is like this:

a = tf.Variable(tf.zeros([100, 100]), dtype=tf.int64)

i = tf.constant(0)
def condition(i, var):
    return tf.less(i, 100)

def body(i, var):
    updated_row = method() # This method returns a [1, 100] tensor which is the updated row for the variable
    temp = tf.assign(a[i], updated_row)
    return [tf.add(i, 1), temp]

z = tf.while_loop(condition, body, [i, a], back_prop=False, parallel_iterations=10)


The iterations are completely independent and I do not know what is the problem.

Strangely If I change the code like this:

a = tf.Variable(tf.zeros([100, 100]), dtype=tf.int64)

i = tf.constant(0)
def condition(i, var):
    return tf.less(i, 100)

def body(i, var):
    zeros = lambda: tf.zeros([100, 100], dtype=tf.int64)
    temp = tf.Variable(initial_value=zeros, dtype=tf.int64)
    updated_row = method() # This method returns a [1, 100] tensor which is the updated row for the variable
    temp = tf.assign(temp[i], updated_row)
    return [tf.add(i, 1), temp]

z = tf.while_loop(condition, body, [i, a], back_prop=False, parallel_iterations=10)


the code gives the correct outcome for parallel_iterations&gt;1. Can someone explain me what is going on here and give me an efficient solution to update the variable because the original variable I want to update is huge and the solution I found is very inefficient.
",1,598,"In tf.function I've found the following:


  Key Point: Any Python side-effects (appending to a list, printing with print, etc) will only happen once, when func is traced. To have side-effects executed into your tf.function they need to be written as TF ops:


I'm pretty sure that's what's going on here. You're expecting a to change but that is a ""side effect"" ( https://runestone.academy/runestone/books/published/fopp/Functions/SideEffects.html ) which tensorflow does not fully support. When you change a to temp you're no longer relying on the side effect and the code works.
",,
tensorflow strange result,https://stackoverflow.com/questions/48119473,Gram-Schmidt orthogonalization in pure Tensorflow: performance for iterative solution is much slower than numpy,,1,2097,,,
tensorflow strange result,https://stackoverflow.com/questions/46771019,tensorflow keras do not use all available resources,,1,1464,,,
tensorflow strange result,https://stackoverflow.com/questions/45577747,CNN in Tensorflow - loss remains constant,,1,7390,,,
tensorflow strange result,https://stackoverflow.com/questions/43872812,Tensorflow AdamOptimizer vs Gradient Descent,,1,4985,,,
tensorflow strange result,https://stackoverflow.com/questions/42938603,tensorflow conv3d ignoring 1st spacial dimension,,1,419,,,
tensorflow strange result,https://stackoverflow.com/questions/39152282,Multilayer feedforward net fails to train in TensorFlow,,1,412,,,
tensorflow strange result,https://stackoverflow.com/questions/35500242,"Implement Logistic Regression(LR) in Tensorflow,But Result looks strange when using bias but good without bias",,1,516,,,
tensorflow strange result,https://stackoverflow.com/questions/54152947,Tensorflow Object detection: Trained model does not predict ALL instances of the same object in the image,,0,523,,,
tensorflow strange result,https://stackoverflow.com/questions/56700022,tensorflow-hub: Python import error when file is in a specific folder,,0,1675,,,
tensorflow strange result,https://stackoverflow.com/questions/73804090,Is it possible to convert a TensorFlow (Keras) model from BGR to RGB?,"I have converted a Caffe model, learned on BGR data, to ONNX format and then from ONNX to TensorFlow (Keras)
So now I have a Keras model, learned on BGR data. Is it possible to convert it in such a way, that it will properly work with RGB data?
I've tried to convert it to OpenVINO with the --reverse_input_channel flag and then back to TensorFlow, but it seems that openvino2tensorflow works very poorly, so it didn't work. Maybe there is some simpler way?
Update
I've realized that I get SavedModel through Keras model, so I've updated the question.
Update 2
I've applied AndrzejO solution. However, now my model gives much worse results than before. Am I doing something wrong?
from keras.layers import Input, Lambda
from keras.models import Model

input_shape = k_model.get_layer(index = 0).input_shape[0][1:]
inputs = Input(shape=input_shape)
lambda_layer = Lambda(lambda x: x[:,:,:,::-1])(inputs)
outputs = k_model(lambda_layer)
k_model = Model(inputs=inputs, outputs=outputs)

Update 3
Regarding AndrdzejO hint, I've tested the reversed model on the reversed (BGR) images and compared results with the former model on normal (RGB) images. That's strange - they are similar but not the same. Below is the code (in Java) that reverses the image:
  public static byte[] getPixelsBGR(Bitmap image) {
    // calculate how many bytes our image consists of
    int bytes = image.getByteCount();

    ByteBuffer buffer = ByteBuffer.allocate(bytes); // Create a new buffer
    image.copyPixelsToBuffer(buffer); // Move the byte data to the buffer

    byte[] pixels = buffer.array(); // Get the underlying array containing the data.

    // Copy pixels into place
    for (int i = 0; i &lt; pixels.length/4; i++)
    {
      byte pom = pixels[i*4];
      pixels[i * 4] = pixels[i * 4 + 2];
      pixels[i * 4 + 2] = pom;
    }

    return pixels;
  }

if (!modelBGR)
{
    byte[] pixels = getPixelsBGR(resizedBitmap);
    ByteBuffer pixelBuffer = ByteBuffer.wrap(pixels);
    resizedBitmap.copyPixelsFromBuffer(pixelBuffer);
}

**Update 4**

**AndrzejO**'s solution works perfectly. It correctly reverses the channel order. The thing was I had been subtracting channels mean in the tflite metadata and forgot that I need to also reverse the order of channels there. After I've corrected this, I have the exact same results, which implies that the reversion of channels has worked perfectly.

For some reason, reversing the channel order in my case makes inference less accurate (as if though the channels have alreasdy been reversed in some earlier conversion process), but that's a different thing to investigate

",0,367,"You can create a new model: first a lambda layer which will reverse the channel order, than your saved model:
input_shape = old_model.get_layer(index = 0).input_shape[0][1:]
inputs = Input(shape=input_shape)
lambda_layer = Lambda(lambda x: x[:,:,:,::-1])(inputs)
outputs = old_model(lambda_layer)
new_model = Model(inputs=inputs, outputs=outputs)

",,
tensorflow strange result,https://stackoverflow.com/questions/66675299,Pytorch -&gt; [Onnx -&gt; tensorflow] -&gt; tflite generates lots of redundant conv2d operators,"I am converting efficientnet from onnx to tensorflow for further conversion to tflite. The conversion from onnx to tensorflow yields strange results
Onnx has 1 conv2d operator

And after converting to tensorflow and later tflite model I get this stack of convs(All of them couldn't fit in the picture, there are 32 of the in total, as the number of kernels in onnx)

I suspect the issue is with onnx-tf conversion. It seems like the conv2d output are split into 32 separate outputs, then they are processed individually on separate conv2d operator and then concatenated together.
There are 2 connected issues on their github.
First has a fix for depthwise convs but it can't be applied in my case
https://github.com/onnx/onnx-tensorflow/issues/473
https://github.com/onnx/onnx-tensorflow/issues/754
Looking at the source of https://github.com/onnx/onnx-tensorflow/blob/master/onnx_tf/handlers/backend/conv_mixin.py
They do indeed split the output produce separate convolutions and later concatenate them.
Can this split to multiple convolutions be avoided?
",0,1133,"After some additional digging I've found the following

My convs were depthwise(conv2d is depthwise in pytorch and onnx if it has groups parameter &gt; 1)
This bunch of convs is an inefficient way of doing a depthwise conv. To do it efficiently we need to use tf.depthwiseconv

To fix this in onnx-tf v1.7.0 you should apply a patch to onnx-tf source code posted here https://github.com/onnx/onnx-tensorflow/issues/473#issuecomment-533522917
In current master branch there is an attempt to fix the issue by detecting depthwise convs but it currently contains a bug. To fix the bug in master branch you can apply a fix I posted here https://github.com/onnx/onnx-tensorflow/issues/754#issuecomment-801775203
I've used the master branch and the fix above resulting in a small graph with depthwise conv
I've also created a fork with the fix above, so you can do
pip install git+https://github.com/Vozf/onnx-tensorflow

to apply it instead of patching on your own

It seems like the issue should be fixed in the next major release(probably 1.8.0)
Also consider using https://github.com/nerox8664/onnx2keras as it supports depthwise convolutions and moreover supports full nhwc conversion which allows removing all the transpose ops.
",,
tensorflow strange result,https://stackoverflow.com/questions/65512540,TensorFlow: &quot;Expected axis -1 of input shape...&quot;,"import tensorflow.keras as keras
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import OneHotEncoder

if __name__ == ""__main__"":
    file = pd.read_csv(""datensatz.csv"")

    file.loc[file['species'] == 'Iris-setosa', 'species'] = 0
    file.loc[file['species'] == 'Iris-versicolor', 'species'] = 1
    file.loc[file['species'] == 'Iris-virginica', 'species'] = 2

    data = file.to_numpy()

    train_x, test_x, train_y, test_y = train_test_split(data[:, :4], data[:, 4], test_size=0.20)

    encoder = OneHotEncoder(sparse=False)

    train_y = encoder.fit_transform(train_y.reshape(-1, 1))
    test_y = encoder.fit_transform(test_y.reshape(-1, 1))

    model = keras.models.Sequential([
        # sepal length, sepal width, petal length, petal width, species as number
        Dense(units=1, name=""Input"", input_shape=(4,), activation=""relu""),
        Dense(units=4, activation='relu', name=""Hidden""),
        Dense(units=3, activation='softmax', name=""Output"")
    ])

    model.compile(optimizer=keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    model.summary()

    model.fit(train_x.astype('float'), train_y, verbose=2, batch_size=5, epochs=100)
    print('Accuracy:', model.evaluate(test_x.astype('float'), test_y)[1])

    insert = np.array([6.1, 3.0, 4.6, 1.4]).astype('float')
    print(model.predict(x=insert))

Hi guys, I'm trying to predict something, but I get a really strange error(TensorFlow is really known for that!), btw. I use the famous Iris dataset.
At first, I could train the modul, that works fine, as expected.

Epoch 100/100
24/24 - 0s - loss: 0.2863 - accuracy: 0.9333
1/1 [==============================] - 0s 999us/step - loss: 0.2402 - accuracy: 0.9333
That's good enough! The model evaluate works as well.
Accuracy: 0.9333333373069763

My train_x data looks like this:
[[5.9 3.0 4.2 1.5]
 [5.4 3.0 4.5 1.5]
 [5.6 3.0 4.5 1.5]
 [5.7 3.8 1.7 0.3]
 [5.1 3.3 1.7 0.5]
 [6.4 2.9 4.3 1.3]
 [7.7 3.8 6.7 2.2]
 [6.0 3.4 4.5 1.6]
...]

insert = np.array([6.1, 3.0, 4.6, 1.4]).astype('float')
print(model.predict(x=insert))
That dosen't work! I tried it with an element of the testset as well, but that gave me the same result, a really nice and bad to read error.

ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 4 but received input with shape [None, 1]

Frankly I have no idea, what tensorflow is trying to tell me.
How can I fix it? I just want to predict, what flower could be considered.
If anyone is interessted in the dataset:
https://www.kaggle.com/akashsri99/deep-learning-iris-dataset-keras
",0,320,"Predict need batch dimension. Try this:
insert = np.array([[6.1, 3.0, 4.6, 1.4]]).astype('float') 

",,
tensorflow strange result,https://stackoverflow.com/questions/56199905,How to create sum of different kernel objects in TensorFlow Probability?,,0,374,,,
tensorflow strange result,https://stackoverflow.com/questions/52584378,How to execute inference of tensorflow model in Android,,0,459,,,
tensorflow strange result,https://stackoverflow.com/questions/51665533,"validation and training don&#39;t converge at the same time, but validation still converges",,0,754,,,
tensorflow strange result,https://stackoverflow.com/questions/51617315,Tensorflow lite accuracy drop on mobile device,,0,374,,,
tensorflow strange result,https://stackoverflow.com/questions/50512783,TensorRT for upscaling images serves not expected results,,0,799,,,
tensorflow strange result,https://stackoverflow.com/questions/49351711,Saved multi-GPU trained model loaded into single-GPU; inconsistent results,,0,1298,,,
tensorflow strange result,https://stackoverflow.com/questions/47694415,Tensorboard not able to display all summaries,,0,1922,,,
tensorflow strange result,https://stackoverflow.com/questions/46290884,FailedPreconditionError: sequence_length(0) &lt;= X,,0,708,,,
tensorflow strange result,https://stackoverflow.com/questions/42816644,"tensorflow: Strange result from convolution compared to theano (not flipping, though)",,0,518,,,
tensorflow strange result,https://stackoverflow.com/questions/38030552,Basic softmax model implementation on 150x150 images,,0,230,,,
tensorflow strange issue,https://stackoverflow.com/questions/52700621,Tensorflow server: I don&#39;t want to initialize global variables for every session,,15,1780,,,
tensorflow strange issue,https://stackoverflow.com/questions/67045622,tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error,"I am trying to use GPU with Tensorflow. My Tensorflow version is 2.4.1 and I am using Cuda version 11.2. Here is the output of nvidia-smi.
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce MX110       Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   52C    P0    N/A /  N/A |    254MiB /  2004MiB |      8%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1151      G   /usr/lib/xorg/Xorg                 37MiB |
|    0   N/A  N/A      1654      G   /usr/lib/xorg/Xorg                136MiB |
|    0   N/A  N/A      1830      G   /usr/bin/gnome-shell               68MiB |
|    0   N/A  N/A      5443      G   /usr/lib/firefox/firefox            0MiB |
|    0   N/A  N/A      5659      G   /usr/lib/firefox/firefox            0MiB |
+-----------------------------------------------------------------------------+

I am facing a strange issue. Previously when I was trying to list all the physical devices using tf.config.list_physical_devices() it was identifying one cpu and one gpu. AFter that I tried to do a simple matrix multiplication on the GPU. It failed with this error : failed to synchronize cuda stream CUDA_LAUNCH_ERROR (the error code was something like that, I forgot to note it). But after that when I again tried the same thing from another terminal, it failed to recognise any GPU. This time, listing physical devices produce this:
&gt;&gt;&gt; tf.config.list_physical_devices()
2021-04-11 18:56:47.504776: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-04-11 18:56:47.507646: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-04-11 18:56:47.534189: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2021-04-11 18:56:47.534233: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: debadri-HP-Laptop-15g-dr0xxx
2021-04-11 18:56:47.534244: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: debadri-HP-Laptop-15g-dr0xxx
2021-04-11 18:56:47.534356: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.39.0
2021-04-11 18:56:47.534393: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.39.0
2021-04-11 18:56:47.534404: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.39.0
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]

My OS is Ubuntu 20.04, Python version 3.8.5 and Tensorflow , as mentioned before 2.4.1 with Cuda version 11.2. I installed cuda from these instructions. One additional piece of information; when I import tensorflow , it shows the following output:
import tensorflow as tf
2021-04-11 18:56:07.716683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0

What am I missing? Why is it failing to recognise the GPU even though it was recognising previously?
",12,15698,"tldr: Disable Secure Boot before installing the Nvidia Driver.
I had the exact same error, and I spent a ton of time trying to figure out if I had installed Tensorflow related stuff incorrectly. After many hours of problem solving, I found that my NVIDIA driver was having some problems because I never disabled secure boot in my BIOS when setting up Ubuntu 20.4. Here's what I suggest (I opted for using Docker w/ Tensorflow, which avoids having to install all theCuda related stuff) - I hope it works for you!

Disable Secure Boot in your BIOS
Make a fresh install on Ubuntu 20.4
Install Docker according to nvidia-container-toolkit's page.

curl https://get.docker.com | sh \
  &amp;&amp; sudo systemctl --now enable docker


Install nvidia-container-toolkit from the same page.

distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \
   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update

sudo apt-get install -y nvidia-docker2

sudo systemctl restart docker


Test to make sure that's working with

sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi


Finally, use Tensorflow with Docker w/ GPU support!

docker run --gpus all -u $(id -u):$(id -g) -it -p 8888:8888 tensorflow/tensorflow:latest-gpu-jupyter jupyter notebook --ip=0.0.0.0

","Disabling Secure Boot solved the problem immediately. No need to reinstall anything.
&gt; import tensorflow as tf
&gt; tf.config.list_physical_devices(""GPU"")
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

","I just made an account to say that @Nate's answer worked for me.
I have the exact same setting as you and have been trying for two days.
What I did in the end was
Reboot - F10 to the setting - Security - BIOS Secure Boot (or something like that I don't remember exactly) - Disabled
Then there was some extra steps with the confirmation but it worked fine. I did not re-install the whole Unbuntu. It was a bit too technically risky for me.
Then I tried the tf.config line and I got this:
2021-06-14 17:12:19.546509: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1

2021-06-14 17:12:26.754680: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

2021-06-14 17:12:26.909679: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3593460000 Hz

2021-06-14 17:12:26.910016: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a8352501c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:

2021-06-14 17:12:26.910040: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version

2021-06-14 17:12:26.972350: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1

2021-06-14 17:12:27.074861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2021-06-14 17:12:27.075289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:0c:00.0 name: GeForce GTX 1650 computeCapability: 7.5
coreClock: 1.665GHz coreCount: 14 deviceMemorySize: 3.81GiB deviceMemoryBandwidth: 119.24GiB/s

There are more red lines on devices properties towards the end but I got
Default GPU Device: /device:GPU:0

Don't know why it works, but it works. Just change the security boot setting.
I don't have enough experience points to upvote Nate's answer. I will come back later. But he/she really offers a good solution.
"
tensorflow strange issue,https://stackoverflow.com/questions/57122907,Tensorboard AttributeError: &#39;ModelCheckpoint&#39; object has no attribute &#39;on_train_batch_begin&#39;,"I'm currently using Tensorboard using the below callback as outlined by this SO post as shown below.

from keras.callbacks import ModelCheckpoint

CHECKPOINT_FILE_PATH = '/{}_checkpoint.h5'.format(MODEL_NAME)
checkpoint = ModelCheckpoint(CHECKPOINT_FILE_PATH, monitor='val_acc', verbose=1, save_best_only=True, mode='max', period=1)


When I run Keras' dense net model, I get the following error. I haven't had any issues running Tensorboard in this manner with any of my other models, which makes this error very strange. According to this Github post, the official solution is to use the official Tensorboard implementation; however, this requires upgrading to Tensorflow 2.0, which is not ideal for me. Anyone know why I'm getting the following error for this specific densenet and is there a workaround/fix that someone knows?


  AttributeError                            Traceback (most recent call
  last)  in ()
       26                     batch_size=32,
       27                     class_weight=class_weights_dict,
  ---&gt; 28                     callbacks=callbacks_list
       29                    )
       30 
  
  2 frames
  /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py
  in _call_batch_hook(self, mode, hook, batch, logs)
      245     t_before_callbacks = time.time()
      246     for callback in self.callbacks:
  --&gt; 247       batch_hook = getattr(callback, hook_name)
      248       batch_hook(batch, logs)
      249     self._delta_ts[hook_name].append(time.time() - t_before_callbacks)
  
  AttributeError: 'ModelCheckpoint' object has no attribute
  'on_train_batch_begin'


The dense net I'm running

from tensorflow.keras import layers, Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.densenet import preprocess_input, DenseNet121
from keras.optimizers import SGD, Adagrad
from keras.utils.np_utils import to_categorical

IMG_SIZE = 256
NUM_CLASSES = 5
NUM_EPOCHS = 100

x_train = np.asarray(x_train)
x_test = np.asarray(x_test)

y_train = to_categorical(y_train, NUM_CLASSES)
y_test = to_categorical(y_test, NUM_CLASSES)


x_train = x_train.reshape(x_train.shape[0], IMG_SIZE, IMG_SIZE, 3)
x_test = x_test.reshape(x_test.shape[0], IMG_SIZE, IMG_SIZE, 3)

densenet = DenseNet121(
    include_top=False,
    input_shape=(IMG_SIZE, IMG_SIZE, 3)
)

model = Sequential()
model.add(densenet)
model.add(layers.GlobalAveragePooling2D())
model.add(layers.Dense(NUM_CLASSES, activation='softmax'))
model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model.fit(x_train,
                    y_train,
                    epochs=NUM_EPOCHS,
                    validation_data=(x_test, y_test),
                    batch_size=32,
                    class_weight=class_weights_dict,
                    callbacks=callbacks_list
                   )

",10,14410,"I replace this line 

from keras.callbacks import EarlyStopping, ModelCheckpoint


To this line 

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

","Yes imports are mixed from keras and tensorflow 

try sticking on to tensorflow.keras for example :

from tensorflow.keras.callbacks import EarlyStopping

","Make all imports from either keras or tensorflow.keras

I hope this will sort it out!
"
tensorflow strange issue,https://stackoverflow.com/questions/65402617,Tensorflow automl model in react,"I'm trying to move a tensorflow model from its original html into a react app (built with create-react-app).
My App.js looks like this:
import logo from './logo.svg';
import * as tf from ""@tensorflow/tfjs"";
// import { loadImageclassification } from ""@tensorflow/tfjs"";
import './App.css';
import * as automl from ""@tensorflow/tfjs-automl"";
import * as modelJSON from './model.json';

function App() {

var loadFile = function(event) {
    var image = document.getElementById('output');
    image.src = URL.createObjectURL(event.target.files[0]);
  run();
};

async function run() {
  console.log(modelJSON);
        // const model = await tf.loadImageclassification('model.json');
        const model = await automl.loadImageClassification(modelJSON);
        const image = document.getElementById('output');
        const predictions = await model.classify(image);
        console.log(predictions);

        const pre = document.getElementById('result');
        pre.textContent = JSON.stringify(predictions, null, 2);
}

  return (
  &lt;div className=""App""&gt;
    &lt;div className=""hero-text""&gt;
      &lt;h1&gt;classifier&lt;/h1&gt;
      &lt;h3&gt;Upload a picture to see what type it is! &lt;/h3&gt;
      &lt;p&gt;
        &lt;input type=""file""  accept=""image/*"" name=""image"" id=""file""  onChange={loadFile} /&gt;
      &lt;/p&gt;
      &lt;div id=""demobox""&gt;
        &lt;p&gt;
          &lt;label htmlFor=""file""&gt;Upload your image&lt;/label&gt;
        &lt;/p&gt;
      &lt;/div&gt; 
      &lt;p&gt;&lt;img id=""output"" width=""200"" alt=""output"" /&gt;&lt;/p&gt;
      &lt;div className=""result"" id=""result""&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  );
}

export default App;

My index.html looks like this:
&lt;!DOCTYPE html&gt;
&lt;html lang=""en""&gt;
  &lt;head&gt;
    &lt;meta charset=""utf-8"" /&gt;
    &lt;link rel=""icon"" href=""%PUBLIC_URL%/favicon.ico"" /&gt;
    &lt;meta name=""viewport"" content=""width=device-width, initial-scale=1"" /&gt;
    &lt;meta name=""theme-color"" content=""#000000"" /&gt;
    &lt;meta
      name=""description""
      content=""Web site created using create-react-app""
    /&gt;
    &lt;link rel=""apple-touch-icon"" href=""%PUBLIC_URL%/logo192.png"" /&gt;
    &lt;link rel=""manifest"" href=""%PUBLIC_URL%/manifest.json"" /&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;noscript&gt;You need to enable JavaScript to run this app.&lt;/noscript&gt;
    &lt;div id=""root""&gt;&lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;

I am getting the following error, which seems to be issuing from somewhere in the loadImageClassification method:
Unhandled Rejection (TypeError): modelUrl.lastIndexOf is not a function

Edit:
Apparently loadImageClassification uses a fetch request under the hood and so requires a remote file (which is strange, because it seemed to work fine in the static index.html original version of this same project).
So I am now trying it just with a localhost express server, which at present looks like this:
const modelJSON = require('./model.json');

const express = require(""express"");
const bodyParser = require(""body-parser"");
const CORS = require(""cors"");

const app = express();

app.use(bodyParser.json());
app.use(CORS());

let modelObj = modelJSON;

app.get(""/"", (req, res) =&gt; {
  // console.log(modelObj);
  res.send(modelObj);
});

app.listen(5000, () =&gt; {
  console.log(""Server listening on port 5000"");
});


I can see the correct data when I navigate to localhost5000, but when I change
async function run() {
  const model = await automl.loadImageClassification(modelJSON);

to
async function run() {
  const modelUrl = ""http://localhost:5000/"";
  const model = await automl.loadImageClassification(modelUrl);

I get these errors:

Edit 2:
My server.js file now looks like this:

This produces the same errors as in the previous screenshot. (I am leaving in the comments that mess of an attempt to include all the shard files in this server.js file screenshot just because it may illustrate that I don't understand how to pass those ancillary model files to loadImageClassification when it makes its fetch request.)
So presumably the problem now has to do with the fact that loadImageClassification assumes that the ...shard__of6.bin and dict files are in the same directory as the model.json file.
So the question may (?) be: how to simulate the file structure that it (i.e., loadImageClassification) is expecting within a remote node server.
Fundamental confusion:
I'm don't understand why, when loadImageClassification is in the original static html, it does not seem to require a remote url from which to fetch model.json  but then when I put it in my react app, it suddenly gives me this error: ""Fetch API cannot load file:///Users///client/src/model.json. URL scheme must be 'http' or 'https' for CORS request.""
",8,730,"What's the location of the model on your local device?
Try changing
const modelUrl = ""http://localhost:5000/""

to
const modelUrl = 'model/model.json'

if the model is in build/model/, or to whatever the location is.
",,
tensorflow strange issue,https://stackoverflow.com/questions/46355068,keras loss function for 360 degree prediction,"I'm trying to predict azimuths using keras/tensorflow.  y_true ranges from 0-359, but I need a loss function that handles predictions that have wrapped around and are outside that range.  Unfortunately, when I try any kind of modular division tf.mod() or %, i get an error...

LookupError: No gradient defined for operation 'FloorMod' (op type: FloorMod)


so I think I've worked around this with the following...

def mean_squared_error_360(y_true, y_pred):
  delta = K.minimum(K.minimum(K.abs(y_pred - y_true),
                              K.abs(y_pred - (360+y_true))),
                              K.abs(y_true - (360+y_pred)))
  return K.mean(K.square(delta), axis=-1)

def rmse_360(y_true, y_pred):
  return K.sqrt(mean_squared_error_360(y_true, y_pred))


model.compile(loss=mean_squared_error_360,
              optimizer=rmsprop(lr=0.0001),
              metrics=[rmse_360])


this handles the following edge cases...  I haven't come across predictions &lt; 0, so I'm not addressing.

y =   1  y_pred = 361  err = 0
y = 359  y_pred =   1  err = 2
y = 359  y_pred = 361  err = 2


Questions


this feels clunky; is there a smarter solution?
intuitively, I think there's no difference in outcome between using mean_squared_error and root_mean_squared_error as the loss...  the gradients will be different, but the same optimum weights will solve both, right?  Is there any reason to pick one over the other?  I'd guess mse is slightly simpler than rmse, but that should be trivial.  I've tried both, and using rmse 'feels' like a more orderly descent than mse...  is there something about the magnitude of those squared errors that make it jump around more?


Thanks in advance.

EDIT

for whatever reason...  my original mse seemed to be fitting the training set, but the validation set seemed pretty noisy epoch to epoch, without any real improvement after a few epochs.  rmse seemed like a more orderly descent...  until the loss went to inf after improving for a couple dozen epochs.  I might have bigger issues than the loss function.

EDIT 2 - adding my implementation @Patwie answer below

ah...  trig!!  of course!!  unfortunately, I'm using tf v1.0 which doesn't seem to have tf.atan2().  strangely, I couldn't find atan2 implementation in tf repository, but I think asos-ben's suggestion in issue 6095 does the trick.  see here:  https://github.com/tensorflow/tensorflow/issues/6095

def atan2(x, y, epsilon=1.0e-12):
  x = tf.where(tf.equal(x, 0.0), x+epsilon, x)
  y = tf.where(tf.equal(y, 0.0), y+epsilon, y)    
  angle = tf.where(tf.greater(x,0.0), tf.atan(y/x), tf.zeros_like(x))
  angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)
  angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)
  angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)
  angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)
  angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), tf.zeros_like(x), angle)
  return angle

# y in radians
def rmse_360_2(y_true, y_pred):
  return K.mean(K.abs(atan2(K.sin(y_true - y_pred), K.cos(y_true - y_pred))))


Only about 7 epochs in on a test run, but it seems promising.
",8,2817,,,
tensorflow strange issue,https://stackoverflow.com/questions/47743936,Tensorflow ResourceExhaustedError after first batch,,7,3791,,,
tensorflow strange issue,https://stackoverflow.com/questions/37956242,Saving a collection of variable length tensors to a TFRecords file in TensorFlow,,6,2649,,,
tensorflow strange issue,https://stackoverflow.com/questions/42029046,Tensorflow batch loss spikes when restoring model for training from saved checkpoint?,,6,1860,"I have also seen this issue in PyTorch, resulting from restoring model weights but not optimizer parameters. For example, the Adam optimizer stores estimates of the first and second moments of the gradient. If you only reload the model and don't save/reload the optimizer, the momentum parameters are reinitialized with the optimizer, which often causes a spike in the loss.
",,
tensorflow strange issue,https://stackoverflow.com/questions/33699174,TensorFlow&#39;s ReluGrad claims input is not finite,,6,4952,,,
tensorflow strange issue,https://stackoverflow.com/questions/42473052,TensorFlow 1.0 does not see GPU on Windows (but Theano does),,5,1457,,,
tensorflow strange issue,https://stackoverflow.com/questions/62627157,Failed to run the tflite model on Interpreter due to Internal Error,"I am trying to build an offline translator for android. My model is highly inspired from this guide: https://www.tensorflow.org/tutorials/text/nmt_with_attention. I just did some modifications to make sure the model is serialisable. (You can find the code for the model at the end)
The model works perfectly on my jupyter notebook. I am using Tensorflow version: 2.3.0-dev20200617, I also was able to generate the tflite file using the following snippet:
converter = tf.lite.TFLiteConverter.from_keras_model(partial_model)
tflite_model = converter.convert()

with tf.io.gfile.GFile('goog_nmt_v2.tflite', 'wb') as f:
  f.write(tflite_model)

However when I used the generated tflite model to get predictions on android, it throws the error java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/concatenation.cc:73 t-&gt;dims-&gt;data[d] != t0-&gt;dims-&gt;data[d] (8 != 1) Node number 84 (CONCATENATION) failed to prepare.
This is strange because I have provided the exact same input dimensions as I did in my jupyter notebook. Here is the java code that is used to test (dummy inputs) if model runs on android:
 HashMap&lt;Integer, Object&gt; outputVal = new HashMap&lt;&gt;();
        for(int i=0; i&lt;2; i++) outputVal.put(i, new float[1][5]);
        float[][] inp_test = new float[1][8];
        float[][] enc_hidden = new float[1][1024];
        float[][] dec_input = new float[1][1];
        float[][] dec_test = new float[1][8];

        tfLite.runForMultipleInputsOutputs(new Object[] {inp_test,enc_hidden, dec_input, dec_test},outputVal);

And here are my gradle dependencies:
dependencies {
    implementation fileTree(dir: 'libs', include: ['*.jar'])

    implementation 'androidx.appcompat:appcompat:1.1.0'
    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'
    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'
    // This dependency adds the necessary TF op support.
    implementation 'androidx.constraintlayout:constraintlayout:1.1.3'
    testImplementation 'junit:junit:4.12'
    androidTestImplementation 'androidx.test.ext:junit:1.1.1'
    androidTestImplementation 'androidx.test.espresso:espresso-core:3.2.0'
}

As the error pointed, there was something wrong with dimensions at node 84. So I went ahead and visualised the tflite file using Netron. I have zoomed the concatenation node, you can find the pic of the node along with input and output dimensions here. You can find the whole generated graph here.
As it turns out, the concatenation node at location 84 isn't actually concatenating, you can see this from the input and output dimensions. It just spits out a 1X1X1 matrix after processing 1X1X1 and 1X1X256 matrix. I know tflite graph isn't same as the original model graph since a lot of operations are replaced and even removed for optimisations but this seems a little odd.
I can't relate this to the error. And if it runs prefectly on jupyter, is it a framework issue or am I missing something? Also, could anyone please explain me what does the error mean by t-&gt;dims-&gt;data[d] != t0-&gt;dims-&gt;data[d] what is d?
Please if you have answers to even any one of the question, please write it. If you require any extra details please let me know.
Here is the code for the model:

Tx = 8
def Partial_model():
    outputs = []
    X = tf.keras.layers.Input(shape=(Tx,))
    partial = tf.keras.layers.Input(shape=(Tx,))
    enc_hidden = tf.keras.layers.Input(shape=(units,))
    dec_input = tf.keras.layers.Input(shape=(1,))
    
    d_i = dec_input
    e_h = enc_hidden
    X_i = X
    
    enc_output, e_h = encoder(X, enc_hidden)
    
    
    dec_hidden = enc_hidden
    print(dec_input.shape, 'inp', dec_hidden.shape, 'dec_hidd')
    for t in range(1, Tx):
        print(t, 'tt')
      # passing enc_output to the decoder
        predictions, dec_hidden, _ = decoder(d_i, dec_hidden, enc_output)
#         outputs.append(predictions)
        print(predictions.shape, 'pred')
        d_i = tf.reshape(partial[:, t], (-1, 1))
        print(dec_input.shape, 'dec_input')
    
    predictions, dec_hidden, _ = decoder(d_i, dec_hidden, enc_output)
    d_i = tf.squeeze(d_i)
    
    outputs.append(tf.math.top_k(predictions, 5))
    
    return tf.keras.Model(inputs = [X, enc_hidden, dec_input, partial], outputs = [outputs[0][0], outputs[0][1]])




class Encoder():
  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
    self.batch_sz = batch_sz
    self.enc_units = enc_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.enc_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')

  def __call__(self, x, hidden):
    x = self.embedding(x)
    output, state = self.gru(x, initial_state = hidden)
    print(output.shape, hidden.shape, ""out"", ""hid"")
    return output, state


  def initialize_hidden_state(self):
    return tf.zeros((self.batch_sz, self.enc_units))



class BahdanauAttention():
  def __init__(self, units):
    self.W1 = tf.keras.layers.Dense(units)
    self.W2 = tf.keras.layers.Dense(units)
    self.V = tf.keras.layers.Dense(1)

  def __call__(self, query, values):
    # query hidden state shape == (batch_size, hidden size)
    # query_with_time_axis shape == (batch_size, 1, hidden size)
    # values shape == (batch_size, max_len, hidden size)
    # we are doing this to broadcast addition along the time axis to calculate the score
    print(query.shape, 'shape')
    query_with_time_axis = tf.expand_dims(query, 1)
    # score shape == (batch_size, max_length, 1)
    # we get 1 at the last axis because we are applying score to self.V
    # the shape of the tensor before applying self.V is (batch_size, max_length, units)
    print(""2"")
    score = self.V(tf.nn.tanh(
        self.W1(query_with_time_axis) + self.W2(values)))
    print(""3"")

    # attention_weights shape == (batch_size, max_length, 1)
    attention_weights = tf.nn.softmax(score, axis=1)

    # context_vector shape after sum == (batch_size, hidden_size)
    context_vector = attention_weights * values
    context_vector = tf.reduce_sum(context_vector, axis=1)
    
    return context_vector, attention_weights


class Decoder():
  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):
    self.dec_units = dec_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.dec_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')
    self.fc = tf.keras.layers.Dense(vocab_size)

    # used for attention
    self.attention = BahdanauAttention(self.dec_units)

  def __call__(self, x, hidden, enc_output):
    # enc_output shape == (batch_size, max_length, hidden_size)
    context_vector, attention_weights = self.attention(hidden, enc_output)
    
    print(context_vector.shape, 'c_v', attention_weights.shape, ""attention_w"")

    # x shape after passing through embedding == (batch_size, 1, embedding_dim)
    x = self.embedding(x)

    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)
    print(x.shape, 'xshape', context_vector.shape, 'context')
    expanded_dims = tf.expand_dims(context_vector, 1)
    x = tf.concat([expanded_dims, x], axis=-1)

    # passing the concatenated vector to the GRU
    output, state = self.gru(x)

    # output shape == (batch_size * 1, hidden_size)
    output = tf.reshape(output, (-1, output.shape[2]))

    # output shape == (batch_size, vocab)
    x = self.fc(output)

    return x, state, attention_weights





",5,9433,"You can load the generated .tflite file inside python notebook and pass the same inputs as at Keras model. You have to see the exact outputs because during conversion of model there is no loss of accuracy. If there is a problem there...there will be problem during android operations. If not...everything will work fine. Use below code from Tensorflow guide to run inference in Python:
import numpy as np
import tensorflow as tf

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""converted_model.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)

Happy coding!
",,
tensorflow strange issue,https://stackoverflow.com/questions/47947629,"Tensorflow: Keras, Estimators and custom input function",,5,1544,,,
tensorflow strange issue,https://stackoverflow.com/questions/41079660,Why I need a password to get access to the jupyter notebook when I use docker machine?,,5,1734,,,
tensorflow strange issue,https://stackoverflow.com/questions/64645579,Keras: custom data validation callback on training data always returns validation data results,,4,1136,"It seems that this is a bug in tensorflow versions 2.3.x (tested with 2.3.0 and 2.3.1). In versions 2.4.0-rc0 and 2.2.1, the loss outputs of loss and custom_loss differ, which is the expected behavior:
{'custom_loss': [0.7694963216781616, 0.541864812374115],
'loss': [0.9665887951850891, 0.6637843251228333],
'val_loss': [0.7624925374984741, 0.5331208109855652]}

",,
tensorflow strange issue,https://stackoverflow.com/questions/56660164,"Jupyter Notebook, NameError: is not defined, %%time prevents assignment",,4,3006,,,
tensorflow strange issue,https://stackoverflow.com/questions/47962156,Unable to build Tensorflow from source MacOS High Sierra,,4,716,,,
tensorflow strange issue,https://stackoverflow.com/questions/45644606,Illegal ambiguous match on configurable attribute &quot;deps&quot; in //tensorflow/core/grappler/costs:utils: when trying to build Tensorflow with GPU support,,4,2231,,,
tensorflow strange issue,https://stackoverflow.com/questions/46580112,R-squared results of test and validation differ by a huge margin,,3,4042,"Tensorflow's built-in evaluate method evaluates your test set batch by batch and hence calculates r2 at each batch. The metrics produced from model.evaluate() is then simple average of all r2 from each batch. While in model.fit(), r2 (and all metrics on validation set) are calculated per epoch (instead of per batch and then take avg.)
You may slice your output and output_pred into batches of the same batch size you used in model.evaluate() and calculate r2 on each batch. I guess the model produces high r2 on batches with high total sum of squares (SS_tot) and bad r2 on lower ones. So when taken average, result would be poor (however when calculate r2 on entire dataset, samples with higher ss_tot usually dominate the result).
",,
tensorflow strange issue,https://stackoverflow.com/questions/55772880,TF 2.0 - Method estimator.model_to_estimator( ) fails but model.fit works for tf.keras created model,,3,408,,,
tensorflow strange issue,https://stackoverflow.com/questions/57311222,Limitation of number of predictions in Tensorflow Object Detection API,"Ive trained a Faster R-CNN model with Tensorflow Object Detection API with and encountered a strange issue. The output of the model has max 100 predictions, despite, there are many more objects in the image. This is a case for each image Ive tested.

Ive found similar issue on Ten GitHub, but from what I can see they are not doing much in these regards.
https://github.com/tensorflow/tensorflow/issues/30464

Maybe you had a similar issue in the past? Any idea how to tackle this?


",3,1970,,,
tensorflow strange issue,https://stackoverflow.com/questions/40406469,"tf.train.range_input_producer(epoch_size, shuffle=True) does not terminate nor induce CPU/GPU load",,3,1348,,,
tensorflow strange issue,https://stackoverflow.com/questions/39774449,Fully-convolutional ResNets using TF-Slim run very slow,,3,2950,,,
tensorflow strange issue,https://stackoverflow.com/questions/39540806,Tensorflow - Any input gives me same output,"I am facing a very strange problem where I am building an RNN model using tensorflow and then storing the model variables (all) using tf.Saver after I finish training.

During testing, I just build the inference part again and restore the variables to the graph. The restoration part does not give any error.

But when I start testing on the evaluation test, I always get same output from the inference all i.e. for all test inputs, I get the same output.

I printed the output during training and I do see that output is different for different training samples and cost is also decreasing.
But when I do testing, it always gives me same output no matter what is the input.

Can someone help me to understand why this could be happening? I want to post some minimal example but as I am not getting any error, I am not sure what should I post here. I will be happy to share more information if it can help the issue. 

One difference I have between the inference graph during training and testing is the number of time steps in RNN. During training I train for n steps (n = 20 or more) for a batch before updating gradients while for testing  I just use one step as I only want to predict for that input.

Thanks
",3,1394,,,
tensorflow strange issue,https://stackoverflow.com/questions/36807107,Strange error when taking gradient of TensorArray,,2,1058,,,
tensorflow strange issue,https://stackoverflow.com/questions/73415068,"HuggingFace Trainer() does nothing - only on Vertex AI workbench, works on colab","I am having issues getting the Trainer() function in huggingface to actually do anything on Vertex AI workbench notebooks.
I'm totally stumped and have no idea how to even begin to try debug this.
I made this small notebook: https://github.com/andrewm4894/colabs/blob/master/huggingface_text_classification_quickstart.ipynb
If you set framework=pytorch and run it in colab it runs fine.
I wanted to move from colab to something more persistent so tried Vertex AI Workbench notebooks on GCP. I created a user managed notebook (PyTorch:1.11, 8 vCPUs, 30 GB RAM, NVIDIA Tesla T4 x 1) and if i try run the same example notebook in jupyterlab on the notebook it just seems to hang on the Trainer() call and do nothing.
It looks like the GPU is not doing anything either for some reason (it might not be supposed to since i think Trainer() is some pretraining step):
(base) jupyter@pytorch-1-11-20220819-104457:~$ nvidia-smi
Fri Aug 19 09:56:10 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   41C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

I found this thread that maybe seems like a similar problem so i played with as many Trainer() args as i could but no luck.
So im kind of totally blocked here - i refactored the code to be able to use Tensorflow which does work for me (after i installed tensorflow on the notebook) but its much slower for some reason.
Basically this was all working great (in my actual real code im working on) on colab's but when i tried to move to Vertex AI Notebooks i seem to be now blocked by this strange issue.
Any help or advice much appreciated, i'm new to HuggingFace and Pytorch etc too so not even sure what things i might try or ways to try run in debug etc maybe.
Workaround
i noticed that if i make a new workbook NumPy/SciPy/scikit-learn 4 vCPUs, 15 GB RAM , NVIDIA Tesla T4 x (instead of the official pytorch one from the dropdown) and install pytorch myself with conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch it all works.
",2,436,"We faced exactly the same issue on Vertex AI, when we selected the pre-built PyTorch=1.13 environment. The training would just freeze and do nothing for hours.
What worked for us was to select an older version of the PyTorch environment. So we went for the PyTorch=1.9 environment and the training worked just fine.

We could see the logs already as the training commenced. So my guess is that there's something specifically about the newer PyTorch environment on Vertex AI that prevents things from working as expected.
",,
tensorflow strange issue,https://stackoverflow.com/questions/55235620,After installing Jupyter Notebook NumPy and TensorFlow are not working,,2,896,,,
tensorflow strange issue,https://stackoverflow.com/questions/42422646,Keras train partial model issue (about GAN model),,2,6004,,,
tensorflow strange issue,https://stackoverflow.com/questions/71475575,cudf instllation issue on centos7,"I'm new to rapids ai libraries. I've an existing conda environment yaml file where I'm using python 3.8.5, tensorflow 2.7.0, opencv-python-headless 4.5.5.62, numpy 1.22.2, pandas 1.4.1, pandas-profiling 3.1.0, seaborn 0.11.2,  matplotlib 3.5.1, jupyterlab 3.2.9.
I've added below 2 channels to the file:

rapidsai
nvidia

And the below packages:

cudf=22.02
cudatoolkit=11.5

The installation is going on for hours and while trying to find incompatible packages, it seems to be in some sort of loop as I keep seeing below message multiple times in the terminal:
Found conflicts! Looking for incompatible packages.
Is there any known issue/limitations that I should be aware of?
Since we don't get interactive shell on GPU h/w easily, I'm trying the conda environment update on non-GPU machine and once installed, I'll try cudf package on GPU machine.
EDIT1:
This is what I have as working without tensorflow and tensorflow-hub

    name: cudf-env
        channels:
          - default
          - rmg
          - rapidsai
          - nvidia
          - numba
          - conda-forge
          - anaconda
        dependencies:
          - glibc=2.19
          - libgcc-ng=11.2.0
          - python=3.8.5
          - cudf=22.02
          - cudatoolkit=11.2
          - pytest=6.1.2
          - pandas=1.3.5
          - numpy=1.21.5
          - requests=2.25.0
          - scikit-learn=0.24.2
          - dill=0.3.4
          - tqdm=4.62.3
          - ruamel.yaml=0.17.19
          - yappi=1.3.3
          - black=22.1.0
          - pillow=9.0.1
          - jupyterlab=3.2.9
          - matplotlib=3.5.1
          - seaborn=0.11.2
          - plotly=5.6.0
          - pandas-profiling=3.1.0
          - black=22.1.0
        #  - pip
        #  - pip:
        #      - tensorflow==2.7.0
        #      - tensorflow-hub==0.12.0
        #      - opencv-python-headless==4.5.5.62
        #      - opencv-contrib-python-headless==4.5.5.62

Now, if I uncomment the pip section, the anaconda crashes while creating the environment. Since pip may not be supported with cudf, I tried following as well, the conda create env hangs while solving the environment (strangely, it's not resolving from conda-forge channel):

    name: cudf-env
        channels:
          - default
          - rmg
          - rapidsai
          - nvidia
          - numba
          - conda-forge
          - anaconda
        dependencies:
          - glibc=2.19
          - libgcc-ng=11.2.0
          - python=3.8.5
          - cudf=22.02
          - cudatoolkit=11.2
          - pytest=6.1.2
          - pandas=1.3.5
          - numpy=1.21.5
          - requests=2.25.0
          - scikit-learn=0.24.2
          - dill=0.3.4
          - tqdm=4.62.3
          - ruamel.yaml=0.17.19
          - yappi=1.3.3
          - black=22.1.0
          - pillow=9.0.1
          - jupyterlab=3.2.9
          - matplotlib=3.5.1
          - seaborn=0.11.2
          - plotly=5.6.0
          - pandas-profiling=3.1.0
          - black=22.1.0
          - tensorflow
          - tensorflow-hub

My system details are following:
$ cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""

$ uname -r
3.10.0-1127.10.1.el7.x86_64

EDIT2: I forgot to mention that if I comment out glibc, cudf and cudatoolkit, the tensorflow installation through pip works fine.
",1,463,"Challenges updating conda environments can be tricky to untangle, but in this case the issue (in terms of cuDF) is likely the pinning of pandas to 1.4.1.
cuDF does not yet support pandas=1.4.1. The cuDF nightly packages currently support pandas &gt;=1.0,&lt;1.4.0dev0 (everything from 1.0 up to a dev build of 1.4.0). For the moment, if you switch to
pandas=1.3.5 things will work (assuming the other packages are compatible -- Tensorflow likely will require CUDA Toolkit 11.2, not 11.5, and some others might not be either). You can also let conda solve for the appropriate pandas version.
","I was finally able to complete the installation of cudf along with all the required packages after I reimaged my machine to Ubuntu20.04.
And I dropped below 2 packages as with Ubuntu20.04 I don't need those -
 - glibc=2.19
 - libgcc-ng=11.2.0

",
tensorflow strange issue,https://stackoverflow.com/questions/66797246,Why is Tensorflow GPU extremely slow when creating models and training models compared to the CPU version?,"I would first like to give you some information about how I installed tensorflow and other packages before explaining the problem. It took me a lot of time to get tensorflow running on my GPU (Nvidia RTX 3070, Windows 10 system). First, I installed Cuda (v.10.1), downloaded CuDDN (v7.6) and copied and pasted the CuDNN files to the correct Cuda installation folders (as described here: https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-windows)
I want to use tensorflow 2.3.0 and checked if the Cuda and cuDNN versions are compatible using the table on this page: https://www.tensorflow.org/install/source
Then I opened the anaconda prompt window, activated my new environment (&gt;&gt; activate [MyEnv]) and installed the required packages. I read that it is important to install tensorflow first, so the first package I installed was tensorflow-gpu, followed by a bunch of other packages. Later, I ran into the problem that my GPU was not found when I typed in
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))

The response was ""Num GPUs Available: 0""
I did a lot of googling and found the following discussion:
https://github.com/ContinuumIO/anaconda-issues/issues/12194#issuecomment-751700156
where it is mentioned that a faulty tensorflow build is installed when using conda install tensorflow-gpu in the anaconda prompt window. Instead (when using Pythin 3.8, as I do), one has to use pass the correct tensorflow build in the prompt window. So I set up a new environment and used
conda install tensorflow-gpu=2.3 tensorflow=2.3=mkl_py38h1fcfbd6_0

to install tensorflow. So far so good. Now, the cudatoolkit (version 10.1.243) and cudnn (version 7.6.5), which were missing in my first environment, are inculded in the the tensorflow package and thus in my second environment [MyEnv2].
I start VSCode, select the correct environment, and retest if the gpu can be found by repeating the test:
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
And ...it works. The GPU is found and everything looks good at the first sight.
So what's the problem?
Using tensorflow on gpu is extremly slow. Not only when training models, but also when creating the model with
model = models.Sequential()
model.add(...)
(...)
model.summary()

Running the same code sample on CPU finishes almost immediately, wheras running the code on GPU needs more than 10 minutes! (When I look into the taskmanager performance tab nothing happens. Neither CPU nor GPU seems to do anything, when I run the Code on the GPU!) And this happens, when just creating the model without training!
After compiling the model and starting the training, the same problem occurs. Training on the CPU gives me a immediate feedback about the epoch process, while training on gpu seems to freeze the program as nothing happens for several minutes (maybe ""freezing"" is the wrong word, because I can still switch between the tabs in VSCode. The program itself is not freezing) Another confusing aspect is that when training on the gpu, I only get nans for the loss and mae when the training finally starts after minutes of waiting. In the task manager I can obeserve that the model needs about 7,5GB of VRAM. The RTX3070 comes with 8GB of VRAM. When I run the same code on the cpu, loss and mae look perfectly fine...
I really have no idea what is happening and why I am getting this strange behaviour when I run tensorflow on my gpu. Do you have any ideas?
",1,2680,"Nvidia RTX 3070 cards are based on the Ampere architecture for which compatible CUDA version start with 11.x.
You can upgrade tensorflow from 2.3 to 2.4, because this version supports Ampere architecture.
So to get benefit from your GPU card, the compatible combination are TF 2.4, CUDA 11.0 and cuDNN is 8.0.
",,
tensorflow strange issue,https://stackoverflow.com/questions/66119206,Could not load dynamic library &#39;libcusparse.so.11&#39;; dlerror: /usr/local/cuda-11.0/lib64/libcusparse.so.11: file too short; LD_LIBRARY_PATH:,"I intalled on my pc tensorflow 2.4.1 and try to test it. I found some simple script at  github that train model on mnist:
'''Trains a simple convnet on the MNIST dataset.
Gets to 99.25% test accuracy after 12 epochs
(there is still a lot of margin for parameter tuning).
16 seconds per epoch on a GRID K520 GPU.
'''

from __future__ import print_function
import tensorflow
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras import backend as K

batch_size = 128
num_classes = 10
epochs = 12

# input image dimensions
img_rows, img_cols = 28, 28

# the data, shuffled and split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)
y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=input_shape))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,
              optimizer=tensorflow.keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

Script works,but does not achieve specified accuracy of 99.25%, and there is some issues. The output is the next:
2021-02-09 13:14:27.697775: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
2021-02-09 13:14:28.833228: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-09 13:14:28.833686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-02-09 13:14:28.860694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-02-09 13:14:28.861019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.77GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2021-02-09 13:14:28.861035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-02-09 13:14:28.862265: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-02-09 13:14:28.862292: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-02-09 13:14:28.862870: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-02-09 13:14:28.862986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-02-09 13:14:28.864383: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-02-09 13:14:28.864417: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: /usr/local/cuda-11.0/lib64/libcusparse.so.11: file too short; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64
2021-02-09 13:14:28.864476: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-02-09 13:14:28.864484: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-02-09 13:14:28.864653: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-02-09 13:14:28.865004: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-09 13:14:28.865020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-09 13:14:28.865026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      
2021-02-09 13:14:28.984319: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-02-09 13:14:28.984924: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2899885000 Hz
Epoch 1/12
469/469 [==============================] - 22s 46ms/step - loss: 2.2985 - accuracy: 0.1157 - val_loss: 2.2555 - val_accuracy: 0.2294
Epoch 2/12
469/469 [==============================] - 21s 46ms/step - loss: 2.2477 - accuracy: 0.2090 - val_loss: 2.1945 - val_accuracy: 0.3538
Epoch 3/12
469/469 [==============================] - 22s 46ms/step - loss: 2.1888 - accuracy: 0.3076 - val_loss: 2.1107 - val_accuracy: 0.4475
Epoch 4/12
469/469 [==============================] - 22s 46ms/step - loss: 2.1076 - accuracy: 0.3839 - val_loss: 1.9968 - val_accuracy: 0.5344
Epoch 5/12
469/469 [==============================] - 21s 46ms/step - loss: 1.9966 - accuracy: 0.4540 - val_loss: 1.8466 - val_accuracy: 0.6096
Epoch 6/12
469/469 [==============================] - 22s 47ms/step - loss: 1.8526 - accuracy: 0.5101 - val_loss: 1.6598 - val_accuracy: 0.6830
Epoch 7/12
469/469 [==============================] - 22s 47ms/step - loss: 1.6882 - accuracy: 0.5572 - val_loss: 1.4507 - val_accuracy: 0.7304
Epoch 8/12
469/469 [==============================] - 22s 47ms/step - loss: 1.5123 - accuracy: 0.5991 - val_loss: 1.2509 - val_accuracy: 0.7636
Epoch 9/12
469/469 [==============================] - 22s 47ms/step - loss: 1.3526 - accuracy: 0.6302 - val_loss: 1.0789 - val_accuracy: 0.7893
Epoch 10/12
469/469 [==============================] - 22s 48ms/step - loss: 1.2158 - accuracy: 0.6566 - val_loss: 0.9410 - val_accuracy: 0.8085
Epoch 11/12
469/469 [==============================] - 22s 46ms/step - loss: 1.1047 - accuracy: 0.6798 - val_loss: 0.8354 - val_accuracy: 0.8250
Epoch 12/12
469/469 [==============================] - 21s 45ms/step - loss: 1.0291 - accuracy: 0.6960 - val_loss: 0.7543 - val_accuracy: 0.8331
Test loss: 0.7542847394943237
Test accuracy: 0.8331000208854675

I'm confused with this issue:

Could not load dynamic library 'libcusparse.so.11'; dlerror: /usr/local/cuda-11.0/lib64/libcusparse.so.11: file too short; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64

It is very strange, I checked the file libcusparse.so.11 and it exist in the same path as other libraries:libcudart.so.11.0, libcublas.so.11, libcublasLt.so.11, libcufft.so.10, libcurand.so.10, libcusolver.so.10,  libcudnn.so.8
Environment:

OS: Ubuntu 18.04
GPU Geforce RTX 3070
Driver version 460.39
Cuda Toolkit V11.0.194
Cudnn V8.0.5.39
Python V3.6.9

Any thoughts?
",1,1354,"I had also an issue Could not load dynamic library 'libcusolver.so.11';
I just installed Tensorflow==2.4.
",,
tensorflow strange issue,https://stackoverflow.com/questions/62466877,self._traceback = tf_stack.extract_stack(),"I am training a custom ssd_mobilenet_v2_quantized_300x300  TensorFlow model for object detection using Google Colab with the downgraded version of TensorFlow 1.15.2 because I use to train my model on previous version of TensorFlow i.e. 1.14.0 but due to the latest update to version 2.2.0, I get the strange errors and therefore I can't use the latest version.
Using 1.15.2 version and selection even batch size of 8 I successfully starts the training process but after some time, the training process stops with the following errors.

TypeError: 'numpy.float64' object cannot be interpreted as an integer

self._traceback = tf_stack.extract_stack()


My complete training log is as follows;

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/evaluation.py"", line 272, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 754, in run
    run_metadata=run_metadata)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1259, in run
    run_metadata=run_metadata)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1360, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/local/lib/python3.6/dist-packages/six.py"", line 693, in reraise
    raise value
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1345, in run
    return self._sess.run(*args, **kwargs)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1418, in run
    run_metadata=run_metadata)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1176, in run
    return self._sess.run(*args, **kwargs)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: 2 root error(s) found.
  (0) Out of range: End of sequence
     [[node IteratorGetNext (defined at tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]
  (1) Out of range: End of sequence
     [[node IteratorGetNext (defined at tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]
     [[Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/non_max_suppression_with_scores_1/NonMaxSuppressionV5/_4683]]
0 successful operations.
0 derived errors ignored.

Original stack trace for 'IteratorGetNext':
  File ""content/models/research/object_detection/model_main.py"", line 114, in &lt;module&gt;
    tf.app.run()
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""content/models/research/object_detection/model_main.py"", line 110, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 473, in train_and_evaluate
    return executor.run()
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 613, in run
    return self.run_local()
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 714, in run_local
    saving_listeners=saving_listeners)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 370, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1161, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1195, in _train_model_default
    saving_listeners)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1494, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 754, in run
    run_metadata=run_metadata)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1259, in run
    run_metadata=run_metadata)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1345, in run
    return self._sess.run(*args, **kwargs)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1426, in run
    run_metadata=run_metadata))
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/training/basic_session_run_hooks.py"", line 594, in after_run
    if self._save(run_context.session, global_step):
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/training/basic_session_run_hooks.py"", line 619, in _save
    if l.after_save(session, step):
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 519, in after_save
    self._evaluate(global_step_value)  # updates self.eval_result
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 539, in _evaluate
    self._evaluator.evaluate_and_export())
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 920, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 480, in evaluate
    name=name)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 522, in _actual_eval
    return _evaluate()
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 504, in _evaluate
    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1511, in _evaluate_build_graph
    self._call_model_fn_eval(input_fn, self.config))
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1544, in _call_model_fn_eval
    input_fn, ModeKeys.EVAL)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1025, in _get_features_and_labels_from_input_fn
    self._call_input_fn(input_fn, mode))
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/util.py"", line 65, in parse_input_fn_result
    result = iterator.get_next()
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/data/ops/iterator_ops.py"", line 426, in get_next
    name=name)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/gen_dataset_ops.py"", line 2518, in iterator_get_next
    output_shapes=output_shapes, name=name)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py"", line 1365, in _do_call
    return fn(*args)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: TypeError: object of type &lt;class 'numpy.float64'&gt; cannot be safely interpreted as an integer.
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py"", line 117, in linspace
    num = operator.index(num)

TypeError: 'numpy.float64' object cannot be interpreted as an integer


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/script_ops.py"", line 235, in __call__
    ret = func(*args)

  File ""/content/models/research/object_detection/metrics/coco_evaluation.py"", line 416, in first_value_func
    self._metrics = self.evaluate()

  File ""/content/models/research/object_detection/metrics/coco_evaluation.py"", line 247, in evaluate
    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)

  File ""/content/models/research/object_detection/metrics/coco_tools.py"", line 178, in __init__
    cocoeval.COCOeval.__init__(self, groundtruth, detections, iouType=iou_type)

  File ""/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py"", line 76, in __init__
    self.params = Params(iouType=iouType) # parameters

  File ""/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py"", line 527, in __init__
    self.setDetParams()

  File ""/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py"", line 507, in setDetParams
    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)

  File ""&lt;__array_function__ internals&gt;"", line 6, in linspace

  File ""/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py"", line 121, in linspace
    .format(type(num)))

TypeError: object of type &lt;class 'numpy.float64'&gt; cannot be safely interpreted as an integer.


     [[{{node PyFunc_3}}]]
     [[cond/Detections_Left_Groundtruth_Right/0/_4927]]
  (1) Invalid argument: TypeError: object of type &lt;class 'numpy.float64'&gt; cannot be safely interpreted as an integer.
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py"", line 117, in linspace
    num = operator.index(num)

TypeError: 'numpy.float64' object cannot be interpreted as an integer


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/script_ops.py"", line 235, in __call__
    ret = func(*args)

  File ""/content/models/research/object_detection/metrics/coco_evaluation.py"", line 416, in first_value_func
    self._metrics = self.evaluate()

  File ""/content/models/research/object_detection/metrics/coco_evaluation.py"", line 247, in evaluate
    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)

  File ""/content/models/research/object_detection/metrics/coco_tools.py"", line 178, in __init__
    cocoeval.COCOeval.__init__(self, groundtruth, detections, iouType=iou_type)

  File ""/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py"", line 76, in __init__
    self.params = Params(iouType=iouType) # parameters

  File ""/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py"", line 527, in __init__
    self.setDetParams()

  File ""/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py"", line 507, in setDetParams
    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)

  File ""&lt;__array_function__ internals&gt;"", line 6, in linspace

  File ""/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py"", line 121, in linspace
    .format(type(num)))

TypeError: object of type &lt;class 'numpy.float64'&gt; cannot be safely interpreted as an integer.


     [[{{node PyFunc_3}}]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/content/models/research/object_detection/model_main.py"", line 114, in &lt;module&gt;
    tf.app.run()
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/content/models/research/object_detection/model_main.py"", line 110, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 473, in train_and_evaluate
    return executor.run()
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 613, in run
    return self.run_local()
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 714, in run_local
    saving_listeners=saving_listeners)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 370, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1161, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1195, in _train_model_default
    saving_listeners)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1494, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 754, in run
    run_metadata=run_metadata)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1259, in run
    run_metadata=run_metadata)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1360, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/local/lib/python3.6/dist-packages/six.py"", line 693, in reraise
    raise value
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1345, in run
    return self._sess.run(*args, **kwargs)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1426, in run
    run_metadata=run_metadata))
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/basic_session_run_hooks.py"", line 594, in after_run
    if self._save(run_context.session, global_step):
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/basic_session_run_hooks.py"", line 619, in _save
    if l.after_save(session, step):
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 519, in after_save
    self._evaluate(global_step_value)  # updates self.eval_result
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 539, in _evaluate
    self._evaluator.evaluate_and_export())
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 920, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 480, in evaluate
    name=name)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 522, in _actual_eval
    return _evaluate()
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 511, in _evaluate
    output_dir=self.eval_dir(name))
  File ""/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1619, in _evaluate_run
    config=self._session_config)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/evaluation.py"", line 272, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 861, in __exit__
    self._close_internal(exception_type)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 894, in _close_internal
    h.end(self._coordinated_creator.tf_sess)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/basic_session_run_hooks.py"", line 951, in end
    self._final_ops, feed_dict=self._final_ops_feed_dict)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: TypeError: object of type &lt;class 'numpy.float64'&gt; cannot be safely interpreted as an integer.
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py"", line 117, in linspace
    num = operator.index(num)

TypeError: 'numpy.float64' object cannot be interpreted as an integer


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/script_ops.py"", line 235, in __call__
    ret = func(*args)

  File ""/content/models/research/object_detection/metrics/coco_evaluation.py"", line 416, in first_value_func
    self._metrics = self.evaluate()

  File ""/content/models/research/object_detection/metrics/coco_evaluation.py"", line 247, in evaluate
    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)

  File ""/content/models/research/object_detection/metrics/coco_tools.py"", line 178, in __init__
    cocoeval.COCOeval.__init__(self, groundtruth, detections, iouType=iou_type)

  File ""/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py"", line 76, in __init__
    self.params = Params(iouType=iouType) # parameters

  File ""/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py"", line 527, in __init__
    self.setDetParams()

  File ""/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py"", line 507, in setDetParams
    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)

  File ""&lt;__array_function__ internals&gt;"", line 6, in linspace

  File ""/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py"", line 121, in linspace
    .format(type(num)))

TypeError: object of type &lt;class 'numpy.float64'&gt; cannot be safely interpreted as an integer.


     [[node PyFunc_3 (defined at tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]
     [[cond/Detections_Left_Groundtruth_Right/0/_4927]]
  (1) Invalid argument: TypeError: object of type &lt;class 'numpy.float64'&gt; cannot be safely interpreted as an integer.
Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py"", line 117, in linspace
    num = operator.index(num)

TypeError: 'numpy.float64' object cannot be interpreted as an integer


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/script_ops.py"", line 235, in __call__
    ret = func(*args)

  File ""/content/models/research/object_detection/metrics/coco_evaluation.py"", line 416, in first_value_func
    self._metrics = self.evaluate()

  File ""/content/models/research/object_detection/metrics/coco_evaluation.py"", line 247, in evaluate
    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)

  File ""/content/models/research/object_detection/metrics/coco_tools.py"", line 178, in __init__
    cocoeval.COCOeval.__init__(self, groundtruth, detections, iouType=iou_type)

  File ""/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py"", line 76, in __init__
    self.params = Params(iouType=iouType) # parameters

  File ""/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py"", line 527, in __init__
    self.setDetParams()

  File ""/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py"", line 507, in setDetParams
    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)

  File ""&lt;__array_function__ internals&gt;"", line 6, in linspace

  File ""/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py"", line 121, in linspace
    .format(type(num)))

TypeError: object of type &lt;class 'numpy.float64'&gt; cannot be safely interpreted as an integer.


     [[node PyFunc_3 (defined at tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]
0 successful operations.
0 derived errors ignored.

Original stack trace for 'PyFunc_3':
  File ""content/models/research/object_detection/model_main.py"", line 114, in &lt;module&gt;
    tf.app.run()
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""content/models/research/object_detection/model_main.py"", line 110, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 473, in train_and_evaluate
    return executor.run()
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 613, in run
    return self.run_local()
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 714, in run_local
    saving_listeners=saving_listeners)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 370, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1161, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1195, in _train_model_default
    saving_listeners)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1494, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 754, in run
    run_metadata=run_metadata)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1259, in run
    run_metadata=run_metadata)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1345, in run
    return self._sess.run(*args, **kwargs)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py"", line 1426, in run
    run_metadata=run_metadata))
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/training/basic_session_run_hooks.py"", line 594, in after_run
    if self._save(run_context.session, global_step):
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/training/basic_session_run_hooks.py"", line 619, in _save
    if l.after_save(session, step):
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 519, in after_save
    self._evaluate(global_step_value)  # updates self.eval_result
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 539, in _evaluate
    self._evaluator.evaluate_and_export())
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/training.py"", line 920, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 480, in evaluate
    name=name)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 522, in _actual_eval
    return _evaluate()
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 504, in _evaluate
    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1511, in _evaluate_build_graph
    self._call_model_fn_eval(input_fn, self.config))
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1547, in _call_model_fn_eval
    features, labels, ModeKeys.EVAL, config)
  File ""tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py"", line 1149, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""content/models/research/object_detection/model_lib.py"", line 570, in model_fn
    eval_config, list(category_index.values()), eval_dict)
  File ""content/models/research/object_detection/eval_util.py"", line 1045, in get_eval_metric_ops_for_evaluators
    eval_dict))
  File ""content/models/research/object_detection/metrics/coco_evaluation.py"", line 426, in get_estimator_eval_metric_ops
    first_value_op = tf.py_func(first_value_func, [], tf.float32)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/script_ops.py"", line 513, in py_func
    return py_func_common(func, inp, Tout, stateful, name=name)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/script_ops.py"", line 495, in py_func_common
    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/script_ops.py"", line 318, in _internal_py_func
    input=inp, token=token, Tout=Tout, name=name)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/gen_script_ops.py"", line 170, in py_func
    ""PyFunc"", input=input, token=token, Tout=Tout, name=name)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()


What are the possibilites of getting over this issue, any kind of recommendations?
",1,8334,"Try adding these lines of code immediately after importing Tensorflow in your train.py
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

",,
tensorflow strange issue,https://stackoverflow.com/questions/57140254,Google Cloud AI Platform Notebook Instance won&#39;t use GPU with Jupyter,"I'm using the pre-built AI Platform Jupyter Notebook instances to train a model with a single Tesla K80 card.  The issue is that I don't believe the model is actually training on the GPU.

nvidia-smi returns the following during training:

No Running Processes Found

Not the ""No Running Process Found"" yet ""Volatile GPU Usage"" is 100%. Something seems strange...

...And the training is excruciatingly slow.

A few days ago, I was having issues with the GPU not being released after each notebook run.  When this occurred I would receive a OOM (Out of memory error).  This required me to go into the console every time, find the GPU running process PID and use kill -9 before re-running the notebook.  However, today, I can't get the GPU to run at all?  It never shows a running process.

I've tried 2 different GCP AI Platform Notebook instances (both of the available tensorflow version options) with no luck.  Am I missing something with these ""pre-built"" instances.  

Pre-Built AI Platform Notebook Section

Just to clarify, I did not build my own instance and then install access to Jupyter notebooks.  Instead, I used the built-in Notebook instance option under the AI Platform submenu.

Do I still need to configure a setting somewhere or install a library to continue using/reset my chosen GPU?  I was under the impression that the virtual machine was already loaded with the Nvidia stack and should be plug and play with GPUs.

Thoughts?

EDIT: Here is a full video of the issue as requested --&gt; https://www.youtube.com/watch?v=N5Zx_ZrrtKE&amp;feature=youtu.be
",1,2759,,,
tensorflow strange issue,https://stackoverflow.com/questions/55605853,&#39;numpy.dtype&#39; object has no attribute &#39;base_dtype&#39;,,1,6026,,,
tensorflow strange issue,https://stackoverflow.com/questions/51802631,Does batch normalization in tensorflow use running averages during training?,,1,631,,,
tensorflow strange issue,https://stackoverflow.com/questions/50930376,TensorFlow :ValueError: None values not supported,,1,3231,,,
tensorflow strange issue,https://stackoverflow.com/questions/49339523,"TensorFlow program sometimes works, sometimes throws different errors related to reshape nodes at different points in training using same random seed","I'm having difficulty with a  particularly spooky-looking TensorFlow issue. (TensorFlow 1.4.1 and Python 2.7)

Errors

I'm seeing a couple different errors when running the same program. Here is an example of one:

W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.377745: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.378256: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.378753: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.379193: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.379692: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.380208: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.380709: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.381166: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.381654: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.382138: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.382601: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.383111: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.383601: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.384107: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.384551: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018-03-15 18:52:25.384792: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]
2018/03/15 18:52:25 ERROR|--|Traceback (most recent call last):
  File ""experiment_runner.py"", line 140, in experimentset
    results = e.run()
  File ""experiment_runner.py"", line 69, in run
    results = run_fn()
  File ""experiment_runner.py"", line 100, in traintest
    return tt.run(self.exp_specs, self.data, model)
  File ""/s/chopin/a/grad/jonbyrd/protqa/protqa/experiment/train_test.py"", line 149, in run
    return self._fit_model(exp_specs, data, model)
  File ""/s/chopin/a/grad/jonbyrd/protqa/protqa/experiment/train_test.py"", line 56, in _fit_model
    self.train_proteins_epoch(data[""train""], model, exp_specs[""args""][""minibatch_size""])
  File ""/s/chopin/a/grad/jonbyrd/protqa/protqa/experiment/train_test.py"", line 186, in train_proteins_epoch
    model.train(minibatch)
  File ""/s/chopin/a/grad/jonbyrd/protqa/protqa/modeling/models/tf_model.py"", line 169, in train
    results = self._train(data, options=run_options, run_metadata=run_metadata, **kwargs)
  File ""/s/chopin/a/grad/jonbyrd/protqa/protqa/modeling/models/tf_model.py"", line 113, in _train
    results = self.run_graph([self.train_op, self.loss], data, ""train"", **kwargs)
  File ""/s/chopin/a/grad/jonbyrd/protqa/protqa/modeling/models/protnet.py"", line 135, in run_graph
    return self.sess.run(outputs, feed_dict=feed_dict, options=options, run_metadata=run_metadata)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
InvalidArgumentError: Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]

Caused by op u'optimizer/gradients/energy_2/map/while/Gather_grad/Reshape', defined at:
  File ""experiment_runner.py"", line 332, in &lt;module&gt;
    main()
  File ""experiment_runner.py"", line 328, in main
    e.run()
  File ""experiment_runner.py"", line 69, in run
    results = run_fn()
  File ""experiment_runner.py"", line 140, in experimentset
    results = e.run()
  File ""experiment_runner.py"", line 69, in run
    results = run_fn()
  File ""experiment_runner.py"", line 99, in traintest
    model = tt.build_model(self.exp_specs, self.data)
  File ""/s/chopin/a/grad/jonbyrd/protqa/protqa/experiment/train_test.py"", line 141, in build_model
    model = eval(hparams[""name""] + ""(exp_specs, data['train'])"")
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""/s/chopin/a/grad/jonbyrd/protqa/protqa/modeling/models/protnet.py"", line 110, in __init__
    self.setup_loss()
  File ""/s/chopin/a/grad/jonbyrd/protqa/protqa/modeling/models/tf_model.py"", line 90, in setup_loss
    self.train_op = self.hparams[""optimizer""](self.loss, **self.hparams[""optimizer_args""])
  File ""/s/chopin/a/grad/jonbyrd/protqa/protqa/modeling/optimizers.py"", line 9, in tf_sgd
    return tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 343, in minimize
    grad_loss=grad_loss)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 414, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 581, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 353, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py"", line 581, in &lt;lambda&gt;
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/ops/array_grad.py"", line 373, in _GatherGrad
    values = array_ops.reshape(grad, values_shape)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3938, in reshape
    ""Reshape"", tensor=tensor, shape=shape, name=name)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op u'energy_2/map/while/Gather', defined at:
  File ""experiment_runner.py"", line 332, in &lt;module&gt;
    main()
[elided 6 identical lines from previous traceback]
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""/s/chopin/a/grad/jonbyrd/protqa/protqa/modeling/models/protnet.py"", line 77, in __init__
    dtype=tf.float32, parallel_iterations=32)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/ops/functional_ops.py"", line 389, in map_fn
    swap_memory=swap_memory)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2816, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2640, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2590, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/ops/functional_ops.py"", line 379, in compute
    packed_fn_values = fn(packed_values)
  File ""/s/chopin/a/grad/jonbyrd/protqa/protqa/modeling/models/protnet.py"", line 75, in &lt;lambda&gt;
    ], None, in_dims=nv, in_dists=self.in_dists, **args)[0],
  File ""/s/chopin/a/grad/jonbyrd/protqa/protqa/modeling/models/nn_components.py"", line 447, in energy
    return tf.reshape(tf.reduce_mean(tf.einsum('abi,abj-&gt;abij', (tf.expand_dims(verts, axis=1) * tf.gather(verts, hood_indices)), dists), axis=[0,1]), [in_dims*in_dists]), None
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 2486, in gather
    params, indices, validate_indices=validate_indices, name=name)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1834, in gather
    validate_indices=validate_indices, name=name)
  File ""/s/jawar/j/nobackup/protein_learning/virtualenv/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)

InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 122496 values, but the requested shape has 0
     [[Node: optimizer/gradients/energy_2/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_2/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_2/map/while/Gather_grad/concat)]]


However, I'm getting several different errors. For this node in my graph:

[[Node: optimizer/gradients/energy_1/map/while/Gather_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_1/map/while/mul_grad/tuple/control_dependency_1, optimizer/gradients/energy_1/map/while/Gather_grad/concat)]]


Here are some of the errors I've seen:

Size 1 must be non-negative, not -1231271574
Size 1 must be non-negative, not -1225669337
Input to reshape is a tensor with 122496 values, but the requested shape has 0
Input to reshape is a tensor with 122496 values, but the requested shape has 1715491170492
Input to reshape is a tensor with 122496 values, but the requested shape has 1693172050944
Input to reshape is a tensor with 122496 values, but the requested shape has 1706639062128


For this node in my graph:

[[Node: optimizer/gradients/energy_1/map/while/Mean_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](optimizer/gradients/energy_1/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/tuple/control_dependency, optimizer/gradients/energy_1/map/while/Mean_grad/DynamicStitch/_203)]]


I've seen these errors for example:

Size 0 must be non-negative, not -1237175937
Input to reshape is a tensor with 512 values, but the requested shape has 0


I also had a ""Nan in summary histogram"" error, but I'm going to assume that was due to model divergence.

I don't understand why I'm getting these reshape-related errors at runtime, halfway through the training process. I also don't understand why the values in these errors change each run.

Circumstances

When running the same program with the same hyperparameters on the same data with the same numpy and tensorflow random seeds, the program will sometimes run without issues, but usually throw one of these errors at different points in the training process. Sometimes this happens at the very first epoch, sometimes after many training epoch (even after 40+ epochs, shortly before the training finishes).

The strange thing is that this seems to be very dependent on the number of latent features/convolution filters in the layer before the one that throws the error. Smaller numbers of filters like 16, 32, 64, and 128 will almost always get errors related to the first computation graph node I mentioned, while a 512 filters will mostly get errors related to the second node. These hyperparameter numbers failed 7-10/10 runs.

However, running the program with that number of filters being either 1 or 1024 was successful 10/10 runs, which confuses me.

Program

The program is part of a research framework for deep learning on protein structures. The part that is giving me errors is part of a graph convolutional/message-passing network that downsamples a variable-size/shape graph to a single latent representation. The number of filters in the previous section corresponds to the number of latent features per node in the graph.

Here is the downsampling method:

def energy(input, _, in_dims, in_dists, **kwargs):
    '''Params:
        input: a tuple representing a single graph containing:
            a 2d tensor of vertex representations(vertices x features)
            a 3d tensor of distance metrics between nodes (vertices x neighbors x distances)
            a 2d tensor containing indices of the neighbors of each vertex in the first tensor(vertices x neighbor indices)
        in_dims: number of incoming features for each vertex
        in_dists: number of distance metrics

    Returns: a 1d tensor of size [in_dims*in_dists] which is the sum over all pairs of neighboring vertices of
    (the outer product of (the elementwise product of the two vertices) and the distances).
    '''
    verts, dists, hood_indices = input
    return tf.reshape(tf.reduce_mean(tf.einsum('abi,abj-&gt;abij', (tf.expand_dims(verts, axis=1) * tf.gather(verts, hood_indices)), dists), axis=[0,1]), [in_dims*in_dists]), None


Here is the map_fn where that method is called, where layer_fn is the above method:

              # downsample each graph using layer_fn
                input = tf.map_fn(
                    lambda ind, data=input[0], merge_fn=layer_fn, nv=input[0].get_shape().as_list()[-1], args=args: merge_fn(
                        [tf.slice(data, [tf.squeeze(tf.slice(ind, [0], [1])), 0],
                                 [tf.squeeze(tf.slice(ind, [1], [1])), nv], name=""merge_vertex_slice""),
                         tf.slice(self.distances, [tf.squeeze(tf.slice(ind, [0], [1])), 0, 0],
                                  [tf.squeeze(tf.slice(ind, [1], [1])), self.in_nhood_size, self.in_dists], name=""merge_distance_slice""),
                         tf.slice(tf.squeeze(self.in_hood_indices), [tf.squeeze(tf.slice(ind, [0], [1])), 0],
                                  [tf.squeeze(tf.slice(ind, [1], [1])), self.in_nhood_size], name=""merge_index_slice""),
                         ], None, in_dims=nv, in_dists=self.in_dists, **args)[0],
                    tf.stack([tf.cumsum(self.graph_orders, exclusive=True), self.graph_orders], axis=-1),
                    dtype=tf.float32, parallel_iterations=32)


Stack Overflow will not allow me to post the class that builds the computation graph as it brings my post over the character limit.

The program runs without errors when using a top_k method to downsample the graph.



Confusion

I don't understand why I would get these reshape errors after successful training epochs, or why the number of filters is affecting the issue in such a way. I also don't understand why I'm getting different values in the reshape errors each time. The tensor dimensions should all be fixed except for the number of examples in a minibatch (which I handle via the map_fn) and the number of vertices in each example graph. 

I'm having considerable difficulty figuring this out and would be very grateful for outside input. Thank you!
",1,1037,"I had a similar error ""tensorflow.python.framework.errors_impl.InvalidArgumentError: Size 0 must be non-negative, not -1610612736 [Op:Reshape]""
It turns out to be related to tf.repeat. Basically, the following simple code will get you an OOM error:
a = tf.range(5000000)
b = tf.concat([tf.zeros(5000000-1, dtype=tf.int32),tf.constant([5000000], dtype=tf.int32)], axis=0)
c = tf.repeat(a,b)

The solution is to tune args of tf.repeat, e.g. make input smaller (discard useless values), remove zeros from repeats
For more details: https://github.com/tensorflow/tensorflow/issues/46648#issuecomment-876168035
",,
tensorflow strange issue,https://stackoverflow.com/questions/49323364,Tensorflow Domain Adaptation How can i use two Bazel commands at the same time?,,1,237,,,
tensorflow strange issue,https://stackoverflow.com/questions/46382866,Tensorflow: Different activation values for same image,,1,594,,,
tensorflow strange issue,https://stackoverflow.com/questions/40381101,Keras writing a Recurrent Layer which accepts images,,1,915,,,
tensorflow strange issue,https://stackoverflow.com/questions/50512783,TensorRT for upscaling images serves not expected results,,0,799,,,
tensorflow strange issue,https://stackoverflow.com/questions/49351711,Saved multi-GPU trained model loaded into single-GPU; inconsistent results,,0,1298,,,
tensorflow strange issue,https://stackoverflow.com/questions/54152947,Tensorflow Object detection: Trained model does not predict ALL instances of the same object in the image,,0,523,,,
tensorflow strange issue,https://stackoverflow.com/questions/66675299,Pytorch -&gt; [Onnx -&gt; tensorflow] -&gt; tflite generates lots of redundant conv2d operators,,0,1133,"After some additional digging I've found the following

My convs were depthwise(conv2d is depthwise in pytorch and onnx if it has groups parameter &gt; 1)
This bunch of convs is an inefficient way of doing a depthwise conv. To do it efficiently we need to use tf.depthwiseconv

To fix this in onnx-tf v1.7.0 you should apply a patch to onnx-tf source code posted here https://github.com/onnx/onnx-tensorflow/issues/473#issuecomment-533522917
In current master branch there is an attempt to fix the issue by detecting depthwise convs but it currently contains a bug. To fix the bug in master branch you can apply a fix I posted here https://github.com/onnx/onnx-tensorflow/issues/754#issuecomment-801775203
I've used the master branch and the fix above resulting in a small graph with depthwise conv
I've also created a fork with the fix above, so you can do
pip install git+https://github.com/Vozf/onnx-tensorflow

to apply it instead of patching on your own

It seems like the issue should be fixed in the next major release(probably 1.8.0)
Also consider using https://github.com/nerox8664/onnx2keras as it supports depthwise convolutions and moreover supports full nhwc conversion which allows removing all the transpose ops.
",,
tensorflow strange issue,https://stackoverflow.com/questions/56700022,tensorflow-hub: Python import error when file is in a specific folder,,0,1675,,,
tensorflow strange issue,https://stackoverflow.com/questions/72961365,The editor could not be opened due to an unexpected error: Unexpected token ? in JSON at position 0,"I'm trying to open a .ipynb file in vscode and encounter this error:

This is strange because of many .ipynb files in the same folder I have, only this one when opened has this issue. Maybe because of tensorflow? Yesterday everything was fine, the whole folder is stored in an SSD drive with a SATA cable. This morning when after I unplugged the SSD drive from the PC and plugged it into the laptop, this error popped up. I have never seen this error before.
I tried uninstalling and reinstalling Jupyter extension, switching to previous release, opened it in the text editor and saw this:

This file is very important and I don't wanna to lose it. Could you show me a way to solve this?
",0,1227,"You can open this file in text editor (as the error suggests). There you'll see highlighted parts of code and several buttons around them: Accept incoming change, Compare changes and some others. You should press one of these buttons (according to what you'd like to do, for me it was ""Accept incoming change"") and the highlight from this part of code will be removed. When you remove all the highlights you right-click on this file tab and choose ""Reopen editor with""-&gt;""Jupiter Notebook"". Your code will be saved :)
",,
tensorflow strange issue,https://stackoverflow.com/questions/72570331,GPU memory nearly full after defining tf.distribute.MirroredStrategy?,"I am coming across a strange issue when using TensorFlow (2.9.1). After defining a distributed training strategy, my GPU memory appears to fill.
Steps to reproduce are simple:
import tensorflow as tf
strat = tf.distribute.MirroredStrategy()

After the first line (importing TensorFlow), nvidia-smi outputs:
Fri Jun 10 03:01:47 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro P6000        Off  | 00000000:04:00.0 Off |                  Off |
| 26%   25C    P8     9W / 250W |      0MiB / 24449MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Quadro P6000        Off  | 00000000:06:00.0 Off |                  Off |
| 26%   20C    P8     7W / 250W |      0MiB / 24449MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

After the second line of code, nvidia-smi outputs:
Fri Jun 10 03:02:43 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro P6000        Off  | 00000000:04:00.0 Off |                  Off |
| 26%   29C    P0    59W / 250W |  23951MiB / 24449MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Quadro P6000        Off  | 00000000:06:00.0 Off |                  Off |
| 26%   25C    P0    58W / 250W |  23951MiB / 24449MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A   1833720      C   python                          23949MiB |
|    1   N/A  N/A   1833720      C   python                          23949MiB |
+-----------------------------------------------------------------------------+

The GPU memory is almost entirely full? There is also some terminal output:
2022-06-10 03:02:37.442336: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-10 03:02:39.136390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23678 MB memory:  -&gt; device: 0, name: Quadro P6000, pci bus id: 0000:04:00.0, compute capability: 6.1
2022-06-10 03:02:39.139204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 23678 MB memory:  -&gt; device: 1, name: Quadro P6000, pci bus id: 0000:06:00.0, compute capability: 6.1
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')

Any ideas on why this is occurring would be helpful! Additional details about my configuration:

Python 3.10.4 [GCC 7.5.0] on linux
tensorflow 2.9.1
cuda/11.2.2 cudnn/v8.2.1

",0,473,"By default, Tensorflow will map almost all of your GPU memory: official guide. This is for performance reasons: by allocating the GPU memory, it reduces latency that memory growth would typically cause.
You can try using tf.config.experimental.set_memory_growth to prevent it from immediately filling up all its memory. There are also some good explanations on this StackOverflow post.
",,
tensorflow strange issue,https://stackoverflow.com/questions/69713596,How to force Python into 64bit mode,"I am having a strange error on my MacBook Air (M1, 2020) where python3.8 is being installed as a 64 bit binary from homebrew brew install python@3.8 and yet, is being run in 32 bit mode, thus causing errors when trying to download packages.
Here is what I have tried:

Reinstalling and confirming that the executable is running in 32 bit

$ brew reinstall python@3.8 #installs to /opt/homebrew/opt/python@3.8/bin/python3
$ python3.8 -c 'import sys;print(""%x"" % sys.maxsize, sys.maxsize &gt; 2**64)'
&gt; 7fffffffffffffff False


Looking at architecture of binary

$ file /opt/homebrew/opt/python@3.8/bin/python3
&gt; /opt/homebrew/opt/python@3.8/bin/python3: Mach-O 64-bit executable arm64

From the above output, one can see that the executable is in fact 64-bit despite being run in 32 bit mode.

Additional installation of Rosetta 2, a mac update supposedly to fix this problem

$ /usr/sbin/softwareupdate --install-rosetta --agree-to-license


Attempt to force binary in 64 bit arch

$ arch -x86_64 /opt/homebrew/opt/python@3.8/bin/python3
&gt; arch: posix_spawnp: /opt/homebrew/opt/python@3.8/bin/python3: Bad CPU type in executable

Here is relevant, but sadly unhelpful for my needs, stack overflow tickets:
How to force using 64 bit python on Mac-os-X 
Installation issue with Tensorflow Package 
How to determine if python is in 32 or 64 bit mode mac 
Bad CPU type in executable Mac Rosetta 2 Instructions 
Any ideas would be helpful, thanks!
",0,618,"Turns out I had an error in my command:
python3.8 -c 'import sys;print(""%x"" % sys.maxsize, sys.maxsize &gt; 2**64)'
should be:
python3.8 -c 'import sys;print(""%x"" % sys.maxsize, sys.maxsize &gt; 2**32)'
Which returns true when you're running python in 64 bit mode.
This specific problem is likely something else, and as such I am closing this question.
",,
tensorflow strange issue,https://stackoverflow.com/questions/68834108,Tensorflow object detection API loss increases dramatically,"I'm training tensorflow object detection API with my own data but for some reason, all losses seem to skyrocket past some thousands of steps. You can see it on this figure:

At the beginning it goes down, as it should, but somehow at some point it just explodes.
The model I'm using is ssd_resnet50_v1_fpn with pretrained coco checkpoint but this information might be irrelevant since I think this happens with any model (I've tried 2 models a couple of times each and currently the behaviour is the same for all of my tries). I'm training it following this tutorial https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html so I use the model_main_tf2.py script to train the model.
What is very strange is that a colleague of mine does not have this issue. His loss evolution seems rather normal as you can see here:

He provided me with all the necessary files to run the training (so, the tf.records, the labelmap.txt, the pipeline.config file, the ckpt-0.index etc.). He might be lucky and I'm not. But it's very strange that each time he runs it, he has a normal behaviour and I don't... At least the odds at very very low that this is what happens.
Don't know if it matters but I have a GeForce RTX 3060 Ti, Python: 3.9.6, Tensorflow: 2.5.0, Cudnn: 8.1.1 and Cuda: 11.2
while he has a Geforce GTX 1650 Super, Python: 3.9.6, Tensorflow: 2.5.1, Cudnn: 8.1.0 and Cuda: 11.2
Any idea what could cause this?
Edit 1: I've re-run this while doing nothing else on my computer and it ran quite well. It now shows an expected behaviour. I'll re-run it while using it normally (maybe watch a video or something), which is what I did in the last runs that showed a strange behaviour, and see if this strange behaviour occurs again or not. Do you think it could be the cause of this behaviour?
",0,557,"I'll have the exact same issue. After a varying number of thousands of steps the loss just explodes. I'm also unsure what could be the cause of the issue.
In my opinion it could either be either an GPU issue or something wrong with the data. But at that point I should have gone through the whole set at least once. Let me know if you make any progress on that :)

",,
tensorflow strange issue,https://stackoverflow.com/questions/65453002,I keep receiving a weird &quot;invalid syntax&quot; error when trying to implement a convolution neural network program using tensorflow in python,"I am currently using whatever version of python comes built-in with Google Colab (I believe it is either 3.7, 3.8, or 3.9). I am trying to execute a CNN program that can be used to recognize images without using Keras, but I keep getting a strange error that says this:
File ""&lt;ipython-input-12-6518b14c949f&gt;"", line 91
    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)
            ^
SyntaxError: invalid syntax

This is the full code that I currently have:
!pip install tensorflow_datasets
!pip install --upgrade tensorflow
!pip install tensorflow-datasets
!pip install mnist
#!pip install tensorflow.examples.tutorials.mnist

import argparse
print ('argparse version: ', argparse.__version__)
import mnist
print ('MNIST version: ', mnist.__version__)
import tensorflow_datasets
print ('tensorflow_datasets version: ', tensorflow_datasets.__version__)
import tensorflow.compat.v1 as tf
print ('tf version: ', tf.__version__)
tf.disable_v2_behavior()
#from tensorflow.examples.tutorials.mnist import input_data


#def build_arg_parser():
#    parser = argparse.ArgumentParser(description='Build a CNN classifier \
#            using MNIST data')
#    parser.add_argument('--input-dir', dest='input_dir', type=str,
#            default='./mnist_data', help='Directory for storing data')
#    return parser

def get_weights(shape):
    data = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(data)

def get_biases(shape):
    data = tf.constant(0.1, shape=shape)
    return tf.Variable(data)

def create_layer(shape):
    # Get the weights and biases
    W = get_weights(shape)
    b = get_biases([shape[-1]])

    return W, b

def convolution_2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1],
            padding='SAME')

def max_pooling(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
            strides=[1, 2, 2, 1], padding='SAME')

if __name__ == '__main__':
    #args = build_arg_parser().parse_args()

    # Get the MNIST data
    mnist = tensorflow_datasets.load('mnist')

    # The images are 28x28, so create the input layer
    # with 784 neurons (28x28=784)
    x = tf.placeholder(tf.float32, [None, 784])

    # Reshape 'x' into a 4D tensor
    x_image = tf.reshape(x, [-1, 28, 28, 1])

    # Define the first convolutional layer
    W_conv1, b_conv1 = create_layer([5, 5, 1, 32])

    # Convolve the image with weight tensor, add the
    # bias, and then apply the ReLU function
    h_conv1 = tf.nn.relu(convolution_2d(x_image, W_conv1) + b_conv1)

    # Apply the max pooling operator
    h_pool1 = max_pooling(h_conv1)

    # Define the second convolutional layer
    W_conv2, b_conv2 = create_layer([5, 5, 32, 64])

    # Convolve the output of previous layer with the
    # weight tensor, add the bias, and then apply
    # the ReLU function
    h_conv2 = tf.nn.relu(convolution_2d(h_pool1, W_conv2) + b_conv2)

    # Apply the max pooling operator
    h_pool2 = max_pooling(h_conv2)

    # Define the fully connected layer
    W_fc1, b_fc1 = create_layer([7 * 7 * 64, 1024])

    # Reshape the output of the previous layer
    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])

    # Multiply the output of previous layer by the
    # weight tensor, add the bias, and then apply
    # the ReLU function
    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

    # Define the dropout layer using a probability placeholder
    # for all the neurons
    keep_prob = tf.placeholder(tf.float32)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

    # Define the readout layer (output layer)
    W_fc2, b_fc2 = create_layer([1024, 10])
    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2

    # Define the entropy loss and the optimizer
    y_loss = tf.placeholder(tf.float32, [None, 10])
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_loss))
    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)

    # Define the accuracy computation
    predicted = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_loss, 1))
    accuracy = tf.reduce_mean(tf.cast(predicted, tf.float32))

    # Create and run a session
    sess = tf.InteractiveSession()
    init = tf.initialize_all_variables()
    sess.run(init)

    # Start training
    num_iterations = 21000
    batch_size = 75
    print('\nTraining the model....')
    for i in range(num_iterations):
        # Get the next batch of images
        batch = mnist.train.next_batch(batch_size)

        # Print progress
        if i % 50 == 0:
            cur_accuracy = accuracy.eval(feed_dict = {
                    x: batch[0], y_loss: batch[1], keep_prob: 1.0})
            print('Iteration', i, ', Accuracy =', cur_accuracy)

        Train on the current batch
        optimizer.run(feed_dict = {x: batch[0], y_loss: batch[1], keep_prob: 0.5})

    # Compute accuracy using test data
    print('Test accuracy =', accuracy.eval(feed_dict = {
            x: mnist.test.images, y_loss: mnist.test.labels,
            keep_prob: 1.0}))

Originally, I had an error with the line
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_loss))

but after I looked it up, I found that switching it to be
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_conv, logits=y_)

would fix that error, and I believe it did.
My next error (and the one that I cannot figure out) came with the line right under that
optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)

which is where the invalid syntax error came up.
I tried switching it to use a different type of optimizer such as
optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

but that hasn't seemed to work.
I have found a few sources that use the exact same code, but seem to somehow have it running - I just can't figure out how.
I also noticed that no matter what I do, I am also not able to install or use tensorflow.examples.tutorials.mnist
Does anyone have any ideas on how to properly install/use tensorflow.examples.tutorials.mnist or on how to fix the error with the AdamOptimizer line? So far I still haven't been able to find anything that shows a method of fixing these issues that actually works for me.
Thank you!
UPDATE:
Some nice people let me know that it was actually just a problem with my parentheses.
I added the missing one and fixed another error - the updated section is now this:
# Define the entropy loss and the optimizer
y_loss = tf.placeholder(tf.float32, [None, 10])
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_conv, logits=y_loss))
optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)  

However, now I get the following error with the same optimizer line as before:
ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [""&lt;tf.Variable 'Variable:0' shape=(5, 5, 1, 32) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_2:0' shape=(5, 5, 32, 64) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_3:0' shape=(64,) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_4:0' shape=(3136, 1024) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_5:0' shape=(1024,) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_6:0' shape=(1024, 10) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_7:0' shape=(10,) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_8:0' shape=(5, 5, 1, 32) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_9:0' shape=(32,) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_10:0' shape=(5, 5, 32, 64) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_11:0' shape=(64,) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_12:0' shape=(3136, 1024) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_13:0' shape=(1024,) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_14:0' shape=(1024, 10) dtype=float32_ref&gt;"", ""&lt;tf.Variable 'Variable_15:0' shape=(10,) dtype=float32_ref&gt;""] and loss Tensor(""Mean:0"", shape=(), dtype=float32).

Any ideas on how to fix this new error?
I am very new to tensorflow and CNNs, so I'm not sure what ""No gradients provided for any variable"" is supposed to point to.
",0,1637,"For anyone else that runs into any similar problems, this is what I ended up doing:

My first mistake was forgetting a parenthesis, so I added that


loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels =
y_conv, logits=y_loss))


This then turned into an args error - so I had to switch the arguments in the parentheses


loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels =
y_loss, logits=y_conv))

Now it works fine and the whole section now looks like this:
    y_loss = tf.placeholder(tf.float32, [None, 10])
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_loss, logits=y_conv))
    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)

",,
tensorflow strange issue,https://stackoverflow.com/questions/65419103,Create Version Failed. Bad model detected with error: &quot;Error loading the model&quot; - AI Platform Prediction,"I created a model through AI Platform UI that uses a global endpoint. I am trying to deploy a basic tensorflow 1.15.0 model I exported using the Saved Model builder. When I try to deploy this model I get a Create Version Failed. Bad model detected with error: ""Error loading the model"" error in the UI and the I see the following in the logs:
ERROR:root:Failed to import GA GRPC module. This is OK if the runtime version is 1.x

Failure: Could not reach metadata service: Internal Server Error.

ERROR:root:Command '['/tools/google-cloud-sdk/bin/gsutil', '-o', 'GoogleCompute:service_account=default', 'cp', '-R', 'gs://cml-365057443918-1608667078774578/models/xsqr_global/v6/7349456410861999293/model/*', '/tmp/model/0001']' returned non-zero exit status 1.

ERROR:root:Error loading model: 'generator' object has no attribute 'next'

ERROR:root:Error loading the model



Framework/ML runtime version: Tensorflow 1.15.0
Python: 3.7.3

What is strange is that the gcloud ai-platform local predict works correctly with this exported model, and I can deploy this exact same model on a regional endpoint with no issues. It only gives this error if I try to use a global endpoint model. But I need the global endpoint because I plan on using a custom prediction routine (if I can get this basic model working first).
The logs seem to suggest an issue with copying the model from storage? I've tried giving various IAM roles additional viewer permissions, but I still get the same errors.
Thanks for the help.
",0,300,"Today I faced the same error (ERROR: (gcloud.ai-platform.versions.create) Create Version failed. Bad model detected with error:  ""Error loading the model"") &amp; for those who wants a summary:
The recommendation is to use n1* machine types (for example: n1-standard-4) via regional endpoints (for example: us-central1) instead of mls1* machines while deploying version. Also I made sure to mention the same region (us-central1) while creating the model itself using the below command, thereby resolving the above mentioned error.

!gcloud ai-platform models create  $model_name 
--region=$REGION

","I think it's the same issue as https://issuetracker.google.com/issues/175316320
The comment in the issue says the fix is now rolling out.
",
tensorflow strange issue,https://stackoverflow.com/questions/61463184,TensorFlow official documentation tutorial error &#39;&#39;numpy.ndarray&#39; object has no attribute &#39;get_shape&#39;,"I was trying to run the exact code on the TensorFlow tutorial here.

When I get to line: 

predictions = model(x_train[:1]).numpy()


I get the error:

'numpy.ndarray' object has no attribute 'get_shape'

I saw here that someone had a similar issue, so I changed the line to:

predictions = model(tf.convert_to_tensor(x_train[:1])).numpy()


And I get the error:

Input 'b' of 'MatMul' Op has type float32 that does not match type float64 of argument 'a'

Could someone tell me the correct code that should be in the tutorial? Or is it an issue on my side?

One thing that is strange is it runs if I run the code in the colab notebook provided, but if I download the jupyter notebook to run locally, that's when I get the error. So I'm wondering if I have a different/wrong version of TensorFlow?

My packages:

tensorflow                1.13.1            
tensorflow-base           1.13.1            
tensorflow-estimator      1.13.0                       
python                    3.7.6                 

",0,496,"The reason of your first question is that your model() accept tensor type parameters not numpy. And model(x)  called x.get_shape().
In x.get_shape(), x must be tensor. So, your need transform numpy to tensor.  You can use tf.convert_to_tensor(x) or tf.constant(x)

The second error message is Input 'b' of 'MatMul' Op has type float32 that does not match type float64 of argument 'a'. The message means that in a x b a type is float64 and b type is float32. They are different type variables, so they can't be multiplied. You can try use tf.cast(a,tf.float32) to change the type of a or change b. But in your code, you should use x_train[:1].dtype to check the type of x_train[:1]. If it is float32 you can try use x_train[:1].astype(np.float64) to transform it or vice versa. Or use tf.constant(x_train[:1],dtype=tf.float64)
","The tutorial you are referencing is in Tensorflow 2.x (where Eager mode is by default). In Eager mode you can access a tensor with .numpy() method to get numpy value of the tensor.

In your local, you have TensorFlow 1.13.1 which runs the model in graph mode. You can access the numpy value of the graph tensor (not eager tensor) by running a session and evaluating the tensor. Here is one tutorial on TF1.x with mnist data. Overall, in TF1.x, you need to create a graph, create placeholders, then run a Session, feed the data to placeholders, and train the model, then predict.

Other alternative is to install Tensorflow 2.x by running ""pip3 install tensorflow==2.1`. After installing, you can run the example without any issue.
",
tensorflow strange issue,https://stackoverflow.com/questions/58141530,"Inception settings changed: Computed output size would be negative: -4 [input_size: 2, effective_filter_size: 7, stride: 1]","I was working along the image classifier ML.net sample code over at https://learn.microsoft.com/en-US/dotnet/machine-learning/tutorials/image-classification

The classification there uses the following inception settings

private struct InceptionSettings
{
    public const int ImageHeight = 224;
    public const int ImageWidth = 224;
    public const float Mean = 117;
    public const float Scale = 1;
    public const bool ChannelsLast = true;
}


while using the tensorflow inception5h model.
It appears to be working. Unclear for me is however, what breaks when I change Height and Width from 224 to say 64 to reduce the load and precision of the prediction the reuse and tune inception model part nearly instantly crashes with

System.InvalidOperationException: Splitter/consolidator worker encountered exception while consuming source data ---&gt; Microsoft.ML.Transforms.TensorFlow.TFException: Computed output size would be negative: -4 [input_size: 2, effective_filter_size: 7, stride: 1]
     [[{{node avgpool0}}]]
   at Microsoft.ML.Transforms.TensorFlow.TFStatus.CheckMaybeRaise(TFStatus incomingStatus, Boolean last)
   at Microsoft.ML.Transforms.TensorFlow.TFSession.Run(TFOutput[] inputs, TFTensor[] inputValues, TFOutput[] outputs, TFOperation[] targetOpers, TFBuffer runMetadata, TFBuffer runOptions, TFStatus status)
   at Microsoft.ML.Transforms.TensorFlow.TFSession.Runner.Run(TFStatus status)
   at Microsoft.ML.Transforms.TensorFlowTransformer.Mapper.UpdateCacheIfNeeded(Int64 position, ITensorValueGetter[] srcTensorGetters, String[] activeOutputColNames, OutputCache outputCache)
   at Microsoft.ML.Transforms.TensorFlowTransformer.Mapper.&lt;&gt;c__DisplayClass8_0`1.&lt;MakeGetter&gt;b__3(VBuffer`1&amp; dst)
   at Microsoft.ML.Data.DataViewUtils.Splitter.InPipe.Impl`1.Fill()
   at Microsoft.ML.Data.DataViewUtils.Splitter.&lt;&gt;c__DisplayClass5_1.&lt;ConsolidateCore&gt;b__2()
   --- End of inner exception stack trace ---
   at Microsoft.ML.Data.DataViewUtils.Splitter.Batch.SetAll(OutPipe[] pipes)
   at Microsoft.ML.Data.DataViewUtils.Splitter.Cursor.MoveNextCore()
   at Microsoft.ML.Data.RootCursorBase.MoveNext()
   at Microsoft.ML.Trainers.TrainingCursorBase.MoveNext()
   at Microsoft.ML.Trainers.LbfgsTrainerBase`3.TrainCore(IChannel ch, RoleMappedData data)
   at Microsoft.ML.Trainers.LbfgsTrainerBase`3.TrainModelCore(TrainContext context)
   at Microsoft.ML.Trainers.TrainerEstimatorBase`2.TrainTransformer(IDataView trainSet, IDataView validationSet, IPredictor initPredictor)
   at Microsoft.ML.Data.EstimatorChain`1.Fit(IDataView input)
   at D:\My\MLTrainer.Program.ReuseAndTuneInceptionModel(MLContext mlContext, TrainerData trainerData, String dataLocation, String inputModelLocation, String outputModelLocation) in MLTrainer\Program.cs:line 66
   at MLTrainer.Program.Main(String[] args) in D:\My\MLTrainer\Program.cs:line 29


Now I don't get what I can do and where in the details the issue is buried. Is the pre-trained model already fixed to this somewhat strange resolution? The resolution itself seems not to be used somewhere else, nor do I get why the splitter don't like it.

Do I hit some sort of min size condition I am not aware of? If so, what are the boundaries? I tried 1024x1024 for instance, which failed with another error.

Any hints on that are appreciated :)
",0,600,"You could install Netron to look at your model.

You'll see that on the first layer will stand something like Nx224x224x3.

The model is fixed to this input resolution, because it is trained with a large dataset in exactly this resolution. You could change the input layer with e.g. keras. 
But I never tried to change the input layer in ML.NET.
",,
tensorflow strange issue,https://stackoverflow.com/questions/55892554,Invoke endpoint after model deployment : [Err 104] Connection reset by peer,"I am new to Sagemaker. I have deployed my well trained model in tensorflow  by using Json and Weight file. But it is strange that in my note book, I didn't see it says ""Endpoint successfully built"". Only the below is shown:

--------------------------------------------------------------------------------!


Instead, I found the endpoint number from my console. 

import sagemaker
from sagemaker.tensorflow.model import TensorFlowModel
        predictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint_name, sagemaker_session)
data= test_out2
predictor.predict(data)


Then I try to invoke the endpoint by using 2D array:
(1) If my 2D array is in size of (5000, 170), I am getting the error:

ConnectionResetError: [Errno 104] Connection reset by peer


(2) If reducing the array to size of (10,170), error is :

ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message """". See https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#logEventViewer:group=/aws/sagemaker/Endpoints/sagemaker-tensorflow-2019-04-28-XXXXXXXXX in account 15XXXXXXXX for more information.


Any suggestion please? Found similar case in github, https://github.com/awslabs/amazon-sagemaker-examples/issues/589.

Is it the similar case please?

Thank you very much in advance!
",0,618,"I had this problem and this post helped me resolve it. There does seem to be a limit to the size of the dataset that the predictor will take. I'm not sure what it is, but in any case I now split my training/test data differently.

I assume there's a limit and the limit is based on raw data volume. In rough terms this would translate to the number of cells in my dataframe, since each cell is probably and integer or a float. 

If I can get a 70%/30% split I use that, but if 30% test data exceeds the maximum number of cells, I split my data to give me the maximum number of rows that will fit into the maximum.

Here's the split code:

# Check that the test data isn't too big for the predictor
max_test_cells = 200000
model_rows, model_cols = model_data.shape
print('model_data.shape=', model_data.shape)
max_test_rows = int(max_test_cells / model_cols)
print('max_test_rows=', max_test_rows)
test_rows = min(int(0.3 * len(model_data)), max_test_rows)
print('actual_test_rows=', test_rows)
training_rows = model_rows - test_rows
print('training_rows=', training_rows)

# Split the data to get the largest test set possible
train_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [training_rows])
print(train_data.shape, test_data.shape)

",,
tensorflow strange issue,https://stackoverflow.com/questions/46890399,Trouble scaling wide and deep model to train on google cloud ML,,0,416,,,
tensorflow strange issue,https://stackoverflow.com/questions/46559655,Queue manager for experiments in neural network training,,0,291,,,
tensorflow strange issue,https://stackoverflow.com/questions/45809554,Cannot run Google ML engine locally due to Tensorflow issues,,0,438,,,
tensorflow strange issue,https://stackoverflow.com/questions/43106054,Tensorflow. Conditionally trainable variables and stochastic depth neural networks,,0,472,,,
tensorflow strange issue,https://stackoverflow.com/questions/42925862,How do I import/use rfft in tensorflow?,,0,344,,,
tensorflow strange issue,https://stackoverflow.com/questions/42360167,In what order does TensorFlow evaluate nodes in a computation graph?,,0,390,,,
tensorflow strange issue,https://stackoverflow.com/questions/41031694,Equivalent of numpy.linalg.norm for TensorFlow,,0,2190,,,
tensorflow strange issue,https://stackoverflow.com/questions/39009808,TensorFlow Bidirectional GRU returns ValueError because of &lt;supposedly&gt; incorrect shape,,0,1141,,,
tensorflow strange issue,https://stackoverflow.com/questions/50891828,Can anyone help me identify the &quot;bug&quot; in my Google Cloud ML training job?,,-1,265,,,
tensorflow strange issue,https://stackoverflow.com/questions/43815733,Tensorflow argmax give false indizes,,-1,255,,,
PyTorch unexpected behavior,https://stackoverflow.com/questions/49103737,PyTorch naive single label classification with embedding layer fails at random,"I am new to PyTorch and I am trying out the Embedding Layer. 

I wrote a naive classification task, where all the inputs are the equal and all the labels are set to 1.0. I hence expect the model to learn quickly to predict 1.0.

The input is always 0, which is fed into a nn.Embedding(1,32) layer, followed by nn.Linear(32,1) and nn.Relu().

However, an unexpected and undesired behavior occurs: training outcome is different for different times I run the code.
For example, 


setting the random seed to 10, model converges: loss decreases and model always predicts 1.0
setting the random seed to 1111, model doesn't converge: loss doesn't decrease and model always predicts 0.5. In those cases the parameters are not updated


Here is the minimal, replicable code:

from torch.nn import BCEWithLogitsLoss
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.autograd import Variable
from torch.utils.data import Dataset
import torch


class MyModel(nn.Module):

    def __init__(self):
        super(MyModel, self).__init__()
        self.vgg_fc = nn.Linear(32, 1)
        self.relu = nn.ReLU()
        self.embeddings = nn.Embedding(1, 32)

    def forward(self, data):
        emb = self.embeddings(data['index'])
        return self.relu(self.vgg_fc(emb))


class MyDataset(Dataset):

    def __init__(self):
        pass
    def __len__(self):
        return 1000
    def __getitem__(self, idx):
        return {'label': 1.0, 'index': 0}


def train():
    model = MyModel()
    db = MyDataset()
    dataloader = DataLoader(db, batch_size=256, shuffle=True, num_workers=16)

    loss_function = BCEWithLogitsLoss()
    optimizer_rel = optim.SGD(model.parameters(), lr=0.1)

    for epoch in range(50):
        for i_batch, sample_batched in enumerate(dataloader):

            model.zero_grad()
            out = model({'index': Variable(sample_batched['index'])})

            labels = Variable(sample_batched['label'].type(torch.FloatTensor).view(sample_batched['label'].shape[0], 1))

            loss = loss_function(out, labels)
            loss.backward()
            optimizer_rel.step()
            print 'Epoch:', epoch, 'batch', i_batch, 'Tr_Loss:', loss.data[0]
    return model


if __name__ == '__main__':

    # please, try seed 10 (converge) and seed 1111 (fails)
    torch.manual_seed(10)
    train()


Without specifying the random seed, different runs have different outcome. 

Why is, in those cases, the model unable to learn such a easy task?
Is there any mistake in the way I use nn.Embedding layer?

Thank you 
",3,1522,"I found the problem was the final relu layer, before the sigmoid.
As stated here, that layer will:


  throw away information without adding any additional benefit


Removing the layer, the network learned as expected with any seed.
",,
PyTorch unexpected behavior,https://stackoverflow.com/questions/68624392,Running out of memory with pytorch,"I am trying to train a model using huggingface's wav2vec for audio classification. I keep getting this error:
The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: name, emotion, path.
***** Running training *****
  Num examples = 2708
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 64
  Gradient Accumulation steps = 2
  Total optimization steps = 42
 [ 2/42 : &lt; :, Epoch 0.02/1]
Step    Training Loss   Validation Loss

RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 61, in _worker
    output = module(*input, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File ""&lt;ipython-input-81-dd9fe3ea0f13&gt;"", line 77, in forward
    return_dict=return_dict,
  File ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 1073, in forward
    return_dict=return_dict,
  File ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 732, in forward
    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions
  File ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 574, in forward
    hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))
  File ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py"", line 510, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File ""/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/functional.py"", line 1555, in gelu
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.17 GiB total capacity; 10.49 GiB already allocated; 11.44 MiB free; 10.68 GiB reserved in total by PyTorch)

I'm on an AWS ubuntu deep learning AMI ec2.
I've been researching this a lot. I've already tried:

reducing the batch size (I want 4, but I've gone down to 1 with no change in error)
adding:
import gc
gc.collect()
torch.cuda.empty_cache()


removing all wav files in my dataset that are longer than 6 seconds

Is there anything else I can do? I'm on a p2.8xlarge dataset with 105 GiB mounted.
Running torch.cuda.memory_summary(device=None, abbreviated=False)  gives me:
|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 3            |        cudaMalloc retries: 4         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |    7550 MB |   10852 MB |  209624 MB |  202073 MB |\n|       from large pool |    7544 MB |   10781 MB |  209325 MB |  201780 MB |\n|       from small pool |       5 MB |      87 MB |     298 MB |     293 MB |\n|---------------------------------------------------------------------------|\n| Active memory         |    7550 MB |   10852 MB |  209624 MB |  202073 MB |\n|       from large pool |    7544 MB |   10781 MB |  209325 MB |  201780 MB |\n|       from small pool |       5 MB |      87 MB |     298 MB |     293 MB |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |   10936 MB |   10960 MB |   63236 MB |   52300 MB |\n|       from large pool |   10928 MB |   10954 MB |   63124 MB |   52196 MB |\n|       from small pool |       8 MB |      98 MB |     112 MB |     104 MB |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |  443755 KB |    1309 MB |  155426 MB |  154992 MB |\n|       from large pool |  443551 KB |    1306 MB |  155081 MB |  154648 MB |\n|       from small pool |     204 KB |      12 MB |     344 MB |     344 MB |\n|---------------------------------------------------------------------------|\n| Allocations           |    1940    |    2622    |   32288    |   30348    |\n|       from large pool |    1036    |    1618    |   21855    |   20819    |\n|       from small pool |     904    |    1203    |   10433    |    9529    |\n|---------------------------------------------------------------------------|\n| Active allocs         |    1940    |    2622    |   32288    |   30348    |\n|       from large pool |    1036    |    1618    |   21855    |   20819    |\n|       from small pool |     904    |    1203    |   10433    |    9529    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |     495    |     495    |    2169    |    1674    |\n|       from large pool |     491    |     491    |    2113    |    1622    |\n|       from small pool |       4    |      49    |      56    |      52    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |     179    |     335    |   15998    |   15819    |\n|       from large pool |     165    |     272    |   12420    |   12255    |\n|       from small pool |      14    |      63    |    3578    |    3564    |\n|===========================================================================|\n'

After reducing data only to inputs that are less tahn 2 seconds in length, it trains a lot further but still errors with this:
The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path, emotion, name.
***** Running training *****
  Num examples = 1411
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp; accumulation) = 64
  Gradient Accumulation steps = 2
  Total optimization steps = 22
/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 [11/22 01:12 &lt; 01:28, 0.12 it/s, Epoch 0.44/1]
Step    Training Loss   Validation Loss Accuracy
10  2.428100    2.257138    0.300283
The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path, emotion, name.
***** Running Evaluation *****
  Num examples = 353
  Batch size = 32
Saving model checkpoint to trainingArgs/checkpoint-10
Configuration saved in trainingArgs/checkpoint-10/config.json
Model weights saved in trainingArgs/checkpoint-10/pytorch_model.bin
Configuration saved in trainingArgs/checkpoint-10/preprocessor_config.json
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)
    378             with _open_zipfile_writer(opened_file) as opened_zipfile:
--&gt; 379                 _save(obj, opened_zipfile, pickle_module, pickle_protocol)
    380                 return

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in _save(obj, zip_file, pickle_module, pickle_protocol)
    498         num_bytes = storage.size() * storage.element_size()
--&gt; 499         zip_file.write_record(name, storage.data_ptr(), num_bytes)
    500 

OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-25-3435b262f1ae&gt; in &lt;module&gt;
----&gt; 1 trainer.train()

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1334                     self.control = self.callback_handler.on_step_end(args, self.state, self.control)
   1335 
-&gt; 1336                     self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
   1337                 else:
   1338                     self.control = self.callback_handler.on_substep_end(args, self.state, self.control)

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py in _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)
   1441 
   1442         if self.control.should_save:
-&gt; 1443             self._save_checkpoint(model, trial, metrics=metrics)
   1444             self.control = self.callback_handler.on_save(self.args, self.state, self.control)
   1445 

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py in _save_checkpoint(self, model, trial, metrics)
   1531         elif self.args.should_save and not self.deepspeed:
   1532             # deepspeed.save_checkpoint above saves model/optim/sched
-&gt; 1533             torch.save(self.optimizer.state_dict(), os.path.join(output_dir, ""optimizer.pt""))
   1534             with warnings.catch_warnings(record=True) as caught_warnings:
   1535                 torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, ""scheduler.pt""))

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)
    378             with _open_zipfile_writer(opened_file) as opened_zipfile:
    379                 _save(obj, opened_zipfile, pickle_module, pickle_protocol)
--&gt; 380                 return
    381         _legacy_save(obj, opened_file, pickle_module, pickle_protocol)
    382 

~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/serialization.py in __exit__(self, *args)
    257 
    258     def __exit__(self, *args) -&gt; None:
--&gt; 259         self.file_like.write_end_of_file()
    260         self.buffer.flush()
    261 

RuntimeError: [enforce fail at inline_container.cc:298] . unexpected pos 1849920000 vs 1849919888

When I run !free in the notebook, I get:
The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.
              total        used        free      shared  buff/cache   available
Mem:      503392908     6223452   478499292      346492    18670164   492641984
Swap:             0           0           0

For training code, I am essentially running this colab notebook as an example:
https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb#scrollTo=6M8bNvLLJnG1
All that I am changing is the incoming data/labels, which I have intentionally fit into the same directory structure used in the tutorial notebook. The tutorial notebook runs fine for some reason, even though my data has comparable size/num classes.
",1,2460,"You might use the DataParallel or DistributedDataParallel framework in Pytorch
model = Model(input_size, output_size)
if torch.cuda.device_count() &gt; 1:
  print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")
  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs
  model = nn.DataParallel(model)

model.to(device)

In this approach the model get replicated on each device (gpu) and the data is distributed across devices

DataParallel splits your data automatically and sends job orders to
multiple models on several GPUs. After each model finishes their job,
DataParallel collects and merges the results before returning it to
you.

Further examples here https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.
If the model does not fit in the memory of one gpu, then a model parallel approach should be resorted to.
From your existing model you might tell which layer sits on which gpu with .to('cuda:0'), .to('cuda:1') etc.
class ModelParallelResNet50(ResNet):
    def __init__(self, *args, **kwargs):
        super(ModelParallelResNet50, self).__init__(
            Bottleneck, [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs)

        self.seq1 = nn.Sequential(
            self.conv1,
            self.bn1,
            self.relu,
            self.maxpool,

            self.layer1,
            self.layer2
        ).to('cuda:0')

        self.seq2 = nn.Sequential(
            self.layer3,
            self.layer4,
            self.avgpool,
        ).to('cuda:1')

        self.fc.to('cuda:1')

    def forward(self, x):
        x = self.seq2(self.seq1(x).to('cuda:1'))
        return self.fc(x.view(x.size(0), -1))

Since you might lose performance, a pipelining approach might be of use, i.e. further chunking input data into batches which are run in parallel on the different devices.
",,
PyTorch unexpected behavior,https://stackoverflow.com/questions/60240434,PyTorch MaxPool2D unexpected behavior with padding=1,"I was playing around with MaxPool2D in PyTorch and discovered strange behavior when setting padding=1. Here is what I got:

Code:

import torch
from torch.nn.functional import max_pool2d

TEST = 1


def test_maxpool(negative=False, tnsr_size=2, kernel_size=2, stride=2, padding=0):
    """"""Test MaxPool2D.
    """"""
    global TEST
    print(f'=== TEST {TEST} ===')
    print(*[f'{i[0]}: {i[1]}' for i in locals().items()], sep=' | ')

    inp = torch.arange(1., tnsr_size ** 2 + 1).reshape(1, tnsr_size, tnsr_size)
    inp = -inp if negative else inp

    print('In:')
    print(inp)

    out = max_pool2d(inp, kernel_size, stride, padding=padding)
    print('Out:')
    print(out)
    print()
    TEST += 1


test_maxpool()
test_maxpool(True)

test_maxpool(padding=1)
test_maxpool(True, padding=1)


Out:

=== TEST 1 ===
negative: False | tnsr_size: 2 | kernel_size: 2 | stride: 2 | padding: 0
In:
tensor([[[1., 2.],
         [3., 4.]]])
Out:
tensor([[[4.]]])

=== TEST 2 ===
negative: True | tnsr_size: 2 | kernel_size: 2 | stride: 2 | padding: 0
In:
tensor([[[-1., -2.],
         [-3., -4.]]])
Out:
tensor([[[-1.]]])

=== TEST 3 ===
negative: False | tnsr_size: 2 | kernel_size: 2 | stride: 2 | padding: 1
In:
tensor([[[1., 2.],
         [3., 4.]]])
Out:
tensor([[[1., 2.],
         [3., 4.]]])

=== TEST 4 ===
negative: True | tnsr_size: 2 | kernel_size: 2 | stride: 2 | padding: 1
In:
tensor([[[-1., -2.],
         [-3., -4.]]])
Out:
tensor([[[-1., -2.],
         [-3., -4.]]])


Tests 1, 2, 3 are fine but Test 4 is weird, I expected to get [[0 0], [0 0]] tensor:

In:
[[-1 -2]
 [-3 -4]]

+ padding -&gt; 

[[ 0  0  0  0]
 [ 0 -1 -2  0]
 [ 0 -3 -4  0]
 [ 0  0  0  0]]

-&gt; kernel_size=2, stride=2 -&gt;

[[0 0]
 [0 0]]


According to Test 3 zero padding was used but Test 4 produced controversial result.

What kind of padding (if any) was that? Why does MaxPool2D behave like that?

pytorch 1.3.1
",1,778,"The documentation is still incorrect in the Parameters section, where it says:

padding  implicit zero padding to be added on both sides

","This was expected behavior since negative infinity padding is done by default.
The documentation for MaxPool is now fixed. See this PR: Fix MaxPool default pad documentation #59404 .
",
PyTorch unexpected behavior,https://stackoverflow.com/questions/73374386,"why is ray Tune with pytorch HPO error &#39;trials did not complete, incomplete trials&#39;?","Could someone explain why this code (that I took from here):
## Standard libraries
import os
import json
import math
import numpy as np 
import time

## Imports for plotting
import matplotlib.pyplot as plt
#%matplotlib inline 
#from IPython.display import set_matplotlib_formats
#set_matplotlib_formats('svg', 'pdf') # For export
from matplotlib.colors import to_rgb
import matplotlib
matplotlib.rcParams['lines.linewidth'] = 2.0
import seaborn as sns
sns.reset_orig()
sns.set()
import torch_geometric
import torch_geometric.nn as geom_nn
import torch_geometric.data as geom_data
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint
## Progress bar
from tqdm.notebook import tqdm

## PyTorch
import torch
import torchmetrics
from torchmetrics.functional import precision_recall
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as data
import torch.optim as optim
# Torchvision
import torchvision
from torchvision.datasets import CIFAR10
from torchvision import transforms
# PyTorch Lightning
import pytorch_lightning as pl
from ray import tune

def __init__(self, config):
  super(LightningMNISTClassifier, self).__init__()
  self.layer_1_size = config[""layer_1_size""]
  self.layer_2_size = config[""layer_2_size""]
  self.lr = config[""lr""]
  self.batch_size = config[""batch_size""]

from ray.tune.integration.pytorch_lightning import TuneReportCallback
callback = TuneReportCallback(
    {
        ""loss"": ""val_loss"",
        ""mean_accuracy"": ""val_accuracy""
    },
    on=""validation_end"")


def train_tune(config, epochs=10, gpus=0):
  model = LightningMNISTClassifier(config)
  trainer = pl.Trainer(
    max_epochs=epochs,
    gpus=gpus,
    progress_bar_refresh_rate=0,
    callbacks=[callback])
  trainer.fit(model)

config = {
  ""layer_1_size"": tune.choice([32, 64, 128]),
  ""layer_2_size"": tune.choice([64, 128, 256]),
  ""lr"": tune.loguniform(1e-4, 1e-1),
  ""batch_size"": tune.choice([32, 64, 128])
}


def train_tune(config, epochs=10, gpus=0):
  model = LightningMNISTClassifier(config)
  trainer = pl.Trainer(
    max_epochs=epochs,
    gpus=gpus,
    progress_bar_refresh_rate=0,
    callbacks=[callback])
  trainer.fit(model)


from functools import partial
tune.run(
  partial(train_tune, epochs=10, gpus=0),
  config=config,
  num_samples=10)

generates this error:
Traceback (most recent call last):
  File ""example_hpo_working.py"", line 89, in &lt;module&gt;
    num_samples=10)
  File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/tune.py"", line 741, in run
    raise TuneError(""Trials did not complete"", incomplete_trials)
ray.tune.error.TuneError: ('Trials did not complete', [train_tune_6f362_00000, train_tune_6f362_00001, train_tune_6f362_00002, train_tune_6f362_00003, train_tune_6f362_00004, train_tune_6f362_00005, train_tune_6f362_00006, train_tune_6f362_00007, train_tune_6f362_00008, train_tune_6f362_00009])

I can see a similar question was asked here but not answered (the ultimate aim is to use ray hyperparameter optimisation with a pytorch network).
This is the full trace from the code:
2022-08-16 15:44:08,204 WARNING function_runner.py:604 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.
2022-08-16 15:44:08,411 ERROR syncer.py:147 -- Log sync requires rsync to be installed.
== Status ==
Memory usage on this node: 16.8/86.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 1.0/64 CPUs, 0/0 GPUs, 0.0/62.79 GiB heap, 0.0/9.31 GiB objects
Result logdir: /root/ray_results/train_tune_2022-08-16_15-44-08
Number of trials: 10/10 (9 PENDING, 1 RUNNING)
+------------------------+----------+------------------+--------------+----------------+----------------+-------------+
| Trial name             | status   | loc              |   batch_size |   layer_1_size |   layer_2_size |          lr |
|------------------------+----------+------------------+--------------+----------------+----------------+-------------|
| train_tune_43fd5_00000 | RUNNING  | 172.17.0.2:41684 |           64 |             64 |            256 | 0.00233834  |
| train_tune_43fd5_00001 | PENDING  |                  |           64 |             64 |            256 | 0.00155955  |
| train_tune_43fd5_00002 | PENDING  |                  |          128 |            128 |             64 | 0.00399358  |
| train_tune_43fd5_00003 | PENDING  |                  |          128 |            128 |             64 | 0.000184477 |

...deleted a few similar lines here

..and then there's:
    

(func pid=41684) 2022-08-16 15:44:10,774        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41684) Traceback (most recent call last):
(func pid=41684)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41684)     self._entrypoint()
(func pid=41684)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41684)     self._status_reporter.get_checkpoint(),
(func pid=41684)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41684)     return method(self, *_args, **_kwargs)
(func pid=41684)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41684)     output = fn()
(func pid=41684)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41684)     model = LightningMNISTClassifier(config)
(func pid=41684) NameError: name 'LightningMNISTClassifier' is not defined
2022-08-16 15:44:10,977 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00000: Error processing event.
NoneType: None
Result for train_tune_43fd5_00000:
  date: 2022-08-16_15-44-10
  experiment_id: c8977e85cbf84a9badff15fb2de6f516
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41684
  timestamp: 1660664650
  trial_id: 43fd5_00000
  
(func pid=41722) 2022-08-16 15:44:13,241        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41722) Traceback (most recent call last):
(func pid=41722)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41722)     self._entrypoint()
(func pid=41722)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41722)     self._status_reporter.get_checkpoint(),
(func pid=41722)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41722)     return method(self, *_args, **_kwargs)
(func pid=41722)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41722)     output = fn()
(func pid=41722)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41722)     model = LightningMNISTClassifier(config)
(func pid=41722) NameError: name 'LightningMNISTClassifier' is not defined
(func pid=41720) 2022-08-16 15:44:13,253        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41720) Traceback (most recent call last):
(func pid=41720)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41720)     self._entrypoint()
(func pid=41720)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41720)     self._status_reporter.get_checkpoint(),
(func pid=41720)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41720)     return method(self, *_args, **_kwargs)
(func pid=41720)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41720)     output = fn()
(func pid=41720)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41720)     model = LightningMNISTClassifier(config)
(func pid=41720) NameError: name 'LightningMNISTClassifier' is not defined
(func pid=41718) 2022-08-16 15:44:13,253        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41718) Traceback (most recent call last):
(func pid=41718)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41718)     self._entrypoint()
(func pid=41718)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41718)     self._status_reporter.get_checkpoint(),
(func pid=41718)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41718)     return method(self, *_args, **_kwargs)
(func pid=41718)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41718)     output = fn()
(func pid=41718)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41718)     model = LightningMNISTClassifier(config)
(func pid=41718) NameError: name 'LightningMNISTClassifier' is not defined
(func pid=41734) 2022-08-16 15:44:13,340        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41734) Traceback (most recent call last):
(func pid=41734)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41734)     self._entrypoint()
(func pid=41734)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41734)     self._status_reporter.get_checkpoint(),
(func pid=41734)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41734)     return method(self, *_args, **_kwargs)
(func pid=41734)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41734)     output = fn()
(func pid=41734)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41734)     model = LightningMNISTClassifier(config)
(func pid=41734) NameError: name 'LightningMNISTClassifier' is not defined
(func pid=41732) 2022-08-16 15:44:13,325        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41732) Traceback (most recent call last):
(func pid=41732)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41732)     self._entrypoint()
(func pid=41732)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41732)     self._status_reporter.get_checkpoint(),
(func pid=41732)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41732)     return method(self, *_args, **_kwargs)
(func pid=41732)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41732)     output = fn()
(func pid=41732)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41732)     model = LightningMNISTClassifier(config)
(func pid=41732) NameError: name 'LightningMNISTClassifier' is not defined
(func pid=41728) 2022-08-16 15:44:13,309        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41728) Traceback (most recent call last):
(func pid=41728)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41728)     self._entrypoint()
(func pid=41728)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41728)     self._status_reporter.get_checkpoint(),
(func pid=41728)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41728)     return method(self, *_args, **_kwargs)
(func pid=41728)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41728)     output = fn()
(func pid=41728)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41728)     model = LightningMNISTClassifier(config)
(func pid=41728) NameError: name 'LightningMNISTClassifier' is not defined
(func pid=41730) 2022-08-16 15:44:13,272        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41730) Traceback (most recent call last):
(func pid=41730)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41730)     self._entrypoint()
(func pid=41730)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41730)     self._status_reporter.get_checkpoint(),
(func pid=41730)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41730)     return method(self, *_args, **_kwargs)
(func pid=41730)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41730)     output = fn()
(func pid=41730)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41730)     model = LightningMNISTClassifier(config)
(func pid=41730) NameError: name 'LightningMNISTClassifier' is not defined
2022-08-16 15:44:13,444 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00003: Error processing event.
NoneType: None
Result for train_tune_43fd5_00003:
  date: 2022-08-16_15-44-13
  experiment_id: 02204d81b72943e3bbfcc822d35f02a0
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41722
  timestamp: 1660664653
  trial_id: 43fd5_00003
  
(func pid=41724) 2022-08-16 15:44:13,457        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41724) Traceback (most recent call last):
(func pid=41724)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41724)     self._entrypoint()
(func pid=41724)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41724)     self._status_reporter.get_checkpoint(),
(func pid=41724)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41724)     return method(self, *_args, **_kwargs)
(func pid=41724)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41724)     output = fn()
(func pid=41724)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41724)     model = LightningMNISTClassifier(config)
(func pid=41724) NameError: name 'LightningMNISTClassifier' is not defined
== Status ==
Current time: 2022-08-16 15:44:13 (running for 00:00:05.24)
Memory usage on this node: 17.6/86.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 8.0/64 CPUs, 0/0 GPUs, 0.0/62.79 GiB heap, 0.0/9.31 GiB objects
Result logdir: /root/ray_results/train_tune_2022-08-16_15-44-08
Number of trials: 10/10 (2 ERROR, 8 RUNNING)
+------------------------+----------+------------------+--------------+----------------+----------------+-------------+
| Trial name             | status   | loc              |   batch_size |   layer_1_size |   layer_2_size |          lr |
|------------------------+----------+------------------+--------------+----------------+----------------+-------------|
| train_tune_43fd5_00001 | RUNNING  | 172.17.0.2:41718 |           64 |             64 |            256 | 0.00155955  |
| train_tune_43fd5_00002 | RUNNING  | 172.17.0.2:41720 |          128 |            128 |             64 | 0.00399358  |
| train_tune_43fd5_00004 | RUNNING  | 172.17.0.2:41724 |          128 |             64 |            128 | 0.0221855   |
| train_tune_43fd5_00005 | RUNNING  | 172.17.0.2:41726 |           64 |            128 |            128 | 0.00041038  |
| train_tune_43fd5_00006 | RUNNING  | 172.17.0.2:41728 |           64 |             64 |            256 | 0.0105243   |
| train_tune_43fd5_00007 | RUNNING  | 172.17.0.2:41730 |          128 |             32 |            256 | 0.000929454 |
| train_tune_43fd5_00008 | RUNNING  | 172.17.0.2:41732 |           64 |             64 |            128 | 0.00176483  |
| train_tune_43fd5_00009 | RUNNING  | 172.17.0.2:41734 |          128 |             32 |            256 | 0.000113077 |
| train_tune_43fd5_00000 | ERROR    | 172.17.0.2:41684 |           64 |             64 |            256 | 0.00233834  |
| train_tune_43fd5_00003 | ERROR    | 172.17.0.2:41722 |          128 |            128 |             64 | 0.000184477 |
+------------------------+----------+------------------+--------------+----------------+----------------+-------------+
Number of errored trials: 2
+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name             |   # failures | error file                                                                                                                                                        |
|------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_tune_43fd5_00000 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00000_0_batch_size=64,layer_1_size=64,layer_2_size=256,lr=0.0023_2022-08-16_15-44-08/error.txt  |
| train_tune_43fd5_00003 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00003_3_batch_size=128,layer_1_size=128,layer_2_size=64,lr=0.0002_2022-08-16_15-44-10/error.txt |
+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+

2022-08-16 15:44:13,487 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00001: Error processing event.
NoneType: None
Result for train_tune_43fd5_00001:
  date: 2022-08-16_15-44-13
  experiment_id: e738348e77c64919931d70c916cbfaf8
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41718
  timestamp: 1660664653
  trial_id: 43fd5_00001
  
2022-08-16 15:44:13,490 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00007: Error processing event.
NoneType: None
Result for train_tune_43fd5_00007:
  date: 2022-08-16_15-44-13
  experiment_id: f79be7b9e98a43f1a41893071c4e1f6b
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41730
  timestamp: 1660664653
  trial_id: 43fd5_00007
  
2022-08-16 15:44:13,493 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00002: Error processing event.
NoneType: None
Result for train_tune_43fd5_00002:
  date: 2022-08-16_15-44-13
  experiment_id: 8e7422287e3e44f9b2e7b249a8ae18cd
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41720
  timestamp: 1660664653
  trial_id: 43fd5_00002
  
2022-08-16 15:44:13,512 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00006: Error processing event.
NoneType: None
Result for train_tune_43fd5_00006:
  date: 2022-08-16_15-44-13
  experiment_id: 2d56b152a6a34e1f9e26dad1aec25d00
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41728
  timestamp: 1660664653
  trial_id: 43fd5_00006
  
2022-08-16 15:44:13,527 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00008: Error processing event.
NoneType: None
Result for train_tune_43fd5_00008:
  date: 2022-08-16_15-44-13
  experiment_id: b2158026b3b947bfbb9c3da4e6f7b977
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41732
  timestamp: 1660664653
  trial_id: 43fd5_00008
  
2022-08-16 15:44:13,543 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00009: Error processing event.
NoneType: None
Result for train_tune_43fd5_00009:
  date: 2022-08-16_15-44-13
  experiment_id: 6b5a73f09241440085bd6c09f6f681e9
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41734
  timestamp: 1660664653
  trial_id: 43fd5_00009
  
(func pid=41726) 2022-08-16 15:44:13,484        ERROR function_runner.py:286 -- Runner Thread raised error.
(func pid=41726) Traceback (most recent call last):
(func pid=41726)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(func pid=41726)     self._entrypoint()
(func pid=41726)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(func pid=41726)     self._status_reporter.get_checkpoint(),
(func pid=41726)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(func pid=41726)     return method(self, *_args, **_kwargs)
(func pid=41726)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(func pid=41726)     output = fn()
(func pid=41726)   File ""example_hpo_working.py"", line 76, in train_tune
(func pid=41726)     model = LightningMNISTClassifier(config)
(func pid=41726) NameError: name 'LightningMNISTClassifier' is not defined
2022-08-16 15:44:13,660 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00004: Error processing event.
NoneType: None
Result for train_tune_43fd5_00004:
  date: 2022-08-16_15-44-13
  experiment_id: 60f51e072c7942bdb5d9298e0e147555
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41724
  timestamp: 1660664653
  trial_id: 43fd5_00004
  
2022-08-16 15:44:13,687 ERROR trial_runner.py:886 -- Trial train_tune_43fd5_00005: Error processing event.
NoneType: None
Result for train_tune_43fd5_00005:
  date: 2022-08-16_15-44-13
  experiment_id: 79701d1c19ac4c55b5a73746c1872724
  hostname: 0e26c6a24ffa
  node_ip: 172.17.0.2
  pid: 41726
  timestamp: 1660664653
  trial_id: 43fd5_00005
  
== Status ==
Current time: 2022-08-16 15:44:13 (running for 00:00:05.46)
Memory usage on this node: 16.4/86.4 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/64 CPUs, 0/0 GPUs, 0.0/62.79 GiB heap, 0.0/9.31 GiB objects
Result logdir: /root/ray_results/train_tune_2022-08-16_15-44-08
Number of trials: 10/10 (10 ERROR)
+------------------------+----------+------------------+--------------+----------------+----------------+-------------+
| Trial name             | status   | loc              |   batch_size |   layer_1_size |   layer_2_size |          lr |
|------------------------+----------+------------------+--------------+----------------+----------------+-------------|
| train_tune_43fd5_00000 | ERROR    | 172.17.0.2:41684 |           64 |             64 |            256 | 0.00233834  |
| train_tune_43fd5_00001 | ERROR    | 172.17.0.2:41718 |           64 |             64 |            256 | 0.00155955  |
| train_tune_43fd5_00002 | ERROR    | 172.17.0.2:41720 |          128 |            128 |             64 | 0.00399358  |
| train_tune_43fd5_00003 | ERROR    | 172.17.0.2:41722 |          128 |            128 |             64 | 0.000184477 |
| train_tune_43fd5_00004 | ERROR    | 172.17.0.2:41724 |          128 |             64 |            128 | 0.0221855   |
| train_tune_43fd5_00005 | ERROR    | 172.17.0.2:41726 |           64 |            128 |            128 | 0.00041038  |
| train_tune_43fd5_00006 | ERROR    | 172.17.0.2:41728 |           64 |             64 |            256 | 0.0105243   |
| train_tune_43fd5_00007 | ERROR    | 172.17.0.2:41730 |          128 |             32 |            256 | 0.000929454 |
| train_tune_43fd5_00008 | ERROR    | 172.17.0.2:41732 |           64 |             64 |            128 | 0.00176483  |
| train_tune_43fd5_00009 | ERROR    | 172.17.0.2:41734 |          128 |             32 |            256 | 0.000113077 |
+------------------------+----------+------------------+--------------+----------------+----------------+-------------+
Number of errored trials: 10
+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Trial name             |   # failures | error file                                                                                                                                                        |
|------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| train_tune_43fd5_00000 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00000_0_batch_size=64,layer_1_size=64,layer_2_size=256,lr=0.0023_2022-08-16_15-44-08/error.txt  |
| train_tune_43fd5_00001 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00001_1_batch_size=64,layer_1_size=64,layer_2_size=256,lr=0.0016_2022-08-16_15-44-10/error.txt  |
| train_tune_43fd5_00002 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00002_2_batch_size=128,layer_1_size=128,layer_2_size=64,lr=0.0040_2022-08-16_15-44-10/error.txt |
| train_tune_43fd5_00003 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00003_3_batch_size=128,layer_1_size=128,layer_2_size=64,lr=0.0002_2022-08-16_15-44-10/error.txt |
| train_tune_43fd5_00004 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00004_4_batch_size=128,layer_1_size=64,layer_2_size=128,lr=0.0222_2022-08-16_15-44-10/error.txt |
| train_tune_43fd5_00005 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00005_5_batch_size=64,layer_1_size=128,layer_2_size=128,lr=0.0004_2022-08-16_15-44-10/error.txt |
| train_tune_43fd5_00006 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00006_6_batch_size=64,layer_1_size=64,layer_2_size=256,lr=0.0105_2022-08-16_15-44-10/error.txt  |
| train_tune_43fd5_00007 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00007_7_batch_size=128,layer_1_size=32,layer_2_size=256,lr=0.0009_2022-08-16_15-44-10/error.txt |
| train_tune_43fd5_00008 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00008_8_batch_size=64,layer_1_size=64,layer_2_size=128,lr=0.0018_2022-08-16_15-44-10/error.txt  |
| train_tune_43fd5_00009 |            1 | /root/ray_results/train_tune_2022-08-16_15-44-08/train_tune_43fd5_00009_9_batch_size=128,layer_1_size=32,layer_2_size=256,lr=0.0001_2022-08-16_15-44-10/error.txt |
+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+

Traceback (most recent call last):
  File ""example_hpo_working.py"", line 89, in &lt;module&gt;
    num_samples=10)
  File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/tune.py"", line 741, in run
    raise TuneError(""Trials did not complete"", incomplete_trials)
ray.tune.error.TuneError: ('Trials did not complete', [train_tune_43fd5_00000, train_tune_43fd5_00001, train_tune_43fd5_00002, train_tune_43fd5_00003, train_tune_43fd5_00004, train_tune_43fd5_00005, train_tune_43fd5_00006, train_tune_43fd5_00007, train_tune_43fd5_00008, train_tune_43fd5_00009])

",0,443,"Is there a longer stacktrace where the real error is printed?
Also could you go to the result folder and see the error file?
Usually result folder is under ~/ray_results.
","I believe you have a typo in your code:
def __init__(self, config):
  super(LightningMNISTClassifier, self).__init__()
  self.layer_1_size = config[""layer_1_size""]
  self.layer_2_size = config[""layer_2_size""]
  self.lr = config[""lr""]
  self.batch_size = config[""batch_size""]

You need to define LightningMNISTClassifier properly. Maybe try using this example?
https://docs.ray.io/en/master/tune/examples/includes/mnist_ptl_mini.html
Note that you may need to install the latest version of Ray.
",
PyTorch unexpected result,https://stackoverflow.com/questions/67117097,_C.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN6caffe28TypeMeta21_typeMetaDataInstanceIdEEPKNS_6detail12TypeMetaDataEv,"What is the reason for this error and how can I fix it? I am running the code from this repo: https://github.com/facebookresearch/frankmocap
(frank) mona@goku:~/research/code/frankmocap$ python -m demo.demo_frankmocap --input_path ./sample_data/han_short.mp4 --out_dir ./mocap_output
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/mona/research/code/frankmocap/demo/demo_frankmocap.py"", line 25, in &lt;module&gt;
    from handmocap.hand_bbox_detector import HandBboxDetector
  File ""/home/mona/research/code/frankmocap/handmocap/hand_bbox_detector.py"", line 33, in &lt;module&gt;
    from detectors.hand_object_detector.lib.model.roi_layers import nms # might raise segmentation fault at the end of program
  File ""/home/mona/research/code/frankmocap/detectors/hand_object_detector/lib/model/roi_layers/__init__.py"", line 3, in &lt;module&gt;
    from .nms import nms
  File ""/home/mona/research/code/frankmocap/detectors/hand_object_detector/lib/model/roi_layers/nms.py"", line 3, in &lt;module&gt;
    from model import _C
ImportError: /home/mona/research/code/frankmocap/detectors/hand_object_detector/lib/model/_C.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN6caffe28TypeMeta21_typeMetaDataInstanceIdEEPKNS_6detail12TypeMetaDataEv

I have:
$ lsb_release -a
LSB Version:    core-11.1.0ubuntu2-noarch:security-11.1.0ubuntu2-noarch
Distributor ID: Ubuntu
Description:    Ubuntu 20.04.2 LTS
Release:    20.04
Codename:   focal

and
$ python
Python 3.8.5 (default, Jan 27 2021, 15:41:15) 
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; torch.__version__
'1.8.1+cu111'
&gt;&gt;&gt; import detectron2
&gt;&gt;&gt; detectron2.__version__
'0.4'
&gt;&gt;&gt; from detectron2 import _C

and:
$ python -m detectron2.utils.collect_env
/home/mona/venv/frank/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() &gt; 0
No CUDA runtime is found, using CUDA_HOME='/usr'
---------------------  --------------------------------------------------------------------------
sys.platform           linux
Python                 3.8.5 (default, Jan 27 2021, 15:41:15) [GCC 9.3.0]
numpy                  1.19.5
detectron2             0.4 @/home/mona/venv/frank/lib/python3.8/site-packages/detectron2
Compiler               GCC 7.3
CUDA compiler          CUDA 11.1
DETECTRON2_ENV_MODULE  &lt;not set&gt;
PyTorch                1.8.1+cu111 @/home/mona/venv/frank/lib/python3.8/site-packages/torch
PyTorch debug build    False
GPU available          False
Pillow                 8.1.0
torchvision            0.9.1+cu111 @/home/mona/venv/frank/lib/python3.8/site-packages/torchvision
fvcore                 0.1.3.post20210311
cv2                    4.5.1
---------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

",14,54425,"This error usually shows up when there's a compatibility issue between the installed pytorch version and the detector library version(Detectron2 or mmdet).
Both the detector library and pytorch have to be built by the same CUDA version otherwise some packages will conflict when training your model.
There's a possibility that the Pytorch (1.8.1) + CUDA version (11.1) you have is incompatible with detectron2 v(0.4).
From the repo here, detectron2 v0.4 is built with torch 1.8 + cuda 11.1. It might help if you use torch 1.8.0 instead of 1.8.1
","I tried the solutions mentioned here, but that didn't fully solve the problem. However, when I tried solving a different error using this solution, it also solved this error for me. Use the following command:
pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html

","I've faced of this issue recently and creating a new environment with parameters below solved my problem.
PS : I assume that you're using torch with CUDA, if you're not you can install non-GPU versions.
torch version : 1.9 + cu111
torchvision : 0.10.0 + cu111
torchaudio : 0.9.0

You can download these from ;
pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f 

For Detectron2 ;
python -m pip install detectron2 -f \
  https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.9/index.html

https://download.pytorch.org/whl/torch_stable.html
Source for PyTorch previous versions link
"
PyTorch unexpected result,https://stackoverflow.com/questions/68609414,How to calculate correct Cross Entropy between 2 tensors in Pytorch when target is not one-hot?,,3,4110,"The fundamental problem is that you are incorrectly using the BCELoss function.
Cross-entropy loss is what you want. It is used to compute the loss between two arbitrary probability distributions. Indeed, its definition is exactly the equation that you provided:

where p is the target distribution and q is your predicted distribution. See this StackOverflow post for more information.
In your example where you provide the line
y = tf.convert_to_tensor([0.2, 0.2, 0.6])

you are implicitly modeling a multi-class classification problem where the target class can be one of three classes (the length of that tensor). More specifically, that line is saying that for this one data instance, class 0 has probably 0.2, class 1 has probability 0.2, and class 2 has probability 0.6.
The problem you are having is that PyTorch's BCELoss computes the binary cross-entropy loss, which is formulated differently. Binary cross-entropy loss computes the cross-entropy for classification problems where the target class can be only 0 or 1.
In binary cross-entropy, you only need one probability, e.g. 0.2, meaning that the probability of the instance being class 1 is 0.2. Correspondingly, class 0 has probability 0.8.
If you give the same tensor [0.2, 0.2, 0.6] to BCELoss, you are modeling a situation where there are three data instances, where data instance 0 has probability 0.2 of being class 1, data instance 1 has probability 0.2 of being class 1, and data instance 2 has probability 0.6 of being class 1.
Now, to your original question:

If I want to calculate the cross entropy between 2 tensors and the target tensor is not a one-hot label, which loss should I use?

Unfortunately, PyTorch does not have a cross-entropy function that takes in two probability distributions. See this question:
https://discuss.pytorch.org/t/how-should-i-implement-cross-entropy-loss-with-continuous-target-outputs/10720
The recommendation is to implement your own function using its equation definition. Here is code that works:
def cross_entropy(input, target):
    return torch.mean(-torch.sum(target * torch.log(input), 1))


y = torch.Tensor([[0.2, 0.2, 0.6]])
yhat = torch.Tensor([[0.1, 0.2, 0.7]])
cross_entropy(yhat, y)
# tensor(0.9964)

It provides the answer that you wanted.
","Update: from version 1.10, Pytorch supports class probability targets in CrossEntropyLoss, so you can now simply use:
criterion = torch.nn.CrossEntropyLoss()
loss = criterion(x, y)

where x is the input, y is the target. When y has the same shape as x, it's gonna be treated as class probabilities. Note that x is expected to contain raw, unnormalized scores for each class, while y is expected to contain probabilities for each class (typically the output of the softmax layer). You can find details in the docs.
","Maybe you should try the torch.nn.CrossEntropyLoss function
"
PyTorch unexpected result,https://stackoverflow.com/questions/65206818,Efficient way to speeding up graph theory and complex network algorithms on CPU/GPU using Python?,"P.S.: I've mentioned possible solutions to my problem but have many confusions with them, please provide me suggestions on them. Also if this question is not good for this site, please point me to the correct site and I'll move the question there. Thanks in advance.
I need to perform some repetitive graph theory and complex network algorithms to analyze approx 2000 undirected simple graphs with no self-loops for some research work. Each graph has approx 40,000 nodes and approx 600,000 edges (essentially making them sparse graphs).
Currently, I am using NetworkX for my analysis and currently running nx.algorithms.cluster.average_clustering(G) and nx.average_shortest_path_length(G) for 500 such graphs and the code is running for 3 days and have reached only halfway. This makes me fearful that my full analysis will take a huge and unexpected time.
Before elaborating on my problem and the probable solutions I've thought of, let me mention my computer's configuration as it may help you in suggesting the best approach. I am running Windows 10 on an Intel i7-9700K processor with 32GB RAM and one Zotac GeForce GTX 1050 Ti OC Edition ZT-P10510B-10L 4GB PCI Express Graphics Card.
Explaining my possible solutions and my confusions regarding them:
A) Using GPU with Adjacency Matrix as Graph Data Structure: I can put an adjacency matrix on GPU and perform my analysis by manually coding them with PyCuda or Numba using loops only as recursion cannot be handled by GPU. The nearest I was able to search is this on stackoverflow but it has no good solution.
My Expectations: I hope to speedup algorithms such as All Pair Shortest Path, All Possible Paths between two nodes, Average Clustering, Average Shortest Path Length, and Small World Properties, etc. If it gives a significant speedup per graph, my results can be achieved very fast.
My Confusions:

Could these graph algorithms can be efficiently coded in GPU?
Which will be better to use? PyCuda or Numba?
Is there any other way to store Graphs on GPU that could be more efficient as my graphs are sparse graphs.
I am an average Python Programmer with no experience of GPU programming, so I will have to understand and learn GPU programming with PyCuda/ Numba. Which one is easier to learn?

B) Parallelizing Programs on CPU Itself: I can use Joblib or any other library to parallelly run the program on my CPU itself. I can arrange 2-3 more computers on which I can run small independent portions of programs or can run 500 graphs per computer.
My Expectations: I hope to speedup algorithms by parallelizing and dividing tasks among computers. If the GPU solution does not work, I may still have some hope by this method.
My Confusions:

Which other libraries are available as good alternatives for Joblib?
Should I allot all CPU cores (8 cores in i7) for my programs or use fewer cores?

C) Apart from my probable solutions do you have any other suggestions for me? If a better and faster solution is available in any other language except C/C++, you can also suggest them as well, as I am already considering C++ as a fallback plan if nothing works.

Work In Progress Updates

In different suggestions from comments on this question and discussion in my community, these are the points I've suggested to explore.

GraphBLAS
boost.graph + extensions with python-wrappers
graph-tool
Spark/ Dask
PyCuda/ Numba
Linear Algerbra methods using Pytorch


I tried to run 100 graphs on my CPU (using n_job=-1) using Joblib, the CPU was continuously hitting a temperature of 100C. The processor tripped after running for 3 hours. - As a solution, I am using 75% of available cores on multiple computers (so if available cores are 8, I am using 6 cores) and the program is running fine. the speedup is also good.


",2,3072,"This is a broad but interesting question. Let me try to answer it.

2000 undirected simple graphs [...] Each graph has approx 40,000 nodes and approx 600,000 edges
Currently, I am using NetworkX for my analysis and currently running nx.algorithms.cluster.average_clustering(G) and nx.average_shortest_path_length(G)

NetworkX uses plain Python implementations and is not optimized for performance. It's great for prototyping but if you encounter performance issues, it's best to look to rewrite your code using another library.
Other than NetworkX, the two most popular graph processing libraries are igraph and SNAP. Both are written in C and have Python APIs so you get both good single-threaded performance and ease of use. Their parallelism is very limited but this is not a problem in your use case as you have many graphs, rendering your problem embarrassingly parallel. Therefore, as you remarked in the updated question, you can run 6-8 jobs in parallel using e.g. Joblib or even xargs. If you need parallel processing, look into graph-tool, which also has a Python API.
Regarding your NetworkX algorithms, I'd expect the average_shortest_path_length to be reasonably well-optimized in all libraries. The average_clustering algorithm is tricky as it relies on node-wise triangle counting and a naive implementation takes O(|E|^2) time while an optimized implementation will do it in O(|E|^1.5). Your graphs are large enough so that the difference between these two costs is running the algorithm on a graph in a few seconds vs. running the algorithm for hours.
The ""all-pairs shortest paths"" (APSP) problem is very time-consuming, with most libraries using the FloydWarshall algorithm that has a runtime of O(|V|^3). I'm unsure what output you're looking for with the ""All Possible Paths between two nodes"" algorithm  enumerating all paths leads to an exponential amount of results and is unfeasible at this scale.
I would not start using the GPU for this task: an Intel i7-9700K should be up for this job. GPU-based graph processing libraries are challenging to set up and currently do not provide that significant of a speedup  the gains by using a GPU instead of a CPU are nowhere near as significant for graph processing as for machine learning algorithms. The only problem where you might be able to get a big speedup is APSP but it depends on which algorithms your chosen library uses.
If you are interested in GPU-based libraries, there are promising directions on the topic such as Gunrock, GraphBLAST, and a work-in-progress SuiteSparse:GraphBLAS extension that supports CUDA. However, my estimate is that you should be able to run most of your algorithms (barring APSP) in a few hours using a single computer and its CPU.
","I think GraphScope can fully satisfy your requirements, by offering user-friendly Python interfaces that are compatible with NetworkX and an efficient distributed runtime implemented in C++. That is, users only need to modify their NetworkX applications with a few lines of code, while achieving several orders of magnitude performance improvement. To make NetworkX applications run on GraphScope, users only need to replace import network with import graphscope.nx as networkx, since the graph manipulation and data loading interfaces of GraphScope are fully compatible with NetworkX.
The runtime of GraphScope is implemented with C++ for high efficiency. Taking the clustering algorithm as an example, running it on GraphScope is over 29x faster than running over NetworkX in our testbed. Furthermore, GraphScope enables running NetworkX applications in a distributed manner, allowing for high scalability. To run NeteorkX applications in a distributed fashion on a K8s cluster, users need to replace import graphscope.nx as nwtworkx with
import graphscope
networkx = graphscope.session(num_workers=$NUM_WORKERS).nx()

For more information about how to run NetworkX applications on GraphScope, please check out Analyzing graph with GraphScope in the Style of NetworkX.
Disclaimer: I'm an author of GraphScope.
",
PyTorch unexpected result,https://stackoverflow.com/questions/68727252,cross entropy loss with weight manual calculation,"Hi just playing around with code, I got the unexpected result of cross entropy loss weight implementation.
pred=torch.tensor([[8,5,3,2,6,1,6,8,4],[2,5,1,3,4,6,2,2,6],[1,1,5,8,9,2,5,2,8],[2,2,6,4,1,1,7,8,3],[2,2,2,7,1,7,3,4,9]]).float()
label=torch.tensor([[3],[7],[8],[2],[5]],dtype=torch.int64)
weights=torch.tensor([1,1,1,10,1,6,1,1,1],dtype=torch.float32)

with this kind of sample variables, pytorch's cross entropy loss gives out 4.7894
loss = F.cross_entropy(pred, label, weight=weights,reduction='mean')
&gt; 4.7894

I manually implemented the cross entropy loss code as below
one_hot = torch.zeros_like(pred).scatter(1, label.view(-1, 1), 1)
log_prb = F.log_softmax(pred, dim=1)
loss = -(one_hot * log_prb).sum(dim=1).mean()

this kind of implementation gives same result with pytorch's cross entropy function if given without weight value. However with weight value
one_hot = torch.zeros_like(pred).scatter(1, label.view(-1, 1), 1)
log_prb = F.log_softmax(pred, dim=1)
loss = -(one_hot * log_prb)*weights.sum(dim=1).sum()/weights.sum()
&gt; 3.9564

it gives out different loss value with pytorch module(4.7894).
I can roughly estimate that my understanding of loss's weight have some problem here, but I can't find out the exact reason for this kind of discrepancy.
Can anybody help me handling this issue?
",2,1023,"I found out the problem. It was quite simple...
I shouldn't have divided with the whole sum of weights.
Instead with dividing with wt.sum() (wt=one_hot*weight) got me 4.7894.
&gt;&gt;&gt; wt = one_hot*weights
&gt;&gt;&gt; loss = -(one_hot * log_prb * weights).sum(dim=1).sum() / wt.sum()
4.7894

The denominator was only with 'related' weight value, not whole.
",,
PyTorch unexpected result,https://stackoverflow.com/questions/61933879,Unexpected error when loading the model: problem in predictor - ModuleNotFoundError: No module named &#39;torchvision&#39;,"I've been trying to deploy my model to the AI platform for Prediction through the console on my vm instance, but I've gotten the error ""(gcloud.beta.ai-platform.versions.create) Create Version failed. Bad model detected with error:  ""Failed to load model: Unexpected error when loading the model: problem in predictor - ModuleNotFoundError: No module named 'torchvision' (Error code: 0)""

I need to include both torch and torchvision. I followed the steps in this question Cannot deploy trained model to Google Cloud Ai-Platform with custom prediction routine: Model requires more memory than allowed, but I couldn't fetch the files pointed to by user gogasca. I tried downloading this .whl file from Pytorch website and uploading it to my cloud storage but got the same error that there is no module torchvision, even though this version is supposed to include both torch and torchvision. Also tried using Cloud AI compatible packages here, but they don't include torchvision.

I tried pointing to two separate .whl files for torch and torchvision in the --package-uris arguments, those point to files in my cloud storage, but then I got the error that the memory capacity was exceeded. This is strange, because collectively their size is around 130Mb. An example of my command that resulted in absence of torchvision looked like this:

gcloud beta ai-platform versions create version_1 \
  --model online_pred_1 \
  --runtime-version 1.15 \
  --python-version 3.7 \
  --origin gs://BUCKET/model-dir \
  --package-uris gs://BUCKET/staging-dir/my_package-0.1.tar.gz,gs://BUCKET/torchvision-dir/torch-1.4.0+cpu-cp37-cp37m-linux_x86_64.whl \
  --prediction-class predictor.MyPredictor


I've tried pointing to different combinations of .whl files that I obtained from different sources, but got either the no module error or not enough memory. I don't understand how the modules interact in this case and why the compiler thinks there is no such module. How can I resolve this? Or alternatively, how can I compile a package myself that include both torch and torchvision. Can you please give detailed answers because I'm not very familiar with package management and bash scripting.

Here's the code I used, torch_model.py:

from torch import nn


class EthnicityClassifier44(nn.Module):
    def __init__(self, num_classes=2):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=1, padding=3)
        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)
        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.relu = nn.ReLU(inplace=False)
        self.fc1 = nn.Linear(8*8*128, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc4 = nn.Linear(128, num_classes)


    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.maxpool1(x)
        x = self.relu(self.conv22(x))
        x = self.maxpool2(x)
        x = self.maxpool3(self.relu(self.conv3(x)))
        x = self.maxpool4(self.relu(self.conv4(x)))
        x = self.relu(self.fc1(x.view(x.shape[0], -1)))
        x = self.relu(self.fc2(x))
        x = self.fc4(x)
        return x


This is predictor_py:

from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face
import torch
import torchvision
from torchvision import transforms
from torch.nn import functional as F
from PIL import Image
from sklearn.externals import joblib
import numpy as np
import os
import torch_model


class MyPredictor(object):

    import torch
    import torchvision

    def __init__(self, model, preprocessor, device):
        """"""Stores artifacts for prediction. Only initialized via `from_path`.
        """"""
        self._resnet = model
        self._mtcnn_mult = preprocessor
        self._device = device
        self.get_std_tensor = transforms.Compose([
            np.float32,
            np.uint8,
            transforms.ToTensor(),
        ])
        self.tensor2pil = transforms.ToPILImage(mode='RGB')
        self.trans_resnet = transforms.Compose([
            transforms.Resize((100, 100)),
            np.float32,
            transforms.ToTensor()
        ])

    def predict(self, instances, **kwargs):

        pil_transform = transforms.Resize((512, 512))

        imarr = np.asarray(instances)
        pil_im = Image.fromarray(imarr)
        image = pil_im.convert('RGB')
        pil_im_512 = pil_transform(image)

        boxes, _ = self._mtcnn_mult(pil_im_512)
        box = boxes[0]

        face_tensor = extract_face(pil_im_512, box, margin=40)
        std_tensor = self.get_std_tensor(face_tensor.permute(1, 2, 0))
        cropped_pil_im = self.tensor2pil(std_tensor)

        face_tensor = self.trans_resnet(cropped_pil_im)
        face_tensor4d = face_tensor.unsqueeze(0)
        face_tensor4d = face_tensor4d.to(self._device)

        prediction = self._resnet(face_tensor4d)
        preds = F.softmax(prediction, dim=1).detach().numpy().reshape(-1)
        print('probability of (class1, class2) = ({:.4f}, {:.4f})'.format(preds[0], preds[1]))

        return preds.tolist()

    @classmethod
    def from_path(cls, model_dir):
        import torch
        import torchvision
        import torch_model

        model_path = os.path.join(model_dir, 'class44_M40RefinedExtra_bin_no_norm_7860.joblib')
        classifier = joblib.load(model_path)

        mtcnn_path = os.path.join(model_dir, 'mtcnn_mult.joblib')
        mtcnn_mult = joblib.load(mtcnn_path)

        device_path = os.path.join(model_dir, 'device_cpu.joblib')
        device = joblib.load(device_path)

        return cls(classifier, mtcnn_mult, device)



And setup.py:

from setuptools import setup

REQUIRED_PACKAGES = ['opencv-python-headless', 'facenet-pytorch']

setup(
 name=""my_package"",
 version=""0.1"",
 include_package_data=True,
 scripts=[""predictor.py"", ""torch_model.py""],
 install_requires=REQUIRED_PACKAGES
)

",2,1263,"The solution was to place the following packages in thsetup.py file for the custom prediction code:

REQUIRED_PACKAGES = ['torchvision==0.5.0', 'torch @ https://download.pytorch.org/whl/cpu/torch-1.4.0%2Bcpu-cp37-cp37m-linux_x86_64.whl', 'opencv-python', 'facenet-pytorch']


I then had a different problem with custom class instantiation, but this article explains it well. So I was able to successfully deploy my model to the AI Platform for prediction.
",,
PyTorch unexpected result,https://stackoverflow.com/questions/59219635,PyTorch vectorise sum lookup quantity into buckets,"Using PyTorch, I have figured out the following code for calculating totals of an item's property by some ""bucket index"":

DATASET_SIZE = 10
NUM_BUCKETS = 4
bucket_assignment = torch.tensor([0,1,2,3,0,1,2,3,0,1], dtype = torch.long)
values_to_add = torch.tensor([1,2,3,4,5,6,7,8,9,10], dtype = torch.float)
buckets = torch.zeros(NUM_BUCKETS, dtype = torch.float)
buckets.index_add_(0, bucket_assignment, values_to_add)

# Buckets is now tensor([15., 18., 10., 12.])


In my case this is specifically to check allocation bounds on a problem, and later code checks that no bucket is under- or over- allocated.

I would like to check multiple different possible assignments at once (and later pick a best option, code not shown). I thought I could do this by adding another dimension to bucket_assignment plus to buckets and have each row be a different set of assignments. However, this does not work as intended, because the second argument of index_add_ must be a simple vector, I cannot pass in any higher rank tensor.

E.g.

BATCH_SIZE = 2
DATASET_SIZE = 5
NUM_BUCKETS = 3
bucket_assignment = torch.tensor([[0,1,2,0,1], [1,1,1,2,1]], dtype = torch.long)
values_to_add = torch.tensor([1,2,3,4,5], dtype = torch.float)
buckets = torch.zeros(BATCH_SIZE, NUM_BUCKETS, dtype = torch.float)
buckets.index_add_(0, bucket_assignment, values_to_add)


I would like to get this result:

tensor([[5., 7., 3.], [ 0., 11.,  4.]])


Instead, I get an error:

RuntimeError: invalid argument 3: Index is supposed to be a vector at ../aten/src/TH/generic/THTensorEvenMoreMath.cpp:733


That's not unexpected due to the limitations of .index_add, but I don't know how to progress.

I am not sure what other approach would allow me to solve this problem in PyTorch - is there some other torch method I could use that would allow me to achieve the same thing. The main goal here is vectorisation and avoiding loops in Python, as in reality the batch sizes are large and I will be taking advantage of GPU acceleration.
",1,291,"If the batch size is the problem you could use torch.masked_select to get the values to add up for each bucket torch.masked_select(values_to_add, bucket_assignment == bucket_num), where PyTorch will broadcast the values_to_add and then only iterate over the buckets in plain python like so:

def bucket_sizes(bucket_num):
    mask = bucket_assignment == bucket_num
    buckets = torch.masked_select(values_to_add, mask)
    buckets = torch.split(buckets, list(mask.sum(dim=1)))
    return [bucket.sum() for bucket in buckets]

torch.tensor([bucket_sizes(i) for i in range(NUM_BUCKETS)]).T

",,
PyTorch unexpected result,https://stackoverflow.com/questions/76667318,conda environment.yaml package conflict,"When I'm trying to build a git project, the environment,yml won't work properly. It seems like some package conflict, but other user doesn't seem to run into this issue, and I couldn't figure out a solution. My conda version is conda 4.8.2,and down below is the environment.yaml, requirment.txt and error
environment.yml:
name: oneposeplus
channels:
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - python=3.7
  - pytorch=1.8.0
  - torchvision=0.9.1
  - cudatoolkit=10.1
  - ipython
  - tqdm
  - matplotlib
  - pylint
  - conda-forge::jupyterlab
  - conda-forge::h5py=3.1.0
  - conda-forge::loguru=0.5.3
  - conda-forge::scipy
  - conda-forge::numba
  - conda-forge::ipdb
  - conda-forge::albumentations=0.5.1
  - pip
  - pip:
    - -r requirements.txt

requirment.txt:
pytorch-lightning==1.5.10
ray==1.13.0
aiohttp==3.7
aioredis==1.3.1
pydegensac==0.1.2
opencv_python==4.4.0.46
yacs&gt;=0.1.8
pytorch_memlab
joblib
pytorch3d
open3d
einops==0.3.0
kornia==0.4.1
autopep8
pickle5==0.0.11
timm&gt;=0.3.2
hydra-core
omegaconf
pycocotools
wandb
rich
transforms3d
natsort
plyfile
pycolmap==0.3.0

error:
rvl224@eervl224:~/OnePose_Plus_Plus-main$ conda env create -f environment.yaml
Collecting package metadata (repodata.json): done
Solving environment: \ 
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
Examining conflict for pylint numba python matplotlib jupyterlab ipython tqdm scExamining conflict for pylint numba python matplotlib torchvision jupyterlab ipyExamining conflict for pylint numba python matplotlib torchvision jupyterlab ipyExamining conflict for pylint jupyterlab ipython: : 20it [06:56, 20.25s/it]     Examining conflict for albumentations numba matplotlib torchvision h5py scipy pyExamining conflict for albumentations numba matplotlib torchvision h5py scipy pyExamining conflict for albumentations torchvision: : 24it [08:51,  8.89s/it]    Examining conflict for albumentations pytorch torchvision: : 25it [09:21, 37.46sExamining conflict for albumentations pytorch torchvision: : 26it [09:21, 35.40sExamining conflict for albumentations h5py: : 26it [09:35, 35.40s/it]           Examining conflict for albumentations matplotlib torchvision: : 27it [09:41, 28.Examining conflict for albumentations matplotlib torchvision: : 28it [09:41, 21.Examining conflict for albumentations matplotlib: : 28it [10:08, 21.91s/it]     Examining conflict for albumentations numba matplotlib torchvision scipy: : 29itExamining conflict for albumentations numba matplotlib torchvision scipy: : 30itExamining conflict for albumentations numba python matplotlib torchvision h5py cExamining conflict for albumentations numba python matplotlib torchvision h5py cExamining conflict for albumentations pytorch: : 31it [11:39, 25.74s/it]        failed                                                                  

# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

    Traceback (most recent call last):
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/exceptions.py"", line 1079, in __call__
        return func(*args, **kwargs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/cli/main.py"", line 80, in do_call
        exit_code = getattr(module, func_name)(args, parser)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/cli/main_create.py"", line 111, in execute
        result[installer_type] = installer.install(prefix, pkg_specs, args, env)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda_env/installers/conda.py"", line 32, in install
        prune=getattr(args, 'prune', False), update_modifier=UpdateModifier.FREEZE_INSTALLED)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 117, in solve_for_transaction
        should_retry_solve)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 158, in solve_for_diff
        force_remove, should_retry_solve)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 281, in solve_final_state
        ssc = self._run_sat(ssc)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/core/solve.py"", line 808, in _run_sat
        should_retry_solve=ssc.should_retry_solve
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 1318, in solve
        self.find_conflicts(specs, specs_to_add, history_specs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 347, in find_conflicts
        bad_deps = self.build_conflict_map(specs, specs_to_add, history_specs)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 507, in build_conflict_map
        root, search_node, dep_graph, num_occurances)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/resolve.py"", line 369, in breadth_first_search_for_dep_graph
        last_spec = MatchSpec.union((path[-1], target_paths[-1][-1]))[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 481, in union
        return cls.merge(match_specs, union=True)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 475, in merge
        reduce(lambda x, y: x._merge(y, union), group) if len(group) &gt; 1 else group[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 475, in &lt;lambda&gt;
        reduce(lambda x, y: x._merge(y, union), group) if len(group) &gt; 1 else group[0]
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 502, in _merge
        final = this_component.union(that_component)
      File ""/home/rvl224/anaconda3/lib/python3.7/site-packages/conda/models/match_spec.py"", line 764, in union
        return '|'.join(options)
    TypeError: sequence item 0: expected str instance, Channel found

`$ /home/rvl224/anaconda3/bin/conda-env create -f environment.yaml`

  environment variables:
                 CIO_TEST=&lt;not set&gt;
        CMAKE_PREFIX_PATH=/opt/ros/noetic
  CONDA_AUTO_UPDATE_CONDA=false
                CONDA_EXE=/home/rvl224/anaconda3/bin/conda
         CONDA_PYTHON_EXE=/home/rvl224/anaconda3/bin/python
               CONDA_ROOT=/home/rvl224/anaconda3
              CONDA_SHLVL=0
            DEFAULTS_PATH=/usr/share/gconf/ubuntu.default.path
          LD_LIBRARY_PATH=/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-
                          gnu:/usr/local/cuda/lib64:
           MANDATORY_PATH=/usr/share/gconf/ubuntu.mandatory.path
                     PATH=/home/rvl224/anaconda3/bin:/home/rvl224/anaconda3/condabin:/opt/ros/no
                          etic/bin:/usr/local/cuda/bin:/home/rvl224/.local/bin:/home/rvl224/.loc
                          al/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/u
                          sr/games:/usr/local/games:/snap/bin
          PKG_CONFIG_PATH=/opt/ros/noetic/lib/pkgconfig:/opt/ros/noetic/lib/x86_64-linux-
                          gnu/pkgconfig
               PYTHONPATH=/opt/ros/noetic/lib/python3/dist-packages
       REQUESTS_CA_BUNDLE=&lt;not set&gt;
         ROS_PACKAGE_PATH=/opt/ros/noetic/share
            SSL_CERT_FILE=&lt;not set&gt;
               WINDOWPATH=2

     active environment : None
            shell level : 0
       user config file : /home/rvl224/.condarc
 populated config files : /home/rvl224/.condarc
          conda version : 4.8.2
    conda-build version : 3.18.11
         python version : 3.7.6.final.0
       virtual packages : __cuda=12.0
                          __glibc=2.31
       base environment : /home/rvl224/anaconda3  (writable)
           channel URLs : https://conda.anaconda.org/conda-forge/linux-64
                          https://conda.anaconda.org/conda-forge/noarch
                          https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /home/rvl224/anaconda3/pkgs
                          /home/rvl224/.conda/pkgs
       envs directories : /home/rvl224/anaconda3/envs
                          /home/rvl224/.conda/envs
               platform : linux-64
             user-agent : conda/4.8.2 requests/2.22.0 CPython/3.7.6 Linux/5.15.0-76-generic ubuntu/20.04.4 glibc/2.31
                UID:GID : 1000:1000
             netrc file : None
           offline mode : False


An unexpected error has occurred. Conda has prepared the above report.

If submitted, this report will be used by core maintainers to improve
future releases of conda.
Would you like conda to send this report to the core maintainers?

[y/N]: 
Timeout reached. No report sent.

down below are attempt updates to fix the error:
installing micromamba:
## install micromamba
curl micro.mamba.pm/install.sh | bash
export MAMBA_ROOT_PREFIX=/home/rvl224/anaconda3
micromamba update -n base conda

micromamba create -n oneposeplus -f environment.yaml

stuck at:
pkgs/main/linux-64                                            No change
pkgs/r/linux-64                                               No change
pkgs/r/noarch                                                 No change
pkgs/main/noarch                                              No change
pytorch/linux-64                                              No change
pytorch/noarch                                                No change
conda-forge/noarch                                  11.7MB @   2.3MB/s  5.2s
conda-forge/linux-64                                29.2MB @   4.4MB/s  6.9s

install mamba
#install mamba
micromamba install -c conda-forge mamba --root-prefix=/home/rvl224/anaconda3 -n base

mamba create -n oneposeplus -f environment.yaml

error:
Looking for: ['environment.yaml']

warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
warning  libmamba Could not parse mod/etag header
pkgs/main/noarch                                   851.4kB @   1.3MB/s  0.7s
pkgs/r/linux-64                                      1.4MB @   1.6MB/s  0.9s
pkgs/r/noarch                                        1.3MB @   1.0MB/s  0.6s
pkgs/main/linux-64                                 @   2.5MB/s  2.4s
conda-forge/noarch                                 @   3.0MB/s  4.6s
conda-forge/linux-64                               @   4.8MB/s  7.4s
Encountered problems while solving:
  - nothing provides requested environment.yaml

tried installing packages one by one in a test environmemt,down below are some error while installing:
mamba install -n test_environment cudatoolkit=10.1 --yes

Encountered problems while solving:
  - nothing provides pytorch 1.10.2 cpu_py37h76afcab_0 needed by pytorch-cpu-1.10.2-cpu_py37h718b53a_0


mamba install -n test_environment ""conda-forge::h5py=3.1.0"" --yes

Encountered problems while solving:
  - package pulseaudio-daemon-16.1-ha8d29e2_3 requires openssl &gt;=3.1.0,&lt;4.0a0, but none of the providers can be installed

mamba install -n test_environment -c conda-forge -c pytorch -c defaults --file requirements.txt --yes


Encountered problems while solving:
  - nothing provides requested ray 1.13.0
  - nothing provides requested pydegensac 0.1.2
  - nothing provides requested opencv_python 4.4.0.46
  - nothing provides requested pytorch_memlab
  - nothing provides requested pytorch3d
  - nothing provides requested open3d
  - nothing provides requested kornia 0.4.1
  - package pycolmap-0.3.0-cpu_py39h5202583_1 requires python &gt;=3.9,&lt;3.10.0a0, but none of the providers can be installed

change into:
cudatoolkit=10.2
h5py (without version)

still figuring what to do with requirement.txt
",1,1010,"Had to manually install every package,idk why environment.yml doesn't work
mamba create -n oneposeplus python=3.7 --yes
conda activate oneposeplus

pip install torch==1.8.0 --no-input
pip install torchvision==0.9.1 --no-input
pip install matplotlib --no-input
pip install ipython --no-input
pip install tqdm --no-input
pip install pylint --no-input
pip install jupyterlab --no-input
pip install ""h5py==3.1.0"" --no-input
pip install loguru==0.5.3 --no-input
pip install scipy --no-input
pip install numba --no-input
pip install ipdb --no-input
pip install ""albumentations==0.5.1"" --no-input

pip install pytorch-lightning==1.5.10 --no-input
pip install ray==1.13.0 --no-input
pip install aiohttp==3.7 --no-input
pip install aioredis==1.3.1 --no-input
pip install pydegensac==0.1.2 --no-input
pip install opencv-python==4.4.0.46 --no-input
pip install ""yacs&gt;=0.1.8"" --no-input
pip install pytorch_memlab --no-input
pip install joblib --no-input
pip install pytorch3d --no-input
pip install open3d --no-input
pip install ""einops==0.3.0"" --no-input
pip install ""kornia==0.4.1"" --no-input
pip install autopep8 --no-input
pip install ""pickle5==0.0.11"" --no-input
pip install ""timm&gt;=0.3.2"" --no-input
pip install hydra-core --no-input
pip install omegaconf --no-input
pip install pycocotools --no-input
pip install wandb --no-input
pip install rich --no-input
pip install transforms3d --no-input
pip install natsort --no-input
pip install plyfile --no-input
pip install ""pycolmap==0.3.0"" --no-input

Thanks for @merv for helping.
",,
PyTorch unexpected result,https://stackoverflow.com/questions/61081303,PyTorch gradient calculation gives unexpected results,"The following code is intended to explain how the PyTorch gradient calculation works, and it should, IMO return the weights matrix, but it doesn't:

# the code calculates T x W + B ---&gt; K1
# compute the mean of K1 --&gt; km
# compute the gradient of km relative to T
#
import torch
torch.manual_seed(0)
t = torch.rand(2,3)
w = torch.rand(3,4)
b = torch.rand(1,4)
#
k1 = torch.mm(t, w) + b
#
#torch.set_grad_enabled(True)
print('k1_grad_fn ',k1.grad_fn)
#
print('t grad ',t.grad)
#
#
km = k1.mean()
km.requires_grad_(True)
print('k1 mean=',km)
km.backward()
print('t grad ',t.grad)
print('k1 grad ',k1.grad_fn)


The results are:

t grad  None
k1 mean= tensor(1.0396, requires_grad=True)
t grad  None
k1 grad  None```


",1,349,"It's not strange, it is expected as indicated by Autograd Mechanics in PyTorch documentation:


  Backward computation is never performed in the subgraphs, where all
  Tensors didnt require gradients.


Setting km.required_grad_(True) creates a subgraph (with only single operation being a mean of k1 tensor) so you get k1_mean.grad. Please notice k1 grad would also be None in this case as it doesn't require gradient.

By default tensor has required_grad set to False so the graph doesn't have to perform unnecessary operations. To get your desired behaviour change your t to:

t = torch.rand((2, 3), requires_grad=True)


(each tensor function has requires_grad argument). This gives you, as you probably expected:

k1_grad_fn  &lt;AddBackward0 object at 0x7fe3454d6100&gt;
t grad  None
k1 mean= tensor(1.0396, grad_fn=&lt;MeanBackward0&gt;)
t grad  tensor([[0.3093, 0.1177, 0.2888],
        [0.3093, 0.1177, 0.2888]])
k1 grad  &lt;AddBackward0 object at 0x7fe3454d6100&gt;

",,
PyTorch unexpected result,https://stackoverflow.com/questions/73392060,"Pytorch and ray tune: why the error; raise TuneError(&quot;Trials did not complete&quot;, incomplete_trials)?","I want to embed hyperparameter optimisation with ray into my pytorch script.
I wrote this code (which is a reproducible example):
## Standard libraries
CHECKPOINT_PATH = ""/home/ad1/new_dev_v1""
DATASET_PATH = ""/home/ad1/""
import torch
device = torch.device(""cuda:0"") if torch.cuda.is_available() else torch.device(""cpu"")
from importlib import reload
from itertools import *
import matplotlib
from itertools import groupby
from libs_public.api import get_pantry_token
from matplotlib import pyplot as plt
from matplotlib.colors import to_rgb
from openbabel import pybel
from openbabel.pybel import readstring,descs
from operator import itemgetter
from pathlib import Path
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger
from ray import tune
from ray.tune import CLIReporter
from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback
from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining
from sklearn import preprocessing
from sklearn.metrics import f1_score, precision_score, recall_score,roc_auc_score
from socket import TIPC_DEST_DROPPABLE
from torch.nn import Linear
from torch.utils.data import TensorDataset
from torch_geometric.data import Data, Dataset,DataLoader,DenseDataLoader,InMemoryDataset
from torch_geometric.datasets import TUDataset
from torch_geometric.nn import GCNConv
from torch_geometric.nn import global_mean_pool
from torchmetrics.functional import precision_recall
from torchvision import transforms
from torchvision.datasets import CIFAR10
from tqdm.notebook import tqdm
import getpass, argparse
import joblib
import json
import logging
import math
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np 
import openbabel
import os
import pandas as pd
import pytorch_lightning as pl
import random
import re
import requests
import seaborn as sns
import sklearn
import sys
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data as data
import torch_geometric
import torch_geometric.data as geom_data
import torch_geometric.nn as geom_nn
import torchmetrics
import torchvision
import warnings
matplotlib.rcParams['lines.linewidth'] = 2.0
pl.seed_everything(42)
print(device)
sns.reset_orig()
sns.set()
sys.path.append('/home/ad1/git/')
torch.backends.cudnn.deterministic = True
warnings.filterwarnings('ignore')



# Setting the seed
pl.seed_everything(42)

# Ensure that all operations are deterministic on GPU (if used) for reproducibility
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

device = torch.device(""cuda:0"") if torch.cuda.is_available() else torch.device(""cpu"")
print(device)


import torch
from torch_geometric.datasets import TUDataset
from torch.nn import Linear
from torch_geometric.nn import global_mean_pool
from torch_geometric.data import Data, Dataset,DataLoader


from torch.utils.data import TensorDataset
from ray import tune
from ray.tune import CLIReporter
from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining
from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback

dataset = TUDataset(root='/tmp/MUTAG', name='MUTAG', use_node_attr=True)
loader = DataLoader(dataset, batch_size=32, shuffle=True)

train_dataset = dataset
val_dataset = dataset
test_dataset = dataset

graph_train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
graph_val_loader = DataLoader(val_dataset, batch_size=64) # Additional loader if you want to change to a larger dataset
graph_test_loader = DataLoader(test_dataset, batch_size=64)


#will change this when it makes sense
#config = {
#    ""dropout"": tune.uniform(0.4,0.5)
#    } 

config = {'dropout':0.4}

gnn_layer_by_name = {
    ""GCN"": geom_nn.GCNConv,
    ""GAT"": geom_nn.GATConv,
    ""GraphConv"": geom_nn.GraphConv
}

class GCNLayer(nn.Module):
    def __init__(self, c_in, c_out):
        super().__init__()
        self.projection = nn.Linear(c_in, c_out)
        

    def forward(self, node_feats, adj_matrix):
        """"""
        Inputs:
            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]
            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.
                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections. 
                         Shape: [batch_size, num_nodes, num_nodes]
        """"""
        # Num neighbours = number of incoming edges
        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)
        node_feats = self.projection(node_feats)
        node_feats = torch.bmm(adj_matrix, node_feats)
        node_feats = node_feats / num_neighbours
        return node_feats

class GNNModel(nn.Module):
    
    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=""GCN"", dp_rate=config['dropout'], **kwargs):
        """"""
        Inputs:
            c_in - Dimension of input features
            c_hidden - Dimension of hidden features
            c_out - Dimension of the output features. Usually number of classes in classification
            num_layers - Number of ""hidden"" graph layers
            layer_name - String of the graph layer to use
            dp_rate - Dropout rate to apply throughout the network
            kwargs - Additional arguments for the graph layer (e.g. number of heads for GAT)
        """"""
        super().__init__()
        gnn_layer = gnn_layer_by_name[layer_name]
        
        layers = []
        in_channels, out_channels = c_in, c_hidden
        for l_idx in range(num_layers-1):
            layers += [
                gnn_layer(in_channels=in_channels, 
                          out_channels=out_channels,
                          **kwargs),
                nn.ReLU(inplace=True),
                nn.Dropout(config['dropout'])
            ]
            in_channels = c_hidden
        layers += [gnn_layer(in_channels=in_channels, 
                             out_channels=c_out,
                             **kwargs)]
        self.layers = nn.ModuleList(layers)
    
    def forward(self, x, edge_index):
        """"""
        Inputs:
            x - Input features per node
            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)
        """"""
        for l in self.layers:
            # For graph layers, we need to add the ""edge_index"" tensor as additional input
            # All PyTorch Geometric graph layer inherit the class ""MessagePassing"", hence
            # we can simply check the class type.
            if isinstance(l, geom_nn.MessagePassing):
                x = l(x, edge_index)
            else:
                x = l(x)
        return x



class GraphGNNModel(nn.Module):
    
    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):
        """"""
        Inputs:
            c_in - Dimension of input features
            c_hidden - Dimension of hidden features
            c_out - Dimension of output features (usually number of classes)
            dp_rate_linear - Dropout rate before the linear layer (usually much higher than inside the GNN)
            kwargs - Additional arguments for the GNNModel object
        """"""
        super().__init__()
        self.GNN = GNNModel(c_in=c_in, 
                            c_hidden=c_hidden, 
                            c_out=c_hidden, # Not our prediction output yet!
                            **kwargs)
        self.head = nn.Sequential(
            nn.Dropout(config['dropout']),
            nn.Linear(c_hidden, c_out)
        )

    def forward(self, x, edge_index, batch_idx):
        """"""
        Inputs:
            x - Input features per node
            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)
            batch_idx - Index of batch element for each node
        """"""
        x = self.GNN(x, edge_index)
        x = geom_nn.global_mean_pool(x, batch_idx) # Average pooling
        x = self.head(x)
        return x


#see https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html
class GraphLevelGNN(pl.LightningModule):
    
    def __init__(self, **model_kwargs):
        super().__init__()
        # Saving hyperparameters
        self.save_hyperparameters()
        
        self.model = GraphGNNModel(**model_kwargs)
        self.loss_module = nn.BCEWithLogitsLoss() #if self.hparams.c_out == 1 else nn.CrossEntropyLoss()

    def forward(self, data, mode=""train""):
        x, edge_index, batch_idx = data.x, data.edge_index, data.batch
        x = self.model(x, edge_index, batch_idx)
        x = x.squeeze(dim=-1)
        
        if self.hparams.c_out == 1:
            preds = (x &gt; 0).float()
            data.y = data.y.float()
        else:
            preds = x.argmax(dim=-1)

        loss = self.loss_module(x, data.y)
        acc = (preds == data.y).sum().float() / preds.shape[0]
        f1 = f1_score(preds,data.y)  ##change f1/precision and recall was just testing
        precision = precision_score(preds,data.y)
        recall = recall_score(preds,data.y)
        #roc_auc = roc_auc_score(preds,data.y)  ##ADD THIS BACK IN
        return loss, acc, f1,precision, recall

    def configure_optimizers(self):
        optimizer = optim.SGD(self.parameters(),lr=0.1) # High lr because of small dataset and small model
        return optimizer

    def training_step(self, batch, batch_idx):
        loss, acc, _,_, _ = self.forward(batch, mode=""train"")
        self.log('train_loss', loss,on_epoch=True,logger=True)
        self.log('train_acc', acc,on_epoch=True,logger=True)
        #self.log('train_precision',precision_and_recall)
        return loss

    def validation_step(self, batch, batch_idx):
        loss, acc, _,_, _ = self.forward(batch, mode=""val"")
        self.log('val_acc', acc,on_epoch=True,logger=True)
        self.log('val_loss', loss,on_epoch=True,logger=True)

    def test_step(self, batch, batch_idx):
        loss, acc, f1,precision, recall = self.forward(batch, mode=""test"")
        self.log('test_acc', acc,on_epoch=True,logger=True)
        self.log('test_f1', f1,on_epoch=True,logger=True)
        self.log('test_precision', precision,on_epoch=True,logger=True)       
        self.log('test_recall', recall,on_epoch=True,logger=True) 
        #self.log('roc_auc', roc_auc,on_epoch=True,logger=True) 


from pytorch_lightning import loggers as pl_loggers
def train_graph_classifier(model_name, **model_kwargs):
    pl.seed_everything(42)
    
    # Create a PyTorch Lightning trainer with the generation callback
    root_dir = os.path.join(CHECKPOINT_PATH, ""GraphLevel"" + model_name)
    os.makedirs(root_dir, exist_ok=True)
    csv_logger = pl_loggers.CSVLogger(save_dir=""logs/"")

    tune_report_callback = TuneReportCheckpointCallback(
    metrics={
    ""val_loss"": ""val_loss"",
    ""val_acc"": ""val_acc"",
    },
    filename=""ray_ckpt"",
    on=""validation_end"",
    )

    trainer = pl.Trainer(default_root_dir=root_dir,
                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=""max"", monitor=""val_acc""),tune_report_callback],
                                 #   TuneReportCallback(
                                #    {
                                #        ""loss"": ""val_loss"",
                                #        ""mean_accuracy"": ""val_accuracy"" 
                                #    },
                                #        on=""test_end"")] # need to change this to validation but error at the minute
                                 #   ,
                         gpus=1 if str(device).startswith(""cuda"") else 0,
                         max_epochs=3,
                         progress_bar_refresh_rate=1,
                         logger=csv_logger,                         
                         )

    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need

    # Check whether pretrained model exists. If yes, load it and skip training
    pretrained_filename = os.path.join(CHECKPOINT_PATH, f""GraphLevel{model_name}.ckpt"")

    if os.path.isfile(pretrained_filename):
        print(""Found pretrained model, loading..."")
        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)
    else:
        pl.seed_everything(42)
        model = GraphLevelGNN(c_in = dataset.num_node_features, 
                              c_out=1, #if tu_dataset.num_classes==2 else tu_dataset.num_classes, 
                              **model_kwargs)
        trainer.fit(model, graph_train_loader, graph_val_loader)
        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)
        
    # Test best model on validation and test set
    #train_result = trainer.test(model, graph_train_loader, verbose=False)
    #test_result = trainer.test(model, graph_test_loader, verbose=False)
    #result = {""test"": test_result[0]['test_acc'], ""train"": train_result[0]['test_acc']} 
    #return model, result
    return model

# Example of ASHA Scheduler
scheduler_asha = ASHAScheduler(
    max_t=100,
    grace_period=1,
    reduction_factor=2,
)

from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback

reporter = CLIReporter(
    parameter_columns=['dropout'],
    metric_columns=[""val_loss"", ""val_acc"", ""training_iteration""]
)


model = train_graph_classifier(model_name=""GraphConv"", 
                                       c_hidden=128, 
                                       layer_name=""GraphConv"", 
                                       num_layers=3, 
                                       dp_rate_linear=0.5,
                                       dp_rate=0.0)


result = tune.run(
    tune.with_parameters(
        model,
        #feature_size=10,
        #target_size=2,
        epochs=50,
        gpus=0
        ),

    resources_per_trial={
        ""cpu"": 1,
        ""gpu"": 0,
    },
    

    local_dir='/home/ad1/ray_ckpt2',  # path for saving checkpoints
    metric=""val_loss"",
    mode=""min"",
    config=config,
    num_samples=16,
    scheduler=scheduler_asha,
    progress_reporter=reporter,
    name=""test"",
)

And the error returned is:
(tune_with_parameters pid=65319) 2022-08-17 16:28:47,053        ERROR function_runner.py:286 -- Runner Thread raised error.
(tune_with_parameters pid=65319) Traceback (most recent call last):
(tune_with_parameters pid=65319)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 277, in run
(tune_with_parameters pid=65319)     self._entrypoint()
(tune_with_parameters pid=65319)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 352, in entrypoint
(tune_with_parameters pid=65319)     self._status_reporter.get_checkpoint(),
(tune_with_parameters pid=65319)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py"", line 462, in _resume_span
(tune_with_parameters pid=65319)     return method(self, *_args, **_kwargs)
(tune_with_parameters pid=65319)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/function_runner.py"", line 645, in _trainable_func
(tune_with_parameters pid=65319)     output = fn()
(tune_with_parameters pid=65319)   File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/utils/trainable.py"", line 410, in inner
(tune_with_parameters pid=65319)     trainable(config, **fn_kwargs)
(tune_with_parameters pid=65319)   File ""/root/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
(tune_with_parameters pid=65319)     return forward_call(*input, **kwargs)
(tune_with_parameters pid=65319) TypeError: forward() got an unexpected keyword argument 'checkpoint_dir'

Traceback (most recent call last):
  File ""test_pytorch.py"", line 390, in &lt;module&gt;
    name=""test"",
  File ""/root/miniconda3/lib/python3.7/site-packages/ray/tune/tune.py"", line 741, in run
    raise TuneError(""Trials did not complete"", incomplete_trials)
ray.tune.error.TuneError: ('Trials did not complete', [tune_with_parameters_a90c2_00000, tune_with_parameters_a90c2_00001, 
...cut for space
tune_with_parameters_a90c2_00014, tune_with_parameters_a90c2_00015])

Could someone show me where I'm going wrong, how to I run HPO with tune in this network and then train the model with the best hyperparameters and then return the model for prediction?
",1,1678,"Ray Tune expects a function trainable in the form of
def train_fn(config):
    # ...

In your case, it is probably best to wrap the train_graph_classifier function, e.g.
def train_fn(config):
    train_graph_classifier(
        model_name=""GraphConv"", 
        layer_name=""GraphConv"",
        **config)


analysis = tune.run(
    train_fn,
    config={
        # provide your hyperparameter search space here
        ""c_hidden"": tune.choice([64, 128]),
        ""dp_rate_linear"": tune.quniform(0.0, 1.0, 0.1),
        # ...
    },
    metric=""val_loss"",
    mode=""min"",
    # ...


print(analysis.best_checkpoint)

If you provide the TuneReportCheckpointCallback to the trainer, the analysis.best_checkpoint should contain the best model that can be then loaded for prediction, e.g.
with analysis.best_checkpoint.as_directory() as tmpdir:
    trainer = GraphLevelGNN.load_from_checkpoint(tmpdir)

","I faced the same error and solved it. You may have solved it since it is an old topic, but I would like to state it to share the information.
It is caused by there being any ERROR exited trial.
According to the error log, this TuneError is raised on tune.run in /ray/tune/tune.py. The tune.run collects trials with its status other than TEMINATED and raises the error if even one trial exited abnormally (not TERMINATED).
In my case, it occurred due to some trials with its status Trial.ERROR caused by CUDA: Out of Memory.
Please try to fix the error described in error log ""error.txt"" in the checkpoint, or set raise_on_failed_trial=True to ignore the TuneError.

""/ray/tune/tune.py"", line 741, in run :

    incomplete_trials = []
        for trial in runner.get_trials():
            if trial.status != Trial.TERMINATED:
              incomplete_trials += [trial]
    
    if incomplete_trials:
        if raise_on_failed_trial and not state[signal.SIGINT]:
            raise TuneError(""Trials did not complete"", incomplete_trials)
        else:
            logger.error(""Trials did not complete: %s"", incomplete_trials)

","running raise_on_failed_trial=False did the trick for me.
best_trial = trainer.hyperparameter_search(
    direction=""maximize"", 
    backend=""ray"", 
    n_trials=10, # number of trials
    raise_on_failed_trial=False, 
)


"
PyTorch unexpected result,https://stackoverflow.com/questions/69916926,Unexpected error when trying to install conda environment from .yaml file,"I am trying to install a conda environment in WSL2 from a .yaml file, the instructions for the installation saying the first step is to run the following command:
conda env create -f devtools/conda-envs/ael-test.yaml

After the WSL2 terminal collects the package metadata and solves the environment, it encounters an error, which I don't really know how to solve. I run the command above from the base environment, from the directory where /devtools/conda-ens/ael-test.yaml is located.
Collecting package metadata (repodata.json): done
Solving environment: done
Preparing transaction: failed

# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

    Traceback (most recent call last):
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/exceptions.py"", line 1079, in __call__
        return func(*args, **kwargs)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/cli/main.py"", line 80, in do_call
        exit_code = getattr(module, func_name)(args, parser)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/cli/main_create.py"", line 141, in execute
        result[installer_type] = installer.install(prefix, pkg_specs, args, env)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda_env/installers/conda.py"", line 59, in install
        unlink_link_transaction.execute()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 245, in execute
        self.verify()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/common/io.py"", line 88, in decorated
        return f(*args, **kwds)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 222, in verify
        self.prepare()
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 211, in prepare
        grps = self._prepare(self.transaction_context, stp.target_prefix,
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 288, in _prepare
        packages_info_to_link = tuple(read_package_info(prec, pcrec)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/core/link.py"", line 288, in &lt;genexpr&gt;
        packages_info_to_link = tuple(read_package_info(prec, pcrec)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/gateways/disk/read.py"", line 89, in read_package_info
        package_metadata = read_package_metadata(epd)
      File ""/home/gheorghe/anaconda3/lib/python3.8/site-packages/conda/gateways/disk/read.py"", line 144, in read_package_metadata
        data = json.loads(f.read())
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/__init__.py"", line 357, in loads
        return _default_decoder.decode(s)
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/decoder.py"", line 337, in decode
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
      File ""/home/gheorghe/anaconda3/lib/python3.8/json/decoder.py"", line 355, in raw_decode
        raise JSONDecodeError(""Expecting value"", s, err.value) from None
    json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

`$ /home/gheorghe/anaconda3/bin/conda-env create -f devtools/conda-envs/ael-test.yaml`

  environment variables:
                 CIO_TEST=&lt;not set&gt;
  CONDA_AUTO_UPDATE_CONDA=false
        CONDA_DEFAULT_ENV=base
                CONDA_EXE=/home/gheorghe/anaconda3/bin/conda
             CONDA_PREFIX=/home/gheorghe/anaconda3
    CONDA_PROMPT_MODIFIER=(base)
         CONDA_PYTHON_EXE=/home/gheorghe/anaconda3/bin/python
               CONDA_ROOT=/home/gheorghe/anaconda3
              CONDA_SHLVL=1
           CURL_CA_BUNDLE=&lt;not set&gt;
                     PATH=/home/gheorghe/anaconda3/bin:/home/gheorghe/.vscode-server/bin/f4af3cb
                          f5a99787542e2a30fe1fd37cd644cc31f/bin:/home/gheorghe/anaconda3/bin:/ho
                          me/gheorghe/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbi
                          n:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Program Files
                          /WindowsApps/Microsoft.WindowsTerminal_1.11.2921.0_x64__8wekyb3d8bbwe:
                          /mnt/c/Program Files/copasi.org/COPASI 4.29.228/bin:/mnt/c/Windows/sys
                          tem32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/Syste
                          m32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Prog
                          ram Files (x86)/Intel/Intel(R) Management Engine
                          Components/DAL:/mnt/c/Program Files/Intel/Intel(R) Management Engine
                          Components/DAL:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA
                          NvDLISR:/mnt/c/Program Files (x86)/NVIDIA
                          Corporation/PhysX/Common:/mnt/c/Program Files/Mullvad VPN/resources:/m
                          nt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt
                          /c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/Ope
                          nSSH:/mnt/c/Program Files (x86)/Common Files/Propellerhead
                          Software/ReWire:/mnt/c/Program Files/Common Files/Propellerhead
                          Software/ReWire:/mnt/c/Program Files/NCBI/blast-2.12.0+/bin:/mnt/c/Use
                          rs/gheor/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/ghe
                          or/AppData/Local/Microsoft/WindowsApps:/snap/bin
       REQUESTS_CA_BUNDLE=&lt;not set&gt;
            SSL_CERT_FILE=&lt;not set&gt;

     active environment : base
    active env location : /home/gheorghe/anaconda3
            shell level : 1
       user config file : /home/gheorghe/.condarc
 populated config files : /home/gheorghe/.condarc
          conda version : 4.10.3
    conda-build version : 3.21.5
         python version : 3.8.11.final.0
       virtual packages : __linux=5.10.16.3=0
                          __glibc=2.31=0
                          __unix=0=0
                          __archspec=1=x86_64
       base environment : /home/gheorghe/anaconda3  (writable)
      conda av data dir : /home/gheorghe/anaconda3/etc/conda
  conda av metadata url : None
           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /home/gheorghe/anaconda3/pkgs
                          /home/gheorghe/.conda/pkgs
       envs directories : /home/gheorghe/anaconda3/envs
                          /home/gheorghe/.conda/envs
               platform : linux-64
             user-agent : conda/4.10.3 requests/2.26.0 CPython/3.8.11 Linux/5.10.16.3-microsoft-standard-WSL2 ubuntu/20.04.3 glibc/2.31
                UID:GID : 1000:1000
             netrc file : None
           offline mode : False


An unexpected error has occurred. Conda has prepared the above report.

If submitted, this report will be used by core maintainers to improve
future releases of conda.

Edit:
This is the .yaml file:
channels:
  - conda-forge
  - pytorch
dependencies:
  - python
  - pip

  - numpy
  - scipy
  - scikit-learn

  - qcelemental
  - openbabel

  - matplotlib
  - seaborn

  - mlflow

  - pytorch
  - torchvision

  - lark-parser
  - cython

  - pip:
    - torchani
    - ""git+https://github.com/RMeli/mdanalysis.git@develop#egg=MDAnalysis&amp;subdirectory=package""

  - black
  - flake8
  - mypy
  - isort

  - pytest
  - pytest-xdist
  - pytest-cov
  - codecov

",1,917,"There is no problem in your environment, I managed to install it, getting
Successfully built MDAnalysis
Installing collected packages: msgpack, tqdm, mrcfile, mmtf-python, gsd, fasteners, biopython, GridDataFormats, torchani, MDAnalysis
Successfully installed GridDataFormats-1.0.1 MDAnalysis-2.6.0.dev0 biopython-1.81 fasteners-0.18 gsd-3.1.1 mmtf-python-1.1.3 mrcfile-1.4.3 msgpack-1.0.5 torchani-2.2.3 tqdm-4.66.1

done
#
# To activate this environment, use
#
#     $ conda activate ael

in the end.
The problem you encountering is probably related to https://github.com/conda/conda/issues/9590
which has a workaround
sudo rm -r ~/.condarc

or editing the ~/.condarc like https://github.com/conda/conda/issues/9590#issuecomment-1003211237
",,
PyTorch unexpected result,https://stackoverflow.com/questions/55047065,"Unexpected key(s) in state_dict: &quot;model&quot;, &quot;opt&quot;","I'm currently using fast.ai to train an image classifier model.

data = ImageDataBunch.single_from_classes(path, classes, ds_tfms=get_transforms(), size=224).normalize(imagenet_stats)
learner = cnn_learner(data, models.resnet34)

learner.model.load_state_dict(
    torch.load('stage-2.pth', map_location=""cpu"")
)


which results in :


  torch.load('stage-2.pth', map_location=""cpu"")   File
  ""/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py"",
  line 769, in load_state_dict
      self.class.name, ""\n\t"".join(error_msgs))) RuntimeError: Error(s) in loading state_dict for Sequential:
  
  ...
  
  Unexpected key(s) in state_dict: ""model"", ""opt"". 


I have looked around in SO and tried to use the following solution:

# original saved file with DataParallel
state_dict = torch.load('stage-2.pth', map_location=""cpu"")
# create new OrderedDict that does not contain `module.`
from collections import OrderedDict
new_state_dict = OrderedDict()
for k, v in state_dict.items():
    name = k[7:] # remove `module.`
    new_state_dict[name] = v
# load params
learner.model.load_state_dict(new_state_dict)


which results in :


  RuntimeError: Error(s) in loading state_dict for Sequential:
  
  Unexpected key(s) in state_dict: """".


I'm using Google Colab to train my model and then port the trained model into docker and try to host in in a local server.

What could be the issue? Could it be the different version of pytorch which results in model mismatch?

In my docker config:

# Install pytorch and fastai
RUN pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html
RUN pip install fastai


While my Colab is using the following:

 !curl -s https://course.fast.ai/setup/colab | bash

",1,10563,"My strong guess is that stage-2.pth contains two top-level items: the model itself (its weights) and the final state of the optimizer which was used to train it. To load just the model, you need only the former. Assuming things were done in the idiomatic PyTorch way, I would try

learner.model.load_state_dict(
    torch.load('stage-2.pth', map_location=""cpu"")['model']
)


Update: after applying my first round of advice it becomes clear that you're loading a savepoint create with a different (perhaps differently configured?) model than the one you're loading it into. As you can see in the pastebin, the savepoint contains weights for some extra layers, not present in your model, such as bn3, downsample, etc. 


  ""0.4.0.bn3.running_var"", ""0.4.0.bn3.num_batches_tracked"", ""0.4.0.downsample.0.weight""


at the same time some other key names match, but the tensors are of different shapes.


  size mismatch for 0.5.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).


I see a pattern that you consistently try to load a parameter of shape [2^(x+1), 2^x, 1, 1] in place of [2^(x), 2^(x-1), 1, 1]. Perhaps you're trying to load a model of different depth (ex. loading vgg-16 weights for vgg-11?). Either way, you need to figure out the exact architecture used to create your savepoint and then recreate it before loading the savepoint.

PS. In case you weren't sure - savepoints contain model weights, along with their shapes and (autogenerated) names. They do not contain the full specification of the architecture itself - you need to assure yourself, that you're calling model.load_state_dict with model being of exactly the same architecture as was used to create the savepoint. Otherwise you will likely have weight names mismatching.
",,
PyTorch unexpected result,https://stackoverflow.com/questions/46613117,Why does conda install tk not work in my docker container even though it says its installed?,"I was having issues with tk in my python 3 docker container.

I tried:

conda install tk


but it says it did install it

root@36602e2cd649:/home_simulation_research/overparametrized_experiments/pytorch_experiments# conda install tk
Fetching package metadata ...........
Solving package specifications: .

# All requested packages already installed.
# packages in environment at /opt/conda:
#
tk                        8.6.7                h5979e9b_1


but when I go to python and try to import it it does not work:

&gt;&gt;&gt; import Tkinter
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named 'Tkinter'


and other errors:

&gt;&gt;&gt; import tkinter
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/tkinter/__init__.py"", line 35, in &lt;module&gt;
    import _tkinter # If this fails your Python may not be configured for Tk
ImportError: libX11.so.6: cannot open shared object file: No such file or directory


when I run a script that needs it:

Traceback (most recent call last):
  File ""bulk_experiment_dispatcher.py"", line 18, in &lt;module&gt;
    from data_file import *
  File ""/home_simulation_research/overparametrized_experiments/pytorch_experiments/data_file.py"", line 16, in &lt;module&gt;
    import matplotlib.pyplot as plt
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/matplotlib/pyplot.py"", line 115, in &lt;module&gt;
    _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup()
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/matplotlib/backends/__init__.py"", line 32, in pylab_setup
    globals(),locals(),[backend_name],0)
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/matplotlib/backends/backend_tkagg.py"", line 6, in &lt;module&gt;
    from six.moves import tkinter as Tk
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/six.py"", line 92, in __get__
    result = self._resolve()
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/six.py"", line 115, in _resolve
    return _import_module(self.mod)
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/six.py"", line 82, in _import_module
    __import__(name)
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/tkinter/__init__.py"", line 35, in &lt;module&gt;
    import _tkinter # If this fails your Python may not be configured for Tk
ImportError: libX11.so.6: cannot open shared object file: No such file or directory


I tried apt-get install python-tk (from Install tkinter for Python) but it did not work:

root@36602e2cd649:/home_simulation_research/overparametrized_experiments/pytorch_experiments# apt-get install python-tk
Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Unable to locate package python-tk




I tried running ENTERYPOINT as one of the answers suggested but it threw me some more errors:

/path/fake_gui.sh: 8: /home_simulation_research/overparametrized_experiments/docker_files/runtime/fake_gui.sh: source: not found
/path/fake_gui.sh: 12: /home_simulation_research/overparametrized_experiments/docker_files/runtime/fake_gui.sh: function: not found
/path/fake_gui.sh: 13: kill: invalid signal number or name: SIGTERM
/path/fake_gui.sh: 15: /home_simulation_research/overparametrized_experiments/docker_files/runtime/fake_gui.sh: Syntax error: ""}"" unexpected


but not sure what to do...



Helpful questions:

How to install python-tk in my docker image
",0,5456,"Ok so once I put a dummy screen in the image it stopped crashing:

RUN apt-get update
RUN apt-get install -y xvfb
#RUN Xvfb :1 -screen 0 1024x768x16 &amp;&gt; xvfb.log  &amp;


when I ran my docker image.
","You need to use a Docker container that has a virtual framebuffer installed and running.

This blog post explains the ""Why and How"" of putting X11 into a docker container.

You can see how they're doing it in the Selenium Docker container via their entry_point.sh:

#!/bin/bash
#
# IMPORTANT: Change this file only in directory Standalone!

source /opt/bin/functions.sh

export GEOMETRY=""$SCREEN_WIDTH""""x""""$SCREEN_HEIGHT""""x""""$SCREEN_DEPTH""

function shutdown {
  kill -s SIGTERM $NODE_PID
  wait $NODE_PID
}

if [ ! -z ""$SE_OPTS"" ]; then
  echo ""appending selenium options: ${SE_OPTS}""
fi

SERVERNUM=$(get_server_num)

rm -f /tmp/.X*lock

xvfb-run -n $SERVERNUM --server-args=""-screen 0 $GEOMETRY -ac +extension RANDR"" \
  java ${JAVA_OPTS} -jar /opt/selenium/selenium-server-standalone.jar \
  ${SE_OPTS} &amp;
NODE_PID=$!

trap shutdown SIGTERM SIGINT
wait $NODE_PID


Source for the above code is from this repository on GitHub, SeleniumHQ/docker-selenium.
","With python 3, you must import as follows:

import tkinter   # with a small caps 't'

"
PyTorch unexpected result,https://stackoverflow.com/questions/59815381,pytorch 1D Dropout leads to unstable learning,"I'm implementing an Inception-like CNN in pytorch.  After the blocks of convolution layers, I have three fully-connected linear layers followed by a sigmoid activation to give me my final regression output.  I'm testing the effects of dropout layers in this network, but it's giving me some unexpected results.

Here is the code:

class MyInception(nn.Module):
    def __init__(self, in_channels, verbose=False):
        super(MyInception, self).__init__()
        self.v = verbose
        ic=in_channels; oc=16
        self.inceptionBlock1 = InceptionBlock(in_channels=ic, out_channels=oc, maxpool=False, verbose=verbose) 
        self.inceptionBlock2 = InceptionBlock(in_channels=oc * 6, out_channels=oc, maxpool=False, verbose=verbose) 
        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.regressor = nn.Sequential(
            nn.Linear(oc * 6 * 35 * 35, 1024, bias=True),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.2, inplace=False),  # &lt;--- Dropout 1
            nn.Linear(1024, 128, bias=True),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.2, inplace=False),  # &lt;--- Dropout 2
            nn.Linear(128, 1, bias=True),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.inceptionBlock1(x)
        x = self.inceptionBlock2(x)
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.regressor(x)
        return x


def train(epochs=10, dot_every=25):
    running = pd.DataFrame(columns=['Epoch','Round','TrainLoss','TestLoss','LearningRate'])
    for epoch in range(epochs):
        train_losses = []
        model.train()
        counter = 0

        for images, targets in train_loader:
            images = images.to(device)
            targets = targets.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = loss_fn(torch.flatten(outputs), targets)
            train_losses.append( loss.item() )
            loss.backward()
            optimizer.step()

            counter += 1
            if counter % dot_every == 0: 
                print(""."",  end='.', flush=True)
                test_loss = test()
            else:
                test_loss = -1.
            lr = np.squeeze(scheduler.get_lr())
            running = running.append(pd.Series([epoch, counter, loss.item(), test_loss, lr], index=running.columns), ignore_index=True)

        test_loss = test()
        train_loss = np.mean(np.asarray(train_losses))
        running = running.append(pd.Series([epoch, counter, train_loss, test_loss, lr], index=running.columns), ignore_index=True)
        print("""")
        print(f""Epoch {epoch+1}, Train Loss: {np.round(train_loss,4)}, Test Loss: {np.round(test_loss, 4)}, Learning Rate: {np.format_float_scientific(lr, precision=4)}"")
    return running


def test():
    model.eval()
    test_losses = []
    for i, (images,targets) in enumerate(test_loader):
        images = images.to(device)
        targets = targets.to(device)
        outputs = model(images)
        loss = loss_fn(torch.flatten(outputs), targets)
        test_losses.append( loss.item() )

    mean_loss = np.mean(np.asarray(test_losses))
    return mean_loss

# instantiate the model
model = MyInception(in_channels=4, verbose=False).to(device)
# define the optimizer and loss function
optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
loss_fn = nn.MSELoss()

# run it
results = train(epochs=10, dot_every=20)



Here is a plot of the MSE losses for the training data.  (red = no dropout, green = second dropout only, blue = first dropout only, purple = both dropouts)
Runs with dropout have big increases in losses at the epoch boundaries (dashed vertical lines), with the double dropout even having a big jump in loss at the start of epoch 10.



The important thing is the test loss.  That is much more stable and not too different between either condition after the 5th epoch, so maybe I shouldn't care. But I would like to understand what is going on.


",0,1027,"I cracked the case.  I realized that I flip model.train() to model.eval() in the test call without setting it back to train() after.  Since Dropout behaves differently in train and eval modes, adding in Dropout revealed the bug.
",,
PyTorch unexpected result,https://stackoverflow.com/questions/74944881,Building libtorch from source gives unexpected contents,"I'm trying to build libtorch (i.e. the headers and shared object files for pytorch) from source, but get an unexpected result. I expected something that looks like the libtorch downloads from pytorch.org:
bin/
include/
lib/
share/

but instead got this:
abi-check    Caffe2Config.cmake         cmake_uninstall.cmake   empty.cpp             modules/               TorchConfig.cmake
aten/        Caffe2ConfigVersion.cmake  compile_commands.json   FXdiv/                nccl/                  TorchConfigVersion.cmake
bin/         CMakeCache.txt             confu-deps/             include/              nccl_external-prefix/
build.ninja  CMakeFiles/                CTestTestfile.cmake     install_manifest.txt  sleef/
caffe2/      cmake_install.cmake        detect_cuda_version.cc  lib/                  third_party/

It also contains lots of .o files and other junk, which I didn't expect. And it's missing many of the headers, such as the contents of torch/csrc/autograd/.
I followed the instructions for setting up my environment with Conda. Here's the approximate command I used to build:
export BUILD_CAFFE2=1 # probably don't need this
export BUILD_TEST=0
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
export DEBUG=0
export MAX_JOBS=16
export USE_CUDA=1
export USE_CUDNN=1
export CUDA_HOME=...
export NVCC_EXECUTABLE=...
export CUDNN_LIB_DIR=...
export CUDNN_INCLUDE_DIR=...
export CUDNN_LIBRARY=...

python ../tools/build_libtorch.py

I'm doing this on Linux with Python 3.8.
How can I get it to build the usual libtorch package structure?
",0,925,"I found a way to create the intended libtorch structure by directly using cmake, following this document:
cmake \
  -DBUILD_SHARED_LIBS:BOOL=ON \
  -DCMAKE_BUILD_TYPE:STRING=Release \
  -DPYTHON_EXECUTABLE:PATH=`which python3` \
  -DCMAKE_INSTALL_PREFIX:PATH=../pytorch-install \
  -DBUILD_CAFFE2=1 \
  -DCMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""} \
  -DUSE_CUDA=1 \
  -DUSE_CUDNN=1 \
  -DCUDNN_INCLUDE_DIR=... \
  -DCUDNN_LIBRARY=... \
  ../pytorch

cmake --build . --target install -- -j 16

This produces libtorch in ../pytorch-install.
",,
PyTorch unexpected result,https://stackoverflow.com/questions/63533029,Unexpected Standard exception from MEX file (pytorch model forward),"When I call mex api from Matlab, I got an unexpected standard exception.
I exported 2 pytorch DNN models to 'A.pt' and 'B.pt' files.
And I implemented c++ functions that load models from the '.pt 'files and run models (forward).
The c++ implementation works fine, I can get proper results from the models.
I built the load &amp; run forward function into '.dll' library,
and I implemented a mex api function can call them.
When I call the mex api in Matlab environment,
the 2 models are loaded normally, and the first model runs forward properly.
However, when run forward the 2nd model, I got the following exception.

Unexpected Standard exception from MEX file
What():The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: error in LoadLibraryA

I have no clue why the c++ implementation works fine but the exception occurs when call it through mex api from Matlab.
Because the load &amp; run forward functions are unchanged I expected the exactly same results.
It is more difficult to debug because there is no call-stack print.
Is there any way to get call-stack ?
Please give me any advice.
Thanks in advance.
-environment-------------------------------
c++ compiler : visual studio 2017 community
matlab : R2020a
libtorch : 1.6
pytorch : 1.5
python : 3.6
cuda : 10.2

",0,836,"From Mr. Cris Luengo's comments, I solved this problem by copying all libtorch dlls into Matlab's own bin folder. There are several duplicated files but I overwrote them. I'm not sure it is safe or not, so may be backup of previous dlls is good choice. Thank you Mr. Cris Luengo.
",,
PyTorch unexpected result,https://stackoverflow.com/questions/77344627,BlockingIOError: [Errno 11] Resource temporarily unavailable while conda pytorch packages installation,"I tried to install pytorch in a new conda environment.
So I did conda create --name nemo_venv python=3.8 and conda activate nemo_venv.
Then I did conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia.

And now I have this:
done
Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.
Solving environment: unsuccessful attempt using repodata from current_repodata.json, retrying with next repodata source.
Collecting package metadata (repodata.json): failed

# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

    Traceback (most recent call last):
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/exception_handler.py"", line 17, in __call__
        return func(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/cli/main.py"", line 78, in main_subshell
        exit_code = do_call(args, parser)
                    ^^^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/cli/conda_argparse.py"", line 166, in do_call
        result = getattr(module, func_name)(args, parser)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/notices/core.py"", line 124, in wrapper
        return func(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/cli/main_install.py"", line 26, in execute
        install(args, parser, ""install"")
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/cli/install.py"", line 320, in install
        unlink_link_transaction = solver.solve_for_transaction(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/core/solve.py"", line 139, in solve_for_transaction
        unlink_precs, link_precs = self.solve_for_diff(
                                   ^^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/core/solve.py"", line 200, in solve_for_diff
        final_precs = self.solve_final_state(
                      ^^^^^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/core/solve.py"", line 345, in solve_final_state
        ssc = self._collect_all_metadata(ssc)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/common/io.py"", line 85, in decorated
        return f(*args, **kwds)
               ^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/core/solve.py"", line 565, in _collect_all_metadata
        index, r = self._prepare(prepared_specs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/core/solve.py"", line 1280, in _prepare
        reduced_index = get_reduced_index(
                        ^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/core/index.py"", line 267, in get_reduced_index
        new_records = SubdirData.query_all(
                      ^^^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/core/subdir_data.py"", line 165, in query_all
        result = tuple(
                 ^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/concurrent/futures/_base.py"", line 619, in result_iterator
        yield _result_or_cancel(fs.pop())
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/concurrent/futures/_base.py"", line 317, in _result_or_cancel
        return fut.result(timeout)
               ^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/concurrent/futures/_base.py"", line 456, in result
        return self.__get_result()
               ^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/concurrent/futures/_base.py"", line 401, in __get_result
        raise self._exception
      File ""/root/sdb/anaconda3/lib/python3.11/concurrent/futures/thread.py"", line 58, in run
        result = self.fn(*self.args, **self.kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/core/subdir_data.py"", line 150, in subdir_query
        return tuple(
               ^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/core/subdir_data.py"", line 172, in query
        self.load()
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/core/subdir_data.py"", line 272, in load
        _internal_state = self._load()
                          ^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/core/subdir_data.py"", line 315, in _load
        repodata, state = fetcher.fetch_latest_parsed()
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/gateways/repodata/__init__.py"", line 716, in fetch_latest_parsed
        parsed, state = self.fetch_latest()
                        ^^^^^^^^^^^^^^^^^^^
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/gateways/repodata/__init__.py"", line 768, in fetch_latest
        cache.load_state()
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/gateways/repodata/__init__.py"", line 581, in load_state
        self.load(state_only=True)
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/gateways/repodata/__init__.py"", line 528, in load
        with self.cache_path_state.open(""r+"") as state_file, lock(state_file):
      File ""/root/sdb/anaconda3/lib/python3.11/site-packages/conda/gateways/repodata/lock.py"", line 59, in __enter__
        fcntl.lockf(
    BlockingIOError: [Errno 11] Resource temporarily unavailable

`$ /root/sdb/anaconda3/bin/conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia`




An unexpected error has occurred. Conda has prepared the above report.
If you suspect this error is being caused by a malfunctioning plugin,
consider using the --no-plugins option to turn off plugins.

Example: conda --no-plugins install &lt;package&gt;

Alternatively, you can set the CONDA_NO_PLUGINS environment variable on
the command line to run the command without plugins enabled.

Example: CONDA_NO_PLUGINS=true conda install &lt;package&gt;

If submitted, this report will be used by core maintainers to improve
future releases of conda.
Would you like conda to send this report to the core maintainers? [y/N]: y
Upload successful.


I deleted old virtual environment and created new one but nothing's changed.
I don't know is it a conda problem, pytorch problem or python version problem.
I even tried to reinstall conda, but nothing's changed.
Is it possible this message is somehow connected to current error:
Collecting package metadata (current_repodata.json): \ WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1
done
Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.
Solving environment: unsuccessful attempt using repodata from current_repodata.json, retrying with next repodata source.

UPD. I tried to install pytorch, torchvision and torchaudio separately and pytorch and torchvision were installed correctly but torchaduio threw an error:
BlockingIOError: [Errno 11] Resource temporarily unavailable
UPD 2. I install torchaudio by conda install torchaudio -c pytorch. But pytorch-cuda=11.8 gets the same:
An unexpected error has occurred. Conda has prepared the above report.
If you suspect this error is being caused by a malfunctioning plugin,
consider using the --no-plugins option to turn off plugins.

Example: conda --no-plugins install &lt;package&gt;

Alternatively, you can set the CONDA_NO_PLUGINS environment variable on
the command line to run the command without plugins enabled.

Example: CONDA_NO_PLUGINS=true conda install &lt;package&gt;

If submitted, this report will be used by core maintainers to improve
future releases of conda.

UPD 3. Even simple conda search command throws the same error.
",0,338,"I had a similar issue trying to install packages as well. This seemed to fix it:
conda clean -a

",,
PyTorch unexpected result,https://stackoverflow.com/questions/73374386,"why is ray Tune with pytorch HPO error &#39;trials did not complete, incomplete trials&#39;?",,0,443,"Is there a longer stacktrace where the real error is printed?
Also could you go to the result folder and see the error file?
Usually result folder is under ~/ray_results.
","I believe you have a typo in your code:
def __init__(self, config):
  super(LightningMNISTClassifier, self).__init__()
  self.layer_1_size = config[""layer_1_size""]
  self.layer_2_size = config[""layer_2_size""]
  self.lr = config[""lr""]
  self.batch_size = config[""batch_size""]

You need to define LightningMNISTClassifier properly. Maybe try using this example?
https://docs.ray.io/en/master/tune/examples/includes/mnist_ptl_mini.html
Note that you may need to install the latest version of Ray.
",
PyTorch unexpected result,https://stackoverflow.com/questions/76788649,Pytorch Lightning Learning Rate Tuners Giving unexpected results,"I'm trying to find an optimal learning rate using python pl.tuner.Tuner but results aren't as expected
The model I am running is a linear classifier on top of a BertForSequenceClassification Automodel
I want to find the optimum learning rate when the bert model is frozen.
To do this I am running this code:
  
    tuner = pl.tuner.Tuner(trainer)
    results = tuner.lr_find(
        model, 
        # optimizer = optimizer,
        
        train_dataloaders=data_module, 
        min_lr=10e-8,
        max_lr=10.0,
    )
    # Plot with
    fig = results.plot(suggest=True)
    fig.show()

My optimizer is configured like this in the model:
   def configure_optimizers(self):
        """"""
        :return:
        """"""
        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)

        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=self.n_warmup_steps,
            num_training_steps=self.n_training_steps,
        )
        return dict(optimizer=optimizer, lr_scheduler=dict(scheduler=scheduler, interval=""step""))

This produces:
Chart of loss against learning rate
I am confused as to why the loss is increasing at lower learning rates, and this is not what I was expecting.
I have tried:

removing the scheduler
freezing/ unfreezing the weights
Changing the initial learning rate

I was expecting a chart like this: https://github.com/comhar/pytorch-learning-rate-tuner/blob/master/images/learning_rate_tuner_plot.png
Any help appreciated
Many thanks
",0,301,"I'm getting a similar plot to you, and there is a similar question on Stackoverflow: Unusual Learning Rate Finder Curve: Loss Lowest at Smallest Learning Rate
It may be due to the issue reported here: https://github.com/Lightning-AI/pytorch-lightning/issues/14167
i.e., there may be some moving average smoothing applied, which starts at 0, so the first few loss values are averaged along with 0 leading to the low loss observed on the plot.
However, that doesn't explain why there are many images online of results without this behaviour, unless they were generated with different versions of Lightning. If it is the cause, though, I guess we just have to ensure that the lowest learning rate tested is much too low to be near the optimal and then ignore the left-hand side of the resulting plot.
","I'm not sure if you have solved this problem, but I suggest you to use a larger num_training in tuner.lr_find()
Accoriding to the source code, the default value is 100
def _lr_find(
trainer: ""pl.Trainer"",
model: ""pl.LightningModule"",
min_lr: float = 1e-8,
max_lr: float = 1,
num_training: int = 100,
mode: str = ""exponential"",
early_stop_threshold: Optional[float] = 4.0,
update_attr: bool = False,
attr_name: str = """", 
) -&gt; Optional[_LRFinder]:

",
PyTorch unexpected issue,https://stackoverflow.com/questions/44522661,PyTorch specific inspection issues in PyCharm,"Has anyone been able to resolve PyTorch specific inspection issues in PyCharm? Previous posts for non-PyTorch related issues suggest upgrading PyCharm, but I'm currently at the latest version. One option is to of course disable certain inspections entirely, but I'd rather avoid that.

Example: torch.LongTensor(x) gives me ""Unexpected argument..."", whereas both call signatures (with and without x) are both supported.
",8,1664,"I believe it's because torch.LongTensor has no __init__ method for pycharm to find.

According to this source that I found thanks to this SO post :


  Use __new__ when you need to control the creation of a new instance.
  Use __init__ when you need to control initialization of a new instance.
  
  __new__ is the first step of instance creation.  It's called first,
  and is responsible for returning a new instance of your class.  In
  contrast, __init__ doesn't return anything; it's only responsible for
  initializing the instance after it's been created.
  
  In general, you shouldn't need to override __new__ unless you're
  subclassing an immutable type like str, int, unicode or tuple.


Since Tensors are types, it makes sense to define only new and no init.

You can experiment this behavior by testing the following classes :

torch.LongTensor(1)  # Unexpected arguments


Produces the warning while the following doesn't.

class MyLongTensor(torch.LongTensor):
    def __init__(self, *args, **kwargs):
        pass

MyLongTensor(1)  # No error


To confirm that the absence of __init__ is the culprit try :

class Example(object):
    pass

Example(0)  # Unexpected arguments


To find out by yourself, use pycharm to Ctrl+click on LongTensor then _TensorBase and look at the defined methods.
",,
PyTorch unexpected issue,https://stackoverflow.com/questions/68727252,cross entropy loss with weight manual calculation,,2,1023,"I found out the problem. It was quite simple...
I shouldn't have divided with the whole sum of weights.
Instead with dividing with wt.sum() (wt=one_hot*weight) got me 4.7894.
&gt;&gt;&gt; wt = one_hot*weights
&gt;&gt;&gt; loss = -(one_hot * log_prb * weights).sum(dim=1).sum() / wt.sum()
4.7894

The denominator was only with 'related' weight value, not whole.
",,
PyTorch unexpected issue,https://stackoverflow.com/questions/55047065,"Unexpected key(s) in state_dict: &quot;model&quot;, &quot;opt&quot;",,1,10563,"My strong guess is that stage-2.pth contains two top-level items: the model itself (its weights) and the final state of the optimizer which was used to train it. To load just the model, you need only the former. Assuming things were done in the idiomatic PyTorch way, I would try

learner.model.load_state_dict(
    torch.load('stage-2.pth', map_location=""cpu"")['model']
)


Update: after applying my first round of advice it becomes clear that you're loading a savepoint create with a different (perhaps differently configured?) model than the one you're loading it into. As you can see in the pastebin, the savepoint contains weights for some extra layers, not present in your model, such as bn3, downsample, etc. 


  ""0.4.0.bn3.running_var"", ""0.4.0.bn3.num_batches_tracked"", ""0.4.0.downsample.0.weight""


at the same time some other key names match, but the tensors are of different shapes.


  size mismatch for 0.5.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).


I see a pattern that you consistently try to load a parameter of shape [2^(x+1), 2^x, 1, 1] in place of [2^(x), 2^(x-1), 1, 1]. Perhaps you're trying to load a model of different depth (ex. loading vgg-16 weights for vgg-11?). Either way, you need to figure out the exact architecture used to create your savepoint and then recreate it before loading the savepoint.

PS. In case you weren't sure - savepoints contain model weights, along with their shapes and (autogenerated) names. They do not contain the full specification of the architecture itself - you need to assure yourself, that you're calling model.load_state_dict with model being of exactly the same architecture as was used to create the savepoint. Otherwise you will likely have weight names mismatching.
",,
PyTorch unexpected issue,https://stackoverflow.com/questions/61510810,How does one pickle arbitrary pytorch models that use lambda functions?,"I currently have a neural network module:

import torch.nn as nn

class NN(nn.Module):
    def __init__(self,args,lambda_f,nn1, loss, opt):
        super().__init__()
        self.args = args
        self.lambda_f = lambda_f
        self.nn1 = nn1
        self.loss = loss
        self.opt = opt
        # more nn.Params stuff etc...

    def forward(self, x):
        #some code using fields
        return out


I am trying to checkpoint it but because pytorch saves using state_dicts it means I can't save the lambda functions I was actually using if I checkpoint with the pytorch torch.save etc. I literally want to save everything without issue and re-load to train on GPUs later. I currently am using this:

def save_ckpt(path_to_ckpt):
    from pathlib import Path
    import dill as pickle
    ## Make dir. Throw no exceptions if it already exists
    path_to_ckpt.mkdir(parents=True, exist_ok=True)
    ckpt_path_plus_path = path_to_ckpt / Path('db')

    ## Pickle args
    db['crazy_mdl'] = crazy_mdl
    with open(ckpt_path_plus_path , 'ab') as db_file:
        pickle.dump(db, db_file)


currently it throws no errors when I chekpoint it and it saved it. 

I am worried that when I train it there might be a subtle bug even if no exceptions/errors are trained or something unexpected might happen (e.g. weird saving on disks in the clusters etc who knows).

Is this safe to do with pytorch classes/nn models? Especially if we want to resume training with GPUs?

Cross posted:


How does one pickle arbitrary pytorch models that use lambda functions?
https://discuss.pytorch.org/t/how-does-one-pickle-arbitrary-pytorch-models-that-use-lambda-functions/79026
https://www.reddit.com/r/pytorch/comments/gagpjg/how_does_one_pickle_arbitrary_pytorch_models_that/?
https://www.quora.com/unanswered/How-does-one-pickle-arbitrary-PyTorch-models-that-use-lambda-functions

",1,1880,"I'm the dill author.  I use dill (and klepto) to save classes that contain trained ANNs inside of lambda functions.  I tend to use combinations of mystic and sklearn, so I can't speak directly to pytorch, but I can assume it works the same.  The place where you have to be careful is if you have a lambda that contains a pointer to an object external to the lambda... so for example y = 4; f = lambda x: x+y.  This might seem obvious, but dill will pickle the lambda, and depending on the rest of the code and the serialization variant, may not serialize the value of y.  So, I've seen many cases where people serialize a trained estimator inside some function (or lambda, or class) and then the results aren't ""correct"" when they restore the function from serialization. The overarching cause is because the function wasn't encapsulated so all objects required for the function to yield the correct results are stored in the pickle. However, even in that case you can get the ""correct"" results back, but you'd just need to create the same environment you had when you pickled the estimator (i.e. all the same values it depends on in the surrounding namespace).  The takeaway should be, try to make sure that all variables used in the function are defined within the function.  Here's a portion of a class I've recently started to use myself (should be in the next release of mystic):

class Estimator(object):
    ""a container for a trained estimator and transform (not a pipeline)""
    def __init__(self, estimator, transform):
        """"""a container for a trained estimator and transform

    Input:
        estimator: a fitted sklearn estimator
        transform: a fitted sklearn transform
        """"""
        self.estimator = estimator
        self.transform = transform
        self.function = lambda *x: float(self.estimator.predict(self.transform.transform(np.array(x).reshape(1,-1))).reshape(-1))
    def __call__(self, *x):
        ""f(*x) for x of xtest and predict on fitted estimator(transform(xtest))""
        import numpy as np
        return self.function(*x)


Note when the function is called, everything that it uses (including np) is defined in the surrounding namespace.  As long as pytorch estimators serialize as expected (without external references), then you should be fine if you follow the above guidelines.
","this is not a good idea. If you do this then if your code changes to a different github repo then it will be hard restore your models that took a lot of time to train. The cycles spent recovering those or retraining is not worth it. I recommend to instead do it the pytorch way and only save the weights as they recommend in pytorch.
","Yes, I think it is safe to use dill to pickle lambda functions etc. I have been using torch.save with dill to save state dict and have had no problems resuming training over GPU as well as CPU unless the model class was changed. Even if the model class was changed (adding/deleting some parameters), I could load state dict, modify it, and load to the model.  

Also, usually, people don't save the model objects but only state dicts i.e parameter values to resume the training along with hyperparameters/model arguments to get the same model object later.

Saving model object can be sometimes problematic as changes to model class (code) can make the saved object useless. If you don't plan on changing your model class/code at all and hence the model object won't be changed then maybe saving objects can work well but generally, it is not recommended to pickle module object. 
"
PyTorch unexpected issue,https://stackoverflow.com/questions/66044806,Issue installing zip file created by setup.py for deploying custom prediction to AI platform,,0,540,"I was able to make it work by removing the 'requirement.txt' and copy packages directly to setup.py. Don't know the real reason for this weird behaviour.
",,
PyTorch strange behavior,https://stackoverflow.com/questions/66936111,Subclass of PyTorch dataset class cannot find dataset files,"I'm trying to create a subclass of the PyTorch MNIST dataset class, which I call  CustomMNISTDataset, as follows:
import torchvision.datasets as datasets

class CustomMNISTDataset(datasets.MNIST):

    def __init__(self, root='/home/psando'):
        super().__init__(root=root,
                         download=False)

but when I execute:
dataset = CustomMNISTDataset()

it fails with error: ""RuntimeError: Dataset not found. You can use download=True to download it"".
However, when I run the following in the same file:
dataset = datasets.MNIST(root='/home/psando', download=False)
print(len(dataset))

it succeeds and prints ""60000"", as expected.
Since CustomMNISTDataset subclasses datasets.MNIST why is the behavior different? I've verified that the path '/home/psando' contains the MNIST directory with raw and processed subdirectories (otherwise, explicitly calling the constructor for datasets.MNIST() would have failed). The current behavior implies that the call to super().__init__() within CustomMNISTDataset is not calling the constructor for datasets.MNIST which is very strange!
Other details: I'm using Python 3.6.8 with torch==1.6.0 and
torchvision==0.7.0. Any help would be appreciated!
",5,1318,"This requires some source-diving, but your problem is this function. The path to the dataset is dependant on the name of the class, so when you subclass MNIST the root folder changes to /home/psando/CustomMNISTDataset
So if you rename /home/psando/MNIST to /home/psando/CustomMNISTDataset it works.
",,
PyTorch strange behavior,https://stackoverflow.com/questions/74635994,Pytorch&#39;s share_memory_() vs built-in Python&#39;s shared_memory: Why in Pytorch we don&#39;t need to access the shared memory-block?,"Trying to learn about the built-in multiprocessing and Pytorch's multiprocessing packages, I have observed a different behavior between both. I find this to be strange since Pytorch's package is fully-compatible with the built-in package.
Concretely, I'm refering to the way variables are shared between processes. In Pytorch, tensor's are moved to shared_memory via the inplace operation share_memory_(). On the other hand, we can get the same result with the built-in package by using the shared_memory module.
The difference between both that I'm struggling to understand is that, with the built-in version, we have to explicitely access the shared memory-block inside the launched process. However, we don't need to do that with the Pytorch version.
Here is a Pytorch's toy example showing this:
import time

import torch
# the same behavior happens when importing:
# import multiprocessing as mp
import torch.multiprocessing as mp


def get_time(s):
    return round(time.time() - s, 1)


def foo(a):
    # wait ~1sec to print the value of the tensor.
    time.sleep(1.0)
    with lock:
        #-------------------------------------------------------------------
        # WITHOUT explicitely accessing the shared memory block, we can observe
        # that the tensor has changed:
        #-------------------------------------------------------------------
        print(f""{__name__}\t{get_time(s)}\t\t{a}"")


# global variables.
lock = mp.Lock()
s = time.time()


if __name__ == '__main__':
    print(""Module\t\tTime\t\tValue"")
    print(""-""*50)

    # create tensor and assign it to shared memory.
    a = torch.zeros(2).share_memory_()
    print(f""{__name__}\t{get_time(s)}\t\t{a}"")

    # start child process.
    p0 = mp.Process(target=foo, args=(a,))
    p0.start()

    # modify the value of the tensor after ~0.5sec.
    time.sleep(0.5)
    with lock:
        a[0] = 1.0

    print(f""{__name__}\t{get_time(s)}\t\t{a}"")
    time.sleep(1.5)

    p0.join()

which outputs (as expected):
Module          Time            Value
--------------------------------------------------
__main__        0.0             tensor([0., 0.])
__main__        0.5             tensor([1., 0.])
__mp_main__     1.0             tensor([1., 0.])

And here is a toy example with the built-in package:
import time
import multiprocessing as mp
from multiprocessing import shared_memory

import numpy as np


def get_time(s):
    return round(time.time() - s, 1)


def foo(shm_name, shape, type_):
    #-------------------------------------------------------------------
    # WE NEED TO explicitely access the shared memory block to observe
    # that the array has changed:
    #-------------------------------------------------------------------
    existing_shm = shared_memory.SharedMemory(name=shm_name)
    a = np.ndarray(shape, type_, buffer=existing_shm.buf)

    # wait ~1sec to print the value.
    time.sleep(1.0)
    with lock:
        print(f""{__name__}\t{get_time(s)}\t\t{a}"")


# global variables.
lock = mp.Lock()
s = time.time()


if __name__ == '__main__':
    print(""Module\t\tTime\t\tValue"")
    print(""-""*35)

    # create numpy array and shared memory block.
    a = np.zeros(2,)
    shm = shared_memory.SharedMemory(create=True, size=a.nbytes)
    a_shared = np.ndarray(a.shape, a.dtype, buffer=shm.buf)
    a_shared[:] = a[:]
    print(f""{__name__}\t{get_time(s)}\t\t{a_shared}"")

    # start child process.
    p0 = mp.Process(target=foo, args=(shm.name, a.shape, a.dtype))
    p0.start()

    # modify the value of the vaue after ~0.5sec.
    time.sleep(0.5)
    with lock:
        a_shared[0] = 1.0

    print(f""{__name__}\t{get_time(s)}\t\t{a_shared}"")
    time.sleep(1.5)

    p0.join()

which equivalently outputs, as expected:
Module          Time            Value
-----------------------------------
__main__        0.0             [0. 0.]
__main__        0.5             [1. 0.]
__mp_main__     1.0             [1. 0.]

So what I'm strugging to understand is why we don't need to follow the same steps in both versions, built-in and Pytorch's, i.e. how Pytorch is able to avoid the need to explicitely access the shared memory-block?
P.S. I'm using a Windows OS and Python 3.9
",5,2918,"You are writing a love letter to the pytorch authors.
That is, you are patting them on the back,
congratulating their wrapper efforts as ""a job well done!""
It's a lovely library.
Let's take a step back and use a very simple
data structure, a dictionary d.
If parent initializes d with some values,
and then kicks off a pair of worker children,
each child has a copy of d.
How did that happen?
The multiprocessing module forked off
the workers, looked at the set of defined
variables which includes d, and serialized
those (key, value) pairs from parent down to
the children.
So at this point we have 3 independent copies
of d. If parent or either child modifies d,
the other 2 copies are completely unaffected.
Now switch gears to the pytorch wrapper.
You offered some nice concise code that demos
the little .SharedMemory() dance an app would
need to do if we want 3 references to same shared structure
rather than 3 independent copies.
The pytorch wrapper serializes references
to common data structure, rather than producing copies.
Under the hood it's doing exactly the dance that you did.
But with no repeated verbiage up at the app level,
as the details have nicely been abstracted away, FTW!

Why in Pytorch we don't need to access the shared memory-block?

tl;dr: We do need to access it. But the library shoulders the burden of worrying about the details, so we don't have to.
","pytorch has a simple wrapper around shared memory, python's shared memory module is only a wrapper around the underlying OS dependent functions.
the way it can be done is that you don't serialize the array or the shared memory themselves, and only serialize what's needed to create them by using the __getstate__ and __setstate__ methods from the docs, so that your object acts as both a proxy and a container at the same time.
the following bar class can double for a proxy and a container this way, which is useful if the user shouldn't have to worry about the shared memory part.
import time
import multiprocessing as mp
from multiprocessing import shared_memory
import numpy as np

class bar:
    def __init__(self):
        self._size = 10
        self._type = np.uint8
        self.shm = shared_memory.SharedMemory(create=True, size=self._size)
        self._mem_name = self.shm.name
        self.arr = np.ndarray([self._size], self._type, buffer=self.shm.buf)

    def __getstate__(self):
        """"""Return state values to be pickled.""""""
        return (self._mem_name, self._size, self._type)

    def __setstate__(self, state):
        """"""Restore state from the unpickled state values.""""""
        self._mem_name, self._size, self._type = state
        self.shm = shared_memory.SharedMemory(self._mem_name)
        self.arr = np.ndarray([self._size], self._type, buffer=self.shm.buf)

def get_time(s):
    return round(time.time() - s, 1)

def foo(shm, lock):
    # -------------------------------------------------------------------
    # without explicitely access the shared memory block we observe
    # that the array has changed:
    # -------------------------------------------------------------------
    a = shm

    # wait ~1sec to print the value.
    time.sleep(1.0)
    with lock:
        print(f""{__name__}\t{get_time(s)}\t\t{a.arr}"")

# global variables.
s = time.time()

if __name__ == '__main__':
    lock = mp.Lock()  # to work on windows/mac.

    print(""Module\t\tTime\t\tValue"")
    print(""-"" * 35)

    # create numpy array and shared memory block.
    a = bar()
    print(f""{__name__}\t{get_time(s)}\t\t{a.arr}"")

    # start child process.
    p0 = mp.Process(target=foo, args=(a, lock))
    p0.start()

    # modify the value of the vaue after ~0.5sec.
    time.sleep(0.5)
    with lock:
        a.arr[0] = 1.0

    print(f""{__name__}\t{get_time(s)}\t\t{a.arr}"")
    time.sleep(1.5)

    p0.join()

python just makes it much easier to hide such details inside the class without bothering the user with such details.
Edit: i wish they'd make locks non-inheritable so your code can raise an error on the lock, instead you'll find out one day that it doesn't actually lock ... After it crashes your application in production.
",
PyTorch strange behavior,https://stackoverflow.com/questions/66642338,Strange Cuda out of Memory behavior in Pytorch,"Edit: SOLVED- Problem relied on the number of workers, lowered them, problem solved
I am using a 24GB Titan RTX and I am using it for an image segmentation Unet with Pytorch,
it is always throwing Cuda out of Memory at different batch sizes, plus I have more free memory than it states that I need, and by lowering batch sizes, it INCREASES the memory it tries to allocate which doesn't make any sense.
here is what I tried:
Image size = 448, batch size = 8

""RuntimeError: CUDA error: out of memory""

Image size = 448, batch size = 6

""RuntimeError: CUDA out of memory. Tried to allocate 3.12 GiB (GPU 0; 24.00 GiB total capacity; 2.06 GiB already allocated; 19.66 GiB free; 2.31 GiB reserved in total by PyTorch)""

is says it tried to allocate 3.12GB and I have 19GB free and it throws an error??
Image size = 224, batch size = 8

""RuntimeError: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 24.00 GiB total capacity; 2.78 GiB already allocated; 19.15 GiB free; 2.82 GiB reserved in total by PyTorch)""

Image size = 224, batch size = 6

""RuntimeError: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 24.00 GiB total capacity; 2.30 GiB already allocated; 19.38 GiB free; 2.59 GiB reserved in total by PyTorch)""

reduced batch size but tried to allocate more ???
Image size = 224, batch size = 4

""RuntimeError: CUDA out of memory. Tried to allocate 482.00 MiB (GPU 0; 24.00 GiB total capacity; 2.21 GiB already allocated; 19.48 GiB free; 2.50 GiB reserved in total by PyTorch)""

Image size = 224, batch size = 2

""RuntimeError: CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 24.00 GiB total capacity; 1.44 GiB already allocated; 19.88 GiB free; 2.10 GiB reserved in total by PyTorch)""

Image size = 224, batch size = 1

""RuntimeError: CUDA out of memory. Tried to allocate 1.91 GiB (GPU 0; 24.00 GiB total capacity; 894.36 MiB already allocated; 20.94 GiB free; 1.03 GiB reserved in total by PyTorch)""

Even with stupidly low image sizes and batch sizes...
",4,6398,"SOLVED- Problem relied on the number of workers, lowered them, problem solved
",,
PyTorch strange behavior,https://stackoverflow.com/questions/73125231,Pytorch dataloaders : Bad file descriptor and EOF for workers&gt;0,"Description of the problem
I am encountering a strange behavior during a neural network training with Pytorch dataloaders made from a custom dataset. The dataloaders are set with workers=4, pin_memory=False.
Most of the time, the training finished with no problems.
Sometimes, the training stopped at a random moment with the following errors:

OSError: [Errno 9] Bad file descriptor
EOFError

It looks like the error occurs during socket creation to access dataloader elements.
The error disappears when I set the number of workers to 0, but I need to accelerate my training with multiprocessing.
What could be the source of the error ? Thank you !
The versions of python and libraries
Python 3.9.12, Pyorch 1.11.0+cu102
EDIT: The error is occurring only on clusters
Output of error file
Traceback (most recent call last):
  File ""/my_directory/.conda/envs/geoseg/lib/python3.9/multiprocessing/resource_sharer.py"", line 145, in _serve
Epoch 17:  52%|    | 253/486 [01:00&lt;00:55,  4.18it/s, loss=1.73]

Traceback (most recent call last):
  File ""/my_directory/bench/run_experiments.py"", line 251, in &lt;module&gt;
    send(conn, destination_pid)
  File ""/my_directory/.conda/envs/geoseg/lib/python3.9/multiprocessing/resource_sharer.py"", line 50, in send
    reduction.send_handle(conn, new_fd, pid)
  File ""/my_directory/.conda/envs/geoseg/lib/python3.9/multiprocessing/reduction.py"", line 183, in send_handle
    with socket.fromfd(conn.fileno(), socket.AF_UNIX, socket.SOCK_STREAM) as s:
  File ""/my_directory/.conda/envs/geoseg/lib/python3.9/socket.py"", line 545, in fromfd
    return socket(family, type, proto, nfd)
  File ""/my_directory/.conda/envs/geoseg/lib/python3.9/socket.py"", line 232, in __init__
    _socket.socket.__init__(self, family, type, proto, fileno)
OSError: [Errno 9] Bad file descriptor

    main(args)
  File ""/my_directory/bench/run_experiments.py"", line 183, in main
    run_experiments(args, save_path)
  File ""/my_directory/bench/run_experiments.py"", line 70, in run_experiments
    ) = run_algorithm(algorithm_params[j], mp[j], ss, dataset)
  File ""/my_directorybench/algorithms.py"", line 38, in run_algorithm
    data = es(mp,search_space,  dataset, **ps)
  File ""/my_directorybench/algorithms.py"", line 151, in es
   data = ss.generate_random_dataset(mp,
  File ""/my_directorybench/architectures.py"", line 241, in generate_random_dataset
    arch_dict = self.query_arch(
  File ""/my_directory/bench/architectures.py"", line 71, in query_arch
    train_losses, val_losses, model = meta_net.get_val_loss(
  File ""/my_directory/bench/meta_neural_net.py"", line 50, in get_val_loss
    return self.training(
  File ""/my_directorybench/meta_neural_net.py"", line 155, in training
    train_loss = self.train_step(model, device, train_loader, epoch)
  File ""/my_directory/bench/meta_neural_net.py"", line 179, in train_step
    for batch_idx, mini_batch in enumerate(pbar):
  File ""/my_directory/.conda/envs/geoseg/lib/python3.9/site-packages/tqdm/std.py"", line 1195, in __iter__
    for obj in iterable:
  File ""/my_directory/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py"", line 530, in __next__
    data = self._next_data()
  File ""/my_directory/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py"", line 1207, in _next_data
    idx, data = self._get_data()
  File ""/my_directory/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py"", line 1173, in _get_data
    success, data = self._try_get_data()
  File ""/my_directory/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py"", line 1011, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File ""/my_directory/.conda/envs/geoseg/lib/python3.9/multiprocessing/queues.py"", line 122, in get
    return _ForkingPickler.loads(res)
  File ""/my_directory/.local/lib/python3.9/site-packages/torch/multiprocessing/reductions.py"", line 295, in rebuild_storage_fd
    fd = df.detach()
  File ""/my_directory/.conda/envs/geoseg/lib/python3.9/multiprocessing/resource_sharer.py"", line 58, in detach
    return reduction.recv_handle(conn)
  File ""/my_directory/.conda/envs/geoseg/lib/python3.9/multiprocessing/reduction.py"", line 189, in recv_handle
    return recvfds(s, 1)[0]
  File ""/my_directory/.conda/envs/geoseg/lib/python3.9/multiprocessing/reduction.py"", line 159, in recvfds
    raise EOFError
EOFError

EDIT : The way data is accessed
    from PIL import Image
    from torch.utils.data import DataLoader
    
    # extract of code of dataset
    
        class Dataset():
           def __init__(self,image_files,mask_files):
              self.image_files = image_files
              self.mask_files = mask_files
    
           def __getitem__(self, idx):
              img = Image.open(self.image_files[idx]).convert('RGB')
              mask=Image.open(self.mask_files[idx]).convert('L')
              return img, mask
    
    # extract of code of trainloader
      
        train_loader = DataLoader(
                        dataset=train_dataset,
                        batch_size=4,
                        num_workers=4,
                        pin_memory=False,
                        shuffle=True,
                        drop_last=True,
                        persistent_workers=False,
                    )

",4,1920,"I have finally found a solution. Adding this configuration to the dataset script works:
import torch.multiprocessing
torch.multiprocessing.set_sharing_strategy('file_system')

By default, the sharing strategy is set to 'file_descriptor'.
I have tried some solutions explained in :

this issue (increase shared memory, increase max number of opened file descriptors, torch.cuda.empty_cache() at the end of each epoch, ...)
and this other issue, that turns out to solve the problem

As suggested by @AlexMeredith, the error may be linked to the distributed filesystem (Lustre) that some clusters use. The error may also come from distributed shared memory.
","In this example there's only dataset implementation but there's no snippet showing what's happening with the batches.
In my case, I was storing the batches in the index array-like object which fortunately has been described here. The dataloader couldn't close the subprocesses because of that. Implementing something similar to that helped me solving this problem.
import copy

for batch in data_loader:  
    batch_cp = copy.deepcopy(batch)  
    del batch  
    index.append(batch_cp[""index""])

I also got other errors related to this one, such as:

received 0 items of ancdata
bad message length

",
PyTorch strange behavior,https://stackoverflow.com/questions/58839721,How to define a loss function in pytorch with dependency to partial derivatives of the model w.r.t input?,"After reading about how to solve an ODE with neural networks following the paper Neural Ordinary Differential Equations and the blog that uses the library JAX I tried to do the same thing with ""plain"" Pytorch but found a point rather ""obscure"": How to properly use the partial derivative of a function (in this case the model) w.r.t one of the input parameters.

To resume the problem at hand as shown in 2 it is intended to solve the ODE y' = -2*x*y with the condition y(x=0) = 1 in the domain -2 &lt;= x &lt;= 2. Instead of using finite differences the solution is replaced by a NN as y(x) = NN(x) with a single layer with 10 nodes.

I managed to (more or less) replicate the blog with the following code

import torch
import torch.nn as nn
from torch import optim
import matplotlib.pyplot as plt
import numpy as np 

# Define the NN model to solve the problem
class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.lin1 = nn.Linear(1,10)
        self.lin2 = nn.Linear(10,1)

    def forward(self, x):
        x = torch.sigmoid(self.lin1(x))
        x = torch.sigmoid(self.lin2(x))
        return x

model = Model()

# Define loss_function from the Ordinary differential equation to solve
def ODE(x,y):
    dydx, = torch.autograd.grad(y, x, 
    grad_outputs=y.data.new(y.shape).fill_(1),
    create_graph=True, retain_graph=True)

    eq = dydx + 2.* x * y # y' = - 2x*y
    ic = model(torch.tensor([0.])) - 1.    # y(x=0) = 1
    return torch.mean(eq**2) + ic**2

loss_func = ODE

# Define the optimization
# opt = optim.SGD(model.parameters(), lr=0.1, momentum=0.99,nesterov=True) # Equivalent to blog
opt = optim.Adam(model.parameters(),lr=0.1,amsgrad=True) # Got faster convergence with Adam using amsgrad

# Define reference grid 
x_data = torch.linspace(-2.0,2.0,401,requires_grad=True)
x_data = x_data.view(401,1) # reshaping the tensor

# Iterative learning
epochs = 1000
for epoch in range(epochs):
    opt.zero_grad()
    y_trial = model(x_data)
    loss = loss_func(x_data, y_trial)

    loss.backward()
    opt.step()

    if epoch % 100 == 0:
        print('epoch {}, loss {}'.format(epoch, loss.item()))

# Plot Results
plt.plot(x_data.data.numpy(), np.exp(-x_data.data.numpy()**2), label='exact')
plt.plot(x_data.data.numpy(), y_data.data.numpy(), label='approx')
plt.legend()
plt.show()


From here I manage to get the results as shown in the fig.
enter image description here

The problems is that at the definition of the ODE functional, instead of passing (x,y) I would rather prefer to pass something like (x,fun) (where fun is my model) such that the partial derivative and specific evaluations of the model can be done with a call . So, something like

def ODE(x,fun):
    dydx, = ""grad of fun w.r.t x as a function""

    eq = dydx(x) + 2.* x * fun(x)        # y' = - 2x*y
    ic = fun( torch.tensor([0.]) ) - 1.  # y(x=0) = 1
    return torch.mean(eq**2) + ic**2


Any ideas? Thanks in advance

EDIT:

After some trials I found a way to pass the model as an input but found another strange behavior... The new problem is to solve the ODE y'' = -2 with the BC y(x=-2) = -1 and y(x=2) = 1, for which the analytical solution is y(x) = -x^2+x/2+4

Let's modify a bit the previous code as:

import torch
import torch.nn as nn
from torch import optim
import matplotlib.pyplot as plt
import numpy as np 

# Define the NN model to solve the equation
class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.lin1 = nn.Linear(1,10)
        self.lin2 = nn.Linear(10,1)

    def forward(self, x):
        y = torch.sigmoid(self.lin1(x))
        z = torch.sigmoid(self.lin2(y))
        return z

model = Model()

# Define loss_function from the Ordinary differential equation to solve
def ODE(x,fun):
    y = fun(x)

    dydx = torch.autograd.grad(y, x, 
    grad_outputs=y.data.new(y.shape).fill_(1),
    create_graph=True, retain_graph=True)[0]

    d2ydx2 = torch.autograd.grad(dydx, x, 
    grad_outputs=dydx.data.new(dydx.shape).fill_(1),
    create_graph=True, retain_graph=True)[0]

    eq  = d2ydx2 + torch.tensor([ 2.])             # y'' = - 2
    bc1 =  fun(torch.tensor([-2.])) - torch.tensor([-1.]) # y(x=-2) = -1
    bc2 =  fun(torch.tensor([ 2.])) - torch.tensor([ 1.]) # y(x= 2) =  1
    return torch.mean(eq**2) + bc1**2 + bc2**2

loss_func = ODE


So, here I passed the model as argument and managed to derive twice... so far so good. BUT, using the sigmoid function for this case is not only not necessary but also gives a result that is far from the analytical one.

If I change the NN for:

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.lin1 = nn.Linear(1,1)
        self.lin2 = nn.Linear(1,1)

    def forward(self, x):
        y = self.lin1(x)
        z = self.lin2(y)
        return z


In which case I would expect to optimize a double pass through two linear functions that would retrieve a 2nd order function ... I get the error:


  RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.


Adding the option to the definition of dydx doesn't solve the problem, and adding it to d2ydx2 gives a NoneType definition.

Is there something wrong with the layers as they are?
",3,1749,"Quick Solution: 

add allow_unused=True to .grad functions. So, change 

dydx = torch.autograd.grad(
    y, x,
    grad_outputs=y.data.new(y.shape).fill_(1),
    create_graph=True, retain_graph=True)[0]

d2ydx2 = torch.autograd.grad(dydx, x, grad_outputs=dydx.data.new(
    dydx.shape).fill_(1), create_graph=True, retain_graph=True)[0]


To

dydx = torch.autograd.grad(
    y, x,
    grad_outputs=y.data.new(y.shape).fill_(1),
    create_graph=True, retain_graph=True, allow_unused=True)[0]

d2ydx2 = torch.autograd.grad(dydx, x, grad_outputs=dydx.data.new(
    dydx.shape).fill_(1), create_graph=True, retain_graph=True, allow_unused=True)[0]


More explanation:

See what allow_unused do:

allow_unused (bool, optional): If ``False``, specifying inputs that were not
        used when computing outputs (and therefore their grad is always zero)
        is an error. Defaults to ``False``.


So, if you try to differentiate w.r.t to a variable that is not in being used to compute the value, it will give an error. Also, note that error only occurs when you use linear layers. 

This is because when you use linear layers, you have y=W1*W2*x + b = Wx+b and dy/dx is not a function of x, it is simply W. So when you try to differentiate dy/dx w.r.t x it throws an error. This error goes away as soon as you use sigmoid because then dy/dx will be a function of x. To avoid the error, either make sure dy/dx is a function of x or use allow_unused=True  
",,
PyTorch strange behavior,https://stackoverflow.com/questions/54492667,Segfault when throwing std::runtime_error on Ubuntu Xenial with Rcpp,"I am having a very strange behavior with Rcpp together with libtorch. 

I have a file with 2 functions:

#include &lt;torch/torch.h&gt;
#include &lt;Rcpp.h&gt; 

// [[Rcpp::export]]
void test_error () {  
  throw std::runtime_error(""hi this is my error"");
}

// [[Rcpp::export]]
void test_error2 () {  
  Rcpp::Rcout &lt;&lt; torch::arange(1) &lt;&lt; std::endl;
}


When I call test_error() I get a segfault (g++):

terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
Aborted (core dumped)


The clang++ error is:

terminate called after throwing an instance of 'std::length_error'
  what():  basic_string::_M_create
Aborted (core dumped)


test_error2 works as expected.

This error happens only on Ubuntu Xenial. I tested with Ubuntu Trusty and MacOS and no segfaults. 

If I remove the code for test_error2 from the file, I don't have any error, even if I don't remove the #include &lt;torch/torch.h&gt; line.

Also tested compiling with clang++ and g++. Same error.

I created a smalll repo here with the minimal example I could make. 

Does anyone have any idea of what this could be?


  Note configure file will download and install libtorch automatically from pytorch's website. So don't install the package if you don't want this.

",2,520,"Turns out that compiling the package with an older version g++ worked fine. 

I installed g++-4.9:

sudo apt-get install g++-4.9.

Edited the .R/Makevars to use g++-4.9:

CXX=g++-4.9
CXX11=g++-4.9


Then recompiled Rcpp and the package.
","Can you try replacing

throw std::runtime_error(""hi this is my error"");


with what our docs suggest you do (in an Rcpp-called function, no less) 

Rcpp::stop(""hi this is my error"");


and see what happens?
",
PyTorch strange behavior,https://stackoverflow.com/questions/63549826,RuntimeError: CUDA out of memory. Problem with stanza lemmatazation using too much GPU memory,"Good day,
I have 11GB of GPU memory and I run into CUDA memory issue with pretrained lemmatazation.
I used this code:
snlp = stanza.Pipeline(lang=""en"", use_gpu=True) # tried different batch_size/ lemma_batch_size - did not help
nlp = StanzaLanguage(snlp)

def tokenize(text):
     tokens = nlp(text)
     doc_l = [token.lemma_ for token in doc]
     lower_tokens = [t.lower() for t in doc_l]
     alpha_only = [t for t in lower_tokens if t.isalpha()]
     no_stops = [t for t in alpha_only if t not in stopwords]
     #torch.cuda.empty_cache() # Tried this - did not work
     return no_stops

tfidf = TfidfVectorizer(tokenizer=tokenize, min_df=0.1, max_df=0.9)
# Construct the TF-IDF matrix
tfidf_matrix = tfidf.fit_transform(texts)


RuntimeError: CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0;
11.00 GiB total capacity; 6.40 GiB already allocated; 439.75 MiB free; 6.53 GiB reserved in total by PyTorch).

I tried
 [(tokenize(t) for t in test]

It only lasted for 12 texts. They are 200 words on average each. Based on Error message - 'Tried to allocate 978.00 MiB' and this data - SNLP uses 1GiB of GPU memory per step??

This behavior seems strange to me (probably because I dont understand how library works) as model is already pretrained, so it should not get bigger when transforming new texts, right? Why it needs so much GPU memory?
Is there any way to clear memory after each run of lemma_ for each text? (#torch.cuda.empty_cache()-does not work) and batch_size does not work either.

It works on CPU, however allocates all of the available memory (32G of RAM), however. It is much slower on CPU. I need it to make it work on CUDA.
",2,1215,"If you check the full stack trace, there might be a hint which processor runs into the memory issue. For instance, I recently ran into a similar issue with this stack trace:
...
File ""stanza/pipeline/depparse_processor.py"", line 42, in process     
preds += self.trainer.predict(b)   
File ""stanza/models/depparse/trainer.py"", line 74, in predict     
_, preds = self.model(word, word_mask, wordchars,
wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel,
word_orig_idx, sentlens, wordlens)   
... 
RuntimeError: CUDA out of memory.
Tried to allocate 14.87 GiB (GPU 0; 14.76 GiB total capacity; 460.31 MiB already
allocated; 13.39 GiB free; 490.00 MiB reserved in total by PyTorch)

Which pointed me to the fact that I needed to set depparse_batch_size when calling stanza.Pipeline(...). There are other settings like batch_size and lemma_batch_size that you mentioned, as well as pos_batch_size and ner_batch_size etc. These should really help resolve this issue.
",,
PyTorch strange behavior,https://stackoverflow.com/questions/73629682,Error Running Stable Diffusion from the command line in Windows,"I installed Stable Diffusion v1.4 by following the instructions described in https://www.howtogeek.com/830179/how-to-run-stable-diffusion-on-your-pc-to-generate-ai-images/#autotoc_anchor_2
My machine heavily exceeds the min reqs to run Stable Diffusion:
Windows 11 Pro
11th Gen Intel i7 @ 2.30GHz
Latest NVIDIA GeForce GPU
16GB Memory
1TB SSD
Yet, I get an error when trying to run the test prompt
python scripts/txt2img.py --prompt ""a close-up portrait of a cat by pablo picasso, vivid, abstract art, colorful, vibrant"" --plms --n_iter 5 --n_samples 1
RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 8.00 GiB total capacity; 6.13 GiB already allocated; 0 bytes free; 6.73 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Reading a post by Marco Ramos it seems like it relates to the number of workers in PyTorch
Strange Cuda out of Memory behavior in Pytorch
How do I change the number of workers while running Stable Diffusion? And why is it throwing this error if my machine still has lots of memory? Has anyone encountered this same issue while running Stable Diffusion?
",2,20462,"I had the same issue, it's because you're using a non-optimized version of Stable-Diffusion. You have to download basujindal's branch of it, which allows it use much less ram by sacrificing the precision, this is the branch - https://github.com/basujindal/stable-diffusion
Everything else in that guide stays the same just clone from this version. It allow you to even push past 512x512 default resolution, you can use 756x512 to get rectangular images for example (but the results may vary since it was trained on a 512 square set).
the new prompt becomes python optimizedSD/optimized_txt2img.py --prompt ""blue orange"" --H 756 --W 512
Also another note: as of a few days ago an even faster and more optimized version was released by neonsecret (https://github.com/basujindal/stable-diffusion), however I'm having issues installing it, so can't really recommend it but you can try it as well and see if it works for you.
","In addition to the optimized version by basujindal, the additional tags following the prompt allows the model to run properly on a machine with NVIDIA or AMD 8+GB GPU.
So the new prompt would look like this
&gt;&gt; python optimizedSD/optimized_txt2img.py --prompt ""a close-up portrait of a cat by pablo picasso, vivid, abstract art, colorful, vibrant"" --H 512 --W 512 --seed 27 --n_iter 2 --n_samples 10 --ddim_steps 50
",
PyTorch strange behavior,https://stackoverflow.com/questions/57982376,Pytorch tensor multiplication with Float tensor giving wrong answer,"I am seeing some strange behavior when i multiply two pytorch tensors.

x = torch.tensor([99397544.0])
y = torch.tensor([0.1])
x * y


This outputs 

tensor([9939755.])


However, the answer should be 9939754.4
",1,2266,"In default, the tensor dtype is torch.float32 in pytorch. Change it to torch.float64 will give the right result. 

x = torch.tensor([99397544.0], dtype=torch.float64)
y = torch.tensor([0.1], dtype=torch.float64)
x * y
# tensor([9939754.4000])


The mismatched result for torch.float32 caused by rounding error if you do not have enough precision to calculate (represent) it. 

What Every Computer Scientist Should Know About Floating-Point Arithmetic
",,
PyTorch strange behavior,https://stackoverflow.com/questions/61966466,No matching distribution found for torch==1.0.0 (from -r requirements.txt (line 25)),"No matching distribution found for torch==1.0.0

I was instructed to use to use pytorch==1.0.0 for training. Higher versions of pytorch have strange bilinear warping behavior, because of it model diverge. but i cannot install it from requirement.txt
",0,618,"Try looking here: https://pytorch.org/get-started/previous-versions/#v100

This answer may not help at all, I don't know a ton about how requirement.txt files work. However, I do know that you can make a new environment and install older versions, commands are given at the website I gave above.
",,
PyTorch strange behavior,https://stackoverflow.com/questions/63939096,Non-deterministic behavior for training a neural network on GPU implemented in PyTorch and with a fixed random seed,"I observed a strange behavior of the final Accuracy when I run exactly the same experiment (the same code for training neural net for image classification) with the same random seed on different GPUs (machines). I use only one GPU. Precisely, When I run the experiment on one machine_1 the Accuracy is 86,37. When I run the experiment on machine_2 the Accuracy is 88,0.
There is no variability when I run the experiment multiple times on the same machine. PyTorch and CUDA versions are the same. Could you help me to figure out the reason and fix it?
Machine_1:
NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2
Machine_2:
NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2
To fix random seed I use the following code:
random.seed(args.seed)
os.environ['PYTHONHASHSEED'] = str(args.seed)
np.random.seed(args.seed)
torch.manual_seed(args.seed)
torch.cuda.manual_seed(args.seed)
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

",0,1133,"This is what I use:
import torch
import os
import numpy as np
import random

def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

set_seed(13)

Make sure you have a single function that set's the seeds from once. If you are using Jupyter notebooks cell execution timing may cause this. Also the order of functions inside may be important. I never had problems with this code. You may call set_seed() often in code.
",,
PyTorch strange result,https://stackoverflow.com/questions/66542007,Transformer Model Output Nan Values in Pytorch,"2021-03-09
I trained my transformer models in pytrorch. In the first few batches, the loss calculation and gradient updates were all performing well. However, the output of the model turned out to be nan values after several iterations. I am confident that there are no flawed data in the dataset. Besides, it's not a classification problem, the labels are float numbers.

2021-03-10
Follow-up:
What an interesting story! When I ran this transformer model with a larger architecture (like 6 encoder layers, 8 heads, etc.). The NAN values disappeared. It seems that the gradient explosion only existed in tiny models.
Solutions:
I searched the Pytorch forum and Stackoverflow and found out the accurate reason for this NAN instance. First, since the NAN loss didn't appear at the very beginning. We can conclude that the model might be well defined. The cause might be the data or the training process. I ran torch.autograd.set_detect_anomaly(True) as told in https://discuss.pytorch.org/t/gradient-value-is-nan/91663/2. It returned that the RuntimeError: Function StdBackward1 returned nan values in its 0th output.
According to the similar question in https://discuss.pytorch.org/t/gradient-of-standard-deviation-is-nan/14713, I double-checked the output in each layer inside the transformer. Strangely, after dozens of iterations, the positional embedding layer outputted a vector full of zeros. As a result, the LayerNorm that does the normalization job cannot backward the loss well, since it calculated the standard deviations and the standard deviation has no gradient at zero (or you can say it's infinite)! The possible solution is to add x.std(unbiased=False) if you are using pytorch.
This's my encounter with the NAN loss and mse. I hope my experience can give some insights to you when you meet this circumstance!
Relative Questions: Deep-Learning Nan loss reasons
",10,6838,"For what it's worth, I had this problem and it turned out that I had forgot to initialize an embedding vector, so it was just whatever torch.empty() happened to come upon (likely a lot of zeros.)
",,
PyTorch strange result,https://stackoverflow.com/questions/64024312,How to compute hessian matrix for all parameters in a network in pytorch?,"Suppose vector \theta is all the parameters in a neural network, I wonder how to compute hessian matrix for \theta in pytorch.
Suppose the network is as follows:
class Net(Module):
    def __init__(self, h, w):
        super(Net, self).__init__()
        self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)
        self.f2 = torch.nn.Linear(32 * h * w, 5)

    def forward(self, x):
        x = self.c1(x)
        x = x.view(x.size(0), -1)
        x = self.f2(x)
        return x

I know the second derivative can be calculated by calling torch.autograd.grad() twice, but the parameters in pytorch is organized by net.parameters(), and I don't know how to compute the hessian for all parameters.
I have tried to use torch.autograd.functional.hessian() in pytorch 1.5 as follows:
import torch
import numpy as np
from torch.nn import Module
import torch.nn.functional as F


class Net(Module):
    def __init__(self, h, w):
        super(Net, self).__init__()
        self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)
        self.f2 = torch.nn.Linear(32 * h * w, 5)

    def forward(self, x):
        x = self.c1(x)
        x = x.view(x.size(0), -1)
        x = self.f2(x)
        return x


def func_(a, b c, d):
    p = [a, b, c, d]
    x = torch.randn(size=[8, 1, 12, 12], dtype=torch.float32)
    y = torch.randint(0, 5, [8])
    x = F.conv2d(x, p[0], p[1], 1, 1)
    x = x.view(x.size(0), -1)
    x = F.linear(x, p[2], p[3])
    loss = F.cross_entropy(x, y)
    return loss


if __name__ == '__main__':
    net = Net(12, 12)

    h = torch.autograd.functional.hessian(func_, tuple([_ for _ in net.parameters()]))
    print(type(h), len(h))


h is a tuple, and the results are in strange shape. For example, the shape of \frac{\delta Loss^2}{\delta c1.weight^2} is [32,1,3,3,32,1,3,3]. It seems like I can combine them into a complete H, but I don't know which part it is in the whole Hessian Matrix and the corresponding order.
",8,3254,"Here is one solution, I think it's a little too complex but could be instructive.
Considering about these points:

First, about torch.autograd.functional.hessian() the first argument must be a function, and the second argument should be a tuple or list of tensors. That means we cannot directly pass a scalar loss to it. (I don't know why, because I think there is no large difference between a scalar loss or a function that returns a scalar)
Second, I want to obtain a complete Hessian matrix, which is the second derivative of all parameters, and it should be in an appropriate order.

So here is the solution:
import torch
import numpy as np
from torch.nn import Module
import torch.nn.functional as F

class Net(Module):
    def __init__(self, h, w):
        super(Net, self).__init__()
        self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)
        self.f2 = torch.nn.Linear(32 * h * w, 5)

    def forward(self, x):
        x = self.c1(x)
        x = x.view(x.size(0), -1)
        x = self.f2(x)
        return x

def haha(a, b, c, d):
    p = [a.view(32, 1, 3, 3), b, c.view(5, 32 * 12 * 12), d]
    x = torch.randn(size=[8, 1, 12, 12], dtype=torch.float32)
    y = torch.randint(0, 5, [8])
    x = F.conv2d(x, p[0], p[1], 1, 1)
    x = x.view(x.size(0), -1)
    x = F.linear(x, p[2], p[3])
    loss = F.cross_entropy(x, y)
    return loss


if __name__ == '__main__':
    net = Net(12, 12)

    h = torch.autograd.functional.hessian(haha, tuple([_.view(-1) for _ in net.parameters()]))
    
    # Then we just need to fix tensors in h into a big matrix

I build a new function haha that works in the same way with the neural network Net. Notice that arguments a, b, c, d are all expanded into one-dimensional vectors, so that the shapes of tensors in h are all two dimensional, in good order and easy to be combined into a large hessian matrix.
In my example, the shapes of tensors in h is
# with relation to c1.weight and c1.weight, c1.bias, f2.weight, f2.bias
[288,288]
[288,32]
[288,23040]
[288,5]

# with relation to c2.bias and c1.weight, c1.bias, f2.weight, f2.bias
[32, 288]
[32, 32]
[32, 23040]
[32, 5]
...


So it is easy to see the meaning of the tensors and which part it is. All we need to do is to allocate a (288+32+23040+5)*(288+32+23040+5) matrix and fix the tensors in h into the corresponding locations.
I think the solution still could be improved, like we don't need to build a function works the same way with neural network, and transform the shape of parameters twice. But for now I don't have better ideas, if there is any better solution, please let me know.
",,
PyTorch strange result,https://stackoverflow.com/questions/74635994,Pytorch&#39;s share_memory_() vs built-in Python&#39;s shared_memory: Why in Pytorch we don&#39;t need to access the shared memory-block?,,5,2918,"You are writing a love letter to the pytorch authors.
That is, you are patting them on the back,
congratulating their wrapper efforts as ""a job well done!""
It's a lovely library.
Let's take a step back and use a very simple
data structure, a dictionary d.
If parent initializes d with some values,
and then kicks off a pair of worker children,
each child has a copy of d.
How did that happen?
The multiprocessing module forked off
the workers, looked at the set of defined
variables which includes d, and serialized
those (key, value) pairs from parent down to
the children.
So at this point we have 3 independent copies
of d. If parent or either child modifies d,
the other 2 copies are completely unaffected.
Now switch gears to the pytorch wrapper.
You offered some nice concise code that demos
the little .SharedMemory() dance an app would
need to do if we want 3 references to same shared structure
rather than 3 independent copies.
The pytorch wrapper serializes references
to common data structure, rather than producing copies.
Under the hood it's doing exactly the dance that you did.
But with no repeated verbiage up at the app level,
as the details have nicely been abstracted away, FTW!

Why in Pytorch we don't need to access the shared memory-block?

tl;dr: We do need to access it. But the library shoulders the burden of worrying about the details, so we don't have to.
","pytorch has a simple wrapper around shared memory, python's shared memory module is only a wrapper around the underlying OS dependent functions.
the way it can be done is that you don't serialize the array or the shared memory themselves, and only serialize what's needed to create them by using the __getstate__ and __setstate__ methods from the docs, so that your object acts as both a proxy and a container at the same time.
the following bar class can double for a proxy and a container this way, which is useful if the user shouldn't have to worry about the shared memory part.
import time
import multiprocessing as mp
from multiprocessing import shared_memory
import numpy as np

class bar:
    def __init__(self):
        self._size = 10
        self._type = np.uint8
        self.shm = shared_memory.SharedMemory(create=True, size=self._size)
        self._mem_name = self.shm.name
        self.arr = np.ndarray([self._size], self._type, buffer=self.shm.buf)

    def __getstate__(self):
        """"""Return state values to be pickled.""""""
        return (self._mem_name, self._size, self._type)

    def __setstate__(self, state):
        """"""Restore state from the unpickled state values.""""""
        self._mem_name, self._size, self._type = state
        self.shm = shared_memory.SharedMemory(self._mem_name)
        self.arr = np.ndarray([self._size], self._type, buffer=self.shm.buf)

def get_time(s):
    return round(time.time() - s, 1)

def foo(shm, lock):
    # -------------------------------------------------------------------
    # without explicitely access the shared memory block we observe
    # that the array has changed:
    # -------------------------------------------------------------------
    a = shm

    # wait ~1sec to print the value.
    time.sleep(1.0)
    with lock:
        print(f""{__name__}\t{get_time(s)}\t\t{a.arr}"")

# global variables.
s = time.time()

if __name__ == '__main__':
    lock = mp.Lock()  # to work on windows/mac.

    print(""Module\t\tTime\t\tValue"")
    print(""-"" * 35)

    # create numpy array and shared memory block.
    a = bar()
    print(f""{__name__}\t{get_time(s)}\t\t{a.arr}"")

    # start child process.
    p0 = mp.Process(target=foo, args=(a, lock))
    p0.start()

    # modify the value of the vaue after ~0.5sec.
    time.sleep(0.5)
    with lock:
        a.arr[0] = 1.0

    print(f""{__name__}\t{get_time(s)}\t\t{a.arr}"")
    time.sleep(1.5)

    p0.join()

python just makes it much easier to hide such details inside the class without bothering the user with such details.
Edit: i wish they'd make locks non-inheritable so your code can raise an error on the lock, instead you'll find out one day that it doesn't actually lock ... After it crashes your application in production.
",
PyTorch strange result,https://stackoverflow.com/questions/60323897,Tensorflow/Keras Conv2D layers with padding=&#39;SAME&#39; behave strangely,"My question:

A straightforward experiment that I conducted showed that using padding='SAME' in a conv2d layer in Keras/TF is different from using padding='VALID' with a preceding zero-padding layer.


How is that possible?
Does Keras/TF pads zeros symmetrically around the tensor?


Explanation of the experiment - just if you're interested in reading further:

I used the onnx2keras package to convert my Pytorch model into keras/TF.

When onnx2keras encounters a convolutional layer with padding &gt; 0 in the ONNX model, it translates it to Keras' Conv2D with valid padding (i.e., no padding!), preceded by Keras' ZeroPadding2D layer. This works very well and returns outputs that are identical to those produced by the Pytorch network.

I yet thought it was strange that it didn't simply used padding='SAME', as most of the references say that Keras/TF use zero padding, just like Pytorch does.

Nevertheless, I patched onnx2keras and made it produce me Conv2D layers with padding='SAME' rather than the existing solution of 'VALID' padding with a preceding zero-padding layer. This made the resulting model return different outputs than the one with the zero-padding layer, and of course different from my Pytorch model, which was identical until the patch. 
",4,3673,"padding='Same' in Keras means padding is added as required to make up for overlaps when the input size and kernel size do not perfectly fit. 

Example of padding='Same':

# Importing dependency
import keras
from keras.models import Sequential
from keras.layers import Conv2D

# Create a sequential model
model = Sequential()

# Convolutional Layer
model.add(Conv2D(filters=24, input_shape=(5,5,1), kernel_size=(2,2), strides =(2,2) ,padding='Same'))

# Model Summary
model.summary()


Output of the code - 

Model: ""sequential_20""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_28 (Conv2D)           (None, 3, 3, 24)          120       
=================================================================
Total params: 120
Trainable params: 120
Non-trainable params: 0
_________________________________________________________________


Pictorial Representation:
Below image shows how the padding for the input (input_shape=(5,5,1), kernel_size=(2,2), strides =(2,2)) when padding='Same'.



------------------------------------------------------------------------------------------------------------------

padding='Valid' in Keras means no padding is added.

Example of padding='Valid': Have used same input for Conv2D that we used above for padding = 'Same' .i.e. (input_shape=(5,5,1), kernel_size=(2,2), strides =(2,2))

# Importing dependency
import keras
from keras.models import Sequential
from keras.layers import Conv2D

# Create a sequential model
model = Sequential()

# Convolutional Layer
model.add(Conv2D(filters=24, input_shape=(5,5,1), kernel_size=(2,2), strides =(2,2) ,padding='Valid'))

# Model Summary
model.summary()


Output of the code - 

Model: ""sequential_21""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_29 (Conv2D)           (None, 2, 2, 24)          120       
=================================================================
Total params: 120
Trainable params: 120
Non-trainable params: 0
_________________________________________________________________


Pictorial Representation:
Below image shows there is no padding added for the input (input_shape=(5,5,1), kernel_size=(2,2), strides =(2,2)) when padding='Valid'.


------------------------------------------------------------------------------------------------------------------

Now lets try same code that we used for padding='Valid' for the input (input_shape=(6,6,1), kernel_size=(2,2), strides =(2,2)). Here padding='Valid' should behave same as padding='Same'.

Code -

# Importing dependency
import keras
from keras.models import Sequential
from keras.layers import Conv2D

# Create a sequential model
model = Sequential()

# Convolutional Layer
model.add(Conv2D(filters=24, input_shape=(6,6,1), kernel_size=(2,2), strides =(2,2) ,padding='Valid'))

# Model Summary
model.summary()


Output of the code - 

Model: ""sequential_22""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_30 (Conv2D)           (None, 3, 3, 24)          120       
=================================================================
Total params: 120
Trainable params: 120
Non-trainable params: 0
_________________________________________________________________

",,
PyTorch strange result,https://stackoverflow.com/questions/58839721,How to define a loss function in pytorch with dependency to partial derivatives of the model w.r.t input?,,3,1749,"Quick Solution: 

add allow_unused=True to .grad functions. So, change 

dydx = torch.autograd.grad(
    y, x,
    grad_outputs=y.data.new(y.shape).fill_(1),
    create_graph=True, retain_graph=True)[0]

d2ydx2 = torch.autograd.grad(dydx, x, grad_outputs=dydx.data.new(
    dydx.shape).fill_(1), create_graph=True, retain_graph=True)[0]


To

dydx = torch.autograd.grad(
    y, x,
    grad_outputs=y.data.new(y.shape).fill_(1),
    create_graph=True, retain_graph=True, allow_unused=True)[0]

d2ydx2 = torch.autograd.grad(dydx, x, grad_outputs=dydx.data.new(
    dydx.shape).fill_(1), create_graph=True, retain_graph=True, allow_unused=True)[0]


More explanation:

See what allow_unused do:

allow_unused (bool, optional): If ``False``, specifying inputs that were not
        used when computing outputs (and therefore their grad is always zero)
        is an error. Defaults to ``False``.


So, if you try to differentiate w.r.t to a variable that is not in being used to compute the value, it will give an error. Also, note that error only occurs when you use linear layers. 

This is because when you use linear layers, you have y=W1*W2*x + b = Wx+b and dy/dx is not a function of x, it is simply W. So when you try to differentiate dy/dx w.r.t x it throws an error. This error goes away as soon as you use sigmoid because then dy/dx will be a function of x. To avoid the error, either make sure dy/dx is a function of x or use allow_unused=True  
",,
PyTorch strange result,https://stackoverflow.com/questions/71146140,Using RNN Trained Model without pytorch installed,"I have trained an RNN model with pytorch. I need to use the model for prediction in an environment where I'm unable to install pytorch because of some strange dependency issue with glibc. However, I can install numpy and scipy and other libraries. So, I want to use the trained model, with the network definition, without pytorch.
I have the weights of the model as I save the model with its state dict and weights in the standard way, but I can also save it using just json/pickle files or similar.
I also have the network definition, which depends on pytorch in a number of ways. This is my RNN network definition.
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import random

torch.manual_seed(1)
random.seed(1)
device = torch.device('cpu')

class RNN(nn.Module):
  def __init__(self, input_size, hidden_size, output_size,num_layers, matching_in_out=False, batch_size=1):
    super(RNN, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.num_layers = num_layers
    self.batch_size = batch_size
    self.matching_in_out = matching_in_out #length of input vector matches the length of output vector 
    self.lstm = nn.LSTM(input_size, hidden_size,num_layers)
    self.hidden2out = nn.Linear(hidden_size, output_size)
    self.hidden = self.init_hidden()
  def forward(self, feature_list):
    feature_list=torch.tensor(feature_list)
    
    if self.matching_in_out:
      lstm_out, _ = self.lstm( feature_list.view(len( feature_list), 1, -1))
      output_space = self.hidden2out(lstm_out.view(len( feature_list), -1))
      output_scores = torch.sigmoid(output_space) #we'll need to check if we need this sigmoid
      return output_scores #output_scores
    else:
      for i in range(len(feature_list)):
        cur_ft_tensor=feature_list[i]#.view([1,1,self.input_size])
        cur_ft_tensor=cur_ft_tensor.view([1,1,self.input_size])
        lstm_out, self.hidden = self.lstm(cur_ft_tensor, self.hidden)
        outs=self.hidden2out(lstm_out)
      return outs
  def init_hidden(self):
    #return torch.rand(self.num_layers, self.batch_size, self.hidden_size)
    return (torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device),
            torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device))

I am aware of this question, but I'm willing to go as low level as possible. I can work with numpy array instead of tensors, and reshape instead of view, and I don't need a device setting.
Based on the class definition above, what I can see here is that I only need the following components from torch to get an output from the forward function:

nn.LSTM
nn.Linear
torch.sigmoid

I think I can easily implement the sigmoid function using numpy. However, can I have some implementation for the nn.LSTM and nn.Linear using something not involving pytorch? Also, how will I use the weights from the state dict into the new class?
So, the question is, how can I ""translate"" this RNN definition into a class that doesn't need pytorch, and how to use the state dict weights for it?
Alternatively, is there a ""light"" version of pytorch, that I can use just to run the model and yield a result?
EDIT
I think it might be useful to include the numpy/scipy equivalent for both nn.LSTM and nn.linear. It would help us compare the numpy output to torch output for the same code, and give us some modular code/functions to use. Specifically, a numpy equivalent for the following would be great:
rnn = nn.LSTM(10, 20, 2)
input = torch.randn(5, 3, 10)
h0 = torch.randn(2, 3, 20)
c0 = torch.randn(2, 3, 20)
output, (hn, cn) = rnn(input, (h0, c0))

and also for linear:
m = nn.Linear(20, 30)
input = torch.randn(128, 20)
output = m(input)

",3,938,"You should try to export the model using torch.onnx. The page gives you an example that you can start with.
An alternative is to use TorchScript, but that requires torch libraries.
Both of these can be run without python. You can load torchscript in a C++ application https://pytorch.org/tutorials/advanced/cpp_export.html
ONNX is much more portable and you can use in languages such as C#, Java, or Javascript
https://onnxruntime.ai/ (even on the browser)
A running example
Just modifying a little your example to go over the errors I found
Notice that via tracing any if/elif/else, for, while will be unrolled
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import random

torch.manual_seed(1)
random.seed(1)
device = torch.device('cpu')

class RNN(nn.Module):
  def __init__(self, input_size, hidden_size, output_size,num_layers, matching_in_out=False, batch_size=1):
    super(RNN, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.num_layers = num_layers
    self.batch_size = batch_size
    self.matching_in_out = matching_in_out #length of input vector matches the length of output vector 
    self.lstm = nn.LSTM(input_size, hidden_size,num_layers)
    self.hidden2out = nn.Linear(hidden_size, output_size)
  def forward(self, x, h0, c0):
    lstm_out, (hidden_a, hidden_b) = self.lstm(x, (h0, c0))
    outs=self.hidden2out(lstm_out)
    return outs, (hidden_a, hidden_b)
  def init_hidden(self):
    #return torch.rand(self.num_layers, self.batch_size, self.hidden_size)
    return (torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device).detach(),
            torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device).detach())

# convert the arguments passed during onnx.export call
class MWrapper(nn.Module):
    def __init__(self, model):
        super(MWrapper, self).__init__()
        self.model = model;
    def forward(self, kwargs):
        return self.model(**kwargs)

Run an example
rnn = RNN(10, 10, 10, 3)
X = torch.randn(3,1,10)
h0,c0  = rnn.init_hidden()
print(rnn(X, h0, c0)[0])

Use the same input to trace the model and export an onnx file

torch.onnx.export(MWrapper(rnn), {'x':X,'h0':h0,'c0':c0}, 'rnn.onnx', 
                  dynamic_axes={'x':{1:'N'},
                               'c0':{1: 'N'},
                               'h0':{1: 'N'}
                               },
                  input_names=['x', 'h0', 'c0'],
                  output_names=['y', 'hn', 'cn']
                 )

Notice that you can use symbolic values for the dimensions of some axes of some inputs. Unspecified dimensions will be fixed with the values from the traced inputs. By default LSTM uses dimension 1 as batch.
Next we load the ONNX model and pass the same inputs
import onnxruntime
ort_model = onnxruntime.InferenceSession('rnn.onnx')
print(ort_model.run(['y'], {'x':X.numpy(), 'c0':c0.numpy(), 'h0':h0.numpy()}))

","Basically implementing it in numpy and copying weights from your pytorch model can do the trick.  For your usecase you will only need to do a forward pass so we just need to implement that only
#Set Parameters for a small LSTM network
input_size  = 2 # size of one 'event', or sample, in our batch of data
hidden_dim  = 3 # 3 cells in the LSTM layer
output_size = 1 # desired model output

num_layers=3
torch_lstm = RNN( input_size, 
                 hidden_dim ,
                 output_size,
                 num_layers,
                 matching_in_out=True
                 )

state = torch_lstm.state_dict() # state will capture the weights of your model

Now for LSTM in numpy these functions will be used:
got the below code from this link: https://towardsdatascience.com/the-lstm-reference-card-6163ca98ae87
### NOT MY CODE
import numpy as np 
from scipy.special import expit as sigmoid

def forget_gate(x, h, Weights_hf, Bias_hf, Weights_xf, Bias_xf, prev_cell_state):
    forget_hidden  = np.dot(Weights_hf, h) + Bias_hf
    forget_eventx  = np.dot(Weights_xf, x) + Bias_xf
    return np.multiply( sigmoid(forget_hidden + forget_eventx), prev_cell_state )

def input_gate(x, h, Weights_hi, Bias_hi, Weights_xi, Bias_xi, Weights_hl, Bias_hl, Weights_xl, Bias_xl):
    ignore_hidden  = np.dot(Weights_hi, h) + Bias_hi
    ignore_eventx  = np.dot(Weights_xi, x) + Bias_xi
    learn_hidden   = np.dot(Weights_hl, h) + Bias_hl
    learn_eventx   = np.dot(Weights_xl, x) + Bias_xl
    return np.multiply( sigmoid(ignore_eventx + ignore_hidden), np.tanh(learn_eventx + learn_hidden) )


def cell_state(forget_gate_output, input_gate_output):
    return forget_gate_output + input_gate_output

  
def output_gate(x, h, Weights_ho, Bias_ho, Weights_xo, Bias_xo, cell_state):
    out_hidden = np.dot(Weights_ho, h) + Bias_ho
    out_eventx = np.dot(Weights_xo, x) + Bias_xo
    return np.multiply( sigmoid(out_eventx + out_hidden), np.tanh(cell_state) )


We would need the sigmoid function as well so
def sigmoid(x):
    return 1/(1 + np.exp(-x))

Because pytorch stores weights in stacked manner so we need to break it up for that we would need the below function
def get_slices(hidden_dim):
    slices=[]
    breaker=(hidden_dim*4)
    slices=[[i,i+3] for i in range(0, breaker, breaker//4)]
    return slices

Now we have the functions ready for lstm, now we create an lstm class to copy the weights from pytorch class and get the output from it.
class numpy_lstm:
    def __init__( self, layer_num=0, hidden_dim=1, matching_in_out=False):
        self.matching_in_out=matching_in_out
        self.layer_num=layer_num
        self.hidden_dim=hidden_dim
        
    def init_weights_from_pytorch(self, state):
        slices=get_slices(self.hidden_dim)
        print (slices)

        #Event (x) Weights and Biases for all gates
        
        lstm_weight_ih='lstm.weight_ih_l'+str(self.layer_num)
        self.Weights_xi = state[lstm_weight_ih][slices[0][0]:slices[0][1]].numpy()  # shape  [h, x]
        self.Weights_xf = state[lstm_weight_ih][slices[1][0]:slices[1][1]].numpy()  # shape  [h, x]
        self.Weights_xl = state[lstm_weight_ih][slices[2][0]:slices[2][1]].numpy()  # shape  [h, x]
        self.Weights_xo = state[lstm_weight_ih][slices[3][0]:slices[3][1]].numpy() # shape  [h, x]

        
        lstm_bias_ih='lstm.bias_ih_l'+str(self.layer_num)
        self.Bias_xi = state[lstm_bias_ih][slices[0][0]:slices[0][1]].numpy()  #shape is [h, 1]
        self.Bias_xf = state[lstm_bias_ih][slices[1][0]:slices[1][1]].numpy()  #shape is [h, 1]
        self.Bias_xl = state[lstm_bias_ih][slices[2][0]:slices[2][1]].numpy()  #shape is [h, 1]
        self.Bias_xo = state[lstm_bias_ih][slices[3][0]:slices[3][1]].numpy() #shape is [h, 1]
        
        
        lstm_weight_hh='lstm.weight_hh_l'+str(self.layer_num)

        #Hidden state (h) Weights and Biases for all gates
        self.Weights_hi = state[lstm_weight_hh][slices[0][0]:slices[0][1]].numpy()  #shape is [h, h]
        self.Weights_hf = state[lstm_weight_hh][slices[1][0]:slices[1][1]].numpy()  #shape is [h, h]
        self.Weights_hl = state[lstm_weight_hh][slices[2][0]:slices[2][1]].numpy()  #shape is [h, h]
        self.Weights_ho = state[lstm_weight_hh][slices[3][0]:slices[3][1]].numpy() #shape is [h, h]
        
        
        lstm_bias_hh='lstm.bias_hh_l'+str(self.layer_num)

        self.Bias_hi = state[lstm_bias_hh][slices[0][0]:slices[0][1]].numpy()  #shape is [h, 1]
        self.Bias_hf = state[lstm_bias_hh][slices[1][0]:slices[1][1]].numpy()  #shape is [h, 1]
        self.Bias_hl = state[lstm_bias_hh][slices[2][0]:slices[2][1]].numpy()  #shape is [h, 1]
        self.Bias_ho = state[lstm_bias_hh][slices[3][0]:slices[3][1]].numpy() #shape is [h, 1]
    def forward_lstm_pass(self,input_data):
        h = np.zeros(self.hidden_dim)
        c = np.zeros(self.hidden_dim)
        
        output_list=[]
        for eventx in input_data:
            f = forget_gate(eventx, h, self.Weights_hf, self.Bias_hf, self.Weights_xf, self.Bias_xf, c)
            i =  input_gate(eventx, h, self.Weights_hi, self.Bias_hi, self.Weights_xi, self.Bias_xi, 
                        self.Weights_hl, self.Bias_hl, self.Weights_xl, self.Bias_xl)
            c = cell_state(f,i)
            h = output_gate(eventx, h, self.Weights_ho, self.Bias_ho, self.Weights_xo, self.Bias_xo, c)
            if self.matching_in_out: # doesnt make sense but it was as it was in main code :(
                output_list.append(h)
        if self.matching_in_out:
            return output_list
        else:
            return h


Similarly for fully connected layer,
    
    
class fully_connected_layer:
    def __init__(self,state, dict_name='fc', ):
        self.fc_Weight = state[dict_name+'.weight'][0].numpy()
        self.fc_Bias = state[dict_name+'.bias'][0].numpy() #shape is [,output_size]
        
    def forward(self,lstm_output, is_sigmoid=True):
        res=np.dot(self.fc_Weight, lstm_output)+self.fc_Bias
        print (res)
        if is_sigmoid:
            return sigmoid(res)
        else:
            return res
        

Now we would need one class to call all of them together and generalise them with respect to multiple layers
You can modify the below class if you need more Fully connected layers or want to set false condition for sigmoid etc.
        
class RNN_model_Numpy:
    def __init__(self, state, input_size, hidden_dim, output_size, num_layers, matching_in_out=True):
        self.lstm_layers=[]
        for i in range(0, num_layers):
            lstm_layer_obj=numpy_lstm(layer_num=i, hidden_dim=hidden_dim, matching_in_out=True)
            lstm_layer_obj.init_weights_from_pytorch(state) 
            self.lstm_layers.append(lstm_layer_obj)
        
        self.hidden2out=fully_connected_layer(state, dict_name='hidden2out')
        
    def forward(self, feature_list):
        for x in self.lstm_layers:
            lstm_output=x.forward_lstm_pass(feature_list)
            feature_list=lstm_output
            
        return self.hidden2out.forward(feature_list, is_sigmoid=False)

Sanity check on a numpy variable:
data = np.array(
           [[1,1],
            [2,2],
            [3,3]])



check=RNN_model_Numpy(state, input_size, hidden_dim, output_size, num_layers)
check.forward(data)

EXPLANATION:
Since we just need forward pass, we would need certain functions that are required in LSTM, for that we have the forget gate, input gate, cell gate and output gate. They are just some operations that are done on the input that you give.
For get_slices function, this is used to break down the weight matrix that we get from pytorch state dictionary (state dictionary) is the dictionary which contains the weights of all the layers that we have in our network.
For LSTM particularly have it in this order  ignore, forget, learn, output. So for that we would need to break it up for different LSTM cells.
For numpy_lstm class, we have init_weights_from_pytorch function which must be called, what it will do is that it will extract the weights from state dictionary which we got earlier from pytorch model object and then populate the numpy array weights with the pytorch weights. You can first train your model and then save the state dictionary through pickle and then use it.
The fully connected layer class just implements the hidden2out neural network.
Finally our rnn_model_numpy class is there to ensure that if you have multiple layers then it is able to send the output of one layer of lstm to other layer of lstm.
Lastly there is a small sanity check on data variable.
IMPORTANT NOTE: PLEASE NOTE THAT YOU MIGHT GET DIMENSION ERROR AS PYTORCH WAY OF HANDLING INPUT IS COMPLETELY DIFFERENT SO PLEASE ENSURE THAT YOU INPUT NUMPY IS OF SIMILAR SHAPE AS DATA VARIABLE.
Important references:
https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
https://christinakouridi.blog/2019/06/19/backpropagation-lstm/
",
PyTorch strange result,https://stackoverflow.com/questions/57285077,Derivative in both arguments of torch.nn.BCELoss(),"When using a torch.nn.BCELoss() on two arguments that are both results of some earlier computation, I get some curious error, which this question is about:

RuntimeError: the derivative for 'target' is not implemented


The MCVE is as follows: 

import torch
import torch.nn.functional as F

net1 = torch.nn.Linear(1,1)
net2 = torch.nn.Linear(1,1)
loss_fcn = torch.nn.BCELoss()

x = torch.zeros((1,1))

y = F.sigmoid(net1(x)) #make sure y is in range (0,1)
z = F.sigmoid(net2(y)) #make sure z is in range (0,1)

loss = loss_fcn(z, y) #works if we replace y with y.detach()

loss.backward()


It turns out if we call .detach() on y the error disappears. But this results in a different computation, now in the .backward()-pass, the gradients with respect to the second argument of the BCELoss will not be computed.

Can anyone explain what I'm doing wrong in this case? As far as I know all pytorch modules in torch.nn should support computing gradients. And this error message seems to tell me that the derivative is not implemented for y, which is somehow strange, as you can compute the gradient of y, but not of y.detach() which seems to be contradictory.
",3,3100,"I met the same problem too. As far as I know, the second argument of BCELoss(input, target),target should be a tensor without gradient attribute. It means that target.requires_grad should be False. But I don't know why.

Usually, the target(we can also call it Ground Truth) doesn't have a gradient attribute. But the target(y in your code) was calculated by F.sigmoid(net1(x)), which means the target (output of net1) has been a tensor with gradient attribute.

so, you should try:


  loss = loss_fcn(z, y.detach())


or:


  loss = loss_fcn(z, y.data)


maybe this?

import torch
import torch.nn.functional as F

net1 = torch.nn.Linear(1,1)
net2 = torch.nn.Linear(1,1)
loss_fcn = torch.nn.BCELoss()

x = torch.zeros((1,1))

y = F.sigmoid(net1(x)) #make sure y is in range (0,1)
z = F.sigmoid(net2(y)) #make sure z is in range (0,1)

y.retain_grad()
a = y

loss = loss_fcn(z, a.detach()) #works if we replace y with y.detach()

loss.backward()

print(y.grad)

","It seems I misunderstood the error message. It is not y that doesn't allow the computation for gradients, it is BCELoss() that doesn't have the ability to compute gradients with respect to the second argument. A similar problem was discussed here.
",
PyTorch strange result,https://stackoverflow.com/questions/56342193,Matrix multiplication (element-wise) from numpy to Pytorch,"I got two numpy arrays (image and and environment map), 

MatA
MatB


Both with shapes (256, 512, 3)

When I did the multiplication (element-wise) with numpy:

prod = np.multiply(MatA,MatB)


I got the wanted result (visualize via Pillow when turning back to Image)

But when I did it using pytorch, I got a really strange result(not even close to the aforementioned).

I did it with the following code:

MatATensor = transforms.ToTensor()(MatA)
MatBTensor = transforms.ToTensor()(MatB)

prodTensor = MatATensor * MatBTensor


For some reasons, the shape for both MatATensor and MatBtensor is 

torch.Size([3, 256, 512])


Same for the prodTensor too.
When I tried to reshape to (256,512,3), I got an error.

Is there a way to get the same result? 

I am new to pytorch, so any help would be appreciated.
",3,2267,"I suggest you use torch.from_numpy, which will easily convert your ndarrays to torch tensors. As in:

In[1]: MatA = np.random.rand(256, 512, 3)
In[2]: MatB = np.random.rand(256, 512, 3)

In[3]: MatA_torch = torch.from_numpy(MatA)
In[4]: MatB_torch = torch.from_numpy(MatB)

In[5]: mul_np = np.multiply(MatA, MatB)
In[6]: mul_torch = MatA_torch * MatB_torch

In[7]: torch.equal(torch.from_numpy(mul_np), mul_torch)
Out[7]: True

In[8]: mul_torch.shape
Out[8]: torch.Size([256, 512, 3])


If you want it back to numpy, just do:

mul_torch.numpy()

","If you read the documentation of transforms.ToTensor() you'll see this transformation does not only convert a numpy array to torch.FloatTensor, but also transpose its dimensions from HxWx3 to 3xHxW.
To ""undo"" this you'll need to

 prodasNp = (prodTensor.permute(2, 0, 1) * 255).to(torch.uint8).numpy()


See permute for more information.
",
PyTorch strange result,https://stackoverflow.com/questions/61933879,Unexpected error when loading the model: problem in predictor - ModuleNotFoundError: No module named &#39;torchvision&#39;,,2,1263,"The solution was to place the following packages in thsetup.py file for the custom prediction code:

REQUIRED_PACKAGES = ['torchvision==0.5.0', 'torch @ https://download.pytorch.org/whl/cpu/torch-1.4.0%2Bcpu-cp37-cp37m-linux_x86_64.whl', 'opencv-python', 'facenet-pytorch']


I then had a different problem with custom class instantiation, but this article explains it well. So I was able to successfully deploy my model to the AI Platform for prediction.
",,
PyTorch strange result,https://stackoverflow.com/questions/68904210,"I want to use Numpy to simulate the inference process of a quantized MobileNet V2 network, but the outcome is different with pytorch realized one","Python version: 3.8
Pytorch version: 1.9.0+cpu
Platform: Anaconda Spyder5.0
To reproduce this problem, just copy every code below to a single file.
The ILSVRC2012_val_00000293.jpg file used in this code is shown below, you also need to download it and then change its destination in the code.


Some background of this problem:

I am now working on a project that aims to develop a hardware accelerator to complete the inference process of the MobileNet V2 network. I used pretrained quantized Pytorch model to simulate the outcome, and the result comes out very well.
   
In order to use hardware to complete this task, I wish to know every inputs and outputs as well as intermidiate variables during runing this piece of pytorch code. I used a package named torchextractor to fetch the outcomes of first layer, which in this case, is a 3*3 convolution layer.
import numpy as np
import torchvision
import torch
from torchvision import transforms, datasets
from PIL import Image
from torchvision import transforms
import torchextractor as tx
import math
#########################################################################################
##### Processing of input image
#########################################################################################

normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])
test_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        normalize,])

preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


#image file destination 
filename = ""D:\Project_UM\MobileNet_VC709\MobileNet_pytorch\ILSVRC2012_val_00000293.jpg""
input_image = Image.open(filename)
input_tensor = preprocess(input_image)
input_batch = input_tensor.unsqueeze(0)
#########################################################################################
#########################################################################################
#########################################################################################

#----First verify that the torchextractor class should not influent the inference outcome 

# ofmp of layer1 before putting into torchextractor
a,b,c = quantize_tensor(input_batch)# to quantize the input tensor and return an int8 tensor, scale and zero point
input_qa = torch.quantize_per_tensor(torch.tensor(input_batch.clone().detach()), b, c, torch.quint8)# Using quantize_per_tensor method of torch

# Load a quantized mobilenet_v2 model
model_quantized = torchvision.models.quantization.mobilenet_v2(pretrained=True, quantize=True)

model_quantized.eval()
with torch.no_grad():
    output = model_quantized.features[0][0](input_qa)# Ofmp of layer1, datatype : quantized_tensor

# print(""FM of layer1 before tx_extractor:\n"",output.int_repr())# Ofmp of layer1, datatype : int8 tensor
output1_clone = output.int_repr().detach().numpy()# Clone ofmp of layer1, datatype : ndarray



#########################################################################################
#########################################################################################
#########################################################################################

# ofmp of layer1 after adding torchextractor
model_quantized_ex = tx.Extractor(model_quantized, [""features.0.0""])#Capture of the module inside first layer
model_output, features = model_quantized_ex(input_batch)# Forward propagation
# feature_shapes = {name: f.shape for name, f in features.items()}
# print(features['features.0.0']) # Ofmp of layer1, datatype : quantized_tensor
out1_clone = features['features.0.0'].int_repr().numpy() # Clone ofmp of layer1, datatype : ndarray


if(out1_clone.all() == output1_clone.all()):
    print('Model with torchextractor attached output the same value as the original model')
else:
    print('Torchextractor method influence the outcome')

Here I define a numpy quantization scheme based on the quantization scheme proposed by
Quantization and Training of Neural Networks for Efficient
Integer-Arithmetic-Only Inference
# Convert a normal regular tensor to a quantized tensor with scale and zero_point
def quantize_tensor(x, num_bits=8):# to quantize the input tensor and return an int8 tensor, scale and zero point

    qmin = 0.
    qmax = 2.**num_bits - 1.
    min_val, max_val = x.min(), x.max()

    scale = (max_val - min_val) / (qmax - qmin)

    initial_zero_point = qmin - min_val / scale

    zero_point = 0
    if initial_zero_point &lt; qmin:
        zero_point = qmin
    elif initial_zero_point &gt; qmax:
        zero_point = qmax
    else:
        zero_point = initial_zero_point

    # print(zero_point)
    zero_point = int(zero_point)
    q_x = zero_point + x / scale
    q_x.clamp_(qmin, qmax).round_()
    q_x = q_x.round().byte()
    return q_x, scale, zero_point

#%%
# #############################################################################################
# ---------  Simulate the inference process of layer0: conv33 using numpy
# #############################################################################################


# get the input_batch quantized buffer data
input_scale = b.item()
input_zero  = c
input_quantized = a[0].detach().numpy()

# get the layer0 output scale and zero_point
output_scale = model_quantized.features[0][0].state_dict()['scale'].item()
output_zero  = model_quantized.features[0][0].state_dict()['zero_point'].item()

# get the quantized weight with scale and zero_point  
weight_scale = model_quantized.features[0][0].state_dict()[""weight""].q_scale()
weight_zero  = model_quantized.features[0][0].state_dict()[""weight""].q_zero_point()
weight_quantized = model_quantized.features[0][0].state_dict()[""weight""].int_repr().numpy()
# print(weight_quantized)
# print(weight_quantized.shape)


# bias_quantized,bias_scale,bias_zero= quantize_tensor(model_quantized.features[0][0].state_dict()[""bias""])# to quantize the input tensor and return an int8 tensor, scale and zero point
# print(bias_quantized.shape)
bias = model_quantized.features[0][0].state_dict()[""bias""].detach().numpy()
# print(input_quantized)
print(type(input_scale))
print(type(output_scale))
print(type(weight_scale))

Then I write a quantized 2D convolution using numpy, hope to figure out every details in pytorch data flow during the inference.
#%% numpy simulated layer0 convolution function define

def conv_cal(input_quantized, weight_quantized, kernel_size, stride, out_i, out_j, out_k):
    weight = weight_quantized[out_i]
    input = np.zeros((input_quantized.shape[0], kernel_size, kernel_size))
    for i in range(weight.shape[0]):
        for j in range(weight.shape[1]):
            for k in range(weight.shape[2]):
                input[i][j][k] = input_quantized[i][stride*out_j+j][stride*out_k+k]
    # print(np.dot(weight,input))
    # print(input,""\n"")
    # print(weight)

    return np.multiply(weight,input).sum()

def QuantizedConv2D(input_scale, input_zero, input_quantized, output_scale, output_zero, weight_scale, weight_zero, weight_quantized, bias, kernel_size, stride, padding, ofm_size):
    output = np.zeros((weight_quantized.shape[0],ofm_size,ofm_size))

    input_quantized_padding = np.full((input_quantized.shape[0],input_quantized.shape[1]+2*padding,input_quantized.shape[2]+2*padding),0)
    zero_temp = np.full(input_quantized.shape,input_zero)
    input_quantized = input_quantized - zero_temp
    for i in range(input_quantized.shape[0]):
        for j in range(padding,padding + input_quantized.shape[1]):
            for k in range(padding,padding + input_quantized.shape[2]):
                input_quantized_padding[i][j][k] = input_quantized[i][j-padding][k-padding]

    zero_temp = np.full(weight_quantized.shape, weight_zero)
    weight_quantized = weight_quantized - zero_temp

    for i in range(output.shape[0]):
        for j in range(output.shape[1]):
            for k in range(output.shape[2]):
                # output[i][j][k] = (weight_scale*input_scale)*conv_cal(input_quantized_padding, weight_quantized, kernel_size, stride, i, j, k) + bias[i] #floating_output
                output[i][j][k] = weight_scale*input_scale/output_scale*conv_cal(input_quantized_padding, weight_quantized, kernel_size, stride, i, j, k) + bias[i]/output_scale + output_zero
                output[i][j][k] = round(output[i][j][k])
                # int_output
    return output  

Here I input the same image, weight, and bias together with their zero_point and scale, then compare this ""numpy simulated"" result to the PyTorch calculated one.
quantized_model_out1_int8 = np.squeeze(features['features.0.0'].int_repr().numpy())


print(quantized_model_out1_int8.shape)
print(quantized_model_out1_int8)
out1_np = QuantizedConv2D(input_scale, input_zero, input_quantized, output_scale, output_zero, weight_scale, weight_zero, weight_quantized, bias, 3, 2, 1, 112)
np.save(""out1_np.npy"",out1_np)

for i in range(quantized_model_out1_int8.shape[0]):
    for j in range(quantized_model_out1_int8.shape[1]):
        for k in range(quantized_model_out1_int8.shape[2]):
            if(out1_np[i][j][k] &lt; 0):
                out1_np[i][j][k] = 0

print(out1_np)

flag = np.zeros(quantized_model_out1_int8.shape)
for i in range(quantized_model_out1_int8.shape[0]):
    for j in range(quantized_model_out1_int8.shape[1]):
        for k in range(quantized_model_out1_int8.shape[2]):

            if(quantized_model_out1_int8[i][j][k] == out1_np[i][j][k]):
                flag[i][j][k] = 1
                out1_np[i][j][k] = 0
                quantized_model_out1_int8[i][j][k] = 0

# Compare the simulated result to extractor fetched result, gain the total hit rate
print(flag.sum()/(112*112*32)*100,'%')

If the ""numpy simulated"" results are the same as the extracted one, call it a hit. Print the total hit rate, it shows that numpy gets 92% of the values right. Now the problem is, I have no idea why the rest 8% of values come out wrong.


Comparison of two outcomes:
The picture below shows the different values between Numpy one and PyTorch one, the sample channel is index[1]. The left upper corner is Numpy one, and the upright corner is PyTorch one, I have set all values that are the same between them to 0, as you can see, most of the values just have a difference of 1(This can be view as the error brought by the precision loss of fixed point arithmetics), but some have large differences, e.g. the value[1][4], 121 vs. 76 (I don't know why)


Focus on one strange value:
This code is used to replay the calculation process of the value[1][4], originally I was expecting a trial and error process could lead me to solve this problem, to get my wanted number of 76, but no matter how I tried, it didn't output 76. If you want to try this, I paste this code for your convenience.


    #%% A test code to check the calculation process
    weight_quantized_sample = weight_quantized[2]
    M_t = input_scale * weight_scale / output_scale
    ifmap_t = np.int32(input_quantized[:,1:4,7:10])
    weight_t = np.int32(weight_quantized_sample)
    bias_t = bias[2]
    bias_q = bias_t/output_scale
    res_t = 0
    for ch in range(3):
        ifmap_offset = ifmap_t[ch]-np.int32(input_zero)
        weight_offset = weight_t[ch]-np.int32(weight_zero)
        res_ch = np.multiply(ifmap_offset, weight_offset)
        res_ch = res_ch.sum()
        res_t = res_t + res_ch
    res_mul = M_t*res_t
    # for n in range(1, 30):
    #     res_mul = multiply(n, M_t, res_t)
    res_t = round(res_mul + output_zero + bias_q)
    print(res_t)

Could you help me out of this, have been stuck here for a long time.
",2,562,"I implemented my own version of quantized convolution and got from 99.999% to 100% hitrate (and mismatch of a single value is by 1 that I can consider to be a rounding issue). The link on the paper in the question helped a lot.
But I found that your formulas are the same as mine. So I don't know what was your issue. As I understand quantization in pytorch is hardware dependent.
Here is my code:
def my_Conv2dRelu_b2(input_q, conv_layer, output_shape):
    '''

    Args:
        input_q: quantized tensor
        conv_layer: quantized tensor
        output_shape: the pre-computed shape of the result

    Returns:

    '''
    output = np.zeros(output_shape)

    # extract needed float numbers from quantized operations
    weights_scale = conv_layer.weight().q_per_channel_scales()
    input_scale = input_q.q_scale()
    weights_zp = conv_layer.weight().q_per_channel_zero_points()
    input_zp = input_q.q_zero_point()

    # extract needed convolution parameters
    padding = conv_layer.padding
    stride = conv_layer.stride

    # extract float numbers for results
    output_zp = conv_layer.zero_point
    output_scale = conv_layer.scale
    conv_weights_int = conv_layer.weight().int_repr()
    input_int = input_q.int_repr()

    biases = conv_layer.bias().numpy()
    for k in range(input_q.shape[0]):
        for i in range(conv_weights_int.shape[0]):
            output[k][i] = manual_convolution_quant(
                input_int[k].numpy(),
                conv_weights_int[i].numpy(),
                biases[i],
                padding=padding,
                stride=stride,
                image_zp=input_zp, image_scale=input_scale,
                kernel_zp=weights_zp[i].item(), kernel_scale=weights_scale[i].item(),
                result_zp=output_zp, result_scale=output_scale
            )
    return output


def manual_convolution_quant(image, kernel, b, padding, stride, image_zp, image_scale, kernel_zp, kernel_scale,
                             result_zp, result_scale):
    H = image.shape[1]
    W = image.shape[2]
    new_H = H // stride[0]
    new_W = W // stride[1]
    results = np.zeros([new_H, new_W])

    M = image_scale * kernel_scale / result_scale
    bias = b / result_scale
    paddedIm = np.pad(
        image,
        [(0, 0), (padding[0], padding[0]), (padding[1], padding[1])],
        mode=""constant"",
        constant_values=image_zp,
    )
    s = kernel.shape[1]
    for i in range(new_H):
        for j in range(new_W):
            patch = paddedIm[
                    :, i * stride[0]: i * stride[0] + s, j * stride[1]: j * stride[1] + s
                    ]
            res = M * ((kernel - kernel_zp) * (patch - image_zp)).sum() + result_zp + bias
            if res &lt; 0:
                res = 0
            results[i, j] = round(res)

    return results

Code to compare pytorch and my own version.
def calc_hit_rate(array1, array2):
    good = (array1 == array2).astype(np.int).sum()
    all = array1.size
    return good / all


# during inference
y2 = model.conv1(y1)
y2_int = torch.int_repr(y2)

y2_int_manual = my_Conv2dRelu_b2(y1, model.conv1, y2.shape)
print(f'y2 hit rate= {calc_hit_rate(y2.int_repr().numpy(), y2_int_manual)}') #hit_rate=1.0

",,
PyTorch strange result,https://stackoverflow.com/questions/61116433,"Maybe I found something strange on pytorch, which result in property setter not working","Maybe I found something strange on pytorch, which result in property setter not working. Below is a minimal example that demonstrates this:

import torch.nn as nn

class A(nn.Module):
    def __init__(self):
        super(A, self).__init__()
        self.aa = 1
        self.oobj = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    @property
    def obj(self):
        print('get attr [obj]: {0}'.format(self.oobj))
        return self.oobj
    @obj.setter
    def obj(self, val):
        print('set attr [obj] to {0}'.format(val))
        self.oobj = val

class B(nn.Module):
    def get_attr(self):
        print('no any attr.')

class C:
    def get_attr(self):
        print('no any attr.')

b = A()             # set obj, and prints my setter message

b.obj               # get obj using my getter

# respectively run the following 3 lines, only the last line not call the setter I defined explicitly.
b.obj = C()         # set obj, and prints my setter message

# b.obj = [1, 2, 3]   # set obj, and prints my setter message

# b.obj = B()         # set obj, but it doesn't print my setter message



The last line doesn't call property setter I defined on class A, but call setter on torch.nn.Module. Because A
    regard B as a nn.Module, call the setter on nn.Module to set attr [obj] as a Module, but it still strange, why
    not call the setter I explicitly defined on class A?

And my project needs to set a nn.Module attribute via setter I defined explicitly, which causes BUG( because it failed). Now I change my code solved the BUG, but still puzzle with the problem.
",2,1746,"It may not look obvious at first, but up until you set b.obj as a nn.Module object, you are defining a normal attribute; but once you set b.obj as a nn.Module object, then you can ""only"" replace b.obj with another nn.Module, because you registered it to _modules. Let me walk you through the code and you'll get it.

nn.Module()'s __setattr__ implementation can be found here.

First, you defined a new nn.Module:

b = A()  # btw, why not a = A() :)


Then, you set (I'll skip unnecessary steps to reproduce the behavior):

b.obj = [1, 2, 3]


In this case, because


[1,2,3] is not a nn.Parameter;
You haven't set a nn.Parameter as attribute before;
[1,2,3] is not a nn.Module;
You haven't set a nn.Module as attribute before;
You haven't registered a buffer before;


Then, this line will be execute:

object.__setattr__(self, name, value)


which is nothing but a normal attribute set, which calls your setter.

Now, when you set:

b.obj = B()


Then, because B() is a nn.Module, the following block will be executed instead:

modules = self.__dict__.get('_modules')
if isinstance(value, Module):
    if modules is None:
        raise AttributeError(
            ""cannot assign module before Module.__init__() call"")
    remove_from(self.__dict__, self._parameters, self._buffers)
    modules[name] = value


So, now you are actually registering a nn.Module to self.__dict__.get('_modules') (print it before and after and you'll see... do it before and after setting [1,2,3] as well).

After this point, if you are not setting a nn.Parameter, and you try to set .obj again, then it will fall into this block:

elif modules is not None and name in modules:
    if value is not None:
        raise TypeError(""cannot assign '{}' as child module '{}' ""
                        ""(torch.nn.Module or None expected)""
                        .format(torch.typename(value), name))
    modules[name] = value


That is: you already have modules['obj'] set to something and from now on you need to provide another nn.Module or None if you want to set it again. And, as you can see, because you are providing a list if you try to set b.obj = [1,2,3] again, you'll get the error message in the block above, and that is what you get.

If you really want set it to something else, then you have to delete it before:

b.obj = B()
del b.obj
b.obj = [1,2,3]

",,
PyTorch strange result,https://stackoverflow.com/questions/55828687,Pytorch 1.0: what does net.to(device) do in nn.DataParallel?,"The following code from the tutorial to pytorch data paraleelism reads strange to me:

device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")

model = Model(input_size, output_size)
if torch.cuda.device_count() &gt; 1:
  print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")
  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs
  model = nn.DataParallel(model)

model.to(device)


According to my best knowledge, mode.to(device) copy the data to GPU. 


  DataParallel splits your data automatically and sends job orders to multiple models on several GPUs. After each model finishes their job, DataParallel collects and merges the results before returning it to you.


If the DataParallel does the job of copying, what does the to(device) do here?
",2,2274,"They add few lines in the tutorial to explain nn.DataParallel. 


  DataParallel splits your data automatically, and send job orders to multiple models on different GPUs using the data. After each model finishes their job, DataParallel collects and merges the results for you.


The above quote can be understood that nn.DataParallel is just a wrapper class to inform model.cuda() should make a multiple copies to GPUs.

In my case, I don't have any GPU on my laptop. I still call nn.DataParallel() without any problem.

import torch
import torchvision

model = torchvision.models.alexnet()
model = torch.nn.DataParallel(model)
# No error appears if I don't move the model to `cuda`

",,
PyTorch strange result,https://stackoverflow.com/questions/69105473,tflite quantized mobilenet v2 classifier not working,,1,981,"EdgeTPU mapping of PReLU (LeakyReLU) is now supported in openvino2tensorflow v1.20.4.
However, due to the large size of the model, it is not possible to map all operations to the EdgeTPU. Therefore, the part of the EdgeTPU that does not fit in RAM is offloaded to the CPU for inference, which is very slow. In this case, inference by the CPU alone is 4 to 5 times faster. EdgeTPU does not support PReLU (LeakyReLU), so the operations must be replaced. However, openvino2tensorflow v1.20.4 automatically replaces the operations in the conversion process.

Converted model
https://github.com/PINTO0309/PINTO_model_zoo/tree/main/149_depth_estimation

Convert sample


docker run --gpus all -it --rm \
-v `pwd`:/home/user/workdir \
pinto0309/openvino2tensorflow:latest

cd workdir

MODEL=depth_estimation_mbnv2

H=180
W=320
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model ${MODEL}_${H}x${W}.onnx \
--data_type FP32 \
--output_dir ${H}x${W}/openvino/FP32
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model ${MODEL}_${H}x${W}.onnx \
--data_type FP16 \
--output_dir ${H}x${W}/openvino/FP16
mkdir -p ${H}x${W}/openvino/myriad
${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/lib/intel64/myriad_compile \
-m ${H}x${W}/openvino/FP16/${MODEL}_${H}x${W}.xml \
-ip U8 \
-VPU_NUMBER_OF_SHAVES 4 \
-VPU_NUMBER_OF_CMX_SLICES 4 \
-o ${H}x${W}/openvino/myriad/${MODEL}_${H}x${W}.blob

openvino2tensorflow \
--model_path ${H}x${W}/openvino/FP32/${MODEL}_${H}x${W}.xml \
--output_saved_model \
--output_pb \
--output_no_quant_float32_tflite \
--output_weight_quant_tflite \
--output_float16_quant_tflite \
--output_integer_quant_tflite \
--string_formulas_for_normalization 'data / 255' \
--output_integer_quant_type 'uint8' \
--output_tfjs \
--output_coreml \
--output_tftrt
mv saved_model saved_model_${H}x${W}

openvino2tensorflow \
--model_path ${H}x${W}/openvino/FP32/${MODEL}_${H}x${W}.xml \
--output_saved_model \
--output_pb \
--output_edgetpu \
--string_formulas_for_normalization 'data / 255' \
--output_integer_quant_type 'uint8'
mv saved_model/model_full_integer_quant.tflite saved_model_${H}x${W}/model_full_integer_quant.tflite
mv saved_model/model_full_integer_quant_edgetpu.tflite saved_model_${H}x${W}/model_full_integer_quant_edgetpu.tflite

mv ${H}x${W}/openvino saved_model_${H}x${W}/openvino
mv ${MODEL}_${H}x${W}.onnx saved_model_${H}x${W}/${MODEL}_${H}x${W}.onnx


H=240
W=320
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model ${MODEL}_${H}x${W}.onnx \
--data_type FP32 \
--output_dir ${H}x${W}/openvino/FP32
$INTEL_OPENVINO_DIR/deployment_tools/model_optimizer/mo.py \
--input_model ${MODEL}_${H}x${W}.onnx \
--data_type FP16 \
--output_dir ${H}x${W}/openvino/FP16
mkdir -p ${H}x${W}/openvino/myriad
${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/lib/intel64/myriad_compile \
-m ${H}x${W}/openvino/FP16/${MODEL}_${H}x${W}.xml \
-ip U8 \
-VPU_NUMBER_OF_SHAVES 4 \
-VPU_NUMBER_OF_CMX_SLICES 4 \
-o ${H}x${W}/openvino/myriad/${MODEL}_${H}x${W}.blob

openvino2tensorflow \
--model_path ${H}x${W}/openvino/FP32/${MODEL}_${H}x${W}.xml \
--output_saved_model \
--output_pb \
--output_no_quant_float32_tflite \
--output_weight_quant_tflite \
--output_float16_quant_tflite \
--output_integer_quant_tflite \
--string_formulas_for_normalization 'data / 255' \
--output_integer_quant_type 'uint8' \
--output_tfjs \
--output_coreml \
--output_tftrt
mv saved_model saved_model_${H}x${W}

openvino2tensorflow \
--model_path ${H}x${W}/openvino/FP32/${MODEL}_${H}x${W}.xml \
--output_saved_model \
--output_pb \
--output_edgetpu \
--string_formulas_for_normalization 'data / 255' \
--output_integer_quant_type 'uint8'
mv saved_model/model_full_integer_quant.tflite saved_model_${H}x${W}/model_full_integer_quant.tflite
mv saved_model/model_full_integer_quant_edgetpu.tflite saved_model_${H}x${W}/model_full_integer_quant_edgetpu.tflite

mv ${H}x${W}/openvino saved_model_${H}x${W}/openvino
mv ${MODEL}_${H}x${W}.onnx saved_model_${H}x${W}/${MODEL}_${H}x${W}.onnx


PReLU (LeakyReLU) to Maximum (ReLU), Minimum, Mul, Add
From:

To:

EdgeTPU model



",,
PyTorch strange result,https://stackoverflow.com/questions/69293455,Why does Keras BatchNorm produce different output than PyTorch?,,1,1632,"Batchnormalization works differently in training and inference,
During training (i.e. when using fit() or when calling the layer/model with the argument training=True), the layer normalizes its output using the mean and standard deviation of the current batch of inputs. That is to say, for each channel being normalized, the layer returns
gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta

where:

epsilon is small constant (configurable as part of the constructor arguments)
gamma is a learned scaling factor (initialized as 1), which can be disabled by passing scale=False to the constructor.
beta is a learned offset factor (initialized as 0), which can be disabled by passing center=False to the constructor.

During inference (i.e. when using evaluate() or predict() or when calling the layer/model with the argument training=False (which is the default), the layer normalizes its output using a moving average of the mean and standard deviation of the batches it has seen during training. That is to say, it returns
gamma * (batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta.

self.moving_mean and self.moving_var are non-trainable variables that are updated each time the layer in called in training mode, as such:
    moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)
    moving_var = moving_var * momentum + var(batch) * (1 - momentum)

ref: https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization
If you run the pytorch batchnorm in eval mode, you get close results (the rest of the discrepancy comes from the different internal implementation, parameter choices, etc.),
from torch import nn
import torch
x = torch.ones((1, 2, 2, 2))
a = nn.Sequential(
            # nn.Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), padding=0, bias=True),
            nn.BatchNorm2d(2)
        )
a.eval()
b = a(x)
print(b)
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.layers import *
x = tf.ones((1, 2, 2, 2))
a = keras.models.Sequential([
            # Conv2D(128, (1, 1), (1, 1), padding='same', use_bias=True),
            BatchNormalization()
        ])
b = a(x)
print(b)

out:
tensor([[[[1.0000, 1.0000],
          [1.0000, 1.0000]],

         [[1.0000, 1.0000],
          [1.0000, 1.0000]]]], grad_fn=&lt;NativeBatchNormBackward&gt;)
tf.Tensor(
[[[[0.9995004 0.9995004]
   [0.9995004 0.9995004]]

  [[0.9995004 0.9995004]
   [0.9995004 0.9995004]]]], shape=(1, 2, 2, 2), dtype=float32)

",,
PyTorch strange result,https://stackoverflow.com/questions/63629112,Neural network always predicts the same class in each epoch,"I'm trying to implement a mnist classifier with DNN. However, the result I got is quite strange.
enter image description here
In this epoch, this model can only predict number '0' correctly, and incorrect prediction for all the other numbers. This model could only predict a specific number for each epoch. (such predicted number is different in each epoch)
This is how I get the dataset.
from sklearn.datasets import fetch_openml
from keras.utils.np_utils import to_categorical
import numpy as np
from sklearn.model_selection import train_test_split
import time

x, y = fetch_openml('mnist_784', version=1, return_X_y=True)
x = (x/255.).astype('float32')
y = to_categorical(y)

x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)

For this part, this is my model. A two-hidden-layers DNN with activation functions of Relu and softmax, Cross entropy loss for the error function.
I'm not really sure if my backpropagation is correct or not. I think something is wrong here.
import numpy as np


class NN():
    def __init__(self, input_size, hidden_1_size, hidden_2_size, output_size):
        self.input_data = np.random.randn(1, input_size)
        self.w1 = np.random.randn(input_size, hidden_1_size)
        self.b1 = np.random.randn(1, hidden_1_size)
        
        self.w2 = np.random.randn(hidden_1_size, hidden_2_size)
        self.b2 = np.random.randn(1, hidden_2_size) 

        self.w3 = np.random.randn(hidden_2_size, output_size)
        self.b3 = np.random.randn(1, output_size)


    def Sigmoid(self, z):
        return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-7))

    def Softmax(self, z):
        y_logit = np.exp(z - np.max(z, 1, keepdims=True))
        y = y_logit / np.sum(y_logit, 1, keepdims=True)
        return y

    def Relu(self, z):
        return np.maximum(z, 0)

    def acc_test(self, input_data):
        tmp_h1 = self.Relu(input_data.dot(self.w1) + self.b1)
        tmp_h2 = self.Relu(self.h1_out.dot(self.w2) + self.b2)
        tmp_out = self.Softmax(self.h2_out.dot(self.w3) + self.b3)
        return tmp_out

    # Feed Placeholder

    def forward(self, input_data):

        self.input_data = input_data
        self.h1_out = self.Relu(input_data.dot(self.w1) + self.b1)
        self.h2_out = self.Relu(self.h1_out.dot(self.w2) + self.b2)
        self.output_layer = self.Softmax(self.h2_out.dot(self.w3) + self.b3)

    # Backward Propagation

    def backward(self, target):

        # corss_entropy loss derivative
        Loss_to_z_grad = (self.output_layer - target) # correct

        self.b3_grad = Loss_to_z_grad
        self.w3_grad = self.h2_out.T.dot(Loss_to_z_grad) # correct



        Activation_2grad = Loss_to_z_grad.dot(self.w3.T) # correct
        Activation_2_grad[Activation_2_grad&lt;0] = 0

        self.b2_grad = Activation_2grad
        self.w2_grad = self.h1_out.T.dot(Activation_2grad)

        
        Activation_1grad = Activation_2grad.dot(self.w2.T)
        Activation_1_grad[Activation_1_grad&lt;0] = 0     

        self.b1_grad = Activation_1grad
        self.w1_grad = self.input_data.T.dot(Activation_1grad)


    # Update Weights
    def update(self, learning_rate=1e-06):
        self.w1 = self.w1 - learning_rate * self.w1_grad
        self.b1 = self.b1 - learning_rate * self.b1_grad

        self.w2 = self.w2 - learning_rate * self.w2_grad
        self.b2 = self.b2 - learning_rate * self.b2_grad

        self.w3 = self.w3 - learning_rate * self.w3_grad
        self.b3 = self.b3 - learning_rate * self.b3_grad

    # Loss Functions
    def cross_entropy(Y, Y_prediction):
        return -(np.matmul(Y, np.log(Y_prediction)) + np.matmul((1-Y), np.log(1-Y_prediction)))

    def print_accuracy(self):
        correct = 0
        loss = 0
        for i in range(y_val.shape[0]):
            self.acc_test(x_val[i])
            index = self.output_layer
            one_hot = 0
            for check in range(y_val[i].shape[0]):
                if y_val[i][check] == 1:
                    one_hot = check
                    break
            if np.argmax(index) == one_hot:
                correct += 1
                # print('correct: ',check)
            # else:
                # print('incorrect: ', check)
        print('accuracy = ', correct/y_val.shape[0])

import random
 mnist_nn = NN(input_size = 784, hidden_1_size = 200, hidden_2_size = 200,output_size = 10)


 
for i in range(1000):
    for j in range(2000):
        index = random.randint(0,x_train.shape[0]-1)
        mnist_nn.forward(x_train[[index]])
        mnist_nn.backward(y_train[index])
        mnist_nn.update()
    print(i)
    mnist_nn.print_accuracy()

The accuracy is terribly low since it can only predict one number.
I've seen this article, Neural network always predicts the same class and I did change Relu to leaky Relu, but it doesn't really work.
I think my dataset should be ok cause I use the same dataset to train a DNN with pytorch, and it works. Also, the initial value of weights and bias are random values.
",1,2676,"I've had a quick look over your code and if I understand it correctly, then there may be some issues:

It seems like you want it to do multi-class classification with 10 classes, however I believe your cross entropy function looks like binary cross entropy, instead of general cross entropy. Also, you're using matrix multiplication, whereas I think you want to sum y * log(y_pred) over the 10 output probabilities and then take the mean across the batch, so you end up with a scalar valued loss.
When doing the ReLU gradient, you should clip where the actual activation is negative, not where the gradient is negative, I think. So Activation_2_grad[Activation_2_grad&lt;0] = 0 should be Activation_2_grad[self.h2_out &lt; 0] = 0.
The rest of the backprop looks okay.

",,
PyTorch strange result,https://stackoverflow.com/questions/72690448,How should I use num_layers in pytorch LSTM model?,"Hi I am new bee with RNN and pytorch. I am implementing an model to predict data. I first only use single layer and the result was fine. Now I want to improve the accurancy of the model and want to use 2 layers in LSTM. However, the output of LSTM is very strange to me.
I was expecting have a [1, 8] output. However, with num_layers=2 I get a [2, 8] result. What does this result mean? Which should I use as the result of LSTM?
Here is my code:
class LSTM(nn.Module):

    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):

        super(LSTM, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim,
                            num_layers=num_layers, batch_first=True)
        
        self.fc = nn.Linear(hidden_dim, output_dim)

    
    def forward(self, x):

        #x = Variable(torch.Tensor(x).cuda())
        
        h_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_dim).cuda())
        
        c_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_dim).cuda())
        
        
        # Propagate input through LSTM
        ula, (h_out, _) = self.lstm(x, (h_0, c_0))
         
        
        out = self.fc(h_out).cuda()
        
        return out

",0,696,"h_out has the shape of (hidden_size,numlayers). If you want to pass it to the output layer fc, you need to reshape it/flatten and also increase the size of the output layer to hidden_size*numlayers.
class LSTM(nn.Module):

    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):

        super(LSTM, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim,
                            num_layers=num_layers, batch_first=True)
        
        self.fc = nn.Linear(hidden_dim*num_layers, output_dim)

    
    def forward(self, x):

        #x = Variable(torch.Tensor(x).cuda())
        
        h_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_dim).cuda())
        
        c_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_dim).cuda())
        
        
        # Propagate input through LSTM
        ula, (h_out, _) = self.lstm(x, (h_0, c_0))
         
        out = h_out.transpose(0,1)
        out = out.reshape(-1,hidden_dim*num_layers)
        out = self.fc(out).cuda()
        
        return out

",,
PyTorch strange result,https://stackoverflow.com/questions/70970106,Cannot install python wheel file in conda without sudo,"I'm trying to build PyTorch from a .whl file on a jetson nano.
I am able to build and install the file but only while using sudo, attempting to pip install the file without sudo results in this error:
ERROR: torch-1.10.0a0+git36449ea-cp36-cp36m-linux_aarch64.whl is not a supported wheel on this platform.

This is strange as with admin I have no issues installing this file, but I can then only use the library by using the sudo command before going into or running with the python command.
I should note that this is in a conda environment, but even in the base conda environment this issue still occurs.
It seems like I can also install the package by using conda deactivate to deactivate conda.
I am using Python 3.7 in the conda environment and Python 3.6 outside.
",0,688,"
I am using Python 3.7 in the conda environment and Python 3.6 outside.

This is the issue. You have a cp36 whl file, so python 3.6. I am suspecting that when you run sudo pip, your systems pip is invoked, whereas when you run pip, then pip from your conda env is used, and cannot install a python 3.6 whl to a python 3.7 env.
Either you need to get the cp37 whl or create a conda env that has python 3.6 installed
",,
PyTorch strange result,https://stackoverflow.com/questions/69351441,Exponential moving covariance matrix,"I have time series data of a specific dimensionality (e.g. [T, 32]). I filter the data in an online fashion using an exponential moving average and variance (according to Wikipedia and this paper):
mean_n = alpha * mean_{n-1} + (1-alpha) * sample
var_n = alpha * (var_{n-1} + (1-alpha) * (sample - mean_{n-1}) * (sample - mean_{n-1}))

I wanted to replace the moving variance with a moving covariance matrix in order to capture the correlation between the data dimensions (e.g. 32). So I have simply replaced the element-wise variance calculation with an outer product:
covmat_n = alpha * (covmat_{n-1} + (1-alpha) * np.outer((sample - mean_{n-1}), (sample - mean_{n-1})))

However this does not seem to give correct results. For example, when I try to initialize a pytorch multivariate gaussian distribution with such a covariance matrix, it sometimes throws an error saying that the covariance matrix contains invalid values. Since it is always symmetric I assume it breaks the positive-definiteness constraint. Other downstream tasks suggested  the same (e.g. computing the KL-Divergence between two gaussians with such covariance matrices sometimes gave negative results).
Does anyone know what I am doing wrong here? Where is the error in my calculations?
And as a bonus question: Are my calculations for the simple moving variance correct? It seeems strange to multiply the new sample variance with alpha again, but the sources suggest that it is the correct way.
",0,471,"I have found the answer myself. It seemed to be a numerical problem. Since the eigenvalues of a positive definite matrix must be positive, I could solve it by applying an eigenvalue decomposition to every sample's covariance matrix and ensure that its eigenvalues are larger than zero:
diff = sample - last_mean
sample_covmat = np.outer(diff, diff)
w, v = np.linalg.eigh(sample_covmat)
w += 1e-3 # Note: Could also achieve this by e.g. w = np.maximum(w, 0)
sample_covmat = v @ np.diag(w) @ np.linalg.inv(v)

",,
PyTorch strange result,https://stackoverflow.com/questions/47461625,torch.addmm received an invalid combination of arguments,"In the official webpage of pytorch I saw the following code with answers: 

&gt;&gt; a = torch.randn(4, 4)
&gt;&gt; a

0.0692  0.3142  1.2513 -0.5428
0.9288  0.8552 -0.2073  0.6409
1.0695 -0.0101 -2.4507 -1.2230
0.7426 -0.7666  0.4862 -0.6628
torch.FloatTensor of size 4x4]

&gt;&gt;&gt; torch.max(a, 1)
(
 1.2513
 0.9288
 1.0695
 0.7426
[torch.FloatTensor of size 4]
,
 2
 0
 0
 0
[torch.LongTensor of size 4]
)


I know that the first result corresponds to the maximum number per row, however I do not get the second tensor (LongTensor)

I tried other random example and after a pytorch.max, I came to find these results 

0.9477  1.0090  0.8348 -1.3513
-0.4861  1.2581  0.3972  1.5751
-1.2277 -0.6201 -1.0553  0.6069
 0.1688  0.1373  0.6544 -0.7784
[torch.FloatTensor of size 4x4]

(
 1.0090
 1.5751
 0.6069
 0.6544
[torch.FloatTensor of size 4]
, 
 1
 3
 3
 2
[torch.LongTensor of size 4]
)


Can anyone tell me what does it really mean these LongTensor data? I thought it was a strange casting between tensors, however after a simple casting of a float tensor, I see that it just cuts decimals

Thanks
",0,217,"It just tells the index of the max element in your original tensor along the queried dimension.

E.g.

0.9477  1.0090  0.8348 -1.3513
-0.4861  1.2581  0.3972  1.5751
-1.2277 -0.6201 -1.0553  0.6069
 0.1688  0.1373  0.6544 -0.7784
[torch.FloatTensor of size 4x4]

# torch.max(a, 1)
(
 1.0090
 1.5751
 0.6069
 0.6544
[torch.FloatTensor of size 4]
, 
 1
 3
 3
 2
[torch.LongTensor of size 4]
)


In the above example in torch.LongTensor, 

1 is the index of 1.0090 in your original tensor (torch.FloatTensor) 
3 is the index of 1.5751 in your original tensor (torch.FloatTensor) 
3 is the index of 0.6069 in your original tensor (torch.FloatTensor) 
2 is the index of 0.6544 in your original tensor (torch.FloatTensor)

along dimension 1.



Instead, if you'd have requested torch.max(a, 0), the entries in torch.LongTensor would correspond to the indices of max elements in your original tensor along dimension 0.
",,
PyTorch strange issue,https://stackoverflow.com/questions/65107933,Pytorch model training CPU Memory leak issue,"When I trained my pytorch model on GPU device,my python script was killed out of blue.Dives into OS log files , and I find script was killed by OOM killer because my CPU ran out of memory.Its very strange that I trained my model on GPU device but I ran out of my CPU memory.
Snapshot of OOM killer log file

In order to  debug this issue,I install python memory profiler. Viewing log file from memory profiler, I find when column wise -= operation  occurred, my CPU memory gradually increased until OOM killer killed my
program.
Snapshot of Python memory profiler

Its very strange, I try many ways to solve this issue.Finally, I found before assignment operation,I detach Tensor first.Amazingly,it solves this issue.But I dont understand clearly why it works.Here is my original function code.
def GeneralizedNabla(self, image):
        pad_size = 2
        affinity = torch.zeros(image.shape[0], self.window_size**2, self.h, self.w).to(self.device)
        h = self.h+pad_size
        w = self.w+pad_size
        #pad = nn.ZeroPad2d(pad_size)
        image_pad = self.pad(image)
        for i in range(0, self.window_size**2):
            affinity[:, i, :, :] = image[:, :, :].detach()  # initialization
            dy = int(i/5)-2
            dx = int(i % 5)-2
            h_start = pad_size+dy
            h_end = h+dy  # if 0 &lt;= dy else h+dy
            w_start = pad_size+dx
            w_end = w+dx  # if 0 &lt;= dx else w+dx
            affinity[:, i, :, :] -= image_pad[:, h_start:h_end, w_start:w_end].detach()
        self.Nabla=affinity
        return

If everyone has any ideas,I will appreciate very much, thank you.
",6,7697,"Previously when you did not use the .detach() on your tensor, you were also accumulating the computation graph as well and as you went on, you kept acumulating more and more until you ended up exuasting your memory to the point it crashed.
When you do a detach(), you are effectively getting the data without the previously entangled history thats needed for computing the gradients.
",,
PyTorch strange issue,https://stackoverflow.com/questions/71146140,Using RNN Trained Model without pytorch installed,,3,938,"You should try to export the model using torch.onnx. The page gives you an example that you can start with.
An alternative is to use TorchScript, but that requires torch libraries.
Both of these can be run without python. You can load torchscript in a C++ application https://pytorch.org/tutorials/advanced/cpp_export.html
ONNX is much more portable and you can use in languages such as C#, Java, or Javascript
https://onnxruntime.ai/ (even on the browser)
A running example
Just modifying a little your example to go over the errors I found
Notice that via tracing any if/elif/else, for, while will be unrolled
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import random

torch.manual_seed(1)
random.seed(1)
device = torch.device('cpu')

class RNN(nn.Module):
  def __init__(self, input_size, hidden_size, output_size,num_layers, matching_in_out=False, batch_size=1):
    super(RNN, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.num_layers = num_layers
    self.batch_size = batch_size
    self.matching_in_out = matching_in_out #length of input vector matches the length of output vector 
    self.lstm = nn.LSTM(input_size, hidden_size,num_layers)
    self.hidden2out = nn.Linear(hidden_size, output_size)
  def forward(self, x, h0, c0):
    lstm_out, (hidden_a, hidden_b) = self.lstm(x, (h0, c0))
    outs=self.hidden2out(lstm_out)
    return outs, (hidden_a, hidden_b)
  def init_hidden(self):
    #return torch.rand(self.num_layers, self.batch_size, self.hidden_size)
    return (torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device).detach(),
            torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device).detach())

# convert the arguments passed during onnx.export call
class MWrapper(nn.Module):
    def __init__(self, model):
        super(MWrapper, self).__init__()
        self.model = model;
    def forward(self, kwargs):
        return self.model(**kwargs)

Run an example
rnn = RNN(10, 10, 10, 3)
X = torch.randn(3,1,10)
h0,c0  = rnn.init_hidden()
print(rnn(X, h0, c0)[0])

Use the same input to trace the model and export an onnx file

torch.onnx.export(MWrapper(rnn), {'x':X,'h0':h0,'c0':c0}, 'rnn.onnx', 
                  dynamic_axes={'x':{1:'N'},
                               'c0':{1: 'N'},
                               'h0':{1: 'N'}
                               },
                  input_names=['x', 'h0', 'c0'],
                  output_names=['y', 'hn', 'cn']
                 )

Notice that you can use symbolic values for the dimensions of some axes of some inputs. Unspecified dimensions will be fixed with the values from the traced inputs. By default LSTM uses dimension 1 as batch.
Next we load the ONNX model and pass the same inputs
import onnxruntime
ort_model = onnxruntime.InferenceSession('rnn.onnx')
print(ort_model.run(['y'], {'x':X.numpy(), 'c0':c0.numpy(), 'h0':h0.numpy()}))

","Basically implementing it in numpy and copying weights from your pytorch model can do the trick.  For your usecase you will only need to do a forward pass so we just need to implement that only
#Set Parameters for a small LSTM network
input_size  = 2 # size of one 'event', or sample, in our batch of data
hidden_dim  = 3 # 3 cells in the LSTM layer
output_size = 1 # desired model output

num_layers=3
torch_lstm = RNN( input_size, 
                 hidden_dim ,
                 output_size,
                 num_layers,
                 matching_in_out=True
                 )

state = torch_lstm.state_dict() # state will capture the weights of your model

Now for LSTM in numpy these functions will be used:
got the below code from this link: https://towardsdatascience.com/the-lstm-reference-card-6163ca98ae87
### NOT MY CODE
import numpy as np 
from scipy.special import expit as sigmoid

def forget_gate(x, h, Weights_hf, Bias_hf, Weights_xf, Bias_xf, prev_cell_state):
    forget_hidden  = np.dot(Weights_hf, h) + Bias_hf
    forget_eventx  = np.dot(Weights_xf, x) + Bias_xf
    return np.multiply( sigmoid(forget_hidden + forget_eventx), prev_cell_state )

def input_gate(x, h, Weights_hi, Bias_hi, Weights_xi, Bias_xi, Weights_hl, Bias_hl, Weights_xl, Bias_xl):
    ignore_hidden  = np.dot(Weights_hi, h) + Bias_hi
    ignore_eventx  = np.dot(Weights_xi, x) + Bias_xi
    learn_hidden   = np.dot(Weights_hl, h) + Bias_hl
    learn_eventx   = np.dot(Weights_xl, x) + Bias_xl
    return np.multiply( sigmoid(ignore_eventx + ignore_hidden), np.tanh(learn_eventx + learn_hidden) )


def cell_state(forget_gate_output, input_gate_output):
    return forget_gate_output + input_gate_output

  
def output_gate(x, h, Weights_ho, Bias_ho, Weights_xo, Bias_xo, cell_state):
    out_hidden = np.dot(Weights_ho, h) + Bias_ho
    out_eventx = np.dot(Weights_xo, x) + Bias_xo
    return np.multiply( sigmoid(out_eventx + out_hidden), np.tanh(cell_state) )


We would need the sigmoid function as well so
def sigmoid(x):
    return 1/(1 + np.exp(-x))

Because pytorch stores weights in stacked manner so we need to break it up for that we would need the below function
def get_slices(hidden_dim):
    slices=[]
    breaker=(hidden_dim*4)
    slices=[[i,i+3] for i in range(0, breaker, breaker//4)]
    return slices

Now we have the functions ready for lstm, now we create an lstm class to copy the weights from pytorch class and get the output from it.
class numpy_lstm:
    def __init__( self, layer_num=0, hidden_dim=1, matching_in_out=False):
        self.matching_in_out=matching_in_out
        self.layer_num=layer_num
        self.hidden_dim=hidden_dim
        
    def init_weights_from_pytorch(self, state):
        slices=get_slices(self.hidden_dim)
        print (slices)

        #Event (x) Weights and Biases for all gates
        
        lstm_weight_ih='lstm.weight_ih_l'+str(self.layer_num)
        self.Weights_xi = state[lstm_weight_ih][slices[0][0]:slices[0][1]].numpy()  # shape  [h, x]
        self.Weights_xf = state[lstm_weight_ih][slices[1][0]:slices[1][1]].numpy()  # shape  [h, x]
        self.Weights_xl = state[lstm_weight_ih][slices[2][0]:slices[2][1]].numpy()  # shape  [h, x]
        self.Weights_xo = state[lstm_weight_ih][slices[3][0]:slices[3][1]].numpy() # shape  [h, x]

        
        lstm_bias_ih='lstm.bias_ih_l'+str(self.layer_num)
        self.Bias_xi = state[lstm_bias_ih][slices[0][0]:slices[0][1]].numpy()  #shape is [h, 1]
        self.Bias_xf = state[lstm_bias_ih][slices[1][0]:slices[1][1]].numpy()  #shape is [h, 1]
        self.Bias_xl = state[lstm_bias_ih][slices[2][0]:slices[2][1]].numpy()  #shape is [h, 1]
        self.Bias_xo = state[lstm_bias_ih][slices[3][0]:slices[3][1]].numpy() #shape is [h, 1]
        
        
        lstm_weight_hh='lstm.weight_hh_l'+str(self.layer_num)

        #Hidden state (h) Weights and Biases for all gates
        self.Weights_hi = state[lstm_weight_hh][slices[0][0]:slices[0][1]].numpy()  #shape is [h, h]
        self.Weights_hf = state[lstm_weight_hh][slices[1][0]:slices[1][1]].numpy()  #shape is [h, h]
        self.Weights_hl = state[lstm_weight_hh][slices[2][0]:slices[2][1]].numpy()  #shape is [h, h]
        self.Weights_ho = state[lstm_weight_hh][slices[3][0]:slices[3][1]].numpy() #shape is [h, h]
        
        
        lstm_bias_hh='lstm.bias_hh_l'+str(self.layer_num)

        self.Bias_hi = state[lstm_bias_hh][slices[0][0]:slices[0][1]].numpy()  #shape is [h, 1]
        self.Bias_hf = state[lstm_bias_hh][slices[1][0]:slices[1][1]].numpy()  #shape is [h, 1]
        self.Bias_hl = state[lstm_bias_hh][slices[2][0]:slices[2][1]].numpy()  #shape is [h, 1]
        self.Bias_ho = state[lstm_bias_hh][slices[3][0]:slices[3][1]].numpy() #shape is [h, 1]
    def forward_lstm_pass(self,input_data):
        h = np.zeros(self.hidden_dim)
        c = np.zeros(self.hidden_dim)
        
        output_list=[]
        for eventx in input_data:
            f = forget_gate(eventx, h, self.Weights_hf, self.Bias_hf, self.Weights_xf, self.Bias_xf, c)
            i =  input_gate(eventx, h, self.Weights_hi, self.Bias_hi, self.Weights_xi, self.Bias_xi, 
                        self.Weights_hl, self.Bias_hl, self.Weights_xl, self.Bias_xl)
            c = cell_state(f,i)
            h = output_gate(eventx, h, self.Weights_ho, self.Bias_ho, self.Weights_xo, self.Bias_xo, c)
            if self.matching_in_out: # doesnt make sense but it was as it was in main code :(
                output_list.append(h)
        if self.matching_in_out:
            return output_list
        else:
            return h


Similarly for fully connected layer,
    
    
class fully_connected_layer:
    def __init__(self,state, dict_name='fc', ):
        self.fc_Weight = state[dict_name+'.weight'][0].numpy()
        self.fc_Bias = state[dict_name+'.bias'][0].numpy() #shape is [,output_size]
        
    def forward(self,lstm_output, is_sigmoid=True):
        res=np.dot(self.fc_Weight, lstm_output)+self.fc_Bias
        print (res)
        if is_sigmoid:
            return sigmoid(res)
        else:
            return res
        

Now we would need one class to call all of them together and generalise them with respect to multiple layers
You can modify the below class if you need more Fully connected layers or want to set false condition for sigmoid etc.
        
class RNN_model_Numpy:
    def __init__(self, state, input_size, hidden_dim, output_size, num_layers, matching_in_out=True):
        self.lstm_layers=[]
        for i in range(0, num_layers):
            lstm_layer_obj=numpy_lstm(layer_num=i, hidden_dim=hidden_dim, matching_in_out=True)
            lstm_layer_obj.init_weights_from_pytorch(state) 
            self.lstm_layers.append(lstm_layer_obj)
        
        self.hidden2out=fully_connected_layer(state, dict_name='hidden2out')
        
    def forward(self, feature_list):
        for x in self.lstm_layers:
            lstm_output=x.forward_lstm_pass(feature_list)
            feature_list=lstm_output
            
        return self.hidden2out.forward(feature_list, is_sigmoid=False)

Sanity check on a numpy variable:
data = np.array(
           [[1,1],
            [2,2],
            [3,3]])



check=RNN_model_Numpy(state, input_size, hidden_dim, output_size, num_layers)
check.forward(data)

EXPLANATION:
Since we just need forward pass, we would need certain functions that are required in LSTM, for that we have the forget gate, input gate, cell gate and output gate. They are just some operations that are done on the input that you give.
For get_slices function, this is used to break down the weight matrix that we get from pytorch state dictionary (state dictionary) is the dictionary which contains the weights of all the layers that we have in our network.
For LSTM particularly have it in this order  ignore, forget, learn, output. So for that we would need to break it up for different LSTM cells.
For numpy_lstm class, we have init_weights_from_pytorch function which must be called, what it will do is that it will extract the weights from state dictionary which we got earlier from pytorch model object and then populate the numpy array weights with the pytorch weights. You can first train your model and then save the state dictionary through pickle and then use it.
The fully connected layer class just implements the hidden2out neural network.
Finally our rnn_model_numpy class is there to ensure that if you have multiple layers then it is able to send the output of one layer of lstm to other layer of lstm.
Lastly there is a small sanity check on data variable.
IMPORTANT NOTE: PLEASE NOTE THAT YOU MIGHT GET DIMENSION ERROR AS PYTORCH WAY OF HANDLING INPUT IS COMPLETELY DIFFERENT SO PLEASE ENSURE THAT YOU INPUT NUMPY IS OF SIMILAR SHAPE AS DATA VARIABLE.
Important references:
https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
https://christinakouridi.blog/2019/06/19/backpropagation-lstm/
",
PyTorch strange issue,https://stackoverflow.com/questions/73629682,Error Running Stable Diffusion from the command line in Windows,,2,20462,"I had the same issue, it's because you're using a non-optimized version of Stable-Diffusion. You have to download basujindal's branch of it, which allows it use much less ram by sacrificing the precision, this is the branch - https://github.com/basujindal/stable-diffusion
Everything else in that guide stays the same just clone from this version. It allow you to even push past 512x512 default resolution, you can use 756x512 to get rectangular images for example (but the results may vary since it was trained on a 512 square set).
the new prompt becomes python optimizedSD/optimized_txt2img.py --prompt ""blue orange"" --H 756 --W 512
Also another note: as of a few days ago an even faster and more optimized version was released by neonsecret (https://github.com/basujindal/stable-diffusion), however I'm having issues installing it, so can't really recommend it but you can try it as well and see if it works for you.
","In addition to the optimized version by basujindal, the additional tags following the prompt allows the model to run properly on a machine with NVIDIA or AMD 8+GB GPU.
So the new prompt would look like this
&gt;&gt; python optimizedSD/optimized_txt2img.py --prompt ""a close-up portrait of a cat by pablo picasso, vivid, abstract art, colorful, vibrant"" --H 512 --W 512 --seed 27 --n_iter 2 --n_samples 10 --ddim_steps 50
",
PyTorch strange issue,https://stackoverflow.com/questions/63549826,RuntimeError: CUDA out of memory. Problem with stanza lemmatazation using too much GPU memory,,2,1215,"If you check the full stack trace, there might be a hint which processor runs into the memory issue. For instance, I recently ran into a similar issue with this stack trace:
...
File ""stanza/pipeline/depparse_processor.py"", line 42, in process     
preds += self.trainer.predict(b)   
File ""stanza/models/depparse/trainer.py"", line 74, in predict     
_, preds = self.model(word, word_mask, wordchars,
wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel,
word_orig_idx, sentlens, wordlens)   
... 
RuntimeError: CUDA out of memory.
Tried to allocate 14.87 GiB (GPU 0; 14.76 GiB total capacity; 460.31 MiB already
allocated; 13.39 GiB free; 490.00 MiB reserved in total by PyTorch)

Which pointed me to the fact that I needed to set depparse_batch_size when calling stanza.Pipeline(...). There are other settings like batch_size and lemma_batch_size that you mentioned, as well as pos_batch_size and ner_batch_size etc. These should really help resolve this issue.
",,
PyTorch strange issue,https://stackoverflow.com/questions/73415068,"HuggingFace Trainer() does nothing - only on Vertex AI workbench, works on colab",,2,436,"We faced exactly the same issue on Vertex AI, when we selected the pre-built PyTorch=1.13 environment. The training would just freeze and do nothing for hours.
What worked for us was to select an older version of the PyTorch environment. So we went for the PyTorch=1.9 environment and the training worked just fine.

We could see the logs already as the training commenced. So my guess is that there's something specifically about the newer PyTorch environment on Vertex AI that prevents things from working as expected.
",,
PyTorch strange issue,https://stackoverflow.com/questions/67870887,Trouble understanding behaviour of modified VGG16 forward method (Pytorch),"I have modified VGG16 in pytorch to insert things like BN and dropout within the feature extractor. By chance I now noticed something strange when I changed the definition of the forward method from:
def forward(self, x):
    x = self.model(x)
    return x

to:
def forward(self, x):
    x = self.model.features(x)
    x = self.model.avgpool(x)
    x = self.model.classifier(x)
    return x

In the second method I am now getting an error that the sizes of the matrices don't match
(mat1 dim 1 must match mat2 dim 0)
Below is the entire code For the editted version of VGG that I've been using.
class Vgg(nn.Module):
    
    def __init__(self, n_classes, bias= None, dropout = 0.3):
        
        super().__init__()
        
        self.model = models.vgg16()
        #self.bn64 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        #self.bn128 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        #self.bn256 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        #self.bn512 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        
        # change to allow 4 channels input
        self.model.features[0] = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))

        # remove/edit some of the first layers to make it more similar to Resnet
        del self.model.features[2]
        del self.model.features[2]
        del self.model.features[-1]
        self.model.features[2] = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        
        # add dropout
        for m in self.model.modules():
            if isinstance(m, nn.Dropout):
                m.p = dropout
            else:
                pass
            
        self.dropout = nn.Dropout(p=dropout)
        self.r = nn.ReLU(inplace=True)
        
        modules = nn.Sequential(*[self.model.features[0],
                                  #self.bn64,
                                  self.model.features[1:3],
                                  #self.bn64,
                                  self.model.features[3:5],
                                  #self.dropout, 
                                  
                                  self.model.features[5],
                                  #self.bn128,
                                  self.model.features[6:8],
                                  #self.bn128,
                                  self.model.features[8:10],
                                  #self.dropout, 
                                  
                                  self.model.features[10],
                                  #self.bn256,
                                  self.model.features[11:13],
                                  #self.bn256,
                                  self.model.features[13:15],
                                  #self.bn256,
                                  self.model.features[15:17],
                                  #self.dropout,
                                  
                                  self.model.features[17],
                                  #self.bn512,
                                  self.model.features[18:20],
                                  #self.bn512,
                                  self.model.features[20:22],
                                  #self.bn512,
                                  self.model.features[22:24],
                                  #self.dropout,
                                  
                                  self.model.features[24],
                                  #self.bn512,
                                  self.model.features[25:27],
                                  #self.bn512,
                                  self.model.features[27:29],
                                  #self.bn512,
                                  #self.dropout
                                  nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                                 ])
        
        
        self.model.features = modules
        
        # change the pooling layer
        self.model.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))
        
        # set output to correct num classes
        self.model.classifier = nn.Linear(in_features=512, out_features=n_classes, bias=True)
        
        # use predefined bias
        if bias is not None:
            assert isinstance(bias, torch.Tensor), 'bias must be tensor'
            self.model.classifier.bias = nn.Parameter(bias)
        
    def forward(self, x):
        x = self.model.features(x)
        x = self.model.avgpool(x)
        x = self.model.classifier(x)
        return x

I know it is very ugly and hacky looking. I have tried to re-write it but the re-written version does not work either for some reason, and I am assuming that this current issue is related to that as well. I think that the input is not being fed through the forward method the way I think it is. My assumption is that calling x = self.model(x) does not run 'x' through all of the editted layers I have made, otherwise I would get the same behaviour with the two version of the forward method above. But my question is then, what is happening when I call self.model(x) in forward? Is it running the input through the original vgg16 from pytorch? Because when I print self.model in my console it shows the changes I made to the architecture of self.model.features as well as self.model.avgpool and self.model.classifier.
Edit:
Below is the entire trace of the error. Some extra information. t is a class I made to deal with the training steps (so looping through training and validation modes, ect)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-7-0b3983ae9702&gt; in &lt;module&gt;
     77                   print_cl_met = True
     78                   )
---&gt; 79 model = t.run()
     80 t.save_to_json()
     81 print(np.max(np.array(t.f1_tracker)))

/home/stevea/treesat/TreeSat/TreeSat/trainers/basetrainer.py in run(self)
    342             for phase in ['training', 'testing']:
    343                 self.phase = phase
--&gt; 344                 self.model_mode()
    345 
    346                 if self.phase == 'testing':

/home/stevea/treesat/TreeSat/TreeSat/trainers/basetrainer.py in model_mode(self)
    154         if self.phase == 'training':
    155             print('*********TRAINING PHASE*********')
--&gt; 156             self.trainModel()
    157         else:
    158             print('*********VALIDATION PHASE*********')

/home/stevea/treesat/TreeSat/TreeSat/trainers/basetrainer.py in trainModel(self)
    234         # loop through all batches to perform an epoch
    235         for loaded in self.loaders[self.phase]:
--&gt; 236             epoch_loss = self.train_step(loaded, epoch_loss)
    237 
    238         mean_loss = np.mean(np.array(epoch_loss))

/home/stevea/treesat/TreeSat/TreeSat/trainers/basetrainer.py in train_step(self, loaded, epoch_loss)
    262 
    263         # process batch through network
--&gt; 264         self.out = self.model(self.img_batch.float())
    265 
    266         # get loss value

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    887             result = self._slow_forward(*input, **kwargs)
    888         else:
--&gt; 889             result = self.forward(*input, **kwargs)
    890         for hook in itertools.chain(
    891                 _global_forward_hooks.values(),

/home/stevea/treesat/TreeSat/TreeSat/models/vgg.py in forward(self, x)
    143         x = self.model.features(x)
    144         x = self.model.avgpool(x)
--&gt; 145         x = self.model.classifier(x)
    146         return x
    147 

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    887             result = self._slow_forward(*input, **kwargs)
    888         else:
--&gt; 889             result = self.forward(*input, **kwargs)
    890         for hook in itertools.chain(
    891                 _global_forward_hooks.values(),

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py in forward(self, input)
     92 
     93     def forward(self, input: Tensor) -&gt; Tensor:
---&gt; 94         return F.linear(input, self.weight, self.bias)
     95 
     96     def extra_repr(self) -&gt; str:

/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in linear(input, weight, bias)
   1751     if has_torch_function_variadic(input, weight):
   1752         return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
-&gt; 1753     return torch._C._nn.linear(input, weight, bias)
   1754 
   1755 

RuntimeError: mat1 dim 1 must match mat2 dim 0

I also tried to print off the shape of x at each step of the forward method:
def forward(self, x):
        x = self.model.features(x)
        print(x.shape)
        x = self.model.avgpool(x)
        print(x.shape)
        x = self.model.classifier(x)
        print(x.shape)
        return x

And it shows me that the shapes seem to be fine as the classifier should be taking in 512 features:
torch.Size([64, 512, 4, 4])
torch.Size([64, 512, 1, 1])

",2,397,"I can't run your code, but I believe the issue is because linear layers expect 2d data input (as it is really a matrix multiplication), while you provide 4d input (with dims 2 and 3 of size 1).
Please try squeeze
def forward(self, x):
        x = self.model.features(x)
        x = self.model.avgpool(x)
        x = torch.squeeze(x)
        x = self.model.classifier(x)
        return x


For less hacky-looking code in the squeeze part, see torch einops.
",,
PyTorch strange issue,https://stackoverflow.com/questions/66817006,PyTorch CPU memory leak but only when running on a specific machine,"I'm running a model and I've noticed that the RAM usage slowly increases during the training of the model. It's around 200mb-400mb per epoch, but over time it fills up all the RAM on my machine which eventually leads the OS to kill the job. However, the strange thing about this is it's only when running on a specific machine. I've run the same code on a different machine and there's no memory leak whatsoever.
The difference between the two machines is one is running PyTorch 1.7.1 with cuda 10.2 (the machine without the memory leak) and the other machine (the one with the memory leak) is running PyTorch 1.8.1 Both are run with conda and only on the CPU. I have tried using older versions of PyTorch on the machine with the memory leak, but the memory leak still exists so I doubt it's due to a PyTorch version.
I've been using psutil to monitor the RAM usage on the CPU and I've been using the tracemalloc package to print out snapshots during the training loop to see how the memory usage changes from one epoch to the next. Yet when printing out these differences there isn't any within my code that matches anywhere near the 200mb-400mb RAM increase...
An example of this would something like...
Epoch     25/  1000 Loss 1428.8508 RAM:  9.910GB
~/Model.py:46: size=1136 B (-96 B), count=2 (+0), average=568 B
~/utils.py:14: size=1040 B (+24 B), count=8 (+1), average=130 B
~/calc_loss.py:79: size=2128 B (+0 B), count=24 (+0), average=89 B
~/calc_loss.py:78: size=2056 B (+0 B), count=23 (+0), average=89 B
~/utils.py:6: size=1920 B (+0 B), count=21 (+0), average=91 B
Epoch     26/  1000 Loss 1426.0033 RAM: 10.254GB
~/Model.py:46: size=1232 B (+96 B), count=2 (+0), average=616 B
~/utils.py:14: size=1016 B (-24 B), count=7 (-1), average=145 B
~/calc_loss.py:79: size=2128 B (+0 B), count=24 (+0), average=89 B
~/calc_loss.py:78: size=2056 B (+0 B), count=23 (+0), average=89 B
~/utils.py:6: size=1920 B (+0 B), count=21 (+0), average=91 B
~/Layers.py:71: size=992 B (+0 B), count=11 (+0), average=90 B
Epoch     27/  1000 Loss 1436.8241 RAM: 10.606GB
~/utils.py:14: size=1040 B (+24 B), count=8 (+1), average=130 B
~/calc_loss.py:79: size=2128 B (+0 B), count=24 (+0), average=89 B
~/calc_loss.py:78: size=2056 B (+0 B), count=23 (+0), average=89 B
~/utils.py:6: size=1920 B (+0 B), count=21 (+0), average=91 B
~/Model.py:46: size=1232 B (+0 B), count=2 (+0), average=616 B
~/Layers.py:71: size=992 B (+0 B), count=11 (+0), average=90 B
Epoch     28/  1000 Loss 1428.6560 RAM: 10.968GB
~/calc_loss.py:79: size=2128 B (+0 B), count=24 (+0), average=89 B
~/calc_loss.py:78: size=2056 B (+0 B), count=23 (+0), average=89 B
~/utils.py:6: size=1920 B (+0 B), count=21 (+0), average=91 B
~/Model.py:46: size=1232 B (+0 B), count=2 (+0), average=616 B
~/utils.py:14: size=1040 B (+0 B), count=8 (+0), average=130 B
Epoch     29/  1000 Loss 1435.2988 RAM: 11.321GB
~/calc_loss.py:79: size=2128 B (+0 B), count=24 (+0), average=89 B
~/calc_loss.py:78: size=2056 B (+0 B), count=23 (+0), average=89 B
~/utils.py:6: size=1920 B (+0 B), count=21 (+0), average=91 B
~/Model.py:46: size=1232 B (+0 B), count=2 (+0), average=616 B
~/utils.py:14: size=1040 B (+0 B), count=8 (+0), average=130 B
~/Layers.py:71: size=992 B (+0 B), count=11 (+0), average=90 B

The information printed out in between the lines showing the current Epoch are created via the use of this function
def compare_snaps(snap1, snap2, limit=50):
  top_stats=snap1.compare_to(snap2, ""lineno"")
  
  for stat in top_stats[:limit]:
    line=str(stat)
    if(""~/"" in line): #filter only lines from my own code
      print(line)

this function takes two snapshots from  tracemalloc.take_snapshot() from the current epoch and the previous epoch and compares how the memory usage changes. It takes the top 50 memory intensive operations, and filters only the ones that I've written (i.e. it excludes any changes from within anaconda3) and prints these changes to the screen. As can be seen, the changes in memory are negligible. In fact, when comparing the snap shotoutput from both machines, they're near identical.
It seems really weird that PyTorch code would have a memory leak on one machine and not on another... Could this perhaps be a conda environemnt issue? I have tried running this from a pip install and the leak still persists. I understand that this a bit vague (as there's no source code) but are there any ways to check where the leaks coming from? Things like Valgrind comes to mind for compile code but not much for interpretative code. This memory leak issue seems to be outside of my skill-set, so any help would be appreciated.
Thank you! :)
",1,1980,"Try incorporating this in your process:
import gc

# add this after computing one complete operation
gc.collect()

",,
PyTorch strange issue,https://stackoverflow.com/questions/64601301,Pytorch input tensor size with wrong dimension Conv1D," def train(epoch):
      model.train()
      train_loss = 0

  for batch_idx, (data, _) in enumerate(train_loader):
    data = data[None, :, :]
    print(data.size())    # something seems to change between here

    data = data.to(device)
    optimizer.zero_grad()
    recon_batch, mu, logvar = model(data) # and here???

    loss = loss_function(recon_batch, data, mu, logvar)
    loss.backward()
    train_loss += loss.item()

    optimizer.step()

    if batch_idx % 1000 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader),
                loss.item() / len(data)))


  print('====&gt; Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))

for epoch in range(1, 4):
        train(epoch)

This is very strange looking at the training loop it does recognize that the size is [1,1,1998] but then something changes after it is sent to the device?
    torch.Size([1, 1, 1998])
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-138-70cca679f91a&gt; in &lt;module&gt;()
     27 
     28 for epoch in range(1, 4):
---&gt; 29         train(epoch)

5 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py in forward(self, input)
    255                             _single(0), self.dilation, self.groups)
    256         return F.conv1d(input, self.weight, self.bias, self.stride,
--&gt; 257                         self.padding, self.dilation, self.groups)
    258 
    259 

RuntimeError: Expected 3-dimensional input for 3-dimensional weight [12, 1, 1], but got 2-dimensional input of size [1, 1998] instead

Also here is my model (I recognize there is likely a couple of other issues here but I am asking about the tensor size not registering)
class VAE(nn.Module):
  def __init__(self):
    super(VAE, self).__init__()

    self.conv1 = nn.Conv1d( 1,12, kernel_size=1,stride=5,padding=0)
    self.conv1_drop = nn.Dropout2d()
    self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2)

    self.fc21 = nn.Linear(198, 1)
    self.fc22 = nn.Linear(198, 1)

    self.fc3 = nn.Linear(1, 198)
    self.fc4 = nn.Linear(198, 1998)

  def encode(self, x):
    h1 = self.conv1(x)
    h1 = self.conv1_drop(h1)
    h1 = self.pool1(h1)
    h1 = F.relu(h1)
    h1 = h1.view(1, -1) # 1 is the batch size
    return self.fc21(h1), self.fc22(h1)
  
  def reparameterize(self, mu, logvar):
    std = torch.exp(0.5*logvar)
    eps = torch.rand_like(std)
    return mu + eps*std 
  
  def decode(self, z):
    h3 = F.relu(self.fc3(z))
    return torch.sigmoid(self.fc4(h3))
  
  def forward(self, x):
    mu, logvar = self.encode(x.view(-1, 1998))
    z = self.reparameterize(mu, logvar)
    return self.decode(z), mu, logvar

So why doesn't Pytorch keep the dimensions after reshaping and would that be the correct tensor size if it did?
",1,327,"I just found my mistake when I call forward() I am doing self.encode(x.view(-1,1998)) which is reshaping the tensor.
",,
PyTorch strange issue,https://stackoverflow.com/questions/55558978,PyTorch will not fit straight line to two data points,"I'm facing issues in fitting a simple y= 4x1 line with 2 data points using pytorch. While running the inference code, the model seems to output same value to any input which is strange. Pls find the code attached along with the data files used by me. Appreciate any help here.

import torch
import numpy as np
import pandas as pd

df = pd.read_csv('data.csv')
test_data = pd.read_csv('test_data.csv')

inputs = df[['x1']]
target = df['y']
inputs = torch.tensor(inputs.values).float()
target = torch.tensor(target.values).float()

test_data = torch.tensor(test_data.values).float()
#Defining Network Architecture
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):

  def __init__(self):
    super(Net,self).__init__()

    hidden1 = 3
#     hidden2 = 5 

    self.fc1 = nn.Linear(1,hidden1)
    self.fc3 = nn.Linear(hidden1,1)


  def forward(self,x):
    x = F.relu(self.fc1(x))
    x = self.fc3(x)
    return x

#instantiate the model

model = Net()
print(model)

criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(),lr=0.01)

model.train()

#epochs
epochs = 100


for x in range(epochs):
  #initialize the training loss to 0
  train_loss = 0
  #clear out gradients
  optimizer.zero_grad() 

  #calculate the output
  output = model(inputs)

  #calculate loss
  loss = criterion(output,target)

  #backpropagate
  loss.backward() 

  #update parameters
  optimizer.step()

  if ((x%5)==0):
    print('Training Loss after epoch {:2d} is {:2.6f}'.format(x,loss))

#set the model in evaluation mode
model.eval()

#Test the model on unseen data

test_output = model(test_data)

print(test_output)


Below is the model output

#model output
tensor([[56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579],
        [56.7579]], grad_fn=&lt;AddmmBackward&gt;)

",1,369,"Your model is collapsing. You can probably see that based on the prints. You may want to use a lower learning rate (1e-5, 1e-6, etc.). Switching from SGD(...)to Adam(...) may be easier if you do not have experience and want less trouble fine-tuning these hparams. Also, maybe 100 epochs is not enough. As you did not share an MCVE, I cannot tell you for sure what it is. Here is an MCVE of linefitting using the same Net you used:

import torch
import numpy as np
import torch.nn as nn
import torch.nn.functional as F

epochs = 1000
max_range = 40
interval = 4

# DATA
x_train = torch.arange(0, max_range, interval).view(-1, 1).float()
x_train += torch.rand(x_train.size(0), 1) - 0.5  # small noise
y_train = (4 * x_train) 
y_train += torch.rand(x_train.size(0), 1) - 0.5  # small noise

x_test  = torch.arange(interval // 2, max_range, interval).view(-1, 1).float()
y_test  = 4 * x_test

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    hidden1 = 3
    self.fc1 = nn.Linear(1, hidden1)
    self.fc3 = nn.Linear(hidden1, 1)

  def forward(self, x):
    x = F.relu(self.fc1(x))
    x = self.fc3(x)
    return x

model = Net()
print(model)

criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)

# TRAIN
model.train()
for epoch in range(epochs):
  optimizer.zero_grad()
  y_pred = model(x_train)
  loss = criterion(y_pred, y_train)
  loss.backward()
  optimizer.step()

  if epoch % 10 == 0:
    print('Training Loss after epoch {:2d} is {:2.6f}'.format(epoch, loss))

# TEST
model.eval()
y_pred = model(x_test)
print(torch.cat((x_test, y_pred, y_test), dim=-1))


This is what the data looks like:



And this is what the training looks like:

Training Loss after epoch  0 is 7416.805664
Training Loss after epoch 10 is 6645.655273
Training Loss after epoch 20 is 5792.936523
Training Loss after epoch 30 is 4700.106445
Training Loss after epoch 40 is 3245.384277
Training Loss after epoch 50 is 1779.370728
Training Loss after epoch 60 is 747.418579
Training Loss after epoch 70 is 246.781311
Training Loss after epoch 80 is 68.635155
Training Loss after epoch 90 is 17.332235
Training Loss after epoch 100 is 4.280161
Training Loss after epoch 110 is 1.170808
Training Loss after epoch 120 is 0.453974
...
Training Loss after epoch 970 is 0.232296
Training Loss after epoch 980 is 0.232090
Training Loss after epoch 990 is 0.231888


And this is what the output looks like:

|  x_test |  y_pred  |  y_test  |
|:-------:|:--------:|:--------:|
|  2.0000 |   8.6135 |   8.0000 |
|  6.0000 |  24.5276 |  24.0000 |
| 10.0000 |  40.4418 |  40.0000 |
| 14.0000 |  56.3303 |  56.0000 |
| 18.0000 |  72.1884 |  72.0000 |
| 22.0000 |  88.0465 |  88.0000 |
| 26.0000 | 103.9047 | 104.0000 |
| 30.0000 | 119.7628 | 120.0000 |
| 34.0000 | 135.6210 | 136.0000 |
| 38.0000 | 151.4791 | 152.0000 |

",,
PyTorch strange issue,https://stackoverflow.com/questions/70970106,Cannot install python wheel file in conda without sudo,,0,688,"
I am using Python 3.7 in the conda environment and Python 3.6 outside.

This is the issue. You have a cp36 whl file, so python 3.6. I am suspecting that when you run sudo pip, your systems pip is invoked, whereas when you run pip, then pip from your conda env is used, and cannot install a python 3.6 whl to a python 3.7 env.
Either you need to get the cp37 whl or create a conda env that has python 3.6 installed
",,
PyTorch strange issue,https://stackoverflow.com/questions/72223776,Google Colab GPU RAM depletes quickly on test data but not so on training data,"I am training my neural network built with PyTorch under Google Colab Pro+ (Tesla P100-PCIE GPU) but encounters the following strange phenomenon:

The amount of free memory (Mem Free) stays nearly constant when I train the model on training data, so there is no GPU memory issue during training (even over multiple epochs, where I use torch.cuda.empty_cache() at the beginning of each epoch).

However, Mem Free quickly depletes as soon as I deploy the model on test data after each training epoch. Everything except the input data to the network stays the same during the process.


To be more precise, the code during which Mem Free depletes looks like
self.batch_training(train=True)  # no problem
self.batch_training(train=False) # issue occurs,

where self is a class object which stores the model, data, and the training method.
The function batch_training looks like
def batch_training(self, train=True):
    if train:
       X, Y = self.X_train, self.Y_train
    else:
       X, Y = self.X_test, self.Y_test
    ...
    (make prediction and does gradient descent if train == True, otherwise, just compute customized losses)

Below is a screenshot that illustrates the situation (just ignores the training loss_g and in each batch), where I tried the code on a subset of training data so it moves on to the test data quickly. We can see the issue as soon as Test metrics are computed.
Screenshot of the issue (sorry could not make it inline due to ''not enough Reputation'')
Thank you very much for your help!
",0,427,"For anyone who might encounter similar issues:
As explained in PyTorch FAQ,

Dont accumulate history across your training loop. By default, computations involving variables that require gradients will keep history. This means that you should avoid using such variables in computations which will live beyond your training loops, e.g., when tracking statistics. Instead, you should detach the variable or access its underlying data.

In my case, adding with torch.no_grad() before my self.batch_training(train=False) resolves the issue.
",,
Keras unexpected behavior,https://stackoverflow.com/questions/56156646,Keras + Tensorflow Model Optimization: TypeError: clone_model() got an unexpected keyword argument &#39;clone_function&#39;,,3,4105,"I found the answer. There is a tricky workaround: besides fixing the code to:

from tensorflow_model_optimization.sparsity import keras as sparsity

pruning_params = {
      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,
                                                   final_sparsity=0.90,
                                                   begin_step=3,
                                                   end_step=end_step,
                                                   frequency=100)
}

pruned_model = tf.keras.Sequential([
    sparsity.prune_low_magnitude(
        l.Conv2D(32, 5, padding='same', activation='relu'),
        input_shape=input_shape,
        **pruning_params),
    l.MaxPooling2D((2, 2), (2, 2), padding='same'),
    l.BatchNormalization(),
    sparsity.prune_low_magnitude(
        l.Conv2D(64, 5, padding='same', activation='relu'), **pruning_params),
    l.MaxPooling2D((2, 2), (2, 2), padding='same'),
    l.Flatten(),
    sparsity.prune_low_magnitude(l.Dense(1024, activation='relu'),
                                 **pruning_params),
    l.Dropout(0.4),
    sparsity.prune_low_magnitude(l.Dense(num_classes, activation='softmax'),
                                 **pruning_params)
])


... I had to restart Jupyter kernel to get rid of further errors, like Conv2D has no attribute 'kernel', as seen at GitHub:

tf.enable_eager_execution must be called at program startup. #18304
",,
Keras unexpected behavior,https://stackoverflow.com/questions/67014270,TypeError: apply_gradients() got an unexpected keyword argument &#39;global_step&#39;,,2,1047,"You should try to use a different Optimizer. Those in tf.keras.optimizer don't take global_steps as an argument in apply_gradients function.
Instead, use these from tf.compat.v1.train, e.g.,
optimizer = tf.compat.v1.train.AdamOptimizer(learn_rate=0.003)

Note this passes the runtime check, but it makes the training impossible to complete. global_step is supposed to take a Variable and its value will be +1 when apply_gradients is called. However, here you see global_step=0 is passed in making it no effect at all. The train_step_counter you defined above will remain 0.
Also note there's a fix on the way.
",,
Keras unexpected behavior,https://stackoverflow.com/questions/68839035,"Activation function of tf.math.pow(x, 0.5) leading to NaN losses","I'm trying to use a custom square root activation function for my Keras sequential model (specifically for the MNIST dataset). When I use tf.math.sqrt(x), training goes smoothly and the model is quite accurate. However, when I try using tf.math.pow(x, 0.5), the model fails to train and the losses go to NaN.
I'm really unsure why this is happening because I would think that the two alternatives are identical.
Square root function
def tfsqrt(x):
    cond = tf.greater_equal(x, 0)
    return tf.where(cond, tf.math.sqrt(x), -tf.math.sqrt(-x))

Power function
def pwsqrt(x):
  cond = tf.greater_equal(x, 0)
  return tf.where(cond, tf.math.pow(x, 0.5), -tf.math.pow(-x, 0.5))

If anybody could explain this unexpected behavior, that would be much appreciated. Thanks!
",2,442,"Functions are correct:
x=tf.Variable([-2.0,-3.0,0.0, 1.0,2.0])
y=tfsqrt(x)
y
y=pwsqrt(x)
y

The functions work fine in google colab, maybe there are some nan values in data.
Maybe there is some problem in model loss or metric.
",,
Keras unexpected behavior,https://stackoverflow.com/questions/55559944,Keras.util.to_categorical show unexpected behaviour,"Keras .To_Categorical shows unexpected behavior: 

Contrary to documentation in https://keras.io/utils/#to_categorical
Specifying dtype returns an error

from keras.utils import to_categorical
import numpy as np

labels= np.array([0, 2, 1, 2, 0])
to_categorical(labels,dtype='int32')


produce:

TypeError: to_categorical() got an unexpected keyword argument 'dtype'

",0,552,"No, this is in the code, but the issue is that the documentation corresponds to the github master version, while you don't that version installed. You most likely have a keras release installed.

Currently there is no Keras release that contains the commits for this parameter.
",,
Keras unexpected result,https://stackoverflow.com/questions/61919774,unexpected keyword argument &#39;sample_weight&#39; when sub-classing tensor-flow loss class (categorical_crossentropy) to created a weighted loss function,,7,4667,"As per the comments; the issue here is that TensorFlow is now enforcing inheriting from the original method signature.

The following has been tested (by comparing equal weighting in the cost_matrix to weighting all but a single category to nothing) on a toy problem and works:

class WeightedCategoricalCrossentropy(tf.keras.losses.CategoricalCrossentropy):

  def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):

    cost_mat = np.array(cost_mat)   
    ## when loading from config, self.cost_mat returns as a list, rather than an numpy array. 
    ## Adding the above line fixes this issue, enabling .ndim to call sucessfully. 
    ## However, this is probably not the best implementation
    assert(cost_mat.ndim == 2)
    assert(cost_mat.shape[0] == cost_mat.shape[1])
    super().__init__(name=name, **kwargs)
    self.cost_mat = K.cast_to_floatx(cost_mat)

  def __call__(self, y_true, y_pred, sample_weight=None):
    assert sample_weight is None, ""should only be derived from the cost matrix""  
    return super().__call__(
        y_true=y_true, 
        y_pred=y_pred, 
        sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),
    )


  def get_config(self):
    config = super().get_config().copy()
    # Calling .update on the line above, during assignment, causes an error with config becoming None-type.
    config.update({'cost_mat': (self.cost_mat)})
    return config

  @classmethod
  def from_config(cls, config):
    # something goes wrong here and changes self.cost_mat to a list variable.
    # See above for temporary fix
    return cls(**config)

def get_sample_weights(y_true, y_pred, cost_m):
    num_classes = len(cost_m)

    y_pred.shape.assert_has_rank(2)
    assert(y_pred.shape[1] == num_classes)
    y_pred.shape.assert_is_compatible_with(y_true.shape)

    y_pred = K.one_hot(K.argmax(y_pred), num_classes)

    y_true_nk1 = K.expand_dims(y_true, 2)
    y_pred_n1k = K.expand_dims(y_pred, 1)
    cost_m_1kk = K.expand_dims(cost_m, 0)

    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k
    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])

    return sample_weights_n


# Register the loss in the Keras namespace to enable loading of the custom object.
tf.keras.losses.WeightedCategoricalCrossentropy = WeightedCategoricalCrossentropy


Usage

Where cost_matrix is a 2D NumPy array, eg:

[
 [ Weight Category 1 predicted as Category 1, 
   Weight Category 1 predicted as Category 2,
   Weight Category 1 predicted as Category 3 ]
 [ Weight Category 2 predicted as Category 1,
   ...,
   ...                                       ]
 [ ...,
   ...,
   Weight Category 3 predicted as Category 3 ]
]




model.compile(
     optimizer='adam',
     loss=WeightedCategoricalCrossentropy(cost_matrix)
     )


Model Saving

model.save(save_version_dir,save_format='tf')


Model Loading

model = tf.keras.models.load_model(
    save_version_dir,
    compile=True,
    custom_objects={
        'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_matrix)
        }
    )

",,
Keras unexpected result,https://stackoverflow.com/questions/50319066,Creating a neural network in keras to multiply two input integers,,4,3119,"It is due to large gradient updates caused by large numbers in training data. When using a neural network, you should first ensure that the training data falls in a small range (usually [-1,1] or [0,1]) to help the optimization process and prevent disruptive gradient updates. Therefore, you should first normalize data. In this case, one good candidate would be log-normalization.
Further, the 'accuracy' as a metric in Keras is used in case of a classification problem. In a regression problem, using it does not make sense, and instead it's better to use a relevant metric like ""mean absolute error"" or 'mae'.
",,
Keras unexpected result,https://stackoverflow.com/questions/39001104,"In Keras, If samples_per_epoch is less than the &#39;end&#39; of the generator when it (loops back on itself) will this negatively affect result?","I'm using Keras with Theano to train a basic logistic regression model.

Say I've got a training set of 1 million entries, it's too large for my system to use the standard model.fit() without blowing away memory.


I decide to use a python generator function and fit my model using model.fit_generator().  
My generator function returns batch sized chunks of the 1M training examples (they come from a DB table, so I only pull enough records at a time to satisfy each batch request, keeping memory usage in check).  
It's an endlessly looping generator, once it reaches the end of the 1 million, it loops and continues over the set


There is a mandatory argument in fit_generator() to specify samples_per_epoch.  The documentation indicates


  samples_per_epoch: integer, number of samples to process before going to the next epoch.


I'm assuming the fit_generator() doesn't reset the generator each time an epoch runs, hence the need for a infinitely running generator.

I typically set the samples_per_epoch to be the size of the training set the generator is looping over.

However, if samples_per_epoch this is smaller than the size of the training set the generator is working from and the nb_epoch &gt; 1:


Will you get odd/adverse/unexpected training resulting as it seems the epochs will have differing sets training examples to fit to?
If so, do you 'fastforward' you generator somehow?

",4,2031,,,
Keras unexpected result,https://stackoverflow.com/questions/53779968,why Keras fit_generator() load before actually &quot;training&quot;,"sorry to bother:

I am confused by keras function : fit_generator

I use custom generator to generate (image,seg_image)
for training 

Look carefully you can see inside of get_seg() function 

I put the print(path) ann the path is just the
path to read image 

from data ,the one more intention is I would like to know how 

fit_generator() get the data from generator 

#import all the stuff
def get_seg(#parameters ):
    print(path) #to track when this function is called 
    return seg_image     #for training 

#pre-processing image 
def getimage(#parameters):
    #do something to image
    return the imgage      #for training

def data_generator():
#load all the data for training 
    zipped =cycle(zip(images,segmentations))
    while True:
        X = []
        Y = []
        for _ in range(batch_size) :
            im , seg = next(zipped)
            X.append(getimage(#parameters))  
            Y.append(get_seg(#parameters))
    yield np.array(X) , np.array(Y) 

#create an generator 
G = data_generator(#parameters) 
#start training
for ep in range( epochs ):
    m.fit_generator( G , steps_per_epoch=512, 
                     epochs=1,workers=1)


While I start training, I get the really unexpected result ,As it goes 

through training:The terminal looks like:
it print out 24 set of path 

fist which it take the data from custom data_generator

data/train/0000_mask.png
data/train/0001_mask.png
data/train/0002_mask.png
data/train/0003_mask.png
data/train/0004_mask.png
data/train/0005_mask.png
data/train/0006_mask.png
data/train/0007_mask.png
data/train/0008_mask.png
data/train/0009_mask.png
data/train/0010_mask.png
data/train/0011_mask.png
data/train/0012_mask.png
data/train/0013_mask.png
data/train/0014_mask.png
data/train/0015_mask.png
data/train/0016_mask.png
data/train/0017_mask.png
data/train/0018_mask.png
data/train/0019_mask.png
data/train/0020_mask.png
data/train/0021_mask.png
data/train/0022_mask.png
data/train/0023_mask.png


And then :
I do believe the training starts here .

1/512 [..............................] - ETA: 2:14:34 - loss: 2.5879 - acc: 0.1697


load the data(image) again 

data/train/0024_mask.png
data/train/0025_mask.png




After 512(steps_per_epoch) which means the next round training 

begins,it would just print next 24 path before training ....

I would like to know why this is happening? Is this is how keras 

works? To load data before actually pass is through the network?

Or I am misunderstand or miss something basic knowledge?
",3,634,"Yes, this is how Keras works.

Training and loading are two parallel actions. One does not see how the other is going.

In the fit_generator method there is a max_queue_size argument, usually equal to 10 by default. This means the generator will load data at full speed until the queue is full. So you're loading many images in advance (this is good to avoid that the model gets slowed by loading)    

And the training just checks: are there items in the queue? Good, train. 

You're printing more than your batches because you call get_seg inside a loop but only call yield outside this loop. 
",,
Keras unexpected result,https://stackoverflow.com/questions/62518906,Keras ImageDataGenerator : how to use data augmentation with images paths,,2,2679,"Ok so I finally found out how to deal with these issues thanks to this article. My mistake was that I kept using ImageDataGenerator despite its lack of flexibility, the solution is thus simple : use another data augmentation tool.
We can resume the author's method as following :

First, create a personalized batch generator as a subclass of Keras Sequence class (which implies to implement a __getitem__ function that loads the images according to their respective paths).
Use the data augmentation albumentations library. It has the advantages of offering more transformation functions as Imgaug or ImageDataGenerator, while being faster. Moreover, this website allows you to test some of its augmentation methods, even with your own images ! See this one for the exhaustive list.


The drawback of this library is that, as it is relatively new, few documentation can be found online, and I've spent several hours trying to resolve an issue I encountered.
Indeed, when I tried to visualize some augmentation functions, the results were entirely black images (strange fact : these would happen only when I was modifying the intensity of the pixels, with methods like RandomGamma or RandomBrightnessContrast. With transformation functions such as HorizontalFlip or ShiftScaleRotate, it would work normally).
After an entire half day of trying-to-find-what's-wrong, I eventually came up with this solution, that might help you if you were to try this library : the loading of images has to be done with OpenCV (I was using load_img and img_to_array functions from tf.keras.preprocessing.image for the loading and processing). If anyone has an explanation of why this doesn't work, I'd be glad to hear it.
Anyway, here is my final code to display an augmented image :
!pip install -U git+https://github.com/albu/albumentations &gt; /dev/null &amp;&amp; echo ""All libraries are successfully installed!""
from albumentations import Compose, HorizontalFlip, RandomBrightnessContrast, ToFloat, RGBShift
import cv2
import matplotlib.pyplot as plt
import numpy as np
from google.colab.patches import cv2_imshow # I work on a Google Colab, thus I cannot use cv2.imshow()


augmentation = Compose([HorizontalFlip(p = 0.5),
                        RandomBrightnessContrast(p = 1),
                        ToFloat(max_value = 255) # Normalize the pixels values into the [0,1] interval
                        # Feel free to add more !
                        ])

img = cv2.imread('Your_path_here.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # cv2.imread() loads the images in BGR format, thus you have to convert it to RGB before applying any transformation function.
img = augmentation(image = img)['image'] # Apply the augmentation functions to the image.
plt.figure(figsize=(7, 7))
plt.imshow((img*255).astype(np.uint8)) # Put the pixels values back to [0,255]. Replace by plt.imshow(img) if the ToFloat function is not used.
plt.show()


'''
If you want to display using cv2_imshow(), simply replace the last three lines by :

img = cv2.normalize(img, None, 255,0, cv2.NORM_MINMAX, cv2.CV_8UC1) # if the ToFloat argument is set up inside Compose(), you have to put the pixels values back to [0,255] before plotting them with cv2_imshow(). I couldn't try with cv2.imshow(), but according to the documentation it seems this line would be useless with this displaying function.
cv2_imshow(img)

I don't recommend it though, because cv2_imshow() plot the images in BGR format, thus some augmentation methods such as RGBShift will not work properly.
'''


EDIT :
I've encountered several issues with the albumentations library (that I described in this question on Github, but for now I still have had no answers) thus I'd better recommend using Imgaug for your data augmentation : it works just fine and is almost as easy to use as albumentations, even though there is a little bit less available transformation functions.
",,
Keras unexpected result,https://stackoverflow.com/questions/69997327,"Tensorflow: ValueError: Input 0 is incompatible with layer model: expected shape=(None, 99), found shape=(None, 3)",,2,3405,"Maybe try changing the shape of pose_landmarks from (33, 3) to (1, 99) after your assertion and before you make a prediction:
import tensorflow as tf

pose_landmarks = tf.random.normal((33, 3))
assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)

pose_landmarks = tf.expand_dims(pose_landmarks, axis=0)
shape = tf.shape(pose_landmarks)
pose_landmarks = tf.reshape(pose_landmarks, (shape[0], shape[1] * shape[2]))

tf.print(pose_landmarks.shape)

TensorShape([1, 99])

",,
Keras unexpected result,https://stackoverflow.com/questions/66823474,GridSearchCV and Keras gives different MSE than manual search,"I'm running some experiments with a simple Neural Network using Keras and Sklearn and I'm running into some unexpected results.
In my first experiment, the NN has one hidden layer with 64 neurons, and I'm using the StratifiedKFold class to run a KFold with 5 splits.
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import StratifiedKFold
import numpy as np
import tensorflow as tf
import random

seed = 7
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)

for train, test in kfold.split(X, Y):
    model = Sequential()
    model.add(Dense(64, input_dim=12, activation='relu'))
    model.add(Dense(1))
    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
    model.summary()
    model.fit(X_train[train], Y[train], epochs=10,verbose=1)
    y_pred=model.predict(X_train[test])
    mse_value, mae_value=model.evaluate(X_train[test], Y[test], verbose=1)
    print(mse_value)

On the first fold, I get the following information printed:
Model: ""sequential_169""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_342 (Dense)            (None, 64)                832       
_________________________________________________________________
dense_343 (Dense)            (None, 1)                 65        
=================================================================
Total params: 897
Trainable params: 897
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
163/163 [==============================] - 0s 520us/step - loss: 23.5748 - mae: 4.6046
Epoch 2/10
163/163 [==============================] - 0s 503us/step - loss: 1.9301 - mae: 1.0770
Epoch 3/10
163/163 [==============================] - 0s 502us/step - loss: 1.0503 - mae: 0.8026
Epoch 4/10
163/163 [==============================] - 0s 492us/step - loss: 0.7895 - mae: 0.6828
Epoch 5/10
163/163 [==============================] - 0s 503us/step - loss: 0.6499 - mae: 0.6171
Epoch 6/10
163/163 [==============================] - 0s 524us/step - loss: 0.5652 - mae: 0.5795
Epoch 7/10
163/163 [==============================] - 0s 506us/step - loss: 0.5806 - mae: 0.5819
Epoch 8/10
163/163 [==============================] - 0s 506us/step - loss: 0.4949 - mae: 0.5497
Epoch 9/10
163/163 [==============================] - 0s 493us/step - loss: 0.4864 - mae: 0.5418
Epoch 10/10
163/163 [==============================] - 0s 492us/step - loss: 0.4942 - mae: 0.5455
41/41 [==============================] - 0s 474us/step - loss: 0.4861 - mae: 0.5457
0.48606643080711365
...

Note that the loss goes from 23.5748 to 0.4942 while training.
In my second experiment, I use the GridSearchCV class to perform a grid search on the number of layers to use. (To demonstrate my problem, I'm just trying one layer). I'm also passing the same kfold strategy as in the previous experiment to the constructor of GridSearchCV.
from tensorflow.keras.wrappers.scikit_learn import KerasRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import GridSearchCV
import tensorflow as tf
import random

seed = 7
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

def create_model(hidden_layers=1):
    # Initialize the constructor
    model = Sequential()

    # Add hidden layers
    for i in range(hidden_layers):
        if i == 0:
            model.add(Dense(64, input_dim=12, activation='relu'))
        else:
            model.add(Dense(64, activation='relu'))

    # Add an output layer 
    model.add(Dense(1))
        
    model.compile(optimizer='rmsprop', loss='mse', metrics=[""mae""])
    model.summary()
        
    return model

model = KerasRegressor(build_fn=create_model, epochs=10, verbose=1)

param_grid = dict(hidden_layers=[1])

kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)

grid = GridSearchCV(estimator=model, param_grid=param_grid,
                    scoring=[""neg_mean_absolute_error"", ""neg_mean_squared_error"", ""r2""],
                    refit=""r2"",
                    n_jobs=1, cv=kfold)

grid_result = grid.fit(X, Y)

With this method, on the first fold, I get the following output:
Model: ""sequential_180""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_364 (Dense)            (None, 64)                832       
_________________________________________________________________
dense_365 (Dense)            (None, 1)                 65        
=================================================================
Total params: 897
Trainable params: 897
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
163/163 [==============================] - 0s 527us/step - loss: 9.8205 - mae: 2.3366
Epoch 2/10
163/163 [==============================] - 0s 479us/step - loss: 1.0685 - mae: 0.8089
Epoch 3/10
163/163 [==============================] - 0s 503us/step - loss: 0.9351 - mae: 0.7488
Epoch 4/10
163/163 [==============================] - 0s 503us/step - loss: 0.9602 - mae: 0.7560
Epoch 5/10
163/163 [==============================] - 0s 502us/step - loss: 1.0195 - mae: 0.7830
Epoch 6/10
163/163 [==============================] - 0s 494us/step - loss: 0.9774 - mae: 0.7761
Epoch 7/10
163/163 [==============================] - 0s 489us/step - loss: 0.9569 - mae: 0.7413
Epoch 8/10
163/163 [==============================] - 0s 488us/step - loss: 0.9772 - mae: 0.7794
Epoch 9/10
163/163 [==============================] - 0s 464us/step - loss: 0.8716 - mae: 0.7259
Epoch 10/10
163/163 [==============================] - 0s 494us/step - loss: 0.8687 - mae: 0.7248
41/41 [==============================] - 0s 380us/step
...

Here the loss function behaves really differently from the first experiment; going from 9.8205 to 0.8687.
Since I'm:

Setting all random seeds to the same value

seed = 7
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)


Using the same KFold method:

kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)


The neural networks have indeed the same architecture:

Model: ""sequential_XXX""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_XXX (Dense)            (None, 64)                832       
_________________________________________________________________
dense_XXY (Dense)            (None, 1)                 65        
=================================================================
Total params: 897
Trainable params: 897
Non-trainable params: 0


The epochs and batch sizes are the same.

I would expect both neurons to obtain the same results (at least in the first fold), but I'm getting different results in the loss function.
How is it possible that the NN from the first experiment behaves differently than the one from the second?
EDIT
The problem was that I was training with X_train on the first experiment and with X on the second. X_train was the scaled version of X.
Having said that, Marco's points regarding seeds also apply. Please refer to his answer.
",2,329,"This is simply due to the random weights initialization by keras every time you build a new model in each fold. Simply setting the seed one time at the top makes the below code reproducible but dependent on the order you execute it.
To make the results the same you simply have to initialize the same seed every time you fit a new fold. We do this at the top of create_model function and use it to manually operate CV and with KerasRegressor plus cross_val_score (from sklearn).
def set_seed(seed):  
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

def create_model(hidden_layers=1):
    
    set_seed(seed=7) # &lt;==== set the seed at the beginning every time
    
    model = Sequential()
    for i in range(hidden_layers):
        if i == 0:
            model.add(Dense(64, input_dim=12, activation='relu'))
        else:
            model.add(Dense(64, activation='relu'))
    model.add(Dense(1))        
    model.compile(optimizer='rmsprop', loss='mse')
        
    return model

initialize some dummy data
np.random.seed(7)
X = np.random.uniform(0,1, (1000,12))
Y = np.random.randint(0,2, (1000,))

kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)

manual CV
for train, test in kfold.split(X, Y):
    model = create_model()
    model.fit(X[train], Y[train], epochs=1, verbose=1)
    mse_value = model.evaluate(X[test], Y[test], verbose=1)

results:
25/25 [==============================] - 1s 1ms/step - loss: 0.2977
7/7 [==============================] - 0s 2ms/step - loss: 0.2508
25/25 [==============================] - 0s 1ms/step - loss: 0.2789
7/7 [==============================] - 0s 2ms/step - loss: 0.2696
25/25 [==============================] - 0s 979us/step - loss: 0.2760
7/7 [==============================] - 0s 2ms/step - loss: 0.2669
25/25 [==============================] - 0s 1ms/step - loss: 0.3076
7/7 [==============================] - 0s 2ms/step - loss: 0.2538
25/25 [==============================] - 0s 1ms/step - loss: 0.2807
7/7 [==============================] - 0s 2ms/step - loss: 0.2642

sklearn CV
model_wrapper = KerasRegressor(build_fn=create_model, epochs=1, verbose=1)
cross_val_score(model_wrapper, X, Y, cv=kfold)

results:
25/25 [==============================] - 0s 1ms/step - loss: 0.2977
7/7 [==============================] - 0s 2ms/step - loss: 0.2508
25/25 [==============================] - 1s 1ms/step - loss: 0.2789
7/7 [==============================] - 0s 2ms/step - loss: 0.2696
25/25 [==============================] - 0s 1ms/step - loss: 0.2760
7/7 [==============================] - 0s 2ms/step - loss: 0.2669
25/25 [==============================] - 0s 1ms/step - loss: 0.3076
7/7 [==============================] - 0s 2ms/step - loss: 0.2538
25/25 [==============================] - 0s 1ms/step - loss: 0.2807
7/7 [==============================] - 0s 2ms/step - loss: 0.2642

here the running notebook
This is valid only with CPU
",,
Keras unexpected result,https://stackoverflow.com/questions/64620236,Error &quot;ValueError: Graph disconnected&quot; in Keras/Tensorflow,,1,847,"pay attention to not override the input variables. you overrode color, clarity and x input inside the network
here a possible solution:
dense1 = 2**7
dense2 = 2**8
dense3 = 2**9
dropout = 0.8
price_loss = 1
cut_loss = 1
activation= LeakyReLU()
batch_size = 32

#====================================================================
# INPUTS
#====================================================================
carat = Input(shape= (1,), batch_size= batch_size, name= 'carat')
Color = Input(shape= (1,), batch_size= batch_size, name= 'color')
Clarity = Input(shape= (1,), batch_size= batch_size, name= 'clarity')
depth = Input(shape= (1,), batch_size= batch_size, name= 'depth')
table = Input(shape= (1,), batch_size= batch_size, name= 'table')
X = Input(shape= (1,), batch_size= batch_size, name= 'x')
y = Input(shape= (1,), batch_size= batch_size, name= 'y')
z = Input(shape= (1,), batch_size= batch_size, name= 'z')
#====================================================================
# CREATE EMBEDDINGS FOR CATEGORICAL FEATURES ""COLOR"" AND ""CLARITY""
#====================================================================
color = Embedding(input_dim = 7, output_dim = 1, name = 'color_emb')(Color)
clarity = Embedding(input_dim = 8, output_dim = 1, name = 'clarity_emb')(Clarity)
color = Flatten()(color)
clarity = Flatten()(clarity)
#====================================================================
# CONCATENATE FEATURES
#====================================================================
x = Concatenate()([color, clarity, carat, depth, table, X, y, z])
#====================================================================
# DENSE NETWORK
#====================================================================
x = Dense(dense1, activation = activation)(x)
x = BatchNormalization()(x)
x = Dense(dense2, activation = activation)(x)
x = BatchNormalization()(x)
x = Dense(dense3, activation = activation)(x)
x = BatchNormalization()(x)
x = Dropout(dropout)(x)
#====================================================================
# PREDICTIONS
# ====================================================================
cut = Dense(1, activation = 'sigmoid')(x)
price = Dense(1)(x)
#====================================================================
# DEFINE THE MODEL
# ====================================================================
model = Model(inputs = [carat, Color, Clarity, depth, table, X, y, z] , 
              outputs = [cut , price])
model.compile('adam', 'mse')
model.summary()

here the running notebook: https://colab.research.google.com/drive/1akpvuGKHXt6Frrec692zvCMAfRzZbIaM?usp=sharing
","Well, like always when we are stuck with some error, the answer lies somewhere in error message. Please note that while you have been able to build the structure of your model overwriting your variables color, clarity and so on... the variables changed the type. At the beginning you initialize the variables to be tf.keras.Input and then you overwrite it with Embedding and Flatten layers. So when it comes to building your model, you ask to build model with inputs that are no longer tf.keras.Input but something else.
The solution is to not overwrite those variables.
",
Keras unexpected result,https://stackoverflow.com/questions/62675004,Keras initializer error unexpected keyword argument &#39;dtype&#39;,,1,7223,"Keras internally calls supplied initializer as below
weight = K.variable(initializer(shape, dtype=dtype), dtype=dtype, ......

As you can see the second argument of your custom initializer should be dtype not name
Fix
def initialize_weights(shape, dtype=None):
  return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)

def initialize_bias(shape, dtype=None):
  return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)

Now
model = get_siamese_model((105, 105, 1))
model.summary()

Will build the model succesfully
output
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 105, 105, 1)  0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 105, 105, 1)  0                                            
__________________________________________________________________________________________________
sequential_4 (Sequential)       (None, 4096)         38947648    input_7[0][0]                    
                                                                 input_8[0][0]                    
__________________________________________________________________________________________________
lambda_2 (Lambda)               (None, 4096)         0           sequential_4[1][0]               
                                                                 sequential_4[2][0]               
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 1)            4097        lambda_2[0][0]                   
==================================================================================================
Total params: 38,951,745
Trainable params: 38,951,745
Non-trainable params: 0
_________________________

","For some reason this is calling the v2 tf.keras initializers instead of the compat.v1 initializers. You are using v1 tensorflow but trying to call a keyword from v2.  To fix this please follow the steps to migrate to tensor flow V2 in the following link.
https://www.tensorflow.org/guide/migrate
",
Keras unexpected result,https://stackoverflow.com/questions/75728844,TypeError: VariableMetaclass._variable_v1_call() got an unexpected keyword argument &#39;experimental_enable_variable_lifting&#39;,,1,1482,"I was facing the same issue. Have you tried installing keras 2.12.0rc1?
I tried it here (pip install keras==2.12.0rc1 --user) and it seems to be working now.
","Seems like the TensorFlow upgrade to version 2.12.0 from 2.12.0-rc1 fixed the issue.
",
Keras unexpected result,https://stackoverflow.com/questions/65346324,Unable to load .h5 file made in Google Colab to Jupyter Notebook,"I tried a code for Face mask detect and alert system and I am getting an error regarding the same. I trained the model in Google Collaboratory and ran the following code in Jupyter Notebook. The code is as follows:
# Import necessary libraries
from keras.models import load_model
import cv2
import numpy as np
import tkinter
from tkinter import messagebox
import smtplib

# Initialize Tkinter
root = tkinter.Tk()
root.withdraw()

#Load trained deep learning model
model = load_model('face_mask_detection_alert_system.h5')

#Classifier to detect face
face_det_classifier=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

# Capture Video
vid_source=cv2.VideoCapture(0)

# Dictionaries containing details of Wearing Mask and Color of rectangle around face. If wearing mask 
then color would be 
# green and if not wearing mask then color of rectangle around face would be red
text_dict={0:'Mask ON',1:'No Mask'}
rect_color_dict={0:(0,255,0),1:(0,0,255)}

SUBJECT = ""Subject""   
TEXT = ""One Visitor violated Face Mask Policy. See in the camera to recognize user. A Person has been 
detected without a face mask in the Hotel Lobby Area 9. Please Alert the authorities.""


# While Loop to continuously detect camera feed
    while(True):

        ret, img = vid_source.read()
        grayscale_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
        faces = face_det_classifier.detectMultiScale(grayscale_img,1.3,5)  

        for (x,y,w,h) in faces:

            face_img = grayscale_img[y:y+w,x:x+w]
            resized_img = cv2.resize(face_img,(56,56))
            normalized_img = resized_img/255.0
            reshaped_img = np.reshape(normalized_img,(1,56,56,1))
            result=model.predict(reshaped_img)

            label=np.argmax(result,axis=1)[0]
  
            cv2.rectangle(img,(x,y),(x+w,y+h),rect_color_dict[label],2)
            cv2.rectangle(img,(x,y-40),(x+w,y),rect_color_dict[label],-1)
            cv2.putText(img, text_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,0,0),2) 
    
            # If label = 1 then it means wearing No Mask and 0 means wearing Mask
            if (label == 1):
                # Throw a Warning Message to tell user to wear a mask if not wearing one. This will 
                stay
                #open and No Access will be given He/She wears the mask
                messagebox.showwarning(""Warning"",""Access Denied. Please wear a Face Mask"")
        
                # Send an email to the administrator if access denied/user not wearing face mask 
                message = 'Subject: {}\n\n{}'.format(SUBJECT, TEXT)
                mail = smtplib.SMTP('smtp.gmail.com', 587)
                mail.ehlo()
                mail.starttls()
                mail.login('aaaa@gmail.com','bbbb@gmail.com')
                mail.sendmail('aaaa@gmail.com','aaaa@gmail.com',message)
                mail.close
            else:
                pass
                break

    cv2.imshow('LIVE Video Feed',img)
    key=cv2.waitKey(1)

    if(key==27):
        break
    
cv2.destroyAllWindows()
source.release()

Error:
Using TensorFlow backend.
Traceback (most recent call last):
  File ""C:\Users\IZZY\Desktop\Dataset\facemaskalert_2.py"", line 14, in &lt;module&gt;
    model = load_model('face_mask_detection_alert_system.h5')
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\engine\saving.py"", line 492, in load_wrapper
    return load_function(*args, **kwargs)
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\engine\saving.py"", line 584, in load_model
    model = _deserialize_model(h5dict, custom_objects, compile)
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\engine\saving.py"", line 274, in _deserialize_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\engine\saving.py"", line 627, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\layers\__init__.py"", line 168, in deserialize
    printable_module_name='layer')
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\utils\generic_utils.py"", line 147, in deserialize_keras_object
    list(custom_objects.items())))
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\engine\sequential.py"", line 301, in from_config
    custom_objects=custom_objects)
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\layers\__init__.py"", line 168, in deserialize
    printable_module_name='layer')
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\utils\generic_utils.py"", line 149, in deserialize_keras_object
    return cls.from_config(config['config'])
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\engine\base_layer.py"", line 1179, in from_config
    return cls(**config)
  File ""C:\Users\IZZY\AppData\Local\Programs\Python\Python37\lib\site- 
  packages\keras\legacy\interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  TypeError: __init__() got an unexpected keyword argument 'ragged'

Source Code : https://theaiuniversity.com/courses/face-mask-detection-alert-system/
Methods Tried but did not help : Unexpected keyword argument 'ragged' in Keras
EDIT:-h5 FILE
https://drive.google.com/file/d/10oHrqtrYoD2Hx0olLnnf0qYSQLlrt3Kq/view?usp=sharing.
",1,1673,"Seems the issue is similar as mentioned in this stackoverflow question. already.
As the accepted answer here mentions that the exported model may be from tf.keras and not keras directly :

""Don't import keras directly as your model is saved with Tensorflow's
keras high level api. Change all your imports to tensorflow.keras"".

My suggestion:
1.You should try to use tf.keras for all keras imports

And also as Mohammad mentions in the answer , use compile=False in load_model.

Check the version of tf and keras in colab and local environment.Both versions need to be same.


From Keras github issues regarding this.
","I think the version of TensorFlow that you used to train and save your model is different than the one that you are using to load the model.
Try compile=False while loading your model:
#Load trained deep learning model
model = load_model('face_mask_detection_alert_system.h5', compile=False)

",
Keras unexpected result,https://stackoverflow.com/questions/65883675,Keras raises an exception on loading a lambda layer,,1,1743,"I had the same problem while using a lambda layer.
A possible workaround is defining a custom lamda function outside the model and using it inside the lambda layer.
Here is the updated code:
import tensorflow as tf

def custom_one_hot(x):
    return tf.keras.backend.one_hot(x, num_classes=10) #or tf.one_hot(x, depth=10)

model1 = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(81,), dtype='uint8'),
    tf.keras.layers.Lambda(custom_one_hot, output_shape=(81, 10)),
])

tf.keras.models.save_model(model1, './model')
model2 = tf.keras.models.load_model('./model')
model2.summary()

Output:
Model: ""sequential_8""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lambda_8 (Lambda)           (None, 81, 10)            0         
                                                                 
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________

Important: each time we want to use the model we will need to define it before loading the model. Otherwise will run into an error:
NameError: Exception encountered when calling layer ""lambda_8"" (type Lambda).

name 'custom_one_hot' is not defined

To resolve this issue, we need to ensure that the custom function is defined before loading the model. The code defining the custom function used inside the lambda layer should always be executed before loading the saved models.
","You could maybe use the tf.keras.layers.experimental.preprocessing.CategoryEncoding layer.
import tensorflow as tf

model1 = tf.keras.Sequential([
    tf.keras.layers.Input(shape = (81,), dtype=tf.int32), 
    tf.keras.layers.experimental.preprocessing.CategoryEncoding(
        max_tokens=(10), output_mode='count'), # or 'binary'
])

tf.keras.models.save_model(model1, './model')
model2 = tf.keras.models.load_model('./model')
model2.summary()

inp = tf.random.uniform((1, 81), 0, 10, dtype=tf.int32)

model2(inp)

&lt;tf.Tensor: shape=(1, 10), dtype=float32, 
    numpy=array([[ 7., 11.,  5.,  6., 10.,  5.,  5., 12., 10., 10.]], dtype=float32)&gt;

",
Keras unexpected result,https://stackoverflow.com/questions/69466354,MLflow S3UploadFailedError: Failed to upload,"I've created with docker a MinioS3 artifact storage and a MySQL backend storage using the next Docker Compose:
    version: '3.8'
    services:
        db:
           environment:
              - MYSQL_DATABASE=${MYSQL_DATABASE}
              - MYSQL_USER=${MYSQL_USER}
              - MYSQL_PASSWORD=${MYSQL_PASSWORD}
              - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
           expose:
              - '3306'        
           volumes:
              - '(path)/server_backend:/var/lib/mysql '
           image: 'mysql'
           container_name: db

        storage:
            environment:
                - MINIO_ACCESS_KEY=${MINIO_USR}
                - MINIO_SECRET_KEY=${MINIO_PASS}
            expose:
                - '9000'
            ports:
                - '9000:9000'        
            depends_on:
                - db
            command: server /data
            volumes:
                - '(path)/server_artifact:/data'
            image: minio/minio:RELEASE.2021-02-14T04-01-33Z
            container_name: MinIO

        mlflow:
            build: ./mlflow
            environment:
                - AWS_ACCESS_KEY_ID=${MINIO_USR}
                - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}       
            expose:
                - '5000'
            ports:
                - '5000:5000'
            depends_on:
                - storage                       
            image: 'mlflow:Dockerfile'
            container_name: server

The Mlflow server docker was created using the next Dockerfile:
    FROM python:3.8-slim-buster
    WORKDIR /usr/src/app
    RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql
    ENV MLFLOW_S3_ENDPOINT_URL=http://storage:9000
    CMD mlflow server \
        --backend-store-uri mysql+pymysql://MLFLOW:temporal@db:3306/DBMLFLOW \
        --default-artifact-root s3://artifacts \
        --host 0.0.0.0

The credantials are defined in a .env file.
The results of the docker-compose up command:

    [+] Running 21/22
     - mlflow Error                                                                                                                              5.6s
     - storage Pulled                                                                                                                           36.9s
       - a6b97b4963f5 Pull complete                                                                                                             24.6s
       - 13948a011eec Pull complete                                                                                                             24.7s
       - 40cdef9976a6 Pull complete                                                                                                             24.7s
       - f47162848743 Pull complete                                                                                                             24.8s
       - 5f2758d8e94c Pull complete                                                                                                             24.9s
       - c2950439edb8 Pull complete                                                                                                             25.0s
       - 1b08f8a15998 Pull complete                                                                                                             30.7s
     - db Pulled                                                                                                                                45.8s
       - 07aded7c29c6 Already exists                                                                                                             0.0s
       - f68b8cbd22de Pull complete                                                                                                              0.7s
       - 30c1754a28c4 Pull complete                                                                                                              2.1s
       - 1b7cb4d6fe05 Pull complete                                                                                                              2.2s
       - 79a41dc56b9a Pull complete                                                                                                              2.3s
       - 00a75e3842fb Pull complete                                                                                                              6.7s
       - b36a6919c217 Pull complete                                                                                                              6.8s
       - 635b0b84d686 Pull complete                                                                                                              6.8s
       - 6d24c7242d02 Pull complete                                                                                                             39.4s
       - 5be6c5edf16f Pull complete                                                                                                             39.5s
       - cb35eac1242c Pull complete                                                                                                             39.5s
       - a573d4e1c407 Pull complete                                                                                                             39.6s
    [+] Building 1.4s (7/7) FINISHED
     =&gt; [internal] load build definition from Dockerfile                                                                                         0.0s
     =&gt; =&gt; transferring dockerfile: 32B                                                                                                          0.0s
     =&gt; [internal] load .dockerignore                                                                                                            0.0s
     =&gt; =&gt; transferring context: 2B                                                                                                              0.0s
     =&gt; [internal] load metadata for docker.io/library/python:3.8-slim-buster                                                                    1.3s
     =&gt; [1/3] FROM docker.io/library/python:3.8-slim-buster@sha256:13a3f2bffb4b18ff7eda2763a3b0ba316dd82e548f52ea8b4fd11c94b97afa7d              0.0s
     =&gt; CACHED [2/3] WORKDIR /usr/src/app                                                                                                        0.0s
     =&gt; CACHED [3/3] RUN pip install cryptography mlflow psycopg2-binary boto3 pymysql                                                           0.0s
     =&gt; exporting to image                                                                                                                       0.0s
     =&gt; =&gt; exporting layers                                                                                                                      0.0s
     =&gt; =&gt; writing image sha256:76d4e4462b5c7c1826734e59a54488b56660de0dd5ecc188c308202608a8f20b                                                 0.0s
     =&gt; =&gt; naming to docker.io/library/mlflow:Dockerfile                                                                                         0.0s
    
    Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them
    [+] Running 3/3
     - Container db  Created                                                                                                       0.5s
     - Container MinIO      Created                                                                                                       0.1s
     - Container server     Created                                                                                                       0.1s
    Attaching to server, MinIO, db
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.26-1debian10 started.
    db  | 2021-10-06 12:12:57+00:00 [Note] [Entrypoint]: Initializing database files
    db  | 2021-10-06T12:12:57.679527Z 0 [System] [MY-013169] [Server] /usr/sbin/mysqld (mysqld 8.0.26) initializing of server in progress as process 44
    db  | 2021-10-06T12:12:57.687748Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
    db  | 2021-10-06T12:12:58.230036Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
    db  | 2021-10-06T12:12:59.888820Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main
    db  | 2021-10-06T12:12:59.889102Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main
    db  | 2021-10-06T12:12:59.997461Z 6 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.
    MinIO      | Attempting encryption of all config, IAM users and policies on MinIO backend
    MinIO      | Endpoint: http://172.18.0.3:9000  http://127.0.0.1:9000
    MinIO      |
    MinIO      | Browser Access:
    MinIO      |    http://172.18.0.3:9000  http://127.0.0.1:9000
    MinIO      |
    MinIO      | Object API (Amazon S3 compatible):
    MinIO      |    Go:         https://docs.min.io/docs/golang-client-quickstart-guide
    MinIO      |    Java:       https://docs.min.io/docs/java-client-quickstart-guide
    MinIO      |    Python:     https://docs.min.io/docs/python-client-quickstart-guide
    MinIO      |    JavaScript: https://docs.min.io/docs/javascript-client-quickstart-guide
    MinIO      |    .NET:       https://docs.min.io/docs/dotnet-client-quickstart-guide
    server     | 2021/10/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 0.1 seconds
    server     | 2021/10/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 0.3 seconds
    server     | 2021/10/06 12:13:02 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 0.7 seconds
    server     | 2021/10/06 12:13:03 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 1.5 seconds
    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Database files initialized
    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Starting temporary server
    db  | 2021-10-06T12:13:04.422603Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.26) starting as process 93
    db  | 2021-10-06T12:13:04.439806Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
    db  | 2021-10-06T12:13:04.575773Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
    db  | 2021-10-06T12:13:04.827307Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:04.827865Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:04.832827Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
    db  | 2021-10-06T12:13:04.834132Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
    db  | 2021-10-06T12:13:04.841629Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
    db  | 2021-10-06T12:13:04.855748Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: /var/run/mysqld/mysqlx.sock
    db  | 2021-10-06T12:13:04.855801Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.26'  socket: '/var/run/mysqld/mysqld.sock'  port: 0  MySQL Community Server - GPL.
    db  | 2021-10-06 12:13:04+00:00 [Note] [Entrypoint]: Temporary server started.
    server     | 2021/10/06 12:13:05 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 3.1 seconds
    db  | Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
    db  | Warning: Unable to load '/usr/share/zoneinfo/leap-seconds.list' as time zone. Skipping it.
    db  | Warning: Unable to load '/usr/share/zoneinfo/zone.tab' as time zone. Skipping it.
    db  | Warning: Unable to load '/usr/share/zoneinfo/zone1970.tab' as time zone. Skipping it.
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating database DBMLFLOW
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Creating user MLFLOW
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Giving user MLFLOW access to schema DBMLFLOW
    db  |
    db  | 2021-10-06 12:13:06+00:00 [Note] [Entrypoint]: Stopping temporary server
    db  | 2021-10-06T12:13:06.948482Z 13 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.26).
    server     | 2021/10/06 12:13:08 WARNING mlflow.store.db.utils: SQLAlchemy engine could not be created. The following exception is caught.
    server     | (pymysql.err.OperationalError) (2003, ""Can't connect to MySQL server on 'db' ([Errno 111] Connection refused)"")
    server     | (Background on this error at: https://sqlalche.me/e/14/e3q8)
    server     | Operation will be retried in 6.3 seconds
    db  | 2021-10-06T12:13:08.716131Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.26)  MySQL Community Server - GPL.
    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: Temporary server stopped
    db  |
    db  | 2021-10-06 12:13:08+00:00 [Note] [Entrypoint]: MySQL init process done. Ready for start up.
    db  |
    db  | 2021-10-06T12:13:09.159115Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.26) starting as process 1
    db  | 2021-10-06T12:13:09.167405Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
    db  | 2021-10-06T12:13:09.298925Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
    db  | 2021-10-06T12:13:09.488958Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:09.489087Z 0 [Warning] [MY-013746] [Server] A deprecated TLS version TLSv1.1 is enabled for channel mysql_main
    db  | 2021-10-06T12:13:09.489934Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
    db  | 2021-10-06T12:13:09.490169Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
    db  | 2021-10-06T12:13:09.494728Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
    db  | 2021-10-06T12:13:09.509856Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: /var/run/mysqld/mysqlx.sock
    db  | 2021-10-06T12:13:09.509982Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.26'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server - GPL.
    db  | mbind: Operation not permitted
    server     | 2021/10/06 12:13:14 INFO mlflow.store.db.utils: Creating initial MLflow database tables...
    server     | 2021/10/06 12:13:14 INFO mlflow.store.db.utils: Updating database tables
    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.
    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
    server     | INFO  [alembic.runtime.migration] Running upgrade  -&gt; 451aebb31d03, add metric step
    server     | INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -&gt; 90e64c465722, migrate user column to tags
    server     | INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -&gt; 181f10493468, allow nulls for metric values
    server     | INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -&gt; df50e92ffc5e, Add Experiment Tags Table
    server     | INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -&gt; 7ac759974ad8, Update run tags with larger limit
    server     | INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -&gt; 89d4b8295536, create latest metrics table
    server     | INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!
    server     | INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -&gt; 2b4d017a5e9b, add model registry tables to db
    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.
    server     | INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!
    server     | INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -&gt; cfd24bdc0731, Update run status constraint with killed
    server     | INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -&gt; 0a8213491aaa, drop_duplicate_killed_constraint
    server     | INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -&gt; 728d730b5ebd, add registered model tags table
    server     | INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -&gt; 27a6a02d2cf1, add model version tags table
    server     | INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -&gt; 84291f40a231, add run_link to model_version
    server     | INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -&gt; a8c4a736bde6, allow nulls for run_id
    server     | INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -&gt; 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary
    server     | INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -&gt; c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql
    server     | INFO  [alembic.runtime.migration] Context impl MySQLImpl.
    server     | INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
    db  | mbind: Operation not permitted
    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Starting gunicorn 20.1.0
    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Listening at: http://0.0.0.0:5000 (17)
    server     | [2021-10-06 12:13:16 +0000] [17] [INFO] Using worker: sync
    server     | [2021-10-06 12:13:16 +0000] [19] [INFO] Booting worker with pid: 19
    server     | [2021-10-06 12:13:16 +0000] [20] [INFO] Booting worker with pid: 20
    server     | [2021-10-06 12:13:16 +0000] [21] [INFO] Booting worker with pid: 21
    server     | [2021-10-06 12:13:16 +0000] [22] [INFO] Booting worker with pid: 22


It makes me suspect because on the second line appears - mlflow Error but I think that this is why the other builds haven't finished.
Then I've set my environment variables on the client to create the information flow between my script and the storages:

    os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://localhost:9000/'
    os.environ['AWS_ACCESS_KEY_ID'] = 'key'
    os.environ['AWS_SECRET_ACCESS_KEY'] = 'pw'
    
    remote_server_uri = ""http://localhost:5000/"" # server URI
    mlflow.set_tracking_uri(remote_server_uri)
    
    mlflow.set_experiment(""mnist_mLflow_demo"")


finally I trained a TensorFlow network and I didn't have problems storing parameters and metrics but gave me some warnings (referring to next error). But the model haven't been auto log, so I tried to do it manually:
    with mlflow.start_run(run_name = ""test0"") as run:
    
        mlflow.keras.log_model(model2, 'model2')

    mlflow.end_run()

It dosen't work and it gives me the next INFO (but essencialy an error):
    INFO:tensorflow:Assets written to: (path)\Temp\tmpgr5eaha2\model\data\model\assets
    INFO:tensorflow:Assets written to: (path)\Temp\tmpgr5eaha2\model\data\model\assets
    2021/10/06 14:16:00 ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: (path)\AppData\Local\Temp\tmpgr5eaha2\model, flavor: keras)
    Traceback (most recent call last):
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\environment.py"", line 212, in infer_pip_requirements
        return _infer_requirements(model_uri, flavor)
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\requirements_utils.py"", line 263, in _infer_requirements
        modules = _capture_imported_modules(model_uri, flavor)
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\requirements_utils.py"", line 221, in _capture_imported_modules
        _run_command(
      File ""(path)\Python\Python39\lib\site-packages\mlflow\utils\requirements_utils.py"", line 163, in _run_command
        stderr = stderr.decode(""utf-8"")
    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 349: invalid continuation byte


And the next error:

    ClientError                               Traceback (most recent call last)
    ~\Python\Python39\lib\site-packages\boto3\s3\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)
        278         try:
    --&gt; 279             future.result()
        280         # If a client error was raised, add the backwards compatibility layer
    
    ~\Python\Python39\lib\site-packages\s3transfer\futures.py in result(self)
        105             # out of this and propogate the exception.
    --&gt; 106             return self._coordinator.result()
        107         except KeyboardInterrupt as e:
    
    ~\Python\Python39\lib\site-packages\s3transfer\futures.py in result(self)
        264         if self._exception:
    --&gt; 265             raise self._exception
        266         return self._result
    
    ~\Python\Python39\lib\site-packages\s3transfer\tasks.py in __call__(self)
        125             if not self._transfer_coordinator.done():
    --&gt; 126                 return self._execute_main(kwargs)
        127         except Exception as e:
    
    ~\Python\Python39\lib\site-packages\s3transfer\tasks.py in _execute_main(self, kwargs)
        149 
    --&gt; 150         return_value = self._main(**kwargs)
        151         # If the task is the final task, then set the TransferFuture's
    
    ~\Python\Python39\lib\site-packages\s3transfer\upload.py in _main(self, client, fileobj, bucket, key, extra_args)
        693         with fileobj as body:
    --&gt; 694             client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
        695 
    
    ~\Python\Python39\lib\site-packages\botocore\client.py in _api_call(self, *args, **kwargs)
        385             # The ""self"" in this scope is referring to the BaseClient.
    --&gt; 386             return self._make_api_call(operation_name, kwargs)
        387 
    
    ~\Python\Python39\lib\site-packages\botocore\client.py in _make_api_call(self, operation_name, api_params)
        704             error_class = self.exceptions.from_code(error_code)
    --&gt; 705             raise error_class(parsed_response, operation_name)
        706         else:
    
    ClientError: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.
    
    During handling of the above exception, another exception occurred:
    
    S3UploadFailedError                       Traceback (most recent call last)
    C:\Users\FCAIZA~1\AppData\Local\Temp/ipykernel_7164/2476247499.py in &lt;module&gt;
          1 with mlflow.start_run(run_name = ""test0"") as run:
          2 
    ----&gt; 3     mlflow.keras.log_model(model2, 'model2')
          4 
          5 mlflow.end_run()
    
    ~\Python\Python39\lib\site-packages\mlflow\keras.py in log_model(keras_model, artifact_path, conda_env, custom_objects, keras_module, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, **kwargs)
        402             mlflow.keras.log_model(keras_model, ""models"")
        403     """"""
    --&gt; 404     Model.log(
        405         artifact_path=artifact_path,
        406         flavor=mlflow.keras,
    
    ~\Python\Python39\lib\site-packages\mlflow\models\model.py in log(cls, artifact_path, flavor, registered_model_name, await_registration_for, **kwargs)
        186             mlflow_model = cls(artifact_path=artifact_path, run_id=run_id)
        187             flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)
    --&gt; 188             mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)
        189             try:
        190                 mlflow.tracking.fluent._record_logged_model(mlflow_model)
    
    ~\Python\Python39\lib\site-packages\mlflow\tracking\fluent.py in log_artifacts(local_dir, artifact_path)
        582     """"""
        583     run_id = _get_or_start_run().info.run_id
    --&gt; 584     MlflowClient().log_artifacts(run_id, local_dir, artifact_path)
        585 
        586 
    
    ~\Python\Python39\lib\site-packages\mlflow\tracking\client.py in log_artifacts(self, run_id, local_dir, artifact_path)
        975             is_dir: True
        976         """"""
    --&gt; 977         self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)
        978 
        979     @contextlib.contextmanager
    
    ~\Python\Python39\lib\site-packages\mlflow\tracking\_tracking_service\client.py in log_artifacts(self, run_id, local_dir, artifact_path)
        332         :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.
        333         """"""
    --&gt; 334         self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)
        335 
        336     def list_artifacts(self, run_id, path=None):
    
    ~\Python\Python39\lib\site-packages\mlflow\store\artifact\s3_artifact_repo.py in log_artifacts(self, local_dir, artifact_path)
        102                 upload_path = posixpath.join(dest_path, rel_path)
        103             for f in filenames:
    --&gt; 104                 self._upload_file(
        105                     s3_client=s3_client,
        106                     local_file=os.path.join(root, f),
    
    ~\Python\Python39\lib\site-packages\mlflow\store\artifact\s3_artifact_repo.py in _upload_file(self, s3_client, local_file, bucket, key)
         78         if environ_extra_args is not None:
         79             extra_args.update(environ_extra_args)
    ---&gt; 80         s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)
         81 
         82     def log_artifact(self, local_file, artifact_path=None):
    
    ~\Python\Python39\lib\site-packages\boto3\s3\inject.py in upload_file(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)
        128     """"""
        129     with S3Transfer(self, Config) as transfer:
    --&gt; 130         return transfer.upload_file(
        131             filename=Filename, bucket=Bucket, key=Key,
        132             extra_args=ExtraArgs, callback=Callback)
    
    ~\Python\Python39\lib\site-packages\boto3\s3\transfer.py in upload_file(self, filename, bucket, key, callback, extra_args)
        283         # client error.
        284         except ClientError as e:
    --&gt; 285             raise S3UploadFailedError(
        286                 ""Failed to upload %s to %s: %s"" % (
        287                     filename, '/'.join([bucket, key]), e))
    
    S3UploadFailedError: Failed to upload (path)\AppData\Local\Temp\tmpgr5eaha2\model\conda.yaml to artifacts/1/5ae5fcef2d07432d811c3d7eb534382c/artifacts/model2/conda.yaml: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.


",1,2128,"I found the solution of this issue. It is a tricky problem due to spanish characters, my system's user profile in ""C:/"" is ""fcaizares"" (Caizares is my first last name). I have created another user named ""fcanizares"" and all is working fine. Hope you find this solution helpfull.
PS: Moral of the issue, get rid of the extrange characters!
",,
Keras unexpected result,https://stackoverflow.com/questions/61886910,Manipulating data in keras custom loss function for CNN,"I'm trying to write a custom loss function in Keras for a CNN I'm working on. Y_true and Y_pred will both be tensors of grayscale images, so I expect a shape of [a, x, y, 1], where x and y are the dimensions of my images and a is the batch size.

The plan is to:


Threshold each image of Y_true by its mean pixel intensity
Use the non-zero elements of this mask to get an array of pixel values from Y_true and Y_pred
Measure the cosine similarity (using the built-in Keras loss function) of these arrays and return the average result of the batch as the loss


My main question is how I can efficiently implement this process?
Does the cosine_similarity function work on 1D arrays?

I know that I should avoid for loops to maintain efficiency but it's the only way I can think of implementing this function. Is there a more efficient way to implement this function using the Keras backend or numpy?

EDIT

Basic implementation and an unexpected error when compiling the model with this function:

def masked_cosine_similarity(y_true, y_pred):
    loss = 0
    for i in range(y_true.shape[0]):
        true_y = y_true[i,:,:,0]
        pred_y = y_pred[i,:,:,0]
        mask = true_y &gt; np.mean(true_y)
        elements = np.nonzero(mask)
        true_vals = np.array([true_y[x,y] for x, y in zip(elements[0], elements[1])])
        pred_vals = np.array([pred_y[x,y] for x, y in zip(elements[0], elements[1])])
        loss += cosine_similarity(true_vals, pred_vals)
    return loss / y_true.shape[0]


Error message:

     64     loss = 0
---&gt; 65     for i in range(y_true.shape[0]):
     66         true_y = y_true[i,:,:,0]
     67         pred_y = y_pred[i,:,:,0]

TypeError: 'NoneType' object cannot be interpreted as an integer

",1,327,"The shape of a tensor in Keras/TF is usually [None, height, width, channels].
This is due to the support of an arbitrary batch size, you don't want to build a model that only works for a specific batch size. For that reason, your code collapses on:

for i in range(y_true.shape[0]):


since y_true.shape[0] == None.

Why do you loop over the batch? You don't need to do it.
For example, given some element-wize loss function (MSE/cosine loss etc.) you can do something like:

def my_loss(y_true, y_pred):
    mask = tf.keras.backend.cast(y_true &gt;= tf.math.reduce_mean(y_true, axis=[1,2], keepdims=True), 'float32') 
    masked_loss = K.sum(mask * elementwize_loss(y_true, y_pred), axis=-1)
    num_valid_pixels = K.maximum(1.0, K.cast(K.sum(mask), 'float32'))
    return masked_loss / num_valid_pixels

",,
Keras unexpected result,https://stackoverflow.com/questions/43720702,Fitting a sine wave with Keras and PYMC3 yields unexpected results,"I've been trying to fit a sine curve with a keras (theano backend) model using pymc3. I've been using this [http://twiecki.github.io/blog/2016/07/05/bayesian-deep-learning/] as a reference point.

A Keras implementation alone fit using optimization does a good job, however Hamiltonian Monte Carlo and Variational sampling from pymc3 is not fitting the data. The trace is stuck at where the prior is initiated. When I move the prior the posterior moves to the same spot. The posterior predictive of the bayesian model in cell 59 is barely getting the sine wave, whereas the non-bayesian fit model gets it near perfect in cell 63. I created a notebook here: https://gist.github.com/tomc4yt/d2fb694247984b1f8e89cfd80aff8706 which shows the code and the results. 

Here is a snippet of the model below...

class GaussWeights(object):
    def __init__(self):
        self.count = 0

    def __call__(self, shape, name='w'):
        return pm.Normal(
            name, mu=0, sd=.1,
            testval=np.random.normal(size=shape).astype(np.float32),
            shape=shape)


def build_ann(x, y, init):
    with pm.Model() as m:

        i = Input(tensor=x, shape=x.get_value().shape[1:])
        m = i
        m = Dense(4, init=init, activation='tanh')(m)
        m = Dense(1, init=init, activation='tanh')(m)

        sigma = pm.Normal('sigma', 0, 1, transform=None)
        out = pm.Normal('out', 
                         m, 1,
                         observed=y, transform=None)

    return out



 with pm.Model() as neural_network:
    likelihood = build_ann(input_var, target_var, GaussWeights())

#     v_params = pm.variational.advi(
#         n=300, learning_rate=.4
#     )
#     trace = pm.variational.sample_vp(v_params, draws=2000)
    start = pm.find_MAP(fmin=scipy.optimize.fmin_powell)
    step = pm.HamiltonianMC(scaling=start)
    trace = pm.sample(1000, step,   progressbar=True)

",1,753,,,
Keras unexpected result,https://stackoverflow.com/questions/68899955,"loading keras model, TypeError: &#39;module&#39; object is not callable",,0,1917,"
The dataset is classical dog-vs-cat dataset and I'm trying to achieve
one-class classification task.

Then the model should have only one output node because this is an example of Binary Classification. Your model has the last Dense layer with two output units and the softmax as an activation function, which is a possible solution, but for this kind of problem my suggestion is to specify on the last Dense layer only one output unit and the sigmoid as the activation function.
It seems you forgot to add the string dog in the classes list on the train_generator.
train_generator = train_datagen.flow_from_directory(
            base_path + 'training_set/training_set/',
            target_size = (SHAPE[0], SHAPE[1]),
            batch_size = batch_size,
            class_mode = 'categorical',
            shuffle = True,
            seed = 33,
            classes = ['cats']
    )

Now, regards on the main topic of your question.
from keras.models import load_model
loaded_model = load_model('my_custom_model.h5')
#or 
loaded_model = tf.keras.models.load_model('my_custom_model.h5')
#or 
loaded_model = load_model('my_custom_model') # as from folder

I think, but I'm not 100% sure about it, the problem is from this line of code from keras.models import load_model. Starting from Tensorflow 2.x, which includes also the Keras module, every single function, class etc. must be imported by specifying always tensorflow as the first name for import each Keras module. In short, you should write this line of code: from tensorflow.keras.models import load_model and it should work because it calls the function from the Keras module with the Tensorflow as the back engine.
",,
Keras unexpected result,https://stackoverflow.com/questions/48850537,Issue with Imagenet classification with VGG16 pretrained weights,,0,1688,,,
Keras unexpected result,https://stackoverflow.com/questions/76564629,Cannot import protobuf builder when deploying Azure function,,0,1454,"I had a similar issue and found a different solution. There is an app setting that isolates the application's dependencies from the azure function runtime dependencies:
https://learn.microsoft.com/en-us/azure/azure-functions/functions-app-settings#python_isolate_worker_dependencies
Adding the PYTHON_ISOLATE_WORKER_DEPENDENCIES setting with a value of 1 to my function in azure fixed the issue for me.
","The root cause of the problem, as highlighted by @SiddheshDesai, turned out to be that the Azure Functions host was loading a version of protobuf (3.19.6) into the cache which was earlier than the version that the latest Tensorflow needed (4.23.3).
Even when placing my protobuf/TF imports at the top of my function app module, the same error arose.
I was, however, able to reload protobuf from my site-packages using importlib.
This cause was proven by adding the following code to my function:
import logging
import importlib
import google.protobuf

logging.info(google.protobuf.__version__)
logging.info(google.protobuf.__file__)

importlib.reload(google.protobuf)

logging.info(google.protobuf.__version__)
logging.info(google.protobuf.__file__)

However, the reload then interfered with the Azure Functions library and host causing a heap of other exceptions.
My conclusion therefore is that this is an issue with Azure Functions rather than an issue with my code.
Rather than trying to work around these issues, I've now containerised my function and re-deployed. This has worked as expected.
The answer, therefore, turns out to be - as SiddheshDesai suggested above - that any application needing to use versions of protobuf &gt;= 3.20.0 need to be containerised rather than deployed as code/run from packages.
","If you want to use Protobuf module in Azure Functions, You need to downgrade it to 3.20.* and add it in your requirements.txt.
I added protobuf==3.20.* in my requirements.txt and the Http Trigger got deployed successfully in Azure Functions via DevOps YAML pipeline, Refer below:-
My requirements.txt:-
azure-functions
protobuf==3.20.*

My init.py:-
import logging

import azure.functions as func


def main(req: func.HttpRequest) -&gt; func.HttpResponse:
    logging.info('Python HTTP trigger function processed a request.')

    name = req.params.get('name')
    if not name:
        try:
            req_body = req.get_json()
        except ValueError:
            pass
        else:
            name = req_body.get('name')

    if name:
        return func.HttpResponse(f""Hello, {name}. This HTTP triggered function executed successfully."")
    else:
        return func.HttpResponse(
             ""This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response."",
             status_code=200
        )


My YAML pipeline:-
trigger:
- master

variables:
  
  azureSubscription: 'xxxxxxxx-xxxxx-xxx9bbd4354dd'

 
  functionAppName: 'valleyfunc541'

 
  vmImageName: 'ubuntu-latest'

 
  workingDirectory: '$(System.DefaultWorkingDirectory)'

stages:
- stage: Build
  displayName: Build stage

  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName)

    steps:
    - bash: |
        if [ -f extensions.csproj ]
        then
            dotnet build extensions.csproj --runtime ubuntu.16.04-x64 --output ./bin
        fi
      workingDirectory: $(workingDirectory)
      displayName: 'Build extensions'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.10'
      inputs:
        versionSpec: 3.10 # Functions V2 supports Python 3.6 as of today

    - bash: |
        pip install --target=""./.python_packages/lib/site-packages"" -r ./requirements.txt
      workingDirectory: $(workingDirectory)
      displayName: 'Install application dependencies'

    - task: ArchiveFiles@2
      displayName: 'Archive files'
      inputs:
        rootFolderOrFile: '$(workingDirectory)'
        includeRootFolder: false
        archiveType: zip
        archiveFile: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
        replaceExistingArchive: true

    - publish: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip
      artifact: drop

- stage: Deploy
  displayName: Deploy stage
  dependsOn: Build
  condition: succeeded()

  jobs:
  - deployment: Deploy
    displayName: Deploy
    environment: 'development'
    pool:
      vmImage: $(vmImageName)

    strategy:
      runOnce:
        deploy:

          steps:
          - task: AzureFunctionApp@1
            displayName: 'Azure functions app deploy'
            inputs:
              azureSubscription: '$(azureSubscription)'
              appType: functionAppLinux
              appName: $(functionAppName)
              package: '$(Pipeline.Workspace)/drop/$(Build.BuildId).zip'

Output:-

The HTTP Trigger got deployed successfully:-


Reference:- My SO thread answer
"
Keras unexpected result,https://stackoverflow.com/questions/71512318,"I get this Error while trying to Train a Model, Tensorflow ValueError: Unexpected result of `train_function` (Empty logs)",,0,2050,"Can you try a dense layer with a single neuron with sigmoid activation as output (and binary crossentropy as loss)?
",,
Keras unexpected result,https://stackoverflow.com/questions/62529415,TensorFlow 2.0 - Begineer Implementing simple CNN,,0,268,"According to this section of the output :
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Invalid JPEG data or crop window, data size 101360
         [[{{node DecodeJpeg}}]]
         [[IteratorGetNext]]
  (1) Invalid argument:  Invalid JPEG data or crop window, data size 101360
         [[{{node DecodeJpeg}}]]
         [[IteratorGetNext]]
         [[IteratorGetNext/_2]]

It seems like if some of your data are corrupted, check your data and remove the corrupted entries, that is a common error that you can check HERE and THERE
",,
Keras unexpected result,https://stackoverflow.com/questions/72126226,"model.fit in a for loop, for K-fold cross validation","I am trying to code a K-fold cross validation with LSTM architecture.
But I got an this error (edit):
Traceback (most recent call last):
  File ""/Users/me/Desktop/dynamicsoundtreatments/DST-features-RNN.py"", line 58, in &lt;module&gt;
    model.fit(training_data, training_label, epochs=100, batch_size=nbr_de_son)
  File ""/Users/me/miniforge3/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/Users/me/miniforge3/lib/python3.9/site-packages/keras/engine/training.py"", line 1395, in fit
    raise ValueError('Unexpected result of `train_function` '
ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

I tried to add run_eagerly=True but got the same error again.
I tried few alternatives such as def train(training_data, training_label): model.fit(training_data, training_label, epochs=100, batch_size=nbr_de_son) outside of the for loop. Got same error.
I was wondering if I should use Functional API, but I am very new to datascience. I really don't why I got this error. Thanks for your help.
nbr_anal = int(6)
nbr_de_son = int(samples.shape[0]/nbr_anal)
sequence = int(samples.shape[1])
input_shape=(nbr_anal, sequence)

# ------------------------------------------------------------------------
# PREPROCESSING

# batch size, sequence length, features
samples = samples.reshape(nbr_de_son, nbr_anal, sequence)
labels_extrait = np.argmax(labels_extrait, axis=1)
print(labels_extrait.shape)

# ------------------------------------------------------------------------
# K-Fold
k = 4
num_validation_samples =  len(samples) // k
num_validation_labels = len(labels_extrait) // k
validation_scores = []

model = Sequential()
model.add(LSTM(sequence,input_shape=input_shape))
model.add(Dropout(0.3))
model.add(Dense(8, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='sparse_categorical_accuracy', run_eagerly=True)

for fold in range(k):
    rng_state = np.random.get_state()
    np.random.shuffle(samples)
    np.random.set_state(rng_state)
    np.random.shuffle(labels_extrait)

    validation_data = samples[num_validation_samples * fold:num_validation_samples * (fold + 1)]
    print(validation_data.shape)
    validation_label = labels_extrait[num_validation_labels * fold:num_validation_labels * (fold + 1)]
    print(validation_label.shape)

    training_data = samples[:num_validation_samples * (fold + 1)] + samples[num_validation_samples * (fold)]
    training_label = labels_extrait[:num_validation_labels * (fold + 1)] + labels_extrait[num_validation_labels * (fold)]

    model.fit(training_data, training_label, epochs=100, batch_size=nbr_de_son)

    validation_score = evaluate(validation_data, validation_label)
    validation_scores.append(validation_score)

validation_score = np.average(validation_scores)
print(validation_score)

",0,1871,"You can use StratifiedKFold from the sklearn package to do the cross validation. It is much clearer and is the standard way to do it. You should also reset the model weights at each fold before fitting the model, otherwise you will start with the weights initialized in the previous call of the fit method,
The modified code :
from sklearn.model_selection import StratifiedKFold
# K-Fold cross validation
k = 4
skf = StratifiedKFold(n_splits=k)
validation_scores = []

# store initial model's weights
weights_init = model.get_weights()

for train_index, test_index in skf.split(samples, labels_extrait):
    training_data = samples[train_index]
    training_label = labels_extrait[train_index]
    validation_data = samples[test_index]
    validation_label = labels_extrait[test_index]

    # reset mdoel's weights
    model.set_weights(weights_init)
    # fit
    model.fit(training_data, training_label, epochs=100, batch_size=nbr_de_son)

    validation_score = model.evaluate(validation_data, validation_label)
    validation_scores.append(validation_score)


validation_score = np.average(validation_scores)
print(validation_score) 

I don't know exactly where your error comes from, but the above code works
",,
Keras unexpected result,https://stackoverflow.com/questions/72023307,How do I solve LSTM error on predict functions,"This is the code attached below. I'm trying to run it and am getting this error below. Need help resolving it. Graph is defined as the variable name.
graph = Sequential()
graph.add(LSTM(50, return_sequences=True, input_shape = (60, 1)))
graph.add(LSTM(50, return_sequences=False))
graph.add(Dense(25))
graph.add(Dense(1))

graph.compile(optimizer='adam', loss='mean_squared_error')
graph.fit(x_train, y_train, batch_size=1, epochs=1)

test_data = train_data[training_data_length - 60: , : ]
x_test = []
y_test =  dataset[training_data_length : , : ]
for i in range(60,len(test_data)):
    x_test.append(test_data[i-60:i,0])
x_test = np.array(x_test)
x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[0],1))
predictions = graph.predict(x_test)
predictions = transfer.inverse_transform(predictions)

Getting this error
ValueError                                Traceback (most recent call last)
&lt;ipython-input-40-4624ea9a4af1&gt; in &lt;module&gt;()
      6 x_test = np.array(x_test)
      7 x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[0],1))
----&gt; 8 predictions = graph.predict(x_test)
      9 predictions = transfer.inverse_transform(predictions)

1 frames
/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
   1995             callbacks.on_predict_batch_end(end_step, {'outputs': batch_outputs})
   1996       if batch_outputs is None:
-&gt; 1997         raise ValueError('Unexpected result of `predict_function` '
   1998                          '(Empty batch_outputs). Please use '
   1999                          '`Model.compile(..., run_eagerly=True)`, or '

ValueError: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

",0,683,"Your test_data always have length of 60 as per your construction
test_data = train_data[training_data_length - 60: , : ]

So, your for loop is never carried out as it is always equal to for i in range(60,60). Consequently, x_test remains [] and causes the error.
I'm guessing that you want to construct a set of length-60 series, each offset the next by 1 timestep. In that case, the test data should've been constructed as
for i in range(60,len(train_data)):
    x_test.append(train_data[i-60:i,0])

Also, your np.reshape usage seems wrong. The number of elements after reshape will be x_test.shape[0] * x_test.shape[0], but the original x_test have x_test.shape[0] * x_test.shape[1], so unless x_test.shape[0] == x_test.shape[1], you'll always get an error. You don't need reshape anyway, as your x_test should've already been in the right shape.
",,
Keras unexpected result,https://stackoverflow.com/questions/71951540,"ValueError: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`,","I tried using GridSearchCV() but I also got errors so I wrote a very time consuming code that returns the metrics I want to evaluate my model. Here it is :
def df_to_new(df,window_size): 
    df_as_np = df.to_numpy()
    X = []
    y = []
    for i in range(len(df_as_np)-window_size):
        row = [[a] for a in df_as_np[i:i+window_size]]
        X.append(row)
        label = df_as_np[i+window_size]
        y.append(label)
        
    return np.array(X),np.array(y)  

from tensorflow.keras.models import load_model
import os
checkpointpath = 'C:\\Users\\USER\\trainingFORLOOP_daily/cp.ckt'
cp = ModelCheckpoint(checkpointpath, save_best_only=True,verbose=1)     
EPOCH = [30,150,300]
learningRates = [0.0001,0.00001]
batchSize = [15,20,40]
win_size = [5,15,25]
dropout_rate = 0.2
num_features = 1
for i in learningRates:
    for j in EPOCH:
        for k in batchSize:
            for l in win_size:
                X,y = df_to_new(Ac13,l)

                #Split the data
                perc_train = 0.8
                limit_train = int(np.floor(len(Ac13)*perc_train))
                xtrain,ytrain = X[:limit_train],y[:limit_train]
                xval,yval = X[limit_train:],y[limit_train:]
                
                #create the model
                model1 = Sequential() 
                model1.add(InputLayer((l,1))) 
                model1.add(LSTM(128))
                model1.add(Dropout(dropout_rate))
                model1.add(Dense(86,'relu'))
                model1.add(Dropout(dropout_rate))
                model1.add(Dense(1,'linear'))
                model1.summary()
                model1.compile(loss=MeanSquaredError(),optimizer = 
                       Adam(learning_rate=i),
                       metrics=[RootMeanSquaredError()],run_eagerly=True)
                model1.fit(xtrain,ytrain,validation_data=(xval,yval),batch_size=k,epochs=j,callbacks=[cp],shuffle=False)
                model1.save(""my_model"")
                model1 = load_model(""my_model"")
                           
                train_predictions = model1.predict(xtrain).flatten() 
                train_results = pd.DataFrame(data={'TrainPredictions':train_predictions,'Actual values':ytrain})
                train_results
                scale = len(train_predictions)
                val_predictions = model1.predict(xval).flatten()
                val_results = pd.DataFrame(data='ValidatePredictions':val_predictions,'Validation values':yval}) 

                

I am getting the following error (Full traceback):
    ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_19052/292377201.py in &lt;module&gt;
     51                 plt.legend(bbox_to_anchor =(0.75, 1.15), ncol = 2)
     52                 plt.show()
---&gt; 53                 val_predictions = model1.predict(xval).flatten() # flatten() removes the brackets inside the data
     54                 val_results = pd.DataFrame(data={'Validate Predictions':val_predictions,'Validation values':yval}) #yval are the actual values
     55                 val_results

~\anaconda3\envs\tf-gpu-cuda8\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---&gt; 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

~\anaconda3\envs\tf-gpu-cuda8\lib\site-packages\keras\engine\training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
   1995             callbacks.on_predict_batch_end(end_step, {'outputs': batch_outputs})
   1996       if batch_outputs is None:
-&gt; 1997         raise ValueError('Unexpected result of `predict_function` '
   1998                          '(Empty batch_outputs). Please use '
   1999                          '`Model.compile(..., run_eagerly=True)`, or '

ValueError: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.

Any suggestions?
I did the same approach but with hourly data and it worked quite well without any errors. The hourly code is the same as the daily code(this one) the only thing that changed is that the data in the hourly code was summed in a day and the daily code data was obtained.
",0,2549,"There are a few things wrong with your code.

You are creating your model inside the training loop. So thats wrong. Unless you want to train a new version at each epoch. Your model should be defined and compiled before you start training.

Depending on which version of tensorflow you you are using you should enable eager execution before you declare the model. That lets you train the model without creating a session and a scope.


Hope this helps!
",,
Keras unexpected result,https://stackoverflow.com/questions/67448600,Error while loading .h5 model in Flask using keras,"I have built a horse human detector using keras CNN on Google colab the model worked and loaded perfectly on colab. Now I am building a flask application while loading he .h5 model file I was getting error
TypeError: __init__() got an unexpected keyword argument 'ragged'

I reinstall keras  2.3.1 using pip and now I am getting a library error
 NameError: name 'six' is not defined

my App.py
#Import necessary libraries
from flask import Flask, render_template, request

import numpy as np
import os

from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.models import load_model
 

#load model
model = load_model(""predictor.h5"" )
print(""@@ model loaded"")

def pred_human_horse(model , horse_or_human):
  test_image = load_img(horse_or_human , target_size=(150,150)) #resize
  print(""@@ Got Image for predicton"")
  test_image = img_to_array(test_image)/255 #numpy array  between 0-1
  test_image = np.expand_dims(test_image,axis=0) #4 dimension

  result= model.predict(test_image).round(3) #rounding off
  pred =np.argmax(result)
  print(""@@ Raw results = "",result)
  print(""@@ class = "", pred)

  if pred==0:
    return ""Horse""

  else:
    return ""Human""

# Crate flask app
app = Flask(__name__)
    
@app.route(""/"",methods=[""GET"",""POST""])
def home():
    return render_template(""index.html"")


@app.route(""/predict"",methods=[""GET"",""POST""])
def predict():
    if request.method==""POST"":
        #get input image file
        file = request.files[""image""]
        filename= file.filename
        print(""@@ File recieved"",filename)
        
        #save the file
        file_path= os.path.join(""static/user_uploaded"",filename)
        file.save(file_path)
        
        print(""@@ Prediction..."")
        pred=pred_human_horse(horse_or_human=file_path )
        
        return render_template(""predict.html"" ,pred_output= pred , user_image=file_path )


if __name__==""__main__"":
    app.run(threaded=False)

Error I am getting
runfile('F:/INTERNSHIP/Crisp-Metric-MAY21/Human-horse-prediction/app.py', wdir='F:/INTERNSHIP/Crisp-Metric-MAY21/Human-horse-prediction')
Traceback (most recent call last):

  File ""&lt;ipython-input-26-df590f092cb6&gt;"", line 1, in &lt;module&gt;
    runfile('F:/INTERNSHIP/Crisp-Metric-MAY21/Human-horse-prediction/app.py', wdir='F:/INTERNSHIP/Crisp-Metric-MAY21/Human-horse-prediction')

  File ""C:\Users\DANIA NIAZI\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""C:\Users\DANIA NIAZI\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""F:/INTERNSHIP/Crisp-Metric-MAY21/Human-horse-prediction/app.py"", line 13, in &lt;module&gt;
    model = load_model(""predictor.h5"" )

  File ""C:\Users\DANIA NIAZI\Anaconda3\lib\site-packages\keras\engine\saving.py"", line 492, in load_wrapper

  File ""C:\Users\DANIA NIAZI\Anaconda3\lib\site-packages\keras\engine\saving.py"", line 582, in load_model

  File ""C:\Users\DANIA NIAZI\Anaconda3\lib\site-packages\keras\utils\io_utils.py"", line 211, in is_supported_type

NameError: name 'six' is not defined

",0,503,"Maybe you should try installing the six package which will be installed when installing Django. Anyway you can install it using:
pip install six
",,
Keras unexpected result,https://stackoverflow.com/questions/63427411,TypeError: __init__() got an unexpected keyword argument &#39;ragged&#39;? when load model,"I'm using keras to load the model which stored in h5 file, but got this error, I tried to google around but got no result.
please help.
face_classifier  = cv2.CascadeClassifier(cv2.data.haarcascades +'haarcascade_frontalface_alt.xml')
classifier = load_model('facial_expression.h5')

class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']

cap = cv2.VideoCapture(0)

while True:

ret, frame = cap.read()
labels = []

gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
faces = face_classifier.detectMultiScale(gray,1.3,5)

for (x,y,w,h) in faces:
    cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)
    roi_gray = gray[y:y+h,x:x+w]
    roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)
# rect,face,image = face_detector(frame)


    if np.sum([roi_gray])!=0:
        roi = roi_gray.astype('float')/255.0
        roi = img_to_array(roi)
        roi = np.expand_dims(roi,axis=0)

    # make a prediction on the ROI, then lookup the class

        preds = classifier.predict(roi)[0]
        label=class_labels[preds.argmax()]
        label_position = (x,y)
        cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)
    else:
        cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)
cv2.imshow('Emotion Detector',frame)
if cv2.waitKey(1) &amp; 0xFF == ord('q'):
    break

cap.release()
cv2.destroyAllWindows()

error like this :
File ""C:\Users\ADMIN\anaconda3\envs\myenv\lib\site-packages\keras\utils\generic_utils.py"", line 147, in deserialize_keras_object
list(custom_objects.items())))
File ""C:\Users\ADMIN\anaconda3\envs\myenv\lib\site-packages\keras\engine\sequential.py"", line 301, in from_config
custom_objects=custom_objects)
File ""C:\Users\ADMIN\anaconda3\envs\myenv\lib\site-packages\keras\layers_init_.py"", line 168, in deserialize
printable_module_name='layer')
File ""C:\Users\ADMIN\anaconda3\envs\myenv\lib\site-packages\keras\utils\generic_utils.py"", line 149, in deserialize_keras_object
return cls.from_config(config['config'])
File ""C:\Users\ADMIN\anaconda3\envs\myenv\lib\site-packages\keras\engine\base_layer.py"", line 1179, in from_config
return cls(**config)
File ""C:\Users\ADMIN\anaconda3\envs\myenv\lib\site-packages\keras\legacy\interfaces.py"", line 91, in wrapper
return func(*args, **kwargs)
TypeError: init() got an unexpected keyword argument 'ragged'
",0,1067,"Instead of importing load_model from keras ,Import it from tf.keras
from tf.keras.models import load_model
classifier = load_model('facial_expression.h5')

",,
Keras unexpected result,https://stackoverflow.com/questions/60193964,Create generator that shuffle training data for Keras in R/train a Keras model with lowspec computer,"I have a dataset similar to MNIST (200.000 rows of 784 pixels + 1 categorical output (785 columns)) and I want to train a MLP and a CNN using the Keras library in R (in RStudio). I'm currently using a computer with 32 GB of RAM with an intel i7-8700 @3.2 GHz and I have not any problem when training this neural networks using the fit() function from Keras (training time ~4 minutes). However, when I execute the same script in my laptop (8 GB of RAM with intel i5-6300 @2.3 GHz) it can not even do an epoch in least than 10 minutes. 

I work as a laboratory professor in university and I'm worried that my students could not run the script with that database in their laptops due to lack of computation power. My idea was to fit the same models using a generator with the fit_generator() function, and load part of the dataset instead of the whole dataset in each call to the generator function (in order to use less memory than loading the entire dataset and resulting in a faster training). However, this produce some unexpected results. The accuracy reached by the fit() function is ~98.8% in training (120.000 rows) and ~98.4% in test (80.000 rows), but using the fit_generator() function is ~1.05% in the same training and ~1.01% in the same test. I have found related issues  here, here, here and here, and it seems that the problem is that fit_generator() does not shuffle the training data and this produce that the network is always trained with the same batches (e.g. same gradient when computing backpropagation) and does not represent well the entire dataset, resulting in that poor accuracy. I've trained the model using fit() but setting the shuffle argument to FALSE, and the accuracy falls to 0.1% so it confirms that shuffling the training data is crucial to train the model.

My questions are:


Is it a good idea to use a generator to avoid problems/reduce training time when using a computer with lower specifications or is there a better solution? 
I'm training the models using the entire dataset by setting steps_per_epoch argument equal to ceil(nrow(train_dataset)/batch_size), so it should use the same data when using fit() and fit_generator() except from the ""shuffling"" part right?
In case that using a generator that loads part of the dataset is a good solution to train the models with low-spec computers, how can i shuffle the training data in an effective way using a generator?


All the generators i've seen takes the entire dataset and produce a batch of the samples in each call or does not shuffle the data. I have created a generator with the code below. It takes as arguments datafile the file with the data (training data or test data), batch_size the size of the batch to produce in each call, mlp in order to process the data to train an MLP or a CNN, val in order to begin to produce batches in different index for validation data, and shuffle to indicate if we want to shuffle the data or not. My idea to shuffle the data was to create a random index and read only one row of the file for each number in the index (using the skip and nrow arguments in read.table()). This is extremely unefficient due to the several calls to read.table():

data_generator &lt;- function(datafile, batch_size = 128, mlp = TRUE, val = TRUE, shuffle = TRUE) {
  nrow_file &lt;- R.utils::countLines(datafile) - 1
  if (!val) {
    skip &lt;- 0 
  } else {
    skip &lt;- nrow_file / 2
  }
  function() {
    # Calculate the rows to read in this epoch
    rows_to_read &lt;- batch_size
    if (skip + batch_size &gt; nrow_file) {
      rows_to_read &lt;- nrow_file - skip
    }

    if (shuffle) {
      index &lt;- sample (c(1:nrow_file), size=batch_size, replace =F)
    } else {
      index &lt;- (skip + 1):(skip + rows_to_read)
    }

    # Load only the rows that we want to use in training
    trData &lt;- as.list(numeric(batch_size))
    for(i in index) {
      ii &lt;- i - 1 
      trData[[which(i == index)]] &lt;- read.table(datafile, sep = "";"", header = TRUE,
                              skip = ii, nrows = 1)
    }
    trData &lt;- do.call(""rbind"",trData)
    # Upload the rows to train
    skip &lt;&lt;- skip + batch_size
    if (skip &gt;= nrow_file) {
      skip &lt;&lt;- 0
    }
    # Build inputs and output
    y_train &lt;- trData[,1]
    x_train &lt;- trData[,-1]
    if (mlp) {
      # Return data as is for mlp
      list(data.matrix(x_train), data.matrix(y_train))  
    } else {
      # Return data reshaped for CNN
      list(array_reshape(data.matrix(x_train), c(nrow(x_train), 28, 28, 1)),
           data.matrix(y_train))
    }
  }
}


The code I've used to train the MLP model (analogous to CNN) is:

Without generator

MLP_model &lt;- keras_model_sequential()

MLP_model %&gt;% 
  layer_dense(units = 500, activation = 'relu', input_shape = c(784),
              kernel_regularizer = regularizer_l2(l = 0.0001),
              bias_regularizer = regularizer_l2(l = 0.0001)) %&gt;% 
  layer_dropout(rate = 0.4, seed = 150)  %&gt;% 
  layer_batch_normalization() %&gt;%
  layer_dense(units = 300, activation = 'relu',
              kernel_regularizer = regularizer_l2(l = 0.001),
              bias_regularizer = regularizer_l2(l = 0.001)) %&gt;%
  layer_dropout(rate = 0.3, seed = 150) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dense(units = 10, activation = 'softmax',
              kernel_regularizer = regularizer_l2(l = 0.001),
              bias_regularizer = regularizer_l2(l = 0.001))


MLP_model %&gt;% compile(
  loss = loss_categorical_crossentropy,
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

history &lt;- MLP_model %&gt;% fit(
  x_train_mlp, y_train, 
  epochs = 20, batch_size = 124, 
  validation_split = 0.2, 
  shuffle = TRUE
)


With generator:

MLP_model &lt;- keras_model_sequential()

MLP_model %&gt;% 
  layer_dense(units = 500, activation = 'relu', input_shape = c(784),
              kernel_regularizer = regularizer_l2(l = 0.0001),
              bias_regularizer = regularizer_l2(l = 0.0001)) %&gt;% 
  layer_dropout(rate = 0.4, seed = 150)  %&gt;% 
  layer_batch_normalization() %&gt;%
  layer_dense(units = 300, activation = 'relu',
              kernel_regularizer = regularizer_l2(l = 0.001),
              bias_regularizer = regularizer_l2(l = 0.001)) %&gt;%
  layer_dropout(rate = 0.3, seed = 150) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dense(units = 10, activation = 'softmax',
              kernel_regularizer = regularizer_l2(l = 0.001),
              bias_regularizer = regularizer_l2(l = 0.001))

MLP_model %&gt;% compile(
  loss = loss_categorical_crossentropy,
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

history &lt;- MLP_model %&gt;% fit_generator(
  data_generator(traindatafile,
                 batch_size = 128, mlp = TRUE, val = FALSE), 
  steps_per_epoch = round((R.utils::countLines(traindatafile)-1) / (128)),
  epochs = 10)


Thanks in advance!
",0,869,"answering my own question and thanks to @user12728748 for the comments, I change the generator to read random samples from the file:

data_generator &lt;- function(datafile, batch_size = 128, 
                           mlp, val, 
                           shuffle = TRUE, validation_split = 0) {
  nrow_file &lt;- py_eval(paste(""sum(1 for line in open('"", datafile, ""'))"", sep = '')) - 1
  skip &lt;- 0 

  if (val) {
    nrow_file &lt;- validation_split * nrow_file
  } else {
    nrow_file &lt;- (1 - validation_split) * nrow_file
  }


  if (nrow_file &gt; 0) {
    function() {
        # Calculate the rows to read in this epoch
        rows_to_read &lt;- batch_size
        if (skip + batch_size &gt; nrow_file) {
          rows_to_read &lt;- nrow_file - skip
        }

        if (shuffle) {
          index &lt;- sample (c(1:nrow_file), size=batch_size, replace =F)
        } else {
          index &lt;- (skip + 1):(skip + rows_to_read)
        }

        # Create rows to skip
        if (val) {
          # in validation, skip training rows and validation rows that are not found in index
          rows_to_skip &lt;- c(1:ifelse(validation_split &gt; 0,((1 - validation_split) * nrow_file / validation_split),1),
                            setdiff(1:nrow_file, index) + (1 - validation_split) * nrow_file / validation_split)
        } else {
          # in training, skip validation rows and training rows that are not found in index
          rows_to_skip &lt;- c(ifelse(validation_split &gt; 0,
                                   nrow_file + 1, 0):ifelse(validation_split &gt; 0,
                                                            nrow_file/(1 - validation_split), 0),
                            setdiff(1:nrow_file, index))
          if (rows_to_skip[1] == 0) rows_to_skip &lt;- rows_to_skip[-1]
        }


        trData &lt;- import(""pandas"")$read_csv(datafile, 
                                            skiprows = rows_to_skip, 
                                            sep = "";"")

        # Upload the rows to train
        skip &lt;&lt;- skip + batch_size
        if (skip &gt;= nrow_file) {
          skip &lt;&lt;- 0
        }
        # Build inputs and output
        y_train &lt;- to_categorical(trData[,1], num_classes = 10)
        x_train &lt;- trData[,-1]
        if (mlp) {
          # Return data as is for mlp
          list(data.matrix(x_train), data.matrix(y_train))  
        } else {
          # Return data reshaped for CNN
          list(array_reshape(data.matrix(x_train), c(nrow(x_train), 28, 28, 1)),
               data.matrix(y_train))
        }
      }
  } else {
    NULL
  }
}


I have added the validation_split argument to set the percentage of the training data that we want to use as validation. 

However, using the generator to train the model does not seem to improve training time when using the lowspec computer, but it uses almost 4 GB less of RAM (especially when training with small batches (~128 samples)) and you can use the computer to perform other tasks while executing the code without crashing the program. 

Here I leave you the code to train and evaluate a MLP model using the generator:

MLP_model &lt;- keras_model_sequential()

MLP_model %&gt;% 
  layer_dense(units = 500, activation = 'relu', input_shape = c(784),
              kernel_regularizer = regularizer_l2(l = 0.0001),
              bias_regularizer = regularizer_l2(l = 0.0001)) %&gt;% 
  layer_dropout(rate = 0.4, seed = 150)  %&gt;% 
  layer_batch_normalization() %&gt;%
  layer_dense(units = 300, activation = 'relu',
              kernel_regularizer = regularizer_l2(l = 0.001),
              bias_regularizer = regularizer_l2(l = 0.001)) %&gt;%
  layer_dropout(rate = 0.3, seed = 150) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dense(units = 10, activation = 'softmax',
              kernel_regularizer = regularizer_l2(l = 0.001),
              bias_regularizer = regularizer_l2(l = 0.001))

MLP_model %&gt;% compile(
  loss = loss_categorical_crossentropy,
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

validation_split &lt;- 0.2
history &lt;- MLP_model %&gt;% fit_generator(
  data_generator(traindatafile,
                 batch_size_train, mlp = TRUE, 
                 val = FALSE, validation_split = validation_split), 
  steps_per_epoch = round((1 - validation_split) * (py_eval(paste(""sum(1 for line in open('"", traindatafile, ""'))"", sep = '')) - 1) / (batch_size_train)),
  validation_data = data_generator(traindatafile,
                                   batch_size_train, mlp = TRUE, 
                                   val = TRUE, validation_split = validation_split),
  validation_steps = round((validation_split) * (py_eval(paste(""sum(1 for line in open('"", traindatafile, ""'))"", sep = '')) - 1) / (batch_size_train)),
  callbacks = c(early_stopping),
  epochs = 10)

MLP_metrics_train &lt;- MLP_model %&gt;% 
  evaluate_generator(data_generator(traindatafile,
                                    batch_size_eval, 
                                    mlp = TRUE, 
                                    val = FALSE,
                                    shuffle = FALSE),
                     steps = ceiling((py_eval(paste(""sum(1 for line in open('"", traindatafile, ""'))"", sep = '')) - 1) / (batch_size_eval)))
MLP_metrics_test &lt;- MLP_model %&gt;% 
  evaluate_generator(data_generator(testdatafile,
                                    batch_size_eval, 
                                    mlp = TRUE, 
                                    val = FALSE,
                                    shuffle = FALSE),
                     steps = ceiling((py_eval(paste(""sum(1 for line in open('"", testdatafile, ""'))"", sep = '')) - 1) / (batch_size_eval)))


y_pred_mlp &lt;- MLP_model %&gt;% 
  predict_generator(data_generator(testdatafile,
                                   batch_size_eval, 
                                   mlp = TRUE, 
                                   val = FALSE,
                                   shuffle = FALSE),
                    steps = ceiling((py_eval(paste(""sum(1 for line in open('"", testdatafile, ""'))"", sep = '')) - 1) / (batch_size_eval)))


Hope this helps someone!
",,
Keras unexpected result,https://stackoverflow.com/questions/46919205,Keras - Unexpected prediction results,"I'm trying to create a keras LSTM to predict time series. My x_train is shaped like 3000,15,10 (Examples, Timesteps, Features), y_train like 3000,15,1 and I'm trying to build a many to many model (10 input features per sequence make 1 output / sequence).
The code I'm using is this:

model = Sequential()

model.add(LSTM(
    10,
    input_shape=(15, 10),
    return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(
    100,
    return_sequences=True))
model.add(Dropout(0.2))
model.add(Dense(1, activation='linear'))
model.compile(loss=""mse"", optimizer=""rmsprop"")
model.fit(
        x_train, y_train,
        batch_size=1, nb_epoch=1, validation_split=0.05)
predicted = model.predict(x_test)

My data looks like this:
x_train = [
    [
        [3,4,3,1.5,0,7....], # 10 Features
        [2,4,1,1,4,3....],
        .... # 15 Timesteps
    ],
    .... # 3000 Examples
]
y_train = [
    [
        [0],
        [5],
        ....
    ],
    .....
]

The code compiles and fits without errors but the predicted values are not realistic.
The expected values are integers between 0 and ~15. However, the predictions are floats between ~-0.03 and 1. Also, the predictions have 15 timesteps but I am actually interested in the 16th timestep (predicted future result).
Example predictions:
predicted = [
    [
        [0.035478],
        [0.046423],
        [0.948723],
        ....
    ],
    ....
]

My Keras version is 2.0.6 and Python is version 3.4.5.
",0,759,,,
Keras unexpected result,https://stackoverflow.com/questions/46958739,Error trying to pass list to hyperas as a parameter,,-1,2112,,,
Keras unexpected issue,https://stackoverflow.com/questions/52466932,restore_best_weights issue keras early stopping,"I am using EarlyStopping from Keras for my deep learning project. The documentations here mentions a very useful idea of restoring best weights. But somehow I am not able to use it yet. I am using Keras 2.2.2/TF 1.10, installed using Anaconda. 
Call is simple as follows. is there any issue? 

es = EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=patience_,verbose=1,restore_best_weights=True)



  __init__() got an unexpected keyword argument 'restore_best_weights'

",12,12680,"Ah, a very common problem. The keras documentation online is produced from the master branch in github, so features that have been introduced only recently are present there, but not yet in any Keras release.

So the restore_best_weights option of the EarlyStopping callback was added on August 18, 2018, while currently the latest Keras release (2.2.2) was made on July 28, 2018. So you will have to use Keras master, wait for a Keras release, or just not use the option.

Update: It is now included in Keras 2.2.3 release.
",,
Keras unexpected issue,https://stackoverflow.com/questions/61919774,unexpected keyword argument &#39;sample_weight&#39; when sub-classing tensor-flow loss class (categorical_crossentropy) to created a weighted loss function,,7,4667,"As per the comments; the issue here is that TensorFlow is now enforcing inheriting from the original method signature.

The following has been tested (by comparing equal weighting in the cost_matrix to weighting all but a single category to nothing) on a toy problem and works:

class WeightedCategoricalCrossentropy(tf.keras.losses.CategoricalCrossentropy):

  def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):

    cost_mat = np.array(cost_mat)   
    ## when loading from config, self.cost_mat returns as a list, rather than an numpy array. 
    ## Adding the above line fixes this issue, enabling .ndim to call sucessfully. 
    ## However, this is probably not the best implementation
    assert(cost_mat.ndim == 2)
    assert(cost_mat.shape[0] == cost_mat.shape[1])
    super().__init__(name=name, **kwargs)
    self.cost_mat = K.cast_to_floatx(cost_mat)

  def __call__(self, y_true, y_pred, sample_weight=None):
    assert sample_weight is None, ""should only be derived from the cost matrix""  
    return super().__call__(
        y_true=y_true, 
        y_pred=y_pred, 
        sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),
    )


  def get_config(self):
    config = super().get_config().copy()
    # Calling .update on the line above, during assignment, causes an error with config becoming None-type.
    config.update({'cost_mat': (self.cost_mat)})
    return config

  @classmethod
  def from_config(cls, config):
    # something goes wrong here and changes self.cost_mat to a list variable.
    # See above for temporary fix
    return cls(**config)

def get_sample_weights(y_true, y_pred, cost_m):
    num_classes = len(cost_m)

    y_pred.shape.assert_has_rank(2)
    assert(y_pred.shape[1] == num_classes)
    y_pred.shape.assert_is_compatible_with(y_true.shape)

    y_pred = K.one_hot(K.argmax(y_pred), num_classes)

    y_true_nk1 = K.expand_dims(y_true, 2)
    y_pred_n1k = K.expand_dims(y_pred, 1)
    cost_m_1kk = K.expand_dims(cost_m, 0)

    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k
    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])

    return sample_weights_n


# Register the loss in the Keras namespace to enable loading of the custom object.
tf.keras.losses.WeightedCategoricalCrossentropy = WeightedCategoricalCrossentropy


Usage

Where cost_matrix is a 2D NumPy array, eg:

[
 [ Weight Category 1 predicted as Category 1, 
   Weight Category 1 predicted as Category 2,
   Weight Category 1 predicted as Category 3 ]
 [ Weight Category 2 predicted as Category 1,
   ...,
   ...                                       ]
 [ ...,
   ...,
   Weight Category 3 predicted as Category 3 ]
]




model.compile(
     optimizer='adam',
     loss=WeightedCategoricalCrossentropy(cost_matrix)
     )


Model Saving

model.save(save_version_dir,save_format='tf')


Model Loading

model = tf.keras.models.load_model(
    save_version_dir,
    compile=True,
    custom_objects={
        'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_matrix)
        }
    )

",,
Keras unexpected issue,https://stackoverflow.com/questions/62518906,Keras ImageDataGenerator : how to use data augmentation with images paths,,2,2679,"Ok so I finally found out how to deal with these issues thanks to this article. My mistake was that I kept using ImageDataGenerator despite its lack of flexibility, the solution is thus simple : use another data augmentation tool.
We can resume the author's method as following :

First, create a personalized batch generator as a subclass of Keras Sequence class (which implies to implement a __getitem__ function that loads the images according to their respective paths).
Use the data augmentation albumentations library. It has the advantages of offering more transformation functions as Imgaug or ImageDataGenerator, while being faster. Moreover, this website allows you to test some of its augmentation methods, even with your own images ! See this one for the exhaustive list.


The drawback of this library is that, as it is relatively new, few documentation can be found online, and I've spent several hours trying to resolve an issue I encountered.
Indeed, when I tried to visualize some augmentation functions, the results were entirely black images (strange fact : these would happen only when I was modifying the intensity of the pixels, with methods like RandomGamma or RandomBrightnessContrast. With transformation functions such as HorizontalFlip or ShiftScaleRotate, it would work normally).
After an entire half day of trying-to-find-what's-wrong, I eventually came up with this solution, that might help you if you were to try this library : the loading of images has to be done with OpenCV (I was using load_img and img_to_array functions from tf.keras.preprocessing.image for the loading and processing). If anyone has an explanation of why this doesn't work, I'd be glad to hear it.
Anyway, here is my final code to display an augmented image :
!pip install -U git+https://github.com/albu/albumentations &gt; /dev/null &amp;&amp; echo ""All libraries are successfully installed!""
from albumentations import Compose, HorizontalFlip, RandomBrightnessContrast, ToFloat, RGBShift
import cv2
import matplotlib.pyplot as plt
import numpy as np
from google.colab.patches import cv2_imshow # I work on a Google Colab, thus I cannot use cv2.imshow()


augmentation = Compose([HorizontalFlip(p = 0.5),
                        RandomBrightnessContrast(p = 1),
                        ToFloat(max_value = 255) # Normalize the pixels values into the [0,1] interval
                        # Feel free to add more !
                        ])

img = cv2.imread('Your_path_here.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # cv2.imread() loads the images in BGR format, thus you have to convert it to RGB before applying any transformation function.
img = augmentation(image = img)['image'] # Apply the augmentation functions to the image.
plt.figure(figsize=(7, 7))
plt.imshow((img*255).astype(np.uint8)) # Put the pixels values back to [0,255]. Replace by plt.imshow(img) if the ToFloat function is not used.
plt.show()


'''
If you want to display using cv2_imshow(), simply replace the last three lines by :

img = cv2.normalize(img, None, 255,0, cv2.NORM_MINMAX, cv2.CV_8UC1) # if the ToFloat argument is set up inside Compose(), you have to put the pixels values back to [0,255] before plotting them with cv2_imshow(). I couldn't try with cv2.imshow(), but according to the documentation it seems this line would be useless with this displaying function.
cv2_imshow(img)

I don't recommend it though, because cv2_imshow() plot the images in BGR format, thus some augmentation methods such as RGBShift will not work properly.
'''


EDIT :
I've encountered several issues with the albumentations library (that I described in this question on Github, but for now I still have had no answers) thus I'd better recommend using Imgaug for your data augmentation : it works just fine and is almost as easy to use as albumentations, even though there is a little bit less available transformation functions.
",,
Keras unexpected issue,https://stackoverflow.com/questions/64620236,Error &quot;ValueError: Graph disconnected&quot; in Keras/Tensorflow,,1,847,"pay attention to not override the input variables. you overrode color, clarity and x input inside the network
here a possible solution:
dense1 = 2**7
dense2 = 2**8
dense3 = 2**9
dropout = 0.8
price_loss = 1
cut_loss = 1
activation= LeakyReLU()
batch_size = 32

#====================================================================
# INPUTS
#====================================================================
carat = Input(shape= (1,), batch_size= batch_size, name= 'carat')
Color = Input(shape= (1,), batch_size= batch_size, name= 'color')
Clarity = Input(shape= (1,), batch_size= batch_size, name= 'clarity')
depth = Input(shape= (1,), batch_size= batch_size, name= 'depth')
table = Input(shape= (1,), batch_size= batch_size, name= 'table')
X = Input(shape= (1,), batch_size= batch_size, name= 'x')
y = Input(shape= (1,), batch_size= batch_size, name= 'y')
z = Input(shape= (1,), batch_size= batch_size, name= 'z')
#====================================================================
# CREATE EMBEDDINGS FOR CATEGORICAL FEATURES ""COLOR"" AND ""CLARITY""
#====================================================================
color = Embedding(input_dim = 7, output_dim = 1, name = 'color_emb')(Color)
clarity = Embedding(input_dim = 8, output_dim = 1, name = 'clarity_emb')(Clarity)
color = Flatten()(color)
clarity = Flatten()(clarity)
#====================================================================
# CONCATENATE FEATURES
#====================================================================
x = Concatenate()([color, clarity, carat, depth, table, X, y, z])
#====================================================================
# DENSE NETWORK
#====================================================================
x = Dense(dense1, activation = activation)(x)
x = BatchNormalization()(x)
x = Dense(dense2, activation = activation)(x)
x = BatchNormalization()(x)
x = Dense(dense3, activation = activation)(x)
x = BatchNormalization()(x)
x = Dropout(dropout)(x)
#====================================================================
# PREDICTIONS
# ====================================================================
cut = Dense(1, activation = 'sigmoid')(x)
price = Dense(1)(x)
#====================================================================
# DEFINE THE MODEL
# ====================================================================
model = Model(inputs = [carat, Color, Clarity, depth, table, X, y, z] , 
              outputs = [cut , price])
model.compile('adam', 'mse')
model.summary()

here the running notebook: https://colab.research.google.com/drive/1akpvuGKHXt6Frrec692zvCMAfRzZbIaM?usp=sharing
","Well, like always when we are stuck with some error, the answer lies somewhere in error message. Please note that while you have been able to build the structure of your model overwriting your variables color, clarity and so on... the variables changed the type. At the beginning you initialize the variables to be tf.keras.Input and then you overwrite it with Embedding and Flatten layers. So when it comes to building your model, you ask to build model with inputs that are no longer tf.keras.Input but something else.
The solution is to not overwrite those variables.
",
Keras unexpected issue,https://stackoverflow.com/questions/69885201,Mamba can&#39;t create env giving &#39;nothing provides&#39; but pip sees such version of opencv-python,,1,2235,"PyPI and Anaconda Cloud packages don't necessarily go by the same names. The opencv package from Conda Forge will include the OpenCV libs (libopencv) and the Python interface (py-opencv). That is, install opencv with the version you want.
BTW, as mentioned in the comments == is Pip syntax, Conda/Mamba use single =.
",,
Keras unexpected issue,https://stackoverflow.com/questions/60606297,Cannot convert tf.keras.layers.ConvLSTM2D layer to open vino intermediate representation,,1,425,"Actually the script to convert from h5 to .pb suggested by intel was not good enough. Always use the code from here to convert your keras model to .pb. 

Once you obtain your .pb file now convert your model to IR using

python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_model ml_playground/try_directory/tf_model.pb   --output_dir /home/deepanshu/open_vino/udacity_project_custom_model/  --input_shape=[1,30,64,64,1] --data_type FP32


After the execution of this script we can obtain the intermediate representation of the keras model. 
",,
Keras unexpected issue,https://stackoverflow.com/questions/53358033,why ResNet50 include_top argument giving error?,"Keras version - 2.2.4

Python - 3.6.6

issue:

from resnet50 import ResNet50



----&gt; 6 model = ResNet50(input_tensor=image_input, 
include_top=True,weights='imagenet')

TypeError: _obtain_input_shape() got an unexpected keyword argument 'include_top'

is there a fix for this?
",1,1305,"Replace: 

model = ResNet50(input_tensor=image_input, include_top=True,weights='imagenet')


With:

model = ResNet50(input_tensor=image_input,require_flatten=True,weights='imagenet')

",,
Keras unexpected issue,https://stackoverflow.com/questions/50754017,Keras implementation issue unexpected keyword argument &#39;axis&#39;,"The Keras implementation of MNIST CNN shown here shows the following error:

TypeError: softmax() got an unexpected keyword argument 'axis'


I spent a long time trying to fix it but nothing works. I am running TensorFlow 1.1.0 and Keras 2.1.6. Thank you for help
",1,2555,"You need to use this command to update the version of TensorFlow.

pip install --upgrade tensorflow      # for Python 2.7
pip3 install --upgrade tensorflow     # for Python 3.n

","Your TensorFlow version is too old, you need to upgrade, preferrably to the latest version (currently 1.8).
",
Keras unexpected issue,https://stackoverflow.com/questions/49301203,keras error got an unexpected keyword argument &#39;epochs&#39;,"I'm trying to train a network in Keras to classify an image and after debugging the last issue got this one of unexpected keywork epochs

muiruri_samuel@training-2:~/google-landmark-recognition-challenge$ python train.py
Using TensorFlow backend.
Found 981214 images belonging to 14951 classes.
Found 237925 images belonging to 14951 classes.
Epoch 1/1
2018-03-15 13:35:19.822304: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instruc
tions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""train.py"", line 74, in &lt;module&gt;
    validation_data=validation_generator)
  File ""/home/muiruri_samuel/.local/lib/python2.7/site-packages/keras/legacy/interfaces.py"", line 91, in w
rapper
    return func(*args, **kwargs)
  File ""/home/muiruri_samuel/.local/lib/python2.7/site-packages/keras/models.py"", line 1276, in fit_genera
tor
    initial_epoch=initial_epoch)
  File ""/home/muiruri_samuel/.local/lib/python2.7/site-packages/keras/legacy/interfaces.py"", line 91, in w
rapper
    return func(*args, **kwargs)
  File ""/home/muiruri_samuel/.local/lib/python2.7/site-packages/keras/engine/training.py"", line 2224, in f
it_generator
    class_weight=class_weight)
  File ""/home/muiruri_samuel/.local/lib/python2.7/site-packages/keras/engine/training.py"", line 1883, in t
rain_on_batch
    outputs = self.train_function(ins)
  File ""/home/muiruri_samuel/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line
 2478, in __call__
    **self.session_kwargs)
TypeError: run() got an unexpected keyword argument 'epochs'


at this point I've removed epochs but still getting the same error

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint


# dimensions of our images.
img_width, img_height = 150, 150

train_data_dir = 'training_images'
validation_data_dir = 'validation_images'
nb_train_samples = 2000
nb_validation_samples = 800
epochs = 50
batch_size = 16

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)

model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=input_shape))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(14951, activation=""softmax""))

monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=0, mode='auto')
checkpointer = ModelCheckpoint(filepath=""best_weights.hdf5"", verbose=0, save_best_only=True) # save best model

model.compile(loss='categorical_crossentropy', optimizer='adam', callbacks=[monitor,checkpointer], epochs=1000, metrics=['accuracy'])

# this is the augmentation configuration we will use for training
train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

# this is the augmentation configuration we will use for testing:
# only rescaling
test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical')

model.fit_generator(
    train_generator,
    validation_data=validation_generator)

model.load_weights('best_weights.hdf5') # load weights from best model
model.save('last_model.h5')


I'd use epochs and batches too but now I'd need it to work first. The model in logic has a folder training_images which has sub folders which are the the classes of the images with the images in these sub folders. Then there's the validation_images which is randomly sampled 20% of the training images to validation.
",1,9317,"model.compile does not take an epochs parameter. Only fit and fit_generator do.
",,
Keras unexpected issue,https://stackoverflow.com/questions/48850537,Issue with Imagenet classification with VGG16 pretrained weights,,0,1688,"It seems that we can (or need to?) use the session from Keras (which has the associated loaded graph with weights) instead of creating a new session in Tensorflow and using the graph obtained from Keras model like below  

VGG = model.graph  


I think the graph got above has no weights in it (and that is the reason predictions are wrong) and the graph from Keras session as the proper weights (so those two graph instances should be different)  

Below is the full code:  

import tensorflow as tf
import numpy as np
from PIL import Image
from tensorflow.python.keras._impl.keras.applications import imagenet_utils
from tensorflow.python.keras._impl.keras import backend as K


model = tf.keras.applications.VGG16()
sess = K.get_session()
VGG = model.graph #Not needed and also doesnt have weights in it

VGG.get_operations()
input = VGG.get_tensor_by_name(""input_1:0"")
output = VGG.get_tensor_by_name(""predictions/Softmax:0"")
print(input)
print(output)

I = Image.open(""Elephant.jpg"")
new_img = I.resize((224,224))
image_array = np.array(new_img)[:, :, 0:3]
image_array = np.expand_dims(image_array, axis=0)
image_array = image_array.astype(np.float32)
image_array = tf.keras.applications.vgg16.preprocess_input(image_array)

pred = (sess.run(output,{input:image_array}))
print(imagenet_utils.decode_predictions(pred))


And this gives the expected result:


  [[('n02504458', 'African_elephant', 0.8518132), ('n01871265', 'tusker', 0.1398836), ('n02504013', 'Indian_elephant', 0.0082286), ('n01704323', 'triceratops', 6.965483e-05), ('n02397096', 'warthog', 1.8662439e-06)]]


Thanks Idavid for the tip about using preprocess_input() function and Nicolas for tips about unloaded weights.
","I think there is 2 mistakes, the first one is that you must rescale your image by dividing by 255 all pixels.

I = Image.open(""Elephant.jpg"")
new_img = I.resize((224,224))
image_array = np.array(new_img)[:, :, 0:3]
image_array /= 255.
image_array = np.expand_dims(image_array, axis=0)


The second point I got it while looking at prediction values. You have a vector of 1000 element and all of them have 0.1% prediction after rescale. That means you have a non-trained model. I don't know exactly how to have if loaded in tensorflow but on Keras for example you can do :

app = applications.vgg16
model = app.VGG16(
        include_top=False,    # this is to have the classifier Standard from imagenet
        weights='imagenet',   # this load weight, else it's random weight
        pooling=""avg"") 


From what I've read, you have to download another file containing weight from for example github.

I hope it helps,

EDIT1:

I tried the same model usign Keras :

from keras.applications.vgg16 import VGG16, decode_predictions
import numpy as np

model = VGG16(weights='imagenet')

I = Image.open(""Elephant.jpg"")
new_img = I.resize((224,224))
image_array = np.array(new_img)[:, :, 0:3]
image_array = image_array/255.
x = np.expand_dims(image_array, axis=0)

preds = model.predict(x)
print('Predicted:', decode_predictions(preds, top=5)[0])


If I comment the rescaling, I have bad predictions :


  Predicted: [('n03788365', 'mosquito_net', 0.22725257), ('n15075141', 'toilet_tissue', 0.026636025), ('n04209239', 'shower_curtain', 0.019786758), ('n02804414', 'bassinet', 0.01353887), ('n03131574', 'crib', 0.01316699)]


Without the rescale, this is good :


  Predicted: [('n02504458', 'African_elephant', 0.95870858), ('n01871265', 'tusker', 0.040065952), ('n02504013', 'Indian_elephant', 0.0012253703), ('n01704323', 'triceratops', 5.0949382e-08), ('n02454379', 'armadillo', 5.0408511e-10)]


Now if I remove the weight, I have the ""same"" as what I have with Tensorflow:


  Predicted: [('n07717410', 'acorn_squash', 0.0010033853), ('n02980441', 'castle', 0.0010028203), ('n02124075', 'Egyptian_cat', 0.0010028186), ('n04179913', 'sewing_machine', 0.0010027955), ('n02492660', 'howler_monkey', 0.0010027081)]


To me, that means that you have no weight applied. Maybe they are downloaded but not used. 
",
Keras unexpected issue,https://stackoverflow.com/questions/71951540,"ValueError: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`,",,0,2549,"There are a few things wrong with your code.

You are creating your model inside the training loop. So thats wrong. Unless you want to train a new version at each epoch. Your model should be defined and compiled before you start training.

Depending on which version of tensorflow you you are using you should enable eager execution before you declare the model. That lets you train the model without creating a session and a scope.


Hope this helps!
",,
Keras unexpected issue,https://stackoverflow.com/questions/72023307,How do I solve LSTM error on predict functions,,0,683,"Your test_data always have length of 60 as per your construction
test_data = train_data[training_data_length - 60: , : ]

So, your for loop is never carried out as it is always equal to for i in range(60,60). Consequently, x_test remains [] and causes the error.
I'm guessing that you want to construct a set of length-60 series, each offset the next by 1 timestep. In that case, the test data should've been constructed as
for i in range(60,len(train_data)):
    x_test.append(train_data[i-60:i,0])

Also, your np.reshape usage seems wrong. The number of elements after reshape will be x_test.shape[0] * x_test.shape[0], but the original x_test have x_test.shape[0] * x_test.shape[1], so unless x_test.shape[0] == x_test.shape[1], you'll always get an error. You don't need reshape anyway, as your x_test should've already been in the right shape.
",,
Keras unexpected issue,https://stackoverflow.com/questions/71512318,"I get this Error while trying to Train a Model, Tensorflow ValueError: Unexpected result of `train_function` (Empty logs)",,0,2050,"Can you try a dense layer with a single neuron with sigmoid activation as output (and binary crossentropy as loss)?
",,
Keras unexpected issue,https://stackoverflow.com/questions/54027170,CUDA_ERROR_LAUNCH_FAILED with Tensorflow and Keras,,0,1765,"Looks like on the First machine, CUDA version mismatch,Make sure use single version of CUDA and on the second machine the variables of CUDA and cuDNN are not set properly. Follow the instructions mentioned on Tensorflow with GPU support.
Also check the NVIDIA driver compute capability and install CUDA accordingly.
",,
Keras unexpected issue,https://stackoverflow.com/questions/60193964,Create generator that shuffle training data for Keras in R/train a Keras model with lowspec computer,,0,869,"answering my own question and thanks to @user12728748 for the comments, I change the generator to read random samples from the file:

data_generator &lt;- function(datafile, batch_size = 128, 
                           mlp, val, 
                           shuffle = TRUE, validation_split = 0) {
  nrow_file &lt;- py_eval(paste(""sum(1 for line in open('"", datafile, ""'))"", sep = '')) - 1
  skip &lt;- 0 

  if (val) {
    nrow_file &lt;- validation_split * nrow_file
  } else {
    nrow_file &lt;- (1 - validation_split) * nrow_file
  }


  if (nrow_file &gt; 0) {
    function() {
        # Calculate the rows to read in this epoch
        rows_to_read &lt;- batch_size
        if (skip + batch_size &gt; nrow_file) {
          rows_to_read &lt;- nrow_file - skip
        }

        if (shuffle) {
          index &lt;- sample (c(1:nrow_file), size=batch_size, replace =F)
        } else {
          index &lt;- (skip + 1):(skip + rows_to_read)
        }

        # Create rows to skip
        if (val) {
          # in validation, skip training rows and validation rows that are not found in index
          rows_to_skip &lt;- c(1:ifelse(validation_split &gt; 0,((1 - validation_split) * nrow_file / validation_split),1),
                            setdiff(1:nrow_file, index) + (1 - validation_split) * nrow_file / validation_split)
        } else {
          # in training, skip validation rows and training rows that are not found in index
          rows_to_skip &lt;- c(ifelse(validation_split &gt; 0,
                                   nrow_file + 1, 0):ifelse(validation_split &gt; 0,
                                                            nrow_file/(1 - validation_split), 0),
                            setdiff(1:nrow_file, index))
          if (rows_to_skip[1] == 0) rows_to_skip &lt;- rows_to_skip[-1]
        }


        trData &lt;- import(""pandas"")$read_csv(datafile, 
                                            skiprows = rows_to_skip, 
                                            sep = "";"")

        # Upload the rows to train
        skip &lt;&lt;- skip + batch_size
        if (skip &gt;= nrow_file) {
          skip &lt;&lt;- 0
        }
        # Build inputs and output
        y_train &lt;- to_categorical(trData[,1], num_classes = 10)
        x_train &lt;- trData[,-1]
        if (mlp) {
          # Return data as is for mlp
          list(data.matrix(x_train), data.matrix(y_train))  
        } else {
          # Return data reshaped for CNN
          list(array_reshape(data.matrix(x_train), c(nrow(x_train), 28, 28, 1)),
               data.matrix(y_train))
        }
      }
  } else {
    NULL
  }
}


I have added the validation_split argument to set the percentage of the training data that we want to use as validation. 

However, using the generator to train the model does not seem to improve training time when using the lowspec computer, but it uses almost 4 GB less of RAM (especially when training with small batches (~128 samples)) and you can use the computer to perform other tasks while executing the code without crashing the program. 

Here I leave you the code to train and evaluate a MLP model using the generator:

MLP_model &lt;- keras_model_sequential()

MLP_model %&gt;% 
  layer_dense(units = 500, activation = 'relu', input_shape = c(784),
              kernel_regularizer = regularizer_l2(l = 0.0001),
              bias_regularizer = regularizer_l2(l = 0.0001)) %&gt;% 
  layer_dropout(rate = 0.4, seed = 150)  %&gt;% 
  layer_batch_normalization() %&gt;%
  layer_dense(units = 300, activation = 'relu',
              kernel_regularizer = regularizer_l2(l = 0.001),
              bias_regularizer = regularizer_l2(l = 0.001)) %&gt;%
  layer_dropout(rate = 0.3, seed = 150) %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dense(units = 10, activation = 'softmax',
              kernel_regularizer = regularizer_l2(l = 0.001),
              bias_regularizer = regularizer_l2(l = 0.001))

MLP_model %&gt;% compile(
  loss = loss_categorical_crossentropy,
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

validation_split &lt;- 0.2
history &lt;- MLP_model %&gt;% fit_generator(
  data_generator(traindatafile,
                 batch_size_train, mlp = TRUE, 
                 val = FALSE, validation_split = validation_split), 
  steps_per_epoch = round((1 - validation_split) * (py_eval(paste(""sum(1 for line in open('"", traindatafile, ""'))"", sep = '')) - 1) / (batch_size_train)),
  validation_data = data_generator(traindatafile,
                                   batch_size_train, mlp = TRUE, 
                                   val = TRUE, validation_split = validation_split),
  validation_steps = round((validation_split) * (py_eval(paste(""sum(1 for line in open('"", traindatafile, ""'))"", sep = '')) - 1) / (batch_size_train)),
  callbacks = c(early_stopping),
  epochs = 10)

MLP_metrics_train &lt;- MLP_model %&gt;% 
  evaluate_generator(data_generator(traindatafile,
                                    batch_size_eval, 
                                    mlp = TRUE, 
                                    val = FALSE,
                                    shuffle = FALSE),
                     steps = ceiling((py_eval(paste(""sum(1 for line in open('"", traindatafile, ""'))"", sep = '')) - 1) / (batch_size_eval)))
MLP_metrics_test &lt;- MLP_model %&gt;% 
  evaluate_generator(data_generator(testdatafile,
                                    batch_size_eval, 
                                    mlp = TRUE, 
                                    val = FALSE,
                                    shuffle = FALSE),
                     steps = ceiling((py_eval(paste(""sum(1 for line in open('"", testdatafile, ""'))"", sep = '')) - 1) / (batch_size_eval)))


y_pred_mlp &lt;- MLP_model %&gt;% 
  predict_generator(data_generator(testdatafile,
                                   batch_size_eval, 
                                   mlp = TRUE, 
                                   val = FALSE,
                                   shuffle = FALSE),
                    steps = ceiling((py_eval(paste(""sum(1 for line in open('"", testdatafile, ""'))"", sep = '')) - 1) / (batch_size_eval)))


Hope this helps someone!
",,
Keras unexpected issue,https://stackoverflow.com/questions/72126226,"model.fit in a for loop, for K-fold cross validation",,0,1871,"You can use StratifiedKFold from the sklearn package to do the cross validation. It is much clearer and is the standard way to do it. You should also reset the model weights at each fold before fitting the model, otherwise you will start with the weights initialized in the previous call of the fit method,
The modified code :
from sklearn.model_selection import StratifiedKFold
# K-Fold cross validation
k = 4
skf = StratifiedKFold(n_splits=k)
validation_scores = []

# store initial model's weights
weights_init = model.get_weights()

for train_index, test_index in skf.split(samples, labels_extrait):
    training_data = samples[train_index]
    training_label = labels_extrait[train_index]
    validation_data = samples[test_index]
    validation_label = labels_extrait[test_index]

    # reset mdoel's weights
    model.set_weights(weights_init)
    # fit
    model.fit(training_data, training_label, epochs=100, batch_size=nbr_de_son)

    validation_score = model.evaluate(validation_data, validation_label)
    validation_scores.append(validation_score)


validation_score = np.average(validation_scores)
print(validation_score) 

I don't know exactly where your error comes from, but the above code works
",,
Keras unexpected issue,https://stackoverflow.com/questions/59035050,"LSTM class, got error: TypeError: __init__() got an unexpected keyword argument &#39;input_shape&#39;","class LSTM:

    scaler_y_train= MinMaxScaler(feature_range=(0,1))
    scaler_x_train= MinMaxScaler(feature_range=(0,1))
    scaler_forecast= MinMaxScaler(feature_range=(0,1))

    def __init__(self, filename, seq_len, forecast_size):
        self._filename= filename
        self._seq_len= seq_len
        self._forecast_size= forecast_size
        self.model= Sequential()


    def load_y_train(self):
        raw_data_values= self._filename.values
        raw_data= self._filename
        print('Initializing the data loading...')
        forecast_out_variable= math.ceil(len(raw_data_values)* self._forecast_size)
        forecast_out_fixed= None

        print('Creating y_train...')
        y= raw_data.iloc[forecast_out_variable:].values
        len_y= len(y)
        data_windows_y= []
        for i in range(len_y - self._seq_len):
            data_windows_y.append(y[i: i + self._seq_len])
        data_windows_y= np.array(data_windows_y).astype(float)
        y_train= data_windows_y[:, -1, [0]]
        return print(y_train.shape), y_train

    def load_x_train(self):
        raw_data_values= self._filename.values
        raw_data= self._filename
        print('Initializing the data loading...')
        forecast_out_variable= math.ceil(len(raw_data_values)* self._forecast_size)
        forecast_out_fixed= None

        print('Creating x_train...')
        x= raw_data.iloc[:-forecast_out_variable].values
        len_x= len(x)
        data_windows_x= []
        for i in range(len_x - self._seq_len):
            data_windows_x.append(x[i: i + self._seq_len])
        data_windows_x= np.array(data_windows_x).astype(float)
        x_train= data_windows_x[:, :-1]
        return x_train

    def load_x_forecast(self):
        raw_data_values= self._filename.values
        raw_data= self._filename
        print('Initializing the data loading...')
        forecast_out_variable= math.ceil(len(raw_data_values)* self._forecast_size)
        forecast_out_fixed= None

        print('Creating x_train...')
        x_forecast = raw_data.iloc[:-forecast_out_variable].values
        len_x_forecast= len(x_forecast)
        data_windows_x= []
        for i in range(len_x_forecast - self._seq_len):
            data_windows_x.append(x_forecast[i: i + self._seq_len])
        data_windows_x= np.array(data_windows_x).astype(float)
        xx_forecast= data_windows_x[:, :-1]
        return print(xx_forecast.shape), xx_forecast

    def build_model(self):
        x_train= self.load_x_train()
        x_train= np.array(x_train).reshape(1050,49,2).astype(float)
        print(x_train.shape)
        print('Model starting compiling...')
        start= time.time()

        self.model.add(LSTM(50, input_shape=(x_train.shape[1], x_train.shape[-1]), 
        return_sequences=True))
        self.model.add(Dropout(0.2))
        self.model.add(LSTM(100, return_sequences=False))
        self.model.add(Dropout(0.2))
        self.model.add(Dense(1, activation = ""linear""))
        self.model.compile(loss='mse', optimizer='adam')
        end= time.time()
        print ('model compiled in: ' +str((end-start)*1000)+ ' ms')
        return model, print(model.summary())




lstm_01= LSTM(ms_unreal, 50, 0.1)
print(lstm_01.load_x_forecast())
print(lstm_01.load_y_train())
print(lstm_01.load_x_train())
print(lstm_01.build_model(model))


Type error is init() got an unexpected keyword argument 'input_shape',
I do not know how to made it work as im calling correctly the libraries. I searched the problem online but im not able to understand the issue. 
The code is a class named LSTM where basically the first function are finalized to get the train and test data, while the last one (in which there is the issue) is to initialize the LSTM keras model. 
",0,3204,"Your class is called LSTM and you call a function from Keras called LSTM. Try renaming your class My_LSTM or some variant. Otherwise you won't be able to call your class without overwriting the Keras implementation.
",,
Keras unexpected issue,https://stackoverflow.com/questions/55107341,VGG bottleneck features + LSTM in keras,"I have pre-stored bottleneck features (.npy files) obtained from VGG16 for around 10k images. Training a SVM classifier (3-class classification) on these features gave me an accuracy of 90% on the test set. These images are obtained from videos. I want to train an LSTM in keras on top of these features. My code snippet can be found below. The issue is that the training accuracy is not going above 43%, which is unexpected. Please help me in debugging the issue. I have tried with different learning rates.

#Asume all necessary imports done
classes = 3
frames = 5
channels = 3
img_height = 224
img_width = 224
epochs = 20

#Model definition
model = Sequential()
model.add(TimeDistributed(Flatten(),input_shape=(frames,7,7,512)))
model.add(LSTM(256,return_sequences=False))
model.add(Dense(1024,activation=""relu""))
model.add(Dense(3,activation=""softmax""))
optimizer = Adam(lr=0.1,beta_1=0.9,beta_2=0.999,epsilon=None,decay=0.0)

model.compile (loss=""categorical_crossentropy"",optimizer=optimizer,metrics=[""accuracy""])

model.summary()

train_data = np.load(open('bottleneck_features_train.npy','rb'))

#final_img_data shape --&gt; 2342,5,7,7,512
#one_hot_labels shape --&gt; 2342,3
model.fit(final_img_data,one_hot_labels,epochs=epochs,batch_size=2)

",0,434,"You are probably missing the local minimum, because learning rate is too high. Try to decrease learning rate to 0.01 -- 0.001 and increase number of epochs. Also, decrease Dense layer neurons from 1024 to half. Otherwise you may overfit.
",,
Keras strange behavior,https://stackoverflow.com/questions/60267911,Keras inconsistent prediction time,,19,3332,"TF2 generally exhibits poor and bug-like memory management in several instances I've encountered - brief description here and here. With prediction in particular, the most performant feeding method is via model(x) directly - see here, and its linked discussions.

In a nutshell: model(x) acts via its its __call__ method (which it inherits from base_layer.Layer), whereas predict(), predict_classes(), etc. involve a dedicated loop function via _select_training_loop(); each utilize different data pre- and post-processing methods suited for different use-cases, and model(x) in 2.1 was designed specifically to yield fastest small-model / small-batch (and maybe any-size) performance (and still fastest in 2.0).

Quoting a TensorFlow dev from linked discussions:


  You can predict the output using model call, not model predict, i.e., calling model(x) would make this much faster because there are no ""conversion to dataset"" part, and also it's directly calling a cached tf.function.


Note: this should be less of an issue in 2.1, and especially 2.2 - but test each method anyway. Also I realize this doesn't directly answer your question on the time spikes; I suspect it's related to Eager caching mechanisms, but the surest way to determine is via TF Profiler, which is broken in 2.1.



Update: regarding increasing spikes, possible GPU throttling; you've done ~1000 iters, try 10,000 instead - eventually, the increasing should stop. As you noted in your comments, this doesn't occur with model(x); makes sense as one less GPU step is involved (""conversion to dataset"").

Update2: you could bug the devs here about it if you face this issue; it's mostly me singing there
","While I can't explain the inconsistencies in execution time, I can recommend that you try to convert your model to TensorFlow Lite to speed up predictions on single data records or small batches.

I ran a benchmark on this model:

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(384, activation='elu', input_shape=(256,)),
    tf.keras.layers.Dense(384, activation='elu'),
    tf.keras.layers.Dense(256, activation='elu'),
    tf.keras.layers.Dense(128, activation='elu'),
    tf.keras.layers.Dense(32, activation='tanh')
])


The prediction times for single records were:


model.predict(input): 18ms
model(input): 1.3ms
Model converted to TensorFlow Lite: 43us


The time to convert the model was 2 seconds.

The class below shows how to convert and use the model and provides a predict method like the Keras model. Note that it would need to be modified for use with models that dont just have a single 1-D input and a single 1-D output.

class LiteModel:

    @classmethod
    def from_file(cls, model_path):
        return LiteModel(tf.lite.Interpreter(model_path=model_path))

    @classmethod
    def from_keras_model(cls, kmodel):
        converter = tf.lite.TFLiteConverter.from_keras_model(kmodel)
        tflite_model = converter.convert()
        return LiteModel(tf.lite.Interpreter(model_content=tflite_model))

    def __init__(self, interpreter):
        self.interpreter = interpreter
        self.interpreter.allocate_tensors()
        input_det = self.interpreter.get_input_details()[0]
        output_det = self.interpreter.get_output_details()[0]
        self.input_index = input_det[""index""]
        self.output_index = output_det[""index""]
        self.input_shape = input_det[""shape""]
        self.output_shape = output_det[""shape""]
        self.input_dtype = input_det[""dtype""]
        self.output_dtype = output_det[""dtype""]

    def predict(self, inp):
        inp = inp.astype(self.input_dtype)
        count = inp.shape[0]
        out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype)
        for i in range(count):
            self.interpreter.set_tensor(self.input_index, inp[i:i+1])
            self.interpreter.invoke()
            out[i] = self.interpreter.get_tensor(self.output_index)[0]
        return out

    def predict_single(self, inp):
        """""" Like predict(), but only for a single record. The input data can be a Python list. """"""
        inp = np.array([inp], dtype=self.input_dtype)
        self.interpreter.set_tensor(self.input_index, inp)
        self.interpreter.invoke()
        out = self.interpreter.get_tensor(self.output_index)
        return out[0]


The complete benchmark code and a plot can be found here:
https://medium.com/@micwurm/using-tensorflow-lite-to-speed-up-predictions-a3954886eb98
",
Keras strange behavior,https://stackoverflow.com/questions/55902068,Fitting a Keras model yields error &quot;constant folding failed: Invalid argument: Unsupported type: 21&quot;,,7,1158,"There is one key difference between the Tutorial mentioned in the link, https://www.tensorflow.org/beta/tutorials/load_data/text and your Dataset.
In the tutorial, Labels are 0, 1 and 2, i.e., all the sentences in cowper.txt are Labelled as 0, all the sentences in derby.txt are Labelled as 1, all the sentences in butler.txt are Labelled as 2. But in your Dataset, Labels are at the end of each sentence of the Text Files.
I have executed the code taking part of your Dataset, as shown below:
FILE_NAMES = ['001.dev', '001.test', '001.train', '002.dev', '002.test', '002.train']

parent_dir = ""Issue_55902068/OC""

parent_dir

In order to handle the difference mentioned above, the function, labeler should be modified as shown below:
def labeler(example, index):
  Label = tf.strings.split(example, sep="""")[-1] #It will give 0 or 1 in Str format
  Label = tf.strings.to_number(Label)
  return example, tf.cast(Label, tf.int64)

After that, I changed the loss function to binary_crossentropy and the optimizer to RMSprop as shown below:
model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])

It is working as expected. Screenshot of the output is shown below.

",,
Keras strange behavior,https://stackoverflow.com/questions/58455765,Keras sees my GPU but doesn&#39;t use it when training a neural network,,7,7425,"The logs are showing that the GPU does get used. You are almost certainly running into an IO bottleneck: your GPU is processing whatever the CPU is throwing at it way faster than the CPU can load and preprocess it. This is very common in deep learning, and there are ways to address it. 

We cannot provide a lot of help without knowing more about your data pipeline (byte size of a batch, preprocessing steps, ...), and how the data is stored. One typical way to speed things up is to store the data is a binary format, like TFRecords, so that the CPU can load it faster. See the official documentation for this. 



Edit: I quickly went through your input pipeline. The issue is very likely to indeed by IO:


You should run the preprocessing steps on the GPU as well, plenty of the augmentation techniques you use are implemented in tf.image. If you can, you should think about using Tensorflow 2.0, because it includes Keras and there are plenty of helpers in there as well.
Checkout the tf.data.Dataset API, it has plenty of helpers to load all the data in different threads, which can roughly speed up the process by the number of cores you have.
You should store your images as TFRecords. This is likely to speed up the loading by an order of magnitude if your input images are smallish.
You could probably try larger batch sizes as well, I'm thinking your images are probably really small.

",,
Keras strange behavior,https://stackoverflow.com/questions/49678302,Delete layers of keras pretrained model,"I want to use vgg16 pre-trained model of keras. I have notice some strange behavior when trying to change the model. 

1) I have add some layers of the per-trained model. My problem is that tensorboard is showing the layers of the model that I didn't add into the sequence model. This is strange because I have also deleted the imported model. I think this have to do with the dependency between layers so I want to remove this dependencies.  How can I do this?



For example in this picture there is two layers that I didn't add but they are showing in the graph

vgg16_model = keras.applications.vgg16.VGG16()


cnnModel = keras.models.Sequential()

for layer in vgg16_model.layers[0:13]:
    cnnModel.add(layer)

for layer in vgg16_model.layers[14:16]:
    cnnModel.add(layer)

for layer in vgg16_model.layers[17:21]:
    cnnModel.add(layer)

cnnModel.add(keras.layers.Dense(2048, name=""compress_1""))
cnnModel.add(keras.layers.Dense(1024, name=""compress_2""))
cnnModel.add(keras.layers.Dense(512, name=""compress_3""))


for layer in cnnModel.layers[0:4]:
    layer.trainable = False

del vgg16_model


2) the second problem occurs when using cnnModel.pop(). In fact I have add all the layers but I do a pop to the layer I don't want before adding the next one this is the error I get.

Layer block4_conv2 has multiple inbound nodes, hence the notion of ""layer output"" is ill-defined. Use `get_output_at(node_index)` instead.


And this is the code I am using:

for layer in vgg16_model.layers[0:14]:
    cnnModel.add(layer)

cnnModel.pop()

for layer in vgg16_model.layers[14:17]:
    cnnModel.add(layer)

cnnModel.pop()

for layer in vgg16_model.layers[17:21]:
    cnnModel.add(layer)


cnnModel.pop() is working the problem only occurs when trying to add the next layer.

Thank you for your help.
",6,5071,"You can try using Model instead of Sequential, like:

vgg16_model = keras.applications.vgg16.VGG16()

drop_layers = [13, 16]

input_layer = x = vgg16_model.input

for i, layer in enumerate(vgg16_model.layers[1:], 1):
    if i not in drop_layers:
        x = layer(x)

x = keras.layers.Dense(2048, name=""compress_1"")(x)
x = keras.layers.Dense(1024, name=""compress_2"")(x)
x = keras.layers.Dense(512, name=""compress_3"")(x)

cnnModel = keras.models.Model(inputs = input_layer, outputs = x)

for layer in cnnModel.layers[0:4]:
    layer.trainable = False

del vgg16_model

",,
Keras strange behavior,https://stackoverflow.com/questions/60797725,"Keras - GRU layer with recurrent dropout - loss: &#39;nan&#39;, accuracy: 0",,4,1522,"I finally found the solution (sort of). It's enough to change keras to tensorflow.keras.

Revised Code

# Based on examples from ""Deep Learning with Python"" by Franois Chollet:
## Constants, modules:
VERSION = 2

import os
#U: from keras import models
#U: from keras import layers
from tensorflow.keras import models
from tensorflow.keras import layers

import matplotlib.pyplot as plt
import pylab

## Loading data:
from keras.datasets import imdb

(x_train, y_train), (x_test, y_test) = \
    imdb.load_data(num_words=10000)

from keras.preprocessing import sequence

x_train = sequence.pad_sequences(x_train, maxlen=500)
x_test = sequence.pad_sequences(x_test, maxlen=500)

## Dictionary with models' hyperparameters:
MODELS_ALL = [
    # GRU:
    {""no"": 1,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": None},

    {""no"": 2,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": None},

    {""no"": 3,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 4,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},

    {""no"": 5,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": None},

    {""no"": 6,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": None},

    {""no"": 7,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 8,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},

    # LSTM:
    {""no"": 9,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": None},

    {""no"": 10,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": None},

    {""no"": 11,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 12,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},

    {""no"": 13,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": None},

    {""no"": 14,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": None},

    {""no"": 15,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 16,
     ""layer_type"": ""LSTM"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},
]

MODELS_GRU_RECCURENT = [
    # GRU:
    {""no"": 3,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 4,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""rmsprop"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},

    {""no"": 7,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": None,
     ""recurrent_dropout"": 0.3},

    {""no"": 8,
     ""layer_type"": ""GRU"",
     ""optimizer"": ""adam"",
     ""dropout"": 0.3,
     ""recurrent_dropout"": 0.3},
]

MODELS = MODELS_ALL   # ""MODELS = MODELS_ALL"" or ""MODELS = MODELS_GRU_RECCURENT""

## Adding name:
for model_dict in MODELS:
    model_dict[""name""] = f""{model_dict['layer_type']}""
    model_dict[""name""] += f""_d{model_dict['dropout']}"" if model_dict['dropout'] is not None else f""_dN""
    model_dict[""name""] += f""_rd{model_dict['recurrent_dropout']}"" if model_dict['recurrent_dropout'] is not None else f""_rdN""
    model_dict[""name""] += f""_{model_dict['optimizer']}""


## Fucntion - defing and training model:
def train_model(model_dict):
    """"""Defines and trains a model, outputs history.""""""

    ## Defining:
    model = models.Sequential()
    model.add(layers.Embedding(10000, 32))

    recurrent_layer_kwargs = dict()
    if model_dict[""dropout""] is not None:
        recurrent_layer_kwargs[""dropout""] = model_dict[""dropout""]
    if model_dict[""recurrent_dropout""] is not None:
        recurrent_layer_kwargs[""recurrent_dropout""] = model_dict[""recurrent_dropout""]

    if model_dict[""layer_type""] == 'GRU':
        model.add(layers.GRU(32, **recurrent_layer_kwargs))
    elif model_dict[""layer_type""] == 'LSTM':
        model.add(layers.LSTM(32, **recurrent_layer_kwargs))
    else:
        raise ValueError(""Wrong model_dict['layer_type'] value..."")
    model.add(layers.Dense(1, activation='sigmoid'))

    ## Compiling:
    model.compile(
        optimizer=model_dict[""optimizer""],
        loss='binary_crossentropy',
        metrics=['accuracy'])

    ## Training:
    history = model.fit(x_train, y_train,
                        epochs=20,
                        batch_size=64,
                        validation_split=0.2)

    return history


## Multi-model graphs' parameters:
graph_all_nrow = 4
graph_all_ncol = 4
graph_all_figsize = (20, 20)

assert graph_all_nrow * graph_all_nrow &gt;= len(MODELS)

# fig and axes of multi-model graphs:
graph_all_loss_fig, graph_all_loss_axs = plt.subplots(graph_all_nrow, graph_all_ncol, figsize=graph_all_figsize)
graph_all_acc_fig, graph_all_acc_axs = plt.subplots(graph_all_nrow, graph_all_ncol, figsize=graph_all_figsize)

## Loop trough all models:
for i, model_dict in enumerate(MODELS):
    history = train_model(model_dict)

    ## Metrics extraction:
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    epochs = range(1, len(loss) + 1)

    ## Single-model graph - loss:
    graph_loss_fname = fr""{os.path.basename(__file__).replace('.py', '')}""
    graph_loss_fname += fr""_v{VERSION}_{model_dict['no']}_{model_dict['name']}_loss_graph.png""

    graph_loss_fig, graph_loss_ax = plt.subplots()
    graph_loss_ax.plot(epochs, loss, 'bo', label='Training loss')
    graph_loss_ax.plot(epochs, val_loss, 'b', label='Validation loss')
    graph_loss_ax.legend()
    graph_loss_fig.suptitle(""Training and validation loss"")
    graph_loss_fig.savefig(graph_loss_fname)
    pylab.close(graph_loss_fig)

    ## Single-model graph - accuracy:
    graph_acc_fname = fr""{os.path.basename(__file__).replace('.py', '')}""
    graph_acc_fname += fr""_v{VERSION}_{model_dict['no']}_{model_dict['name']}_acc_graph.png""

    graph_acc_fig, graph_acc_ax = plt.subplots()
    graph_acc_ax.plot(epochs, acc, 'bo', label='Training accuracy')
    graph_acc_ax.plot(epochs, val_acc, 'b', label='Validation accuracy')
    graph_acc_ax.legend()
    graph_acc_fig.suptitle(""Training and validation acc"")
    graph_acc_fig.savefig(graph_acc_fname)
    pylab.close(graph_acc_fig)

    ## Position of axes on multi-model graph:
    i_row = i // graph_all_ncol
    i_col = i % graph_all_ncol

    ## Adding model metrics to multi-model graph - loss:
    graph_all_loss_axs[i_row, i_col].plot(epochs, loss, 'bo', label='Training loss')
    graph_all_loss_axs[i_row, i_col].plot(epochs, val_loss, 'b', label='Validation loss')
    graph_all_loss_axs[i_row, i_col].set_title(fr""{model_dict['no']}. {model_dict['name']}"")

    ## Adding model metrics to multi-model graph - accuracy:
    graph_all_acc_axs[i_row, i_col].plot(epochs, acc, 'bo', label='Training acc')
    graph_all_acc_axs[i_row, i_col].plot(epochs, val_acc, 'b', label='Validation acc')
    graph_all_acc_axs[i_row, i_col].set_title(fr""{model_dict['no']}. {model_dict['name']}"")

graph_all_loss_fig.suptitle(f""Loss - binary crossentropy [v{VERSION}]"")
graph_all_acc_fig.suptitle(f""Accuracy [v{VERSION}]"")

## Saving multi-model graphs:
graph_all_loss_fig.savefig(fr""{os.path.basename(__file__).replace('.py', '')}_ALL_v{VERSION}_loss_graph.png"", dpi=400)
graph_all_acc_fig.savefig(fr""{os.path.basename(__file__).replace('.py', '')}_ALL_v{VERSION}_acc_graph.png"", dpi=400)

## Saving multi-model graphs (SMALL):
graph_all_loss_fig.savefig(fr""{os.path.basename(__file__).replace('.py', '')}_ALL_v{VERSION}_loss_graph_SMALL.png"", dpi=150)
graph_all_acc_fig.savefig(fr""{os.path.basename(__file__).replace('.py', '')}_ALL_v{VERSION}_acc_graph_SMALL.png"", dpi=150)


Results

Graphs analogous to those in question: Loss - binary crossentropy, Accuracy

More on keras vs tensorflow.keras

As written in Franois Chollet's tweets (found here: https://stackoverflow.com/a/54117754) instead of stand-alone keras there will be tensorflow.keras (that is Keras as official API to TensorFlow) from now on. (I'm not completely sure if I'm 100% correct, feel free to correct me.)

I think it's better just to use tensorflow.keras instead of keras in future projects.
","Same for me while training using R interface to Keras. The issue seems related to recurrent dropout and the length of the ""time"" dimension. It happens using GRU only (lstm has no problem).



# remotes::install_github(""rstudio/keras#1032"")
library(keras)


reticulate::py_config()
#&gt; python:         /home/clanera/anaconda3/envs/r-tensorflow/bin/python
#&gt; libpython:      /home/clanera/anaconda3/envs/r-tensorflow/lib/libpython3.6m.so
#&gt; pythonhome:     /home/clanera/anaconda3/envs/r-tensorflow:/home/clanera/anaconda3/envs/r-tensorflow
#&gt; version:        3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 21:14:29)  [GCC 7.3.0]
#&gt; numpy:          /home/clanera/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/numpy
#&gt; numpy_version:  1.18.1
#&gt; tensorflow:     /home/clanera/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow
#&gt; 
#&gt; NOTE: Python version was forced by RETICULATE_PYTHON
tensorflow::tf_config()
#&gt; TensorFlow v2.0.0 (~/anaconda3/envs/r-tensorflow/lib/python3.6/site-packages/tensorflow)
#&gt; Python v3.6 (~/anaconda3/envs/r-tensorflow/bin/python)
tensorflow::tf_gpu_configured()
#&gt; TensorFlow built with CUDA:  FALSE 
#&gt; GPU device name:
#&gt; [1] FALSE


n &lt;- 100
t &lt;- 80 # with 72- seams have no problem
q &lt;- 10

x &lt;- array(sample(n*t*q), c(n, t, q))
y &lt;- sample(0:1, n, replace = TRUE)


input &lt;- layer_input(c(t, q))
output &lt;- input %&gt;% 
#  ## no problem using LSTM
#  layer_lstm(units = 2, recurrent_dropout = 0.5) %&gt;%
  layer_gru(units = 2, recurrent_dropout = 0.5) %&gt;%
  layer_dense(units = 1, activation = ""sigmoid"")

model &lt;- keras_model(input, output)

summary(model)
#&gt; Model: ""model""
#&gt; ________________________________________________________________________________
#&gt; Layer (type)                        Output Shape                    Param #     
#&gt; ================================================================================
#&gt; input_1 (InputLayer)                [(None, 80, 10)]                0           
#&gt; ________________________________________________________________________________
#&gt; gru (GRU)                           (None, 2)                       78          
#&gt; ________________________________________________________________________________
#&gt; dense (Dense)                       (None, 1)                       3           
#&gt; ================================================================================
#&gt; Total params: 81
#&gt; Trainable params: 81
#&gt; Non-trainable params: 0
#&gt; ________________________________________________________________________________

history &lt;- model %&gt;%
  compile(optimizer = ""adam"", loss = ""binary_crossentropy"") %&gt;% 
  fit(x, y, 2, 3)

history
#&gt; Trained on 100 samples (batch_size=2, epochs=3)
#&gt; Final epoch (plot to see history):
#&gt; loss: NaN


Created on 2020-05-10 by the reprex package (v0.3.0)

sessionInfo()
#&gt; R version 4.0.0 (2020-04-24)
#&gt; Platform: x86_64-pc-linux-gnu (64-bit)
#&gt; Running under: Ubuntu 18.04.4 LTS
#&gt; 
#&gt; Matrix products: default
#&gt; BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so
#&gt; 
#&gt; locale:
#&gt;  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
#&gt;  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
#&gt;  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
#&gt;  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
#&gt;  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
#&gt; [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       
#&gt; 
#&gt; attached base packages:
#&gt; [1] stats     graphics  grDevices datasets  utils     methods   base     
#&gt; 
#&gt; other attached packages:
#&gt; [1] keras_2.2.5.0
#&gt; 
#&gt; loaded via a namespace (and not attached):
#&gt;  [1] Rcpp_1.0.4.6         whisker_0.4          knitr_1.28          
#&gt;  [4] magrittr_1.5         lattice_0.20-41      R6_2.4.1            
#&gt;  [7] rlang_0.4.6          stringr_1.4.0        highr_0.8           
#&gt; [10] tools_4.0.0          grid_4.0.0           xfun_0.13           
#&gt; [13] htmltools_0.4.0      tfruns_1.4           yaml_2.2.1          
#&gt; [16] digest_0.6.25        tensorflow_2.0.0     Matrix_1.2-18       
#&gt; [19] base64enc_0.1-3      zeallot_0.1.0        evaluate_0.14       
#&gt; [22] rmarkdown_2.1        stringi_1.4.6        compiler_4.0.0      
#&gt; [25] generics_0.0.2       reticulate_1.15-9000 jsonlite_1.6.1      
#&gt; [28] renv_0.10.0

",
Keras strange behavior,https://stackoverflow.com/questions/54283085,Why am I getting strange triplication of video using Webcam and Tensorflow.js?,"I have a keras model trained and now I want to run this on the web. I thought this might be a good way to attempt testing out Tensorflow.js. I downloaded the Tesnroflow.js ""Webcam-transfer-learning"" tutorial and then modified it to get what I currently have. The working keras model performs emotion classification after reducing the size of the image to 48x48. Now in the keras model, I take a snapshot of the webcam, copy it and then draw my box and label. I was trying to do the same thing in tf.js, so I setup a canvas, got a reference to it and tried drawing to the canvas after my conversion to gray scale.

I am seeing a strange behavior where it is correctly showing the gray scale image but it is displaying it 3 times across and not sure what I am doing wrong. I have included the areas I believe the problem might reside below.  Should any more info be needed, I can share more. It was my hope that someone that has already tried performing something similar may see right away what I am clearly doing wrong.  Any info would be helpful.  Thanks!

Modified webcam.js by adding function 

preProc() {
return tf.tidy(() =&gt; {
  // Reads the image as a Tensor from the webcam &lt;video&gt; element.
  const webcamImage = tf.fromPixels(this.webcamElement);

  //Resize to our image and get back single channel for greyscale
  const croppedImage = this.cropImage(webcamImage, 1);

  // Expand the outer most dimension so we have a batch size of 1.
  const batchedImage = croppedImage.expandDims(0);

  // Normalize the image between -1 and 1. The image comes in between 0-255,
  // so we divide by 127 and subtract 1.
  return batchedImage.toFloat().div(tf.scalar(127)).sub(tf.scalar(1));
});
}

/**
* Crops an image tensor so we get a square image with no white space.
* @param {Tensor4D} img An input image Tensor to crop.
*/
cropImage(img, dim=3) {
  const size = Math.min(img.shape[0], img.shape[1]);
  const centerHeight = img.shape[0] / 2;
  const beginHeight = centerHeight - (size / 2);
  const centerWidth = img.shape[1] / 2;
  const beginWidth = centerWidth - (size / 2);
  return img.slice([beginHeight, beginWidth, 0], [size, size, dim]);
}


From ui.js I am using drawFrame

export function drawFrame(image, canvas) {
  const [width, height] = [300, 165];
  const ctx = canvas.getContext('2d');
  const imageData = new ImageData(width, height);
  const data = image.dataSync();
  for (let i = 0; i &lt; height * width; ++i) {
    const j = i * 4;
    imageData.data[j + 0] = (data[i * 3 + 0] + 1) * 127;
    imageData.data[j + 1] = (data[i * 3 + 1] + 1) * 127;
    imageData.data[j + 2] = (data[i * 3 + 2] + 1) * 127;
    imageData.data[j + 3] = 255;
  }
  ctx.putImageData(imageData, 0, 0);
}


Finally in index.js, when the predict button is pressed the below handler executes

async function predict() {
while (isPredicting) {
  const predictedClass = tf.tidy(() =&gt; {
    // Capture the frame from the webcam.
    const imgmod = webcam.preProc();
    ui.drawFrame(imgmod, grayframe);


    // Returns the index with the maximum probability. This number corresponds
    // to the class the model thinks is the most probable given the input.
    //return predictions.as1D().argMax();
    return imgmod;
  });

  const classId = (await predictedClass.data())[0];
  predictedClass.dispose();

  //ui.predictClass(classId);
  await tf.nextFrame();
  }
  ui.donePredicting();
}



",3,403,"drawframe is drawing the image three times.
It has to do with the shape of the input image and the way height and width are used to crop the image. If the input image were of shape [298, 160], the canvas will not be rendered as there will be an error when trying to access index that are not in data. For instance the size of data is 298 * 160 whereas the last element of the loop would try to access the element 3 * 300 * 160. Since there are no error in the code, it indicates that the size of data is bigger than [298, 160]. At any rate, there is a mismatch in data dimension. The image are drawn 3 times because of the three channels, possibly because it was not removed before.

Instead of implementing your own way of drawing the imagedata, you can consider using tf.toPixel method
",,
Keras strange behavior,https://stackoverflow.com/questions/47636980,multiple GPUs keras weird speedup,,2,508,"I tried the exact code of multiGPU tutorial.
It looks like its somehow the expected output. But to see the expected speed differences I had to increase the number of samples (20000) and needed to height and width to 100 (due to RAM limits).

I'm not completely sure why in my case I didn't see a speedup with two GPU. I expect it to be due to limits of the memory speed. Because my batch size is rather small and each sample is also small. This leads to the effect that the managing of the data needs more time than the actual calculation.
The distribution of the data gets even more time consuming when using 2 GPUs, while the actual runtime on each GPU decreases.
This effect could be proven if I could check the utilization of the graphics cards. Sadly I don't know how to do this.

If anyone has other ideas on this, let me know. Thanks
",,
Keras strange behavior,https://stackoverflow.com/questions/57498998,Strange behavior of keras v1.2.2 vs. keras v2+ (HUGE differences in accuracy),,1,235,"In Keras 2.0, many parameters changed names, there is compatibility layer to keep things working, but somehow it did not apply when using KerasClassifier.

In this part of the code:

estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, batch_size=5, verbose=0)


You are using the old name nb_epoch instead of the modern name of epochs. The default value is epochs=1, meaning that your model was only being trained for one epoch, producing very low quality predictions.

Also note that here:

model.add(Dense(3, init='normal', activation='sigmoid'))


You should be using a softmax activation instead of sigmoid, as you are using the categorical cross-entropy loss:

model.add(Dense(3, init='normal', activation='softmax'))

","I've managed to isolate the issue, if you change nb_epoch to epochs, (All else being exactly equal) the model predicts very good again, in keras 2 as well. I don't know if this is intended behavior or a bug.
",
Keras strange behavior,https://stackoverflow.com/questions/70428593,Vertex AI seems to think a deployed model input shape is different then when predicting locally,,1,442,"When using Vertex Prediction service with Tensorflow models, following is the expected format of input request as per the docs
{
  ""instances"": [
    &lt;value&gt;|&lt;simple/nested list&gt;|&lt;object&gt;,
    ...
  ]
}

For your scenario, try with
{
  ""instances"": [
    [
      [
        [138, 30, 66, ...],
        [130, 20, 56, ...],
        ...
      ],
      [
        [126, 38, 61, ...],
        [122, 24, 57, ...],
        ...
      ],
      ...
    ],
    ...
  ]
}

",,
Keras strange behavior,https://stackoverflow.com/questions/71397660,"Keras Flatten layer returns output shape (None, None)","So, I noticed this strange behavior of Flatten layer of Keras. I'm using TF1.15 and Keras 2.3.0.
Basically the output of the Flatten layer has an unknown shape. It's hard to troubleshoot the model when you can't keep track of the shape. Why is this happening with Flatten layer, and can I do something so it recognizes the shape?
from keras.layers import Flatten
inputs = Input(shape=(3,2,4))
prediction = Flatten()(inputs)
print(inputs.shape, prediction.shape)

(?, 3, 2, 4) (?, ?)


",1,822,"Try using tf.keras instead of just keras:
import tensorflow as tf
print(tf.__version__)
inputs = tf.keras.layers.Input(shape=(3,2,4))
prediction = tf.keras.layers.Flatten()(inputs)
print(inputs.shape, prediction.shape)

1.15.2
(?, 3, 2, 4) (?, 24)

",,
Keras strange behavior,https://stackoverflow.com/questions/59105384,Tensorboard graph orphan layers,"While building a model that includes transfer learning (from VGG-16).
I encounter this strange behavior. Tensorboard graph shows the layers which are not part of the new model but part of the old, above the point of seperation, and they are just dangling there.





When investigating further, model.summary() does not show these layers, model.get_layer(""block4_conv1"") can't find them either, and the keras tf.keras.utils.plot_model doesn't show them too. but if they are not part of the graph, how would tensorboard know about them?

To build the new model, I used the recommended method. 

Model first stage:

    vgg_input_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_tensor=x)
    final_vgg_kayer = vgg_input_model.get_layer(""block3_pool"")
    input_model = tf.keras.Model(inputs=vgg_input_model.inputs, outputs=final_vgg_kayer.output)
    input_model.trainable = True

    x = tf.keras.layers.Conv2D(512, 1, padding=""same"", activation='relu', name=""stage0_final_conv1"")(input_model.output)
    x = tf.keras.layers.Conv2D(512, 1, padding=""same"", activation='relu', name=""stage0_final_conv2"")(x)
    x = tf.keras.layers.Conv2D(256, 1, padding=""same"", activation='relu', name=""stage0_final_conv3"")(x)
    x = tf.keras.layers.Conv2D(128, 1, padding=""same"", activation='relu', name=""stage0_final_conv4"")(x)




TF:2.1 (nightly-2.x)
PY:3.5
Tensorboard: 2.1.0a20191124

",1,255,"After trying multiple methods, I came to the conclusion that the recommended way is wrong. doing model_b=tf.keras.Model(inputs=model_a.inputs,outputs=model_a.get_layet(""some_layer"").output) will lead to dangling layers from model_a.
Using tf.keras.backend.clear_session() in between may cleans the keras graph, but tensorboard's graph is left empty then.

The best solution I found is config+weights copy of the required model, layer by layer.
And rebuilding the connections in a new model. that way there is no relationship whatsoever in the Keras graph between the two models.
(This is simple for a sequential model like VGG, but might be more difficult for something like ResNet)



Sample code:

tf.keras.backend.clear_session()

input_shape = (368, 368, 3)  #only the input shape is shared between the models
#transfer learning model definition
input_layer_vgg = tf.keras.layers.Input(shape=input_shape)
vgg_input_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_tensor=input_layer_vgg)
name_last_layer = ""block3_pool"" #the last layer to copy

tf.keras.backend.clear_session() #clean the graph from the transfer learning model

input_layer = tf.keras.layers.Input(shape=input_shape) #define the input layer for the first model
x=input_layer
for layer in vgg_input_model.layers[1:]:    #copy over layers, without the other input layer
    config=layer.get_config()  #get config
    weights=layer.get_weights() #get weights
    #print(config)
    copy_layer=type(layer).from_config(config) #create the new layer from config
    x=copy_layer(x) #connect to previous layers,
                    #required for the proper sizing of the layer,
                    #set_weights will not work without it
    copy_layer.set_weights(weights)    

    if layer.name == name_last_layer:
        break
del vgg_input_model

input_model=tf.keras.Model(inputs=input_layer,outputs=x) #create the new model,
                                                        #if needed x can be used further doen the line

",,
Keras strange behavior,https://stackoverflow.com/questions/52880384,Keras loss becomes nan only at epoch end,"I am observing some strange behavior from Keras. I am training a small model where the training loss becomes nan only at the end of the first epoch.

So if I have 100 batches, and I terminate training at batch 99, then resume for another 99 it trains fine. Otherwise, once it reaches the end of an epoch it always returns nan. 

I am using a custom loss function:

def corr(x, y):
    xc = x - K.mean(x)
    yc = y - K.mean(y)
    r_num = K.mean(xc*yc) 
    r_den = K.std(x)*K.std(y)
    return r_num/r_den


And I have tried all of the standard tricks like dropping my learning rate, clipping the norm and value of my gradient, and increasing batch size. Only in the event of increasing my batch size to something unrealistic like 100,000 (I have 1 million data points) does it actually continue past an epoch, but I would like to understand what is going on at the end that is causing this strange behavior. I also tried different optimizers (currently using Adam), and tried this on different systems to make sure it wasn't a problem on my one computer. 

My input and output is one dimensional and my model is summarized below. 

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_7 (InputLayer)         (None, 1)                 0         
_________________________________________________________________
dense_7 (Dense)              (None, 100)               200       
_________________________________________________________________
dense_8 (Dense)              (None, 100)               10100     
_________________________________________________________________
dense_9 (Dense)              (None, 1)                 101       
=================================================================
Total params: 10,401
Trainable params: 10,401
Non-trainable params: 0
_________________________________________________________________


Does Keras so something special at the end of an epoch? I couldn't find anything other than the standard logger callback. I also wrote a custom callback which evaluates my model each batch and stores the output, and when I plot it over time it does not appear to blow up or do anything strange. It just looks like it's slowly improving, then the training dies.
",1,874,"Probably it is caused by a division by zero in the loss function. Make sure the denominator is always positive by adding a small constant to it. You can use K.epsilon() for this purpose:

    return r_num / (r_den + K.epsilon())

",,
Keras strange behavior,https://stackoverflow.com/questions/49910778,Keras sequence to sequence model loss increases without bound,"I'm building a sequence to sequence model in Keras to correct simple spelling mistakes that might occur. I'm following mostly this tutorial.

I have a pretty involved piece of code to generate random misspellings in words, and then I send the outputs of that: ([misspelled sentence,offset_sentence],original_sentence) into the model. The model I built looks pretty much exactly the same as the one in the tutorial:

    print('Training tokenizer...')
    tokenizer = CharToken()
    tokenizer.train_on_corpus(brown)

    num_chars = len(tokenizer.char_dict)
    alpha = 0.001

    encoder_inputs = Input(shape=(None,num_chars))
    encoder = LSTM(128,return_state=True)
    encoder_outputs,state_h,state_c = encoder(encoder_inputs)

    encoder_states = [state_h,state_c]

    decoder_inputs = Input(shape=(None,num_chars))
    decoder = LSTM(128,return_sequences=True,return_state=True)
    decoder_outputs,_,_ = 
    decoder(decoder_inputs,initial_state=encoder_states)
    decoder_dense = Dense(num_chars,activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)

    model = keras.models.Model(inputs= 
    [encoder_inputs,decoder_inputs],outputs=decoder_outputs)
    optim = keras.optimizers.rmsprop(lr=alpha,decay=1e-6)
    model.compile(optimizer=optim,loss='categorical_crossentropy')
    model.summary()
    model.fit_generator(tokenizer.batch_generator(),
    steps_per_epoch=1000,epochs=1)


I'm sure that the problem is not in the tokenizer since I've gone through it and checked all of the outputs multiple times. The batch_generator method in that class outputs a tuple of one hot vectors representing ([misspelled sentence,offset_sentence],original_sentence). I've tried changing the hyperparmaters, including making the learning rate a miniscule 0.00001 but no matter what I do, the training loss always starts at around 11 and then just keep increasing... 

Can anybody figure out what I did wrong? 

EDIT: I did one more step of debugging where I removed the tokenizer from the equation and just tried to train the network on 3 random one-hot arrays. I reduced the complexity a lot by limiting them to have only 10 possible inputs/outputs (characters). The loss quickly rose to ~100 and stayed there. I expect for 10 possible outcomes that random guessing would get me a loss of around -ln(1/10)~2.3, certainly 100 is way too high. I also expect that even though I fed the network random arrays that it would eventually memorize those arrays and over-fit and the loss would decrease, but that's not the case. The loss stays around 100. I can't figure out what's going wrong...

EDIT 2: Some more debugging. I've beaten the model into a much simpler one by forcing the inputs and outputs to have the same length. This is not too bad for a spell corrector as long as the spelling mistakes aren't deleting or inserting too many characters (I pad the sequences anyways to make them the same length which allows me to be able to train on batches). However, the model still exhibits the same behavior. I've also tried running the model on random numbers and get the same 100ish loss:

    num_chars=10
    alpha = 0.001

    X = np.random.randint(10,size=(10000,30,10))
    Y = np.random.randint(10,size=(10000,30,10))

    inputs = Input(shape=(None,num_chars))
    x = LSTM(128,return_sequences=True)(inputs)
    x = LSTM(128,return_sequences=True)(x)
    output = Dense(num_chars,activation='softmax')(x)

    model = keras.models.Model(inputs=inputs,outputs=output)
    optim = keras.optimizers.rmsprop(lr=alpha,decay=1e-6)
    model.compile(optimizer=optim,loss='categorical_crossentropy')
    model.summary()
    model.fit(X,Y) 


I'm beginning to wonder if there's something wrong with my installation of keras or something like that. I've run sequence models like this before many times on my other machine, and I've never observed this strange behavior. 

EDIT 3: I just realized my debugging was flawed. I didn't turn the random Y array into a one-hot vector. When I do change it to a one-hot vector, the loss is as expected, about -ln(1/num_chars). This means that the problem is probably in my tokenizer generator. But I can't figure out what the problem is since I've printed out the output and saw that they were indeed one-hot vectors.
",1,517,"To those that happen upon this question in the future. I have figured out the problem, and unfortunately the problem was not in the blocks of code I posted so there was no way to debug the code from here. Sorry about that. 

The problem was that I initialized the one hot vectors in the wrong place inside the generator function. So they weren't being re-zeroed after every batch and they just got filled up with 1's, leading to worse and worse loss. It was so hard to catch this problem because the generator actually works perfectly for the first loop so whenever I printed out its outputs I saw the correct output.  
",,
Keras strange behavior,https://stackoverflow.com/questions/46447882,Weights of CNN model go to really small values and after NaN,"I am not able to understand the reason why the weights of following model are going smaller and smaller until NaN during training. 

The model is the following:

def initialize_embedding_matrix(embedding_matrix):
    embedding_layer = Embedding(
        input_dim=embedding_matrix.shape[0],
        output_dim=embedding_matrix.shape[1],
        weights=[embedding_matrix],
        trainable=True)
    return embedding_layer

def get_divisor(x):
    return K.sqrt(K.sum(K.square(x), axis=-1))


def similarity(a, b):
    numerator = K.sum(a * b, axis=-1)
    denominator = get_divisor(a) * get_divisor(b)
    denominator = K.maximum(denominator, K.epsilon())
    return numerator / denominator


def max_margin_loss(positive, negative):
    loss_matrix = K.maximum(0.0, 1.0 + negative - Reshape((1,))(positive))
    loss = K.sum(loss_matrix, axis=-1, keepdims=True)
    return loss


def warp_loss(X):
    z, positive_entity, negatives_entities = X
    positiveSim = Lambda(lambda x: similarity(x[0], x[1]), output_shape=(1,), name=""positive_sim"")([z, positive_entity])
    z_reshaped = Reshape((1, z.shape[1].value))(z)
    negativeSim = Lambda(lambda x: similarity(x[0], x[1]), output_shape=(negatives_titles.shape[1].value, 1,), name=""negative_sim"")([z_reshaped, negatives_entities])
    loss = Lambda(lambda x: max_margin_loss(x[0], x[1]), output_shape=(1,), name=""max_margin"")([positiveSim, negativeSim])
    return loss

def mean_loss(y_true, y_pred):
    return K.mean(y_pred - 0 * y_true)

def build_nn_model():
    wl, tl = load_vector_lookups()
    embedded_layer_1 = initialize_embedding_matrix(wl)
    embedded_layer_2 = initialize_embedding_matrix(tl)

    sequence_input_1 = Input(shape=(_NUMBER_OF_LENGTH,), dtype='int32',name=""text"")
    sequence_input_positive = Input(shape=(1,), dtype='int32', name=""positive"")
    sequence_input_negatives = Input(shape=(10,), dtype='int32', name=""negatives"")

    embedded_sequences_1 = embedded_layer_1(sequence_input_1)
    embedded_sequences_positive = Reshape((tl.shape[1],))(embedded_layer_2(sequence_input_positive))
    embedded_sequences_negatives = embedded_layer_2(sequence_input_negatives)

    conv_step1 = Convolution1D(
        filters=1000,
        kernel_size=5,
        activation=""tanh"",
        name=""conv_layer_mp"",
        padding=""valid"")(embedded_sequences_1)

    conv_step2 = GlobalMaxPooling1D(name=""max_pool_mp"")(conv_step1)
    conv_step3 = Activation(""tanh"")(conv_step2)
    conv_step4 = Dropout(0.2, name=""dropout_mp"")(conv_step3)
    z = Dense(wl.shape[1], name=""predicted_vec"")(conv_step4) # activation=""linear""

    loss = warp_loss([z, embedded_sequences_positive, embedded_sequences_negatives])
    model = Model(
        inputs=[sequence_input_1, sequence_input_positive, sequence_input_negatives],
        outputs=[loss]
        )
    model.compile(loss=mean_loss, optimizer=Adam())
    return model

model = build_nn_model()
x, y_real, y_fake = load_x_y()
    X_train = {
    'text': x_train,
    'positive': y_real_train,
    'negatives': y_fake_train
}

model.fit(x=X_train,  y=np.ones(len(x_train)), batch_size=10, shuffle=True, validation_split=0.1, epochs=10)


To describe the model a bit: 


I have two pre-trained embeddings(wl,tl) and I initialize the Keras embeddings with these values.
There are 3 inputs. The sequence_input_1 has integers as input (indexes of words. ex. [42, 32 .., 4]). On them sequence.pad_sequences(X, maxlen=_NUMBER_OF_LENGTH) is used to have fixed length. sequence_input_positive which is an integer of the positive output and sequence_input_negatives which are N random negative outputs (10 in the code above) for each example.
max_margin_loss measures the difference between the cosinus_similarity(positive_example, sequence_input_1) andcosinus_similarity(negative_example[i], sequence_input_1) and the Adam optimizer is used to minimize loss.


While training this model even with only 20 data points the weights in the Convolution1D and Dense goes to NaN. If I add more data points the embedding weights go to NaN too. I can observe that as the model runs the weights are going smaller and smaller until they go to NaN. Something noticable also is that the loss does not go to NaN. When weights reach NaN, the loss goes to zero.

I am unable to find what is going wrong. 

This is what I tried until now:


I have seen that people are using stochastic gradient descent when hinge loss is used. Using SGD optimizer didn't change something in the behavior here.
changed the number of batch size. No change in behavior.
checked input data not to have nan values.
normalized the input matrix (pre-trained data) for embedding with np.linalg.norm
transform  pre-trained matrix from float64 to float32


Do you see anything strange in the architecture of the model? If not: I am unable to find a way to debug the architecture in order to understand why weights are going smaller and smaller till reach NaN. Is there some steps people are using when they notice this kind of behaviour?

Edit:

By using trainable=False in the Embeddings this behaviour of nan weights is NOT observed, and the training seems to have smooth results. However I want the embeddings to be trainable. So why this behavior when the embeddings are trainable??

Edit2:

Using trainable=True and by uniformly randomly initializing the weights embeddings_initializer='uniform' the training is smooth. So the reason happening is my word embeddings. I have checked my pre-trained word embeddings and there are no NaN values. I have also normalized them in case this was causing it but no lack. Cant think anything else why these specific weights are giving this behaviour.

Edit3:

It seems that what causing this was that a lot of rows from one of the Embeddings trained in gensim where all zeros. ex.

[0.2, 0.1, .. 0.3],
[0.0, 0.0, .. 0.0],
[0.0, 0.0, .. 0.0],
[0.0, 0.0, .. 0.0],
[0.2, 0.1, .. 0.1]


It was not so easy to find it as the dimension of the embeddings where really big.

I am leaving this question open in case someone comes up with something similar or wants to answer the question  asked above: ""Is there some steps people are using when they notice this kind of behaviour?""
",1,689,"By your edits, it got a little easier to find the problem. 

Those zeros passed unchanged to the warp_loss function. 
The part that went through the convolution remained unchanged at first, because any filters multiplied by zero result in zero, and the default bias initializer is also 'zeros'. The same idea applies to the dense (filters * 0 = 0 and bias initializer = 'zeros')

That reached this line: return numerator / denominator and caused an error (division by zero)

It's a common practice I've seen in many codes to add K.epsilon() to avoid this:

return numerator / (denominator + K.epsilon())

",,
Keras strange behavior,https://stackoverflow.com/questions/47897199,Bug when using TensorFlow-GPU + Python multiprocessing?,,0,2676,"I am coming with the same error when trying to use tensorflow and multiprocessing

E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED


but in different environment tf1.4 + cuda 8.0 + cudnn 6.0.
matrixMulCUBLAS in sample codes works fine.
I wonder the correct solution too!
And the reference failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED on a AWS p2.xlarge instance did not work for me.
",,
Keras strange behavior,https://stackoverflow.com/questions/53363060,keras unable to call model.predict_classes for multiple times,,0,1226,"Try loading model from file outside the function, and give the model object as argument to the function def predictOne(imgPath, model). This will also be much faster, since the weights don't need to be loaded from disk every time a prediction is needed. 

If you want to keep loading model inside the function, import the backend:

from keras import backend as K


and then 

K.clear_session() 


before loading the model.
","class one_model:
    session = None
    graph = None 
    loadModel = None
    __instance = None
    @staticmethod
    def getInstance(modelPath):
        """""" Static access method. """"""
        if one_model.__instance == None:
            one_model.__instance = one_model(modelPath)
        return one_model.__instance
        
    def __init__(self, modelPath):
        self.modelPath = modelPath
        self.session = tf.Session(graph=tf.Graph())
        self.loadOneModel()
            
    def loadOneModel(self):
        try:
            with self.session.graph.as_default():
                K.set_session(self.session)
                self.loadModel = keras.models.load_model(self.modelPath)               
        except Exception as e:
            logging.error(str(e))
            print(str(e))
                        
    def getPredictionOne(self, input_file_path): 
        #Predict the data once the model is loaded
        if self.loadModel is not None and self.session is not None: 
            try:
                image = load_img(input_file_path, target_size=inputShape)
                image = img_to_array(image)
                image = np.expand_dims(image, axis=0)
                image = preprocess(image)
                with self.session.graph.as_default():
                    K.set_session(self.session)
                    preds = self.loadModel.predict(image)
                    return preds
            except Exception as e:
                logging.error(str(e))
        
        return -1


if __name__== ""__main__"": 
    #First Model 
    data = web.input()
        fileapth = data.imagefilepath  
        modelfilepath = data.modelfilepath
        one_modelObj = one_model.getInstance(modelfilepath)        
        value = one_modelObj.getPredictionOne(fileapth) 

",
Keras strange behavior,https://stackoverflow.com/questions/56323567,Siamese neural network with two pre-trained ResNet 50 - strange behavior while testing model,"I built siamese neural network, using Keras lib for it.
My model has two inputs with shape (64,64,3), two pre-trained ResNet-50.
Loss function is binary cross entropy.

The model is based on this paper a link

During train I have very good trait/val accuracy, about 0.99/0.98, and low loss 0.01/0.05.

But when I test my saved model, I get bad results. The model can't recognize even two the same pictures. 

Also I noticed strange behavior:
the greater the number of epochs the result is worse. 
For example, comparing two identical images, trained model with 10 epoch  gives prediction:
""8.jpg"": 0.5180479884147644
but the same model trained with 100 epoch gives
""8.jpg"": 5.579867080537926E-13
However for 100 epoch I have better train results.

I've tried different model for CNN: ResNet18, different input shapes, like (224,224,3) or (128,128,3). 

Also I've triad use not pre-train model, only ResNet50/ResNet18 without pre-trained weights. But I have the same bad results while testing real model.

My code is

def create_base_model(image_shape, dropout_rate, suffix=''):
    I1 = Input(shape=image_shape)
    model = ResNet50(include_top=False, weights='imagenet', input_tensor=I1, pooling=None)
    model.layers.pop()
    model.outputs = [model.layers[-1].output]
    model.layers[-1].outbound_nodes = []

    for layer in model.layers:
        layer.name = layer.name + str(suffix)
        layer.trainable = False

    flatten_name = 'flatten' + str(suffix)

    x = model.output
    x = Flatten(name=flatten_name)(x)
    x = Dense(1024, activation='relu')(x)
    x = Dropout(dropout_rate)(x)
    x = Dense(512, activation='relu')(x)
    x = Dropout(dropout_rate)(x)

    return x, model.input


def create_siamese_model(image_shape, dropout_rate):

    output_left, input_left = create_base_model(image_shape, dropout_rate)
    output_right, input_right = create_base_model(image_shape, dropout_rate, suffix=""_2"")

    L1_layer = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))
    L1_distance = L1_layer([output_left, output_right])
    L1_prediction = Dense(1, use_bias=True,
                          activation='sigmoid',
                          kernel_initializer=RandomNormal(mean=0.0, stddev=0.001),
                          name='weighted-average')(L1_distance)

    prediction = Dropout(0.2)(L1_prediction)

    siamese_model = Model(inputs=[input_left, input_right], outputs=prediction)

    return siamese_model

siamese_model = create_siamese_model(image_shape=(64, 64, 3),
                                         dropout_rate=0.2)

siamese_model.compile(loss='binary_crossentropy',
                      optimizer=Adam(lr=0.0001),
                      metrics=['binary_crossentropy', 'acc'])
siamese_model.fit_generator(train_gen,
                            steps_per_epoch=1000,
                            epochs=10,
                            verbose=1,
                            callbacks=[checkpoint, tensor_board_callback, lr_reducer, early_stopper, csv_logger],
                            validation_data=validation_data,
                            max_q_size=3)

siamese_model.save('siamese_model.h5')



# and the my prediction
siamese_net = load_model('siamese_model.h5', custom_objects={""tf"": tf})

X_1 = [image, ] * len(markers)
batch = [markers, X_1]
result = siamese_net.predict_on_batch(batch)

# I've tried also to check identical images 
markers = [image]
X_1 = [image, ] * len(markers)
batch = [markers, X_1]
result = siamese_net.predict_on_batch(batch)


I have some doubts about my prediction method.
Could someone please help me to find what is wrong with predictions?
",0,3993,"What you are getting is expected. I'm not sure what you mean by


  Also I noticed strange behavior: the greater the number of epochs the result is worse.  


But the results you shown are valid and expected. Let's start with what the model is outputting. Your model output is (normalized)distance between the first and second inputs. If the inputs are similar, then the distance should be close to zero. As number of training step increases the model learns to identify the inputs, i.e if the inputs are similar the model learns to output values close to zero, and if the inputs are different the model learns to output values close to one. So, 


  ... trained model with 10 epoch gives prediction: ""8.jpg"": 0.5180479884147644 but the same model trained with 100 epoch gives ""8.jpg"": 5.579867080537926E-13 However for 100 epoch I have better train results.  


, confirms that the model has learned that the two inputs are similar and outputs 5.579867080537926E-13 ~ 0(approximately close to 0). 

Although the model is performing well, there is one issue I've observed in the model definition:- The output layer is dropout layer. Dropout is not valid output layer. What you are doing by this setting is, randomly with probability 0.2 you are setting the output of the model to be zero. 

Let's assume the target variable has 1(the two inputs are different), and model has learnt to identify the images correctly and outputs value close to 1 before the dropout layer. Let's further assume that the dropout layer has decided to set the output to be zero. So the model output will be zero. Even though the layers before dropout layer have performed well, because of the dropout layer, they will be penalized. If this is not what you are looking then remove the last dropout layer.

L1_prediction = Dense(1, use_bias=True,
                    activation='sigmoid',
                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.001),
                    name='weighted-average')(L1_distance)


siamese_model = Model(inputs=[input_left, input_right], outputs=L1_prediction)


However, sometimes this behavior is needed if one want to add noise to the model. This has the same effect with randomly altering the target variable when the value is 1. 
",,
Keras strange behavior,https://stackoverflow.com/questions/56029955,Extremely Strange behavior of my own loss function written in keras (always output similar values),"I am implementing a CNN based deep learning algorithm to provide continuous ranking of a set of images. My plan is to feed the network with batches of images that is already ranked in the correct order, and evaluate the ranking errors in the custom loss function, hopefully the network can learn how to produce a relative number to indicating the position of a given image in a sequence. 

I have come up with the simple custom loss function as below:

def my_loss_function(y_true, y_pred):
    #diff = y_pred[1:,:] - y_pred[:-1, :] //previous implementation, same problem
    indx1 = tf.range(1, 128)
    indx2 = tf.range(0, 127)
    diff = tf.gather(y_pred, indx1) - tf.gather(y_pred, indx2)
    order_loss = -K.sum(K.log(K.sigmoid(diff)+K.epsilon()))

    return order_loss


As you can see that I actually don't need the y-true data, so I generated a dummy y_true data just to avoid the checking error by keras. 

The strange problem I have now is this loss function always starts with almost the same value when I run it, even after I changed the return to 2*order_loss or change K.sum to K.mean or other functions. In training, the loss can reduce very slowly and will plateau quickly. I could not think of any reason that might cause this problem, did anyone have similar experience or know how to solve this problem?



Update:

Even after I changed the loss function to:

def my_loss_function(y_true, y_pred):
    diff = alpha*y_pred  # alpha is a constant
    order_loss = -K.sum(K.log(K.sigmoid(diff)+K.epsilon()))
    return order_loss


it still has the same problem, when I change the alpha value, the output doesn't change accordingly. The int_shape of y_pred is (none, 1).


",0,283,"Your question is difficult to answer without exactly working with the problem, but I can suggest you to do some debugging inside the loss function. Here is an example of how you can print some stuff and see what is actually going in there.

def my_loss_function(y_true, y_pred):
    diff = y_pred[1:,:] - y_pred[:-1, :] 
    order_loss = -K.sum(K.log(K.sigmoid(diff)+K.epsilon()))

    # anything you like to print
    print_op = tf.print(""\nDebug : "", diff, order_loss, K.get_variable_shape(y_pred))  
    with tf.control_dependencies([print_op]): # this will force the print_op to be run
        return K.identity(order_loss)
    #return K.identity(order_loss)



I hope this will be helpful for you to see if the shapes and values are as expected and if you see any discrepancies.
Let me know if you find anything fishy, and then we can take it forward.
",,
Keras strange behavior,https://stackoverflow.com/questions/55954212,Unusual behavior of ADAM optimizer with AMSGrad,"I am trying some 1, 2, and 3 layer LSTM networks to classify land cover of some selected pixels from a Landsat time-series spectral data. I tried different optimizers (as implemented in Keras) to see which of them is better, and generally found AMSGrad variant of ADAM doing a relatively better job in my case. However, one strange thing to me is that for the AMSGrad variant, the training and test accuracies start at a relatively high value from the first epoch (instead of increasing gradually) and it changes only slightly after that, as you see in the below graph.

Performance of ADAM optimizer with AMSGrad on

Performance of ADAM optimizer with AMSGrad off

I have not seen this behavior in any other optimizer. Does it show a problem in my experiment? What can be the explanation for this phenomenon?
",0,704,"Pay attention to the number of LSTM layers. They are notorious for easily overfitting the data. Try a smaller model initially(less number of layers), and gradually increase the number of units in a layer. If you notice poor results, then try adding another LSTM layer, but only after the previous step has been done.

As for the optimizers, I have to admit I have never used AMSGrad. However, the plot with regard to the accuracy does seem to be much better in case of the AMSGrad off. You can see that when you use AMSGrad the accuracy on the training set is much better than that on the test set, which a strong sign of overfitting. 

Remember to keep things simple, experiment with simple models and generic optimizers.
",,
Keras strange behavior,https://stackoverflow.com/questions/47798854,Keras CNN accuracy and loss are constant,"I am building a keras CNN model using ResNet50 utilizing transfer learning. For some reason my accuracy and loss are exactly the same for every epoch. Strangely, I am seeing the same behavior with similar code but with VGG19. This leads me to believe that the issue is not with the actual model code and somewhere in the pre-processing. I have tried adjusting learning rates, changing optimizer, image resolution, freezing layers etc. and the scores do not change. I went into my image directories to check if my two different classes are mixed, and they are not. What is the issue? I just want to say thank you ahead of time.

P.S. I am training on ~2000 images and have two classes. 

import numpy as np
import pandas as pd

import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from keras.models import Sequential, Model, load_model
from keras.layers import Conv2D, GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import applications
from keras import optimizers

img_height, img_width, img_channel = 400, 400, 3 #change chanel to 1 instead of three since it is black and white

base_model = applications.ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, img_channel))

# add a global spatial average pooling layer
x = base_model.output
x = GlobalAveragePooling2D()(x)
# let's add a fully-connected layer
x = Dense(512, activation='relu',name='fc-1')(x)
#x = Dropout(0.5)(x)
x = Dense(256, activation='relu',name='fc-2')(x)
#x = Dropout(0.5)(x)
# and a logistic layer -- let's say we have 2 classes
predictions = Dense(1, activation='softmax', name='output_layer')(x)

model = Model(inputs=base_model.input, outputs=predictions)
model.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=0.1),
              metrics=['accuracy'])

model.summary()

from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint

batch_size = 6

# prepare data augmentation configuration
train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.1,
        height_shift_range=0.1,
        shear_range=0.1,
        zoom_range=0.1,
        horizontal_flip=True,
        vertical_flip=True)


test_datagen = ImageDataGenerator(rescale=1./255)

#possibely resize the image
train_generator = train_datagen.flow_from_directory(
        ""../Train/"",
        target_size=(img_height, img_width),
        batch_size=batch_size,
        class_mode='binary',
        shuffle=True
)

validation_generator = test_datagen.flow_from_directory(
        ""../Test/"",
        target_size=(img_height, img_width),
        batch_size=batch_size,
        class_mode='binary',
        shuffle=True)

epochs = 10

history = model.fit_generator(
        train_generator,
        steps_per_epoch=2046 // batch_size,
        epochs=epochs,
        validation_data=validation_generator,
        validation_steps=512 // batch_size,
        callbacks=[ModelCheckpoint('snapshots/ResNet50-transferlearning.model', monitor='val_acc', save_best_only=True)])


This is the output given by keras:

Epoch 1/10
341/341 [==============================] - 59s 172ms/step - loss: 7.0517 - acc: 0.5577 - val_loss: 7.0334 - val_acc: 0.5588
Epoch 2/10
341/341 [==============================] - 57s 168ms/step - loss: 7.0517 - acc: 0.5577 - val_loss: 7.0334 - val_acc: 0.5588
Epoch 3/10
341/341 [==============================] - 56s 165ms/step - loss: 7.0517 - acc: 0.5577 - val_loss: 7.0334 - val_acc: 0.5588
Epoch 4/10
341/341 [==============================] - 57s 168ms/step - loss: 7.0517 - acc: 0.5577 - val_loss: 7.0334 - val_acc: 0.5588
Epoch 5/10
341/341 [==============================] - 57s 167ms/step - loss: 7.0517 - acc: 0.5577 - val_loss: 7.0334 - val_acc: 0.5588

",0,1226,"predictions = Dense(1, activation='softmax', name='output_layer')(x)


The Dense layer represents how many different classes are there which you want to classify and hence for binary classification you will require 2 where you have written 1.

So change that line to.

 predictions = Dense(2, activation='softmax', name='output_layer')(x)


Just a note, always try to keep a variable to handle number of classes, something like 

predictions = Dense(num_classes, activation='softmax', name='output_layer')(x)


and then define num_classes at the start of your code for better flexibility and readability.

You can check documentation about Dense layer here : https://faroit.github.io/keras-docs/2.0.0/layers/core/
","The final layer should have a 'sigmoid' activation instead of softmax since it is binary classification. 
",
Keras strange behavior,https://stackoverflow.com/questions/64119338,Why Keras MAPE metric is exploding during training but MSE loss is not?,"I implemented an LSTM with attention in Keras to reproduce this paper. The strange behavior is simple: I have an MSE loss function and an MAPE and MAE as metrics. During training the MAPE is exploding but the MSE and MAE seem to  train normally:
Epoch 1/20
275/275 [==============================] - 191s 693ms/step - loss: 0.1005 - mape: 15794.8682 - mae: 0.2382 - val_loss: 0.0334 - val_mape: 24.9470 - val_mae: 0.1607
Epoch 2/20
275/275 [==============================] - 184s 669ms/step - loss: 0.0099 - mape: 6385.5464 - mae: 0.0725 - val_loss: 0.0078 - val_mape: 11.3268 - val_mae: 0.0803
Epoch 3/20
275/275 [==============================] - 186s 676ms/step - loss: 0.0025 - mape: 5909.3735 - mae: 0.0369 - val_loss: 0.0131 - val_mape: 14.9827 - val_mae: 0.1061
Epoch 4/20
275/275 [==============================] - 187s 678ms/step - loss: 0.0015 - mape: 4746.2788 - mae: 0.0278 - val_loss: 0.0142 - val_mape: 16.1894 - val_mae: 0.1122
Epoch 5/20
 30/275 [==&gt;...........................] - ETA: 2:38 - loss: 0.0012 - mape: 9.3647 - mae: 0.0246

The MAPE is exploding at the end of each epoch. What could be the cause of this specific behavior?
The MAPE is still decreasing with each epoch so is this not really an issue since it is not hindering the training process?
",-1,3992,"Your loss and MAPE are decreasing so it sounds good. But if you fear the high values in MAPE you can tell if there is a Y value near zero. Because MAPE is a percentage error.
MAPE results can be misleading. From Wikipedia:

Although the concept of MAPE sounds very simple and convincing, it has
major drawbacks in practical application, and there are many studies
on shortcomings and misleading results from MAPE.

It cannot be used if there are zero values (which sometimes happens for example in demand data) because there would be a division
by zero.
For forecasts which are too low the percentage error cannot exceed 100%, but for forecasts which are too high there is no upper limit to
the percentage error.
MAPE puts a heavier penalty on negative errors, than on positive errors.

To overcome these issues with MAPE, there are some other measures
proposed in literature:

Mean Absolute Scaled Error (MASE)
Symmetric Mean Absolute Percentage Error (sMAPE)
Mean Directional Accuracy (MDA)
Mean Arctangent Absolute Percentage Error (MAAPE)


",,
Keras strange result,https://stackoverflow.com/questions/65023353,Difference between keras.metrics.Accuracy() and &quot;accuracy&quot;,,16,7318,"When you are mentioning keras.metrics.Accuracy() you are explicitly asking the library to calculate the metric Accuracy which is simple comparison between how many target values matches the predicted values.
However, when you mention the string accuracy then depending on the type of loss you have chosen a different Metric gets selected. This is what is mentioned in the documentation of Keras,
When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy, tf.keras.metrics.CategoricalAccuracy, tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well.
Hence, as CategoricalCrossEntropy is the loss so CategoricalAccuracy gets calculated in case 2. This is based on finding argmax and then comparing the one-hot encoding. As a result, you are seeing better accuracy values in case 2 and very bad in case 1.
So the string accuracy will not always mean the metric function Accuracy().
The explanation of different metrics for reference, https://keras.io/api/metrics/accuracy_metrics/
The explanation of argument metrics for reference,
https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile
","It is because you have to specify your metrics into a list. Try this :
model.compile(
        loss=keras.losses.CategoricalCrossentropy(),
        optimizer=keras.optimizers.Adam(),
        metrics=[keras.metrics.Accuracy()]
    )

and you should get the same results.
Edit :
keras.metrics.Accuracy() calculates the accuracy between the equality of the predition and the ground truth (see doc). In your case, you want to calculate the accuracy of the match in the correct class. So you should use keras.metrics.BinaryAccuracy()or keras.metrics.CategroicalAccuracy() according to your problem.
",
Keras strange result,https://stackoverflow.com/questions/45193744,Keras + Tensorflow strange results,,8,965,,,
Keras strange result,https://stackoverflow.com/questions/48728099,Word-level Seq2Seq with Keras,"I was following the Keras Seq2Seq tutorial, and wit works fine. However, this is a character-level model, and I would like to adopt it to a word-level model. The authors even include a paragraph with require changes but all my current attempts result in an error regarding wring dimensions.

If you follow the character-level model, the input data is of 3 dims: #sequences, #max_seq_len, #num_char since each character is one-hot encoded. When I plot the summary for the model as used in the tutorial, I get:

Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, None, 71)     0                                            
_____________________________________________________________________________ __________________
input_2 (InputLayer)            (None, None, 94)     0                                            
__________________________________________________________________________________________________
lstm_1 (LSTM)                   [(None, 256), (None, 335872      input_1[0][0]                    
__________________________________________________________________________________________________
lstm_2 (LSTM)                   [(None, None, 256),  359424      input_2[0][0]                    
                                                                 lstm_1[0][1]                     
                                                                 lstm_1[0][2]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, None, 94)     24158       lstm_2[0][0]                     
==================================================================================================


This compiles and trains just fine.

Now this tutorial has section ""What if I want to use a word-level model with integer sequences?"" And I've tried to follow those changes. Firstly, I encode all sequences using a word index. As such, the input and target data is now 2 dims: #sequences, #max_seq_len since I no longer one-hot encode but use now Embedding layers. 

encoder_input_data_train.shape   =&gt;  (90000, 9)
decoder_input_data_train.shape   =&gt;  (90000, 16)
decoder_target_data_train.shape  =&gt;  (90000, 16)


For example, a sequence might look like this:

[ 826.  288. 2961. 3127. 1260. 2108.    0.    0.    0.]


When I use the listed code:

# encoder
encoder_inputs = Input(shape=(None, ))
x = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)
x, state_h, state_c = LSTM(latent_dim, return_state=True)(x)
encoder_states = [state_h, state_c]

# decoder
decoder_inputs = Input(shape=(None,))
x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)
x = LSTM(latent_dim, return_sequences=True)(x, initial_state=encoder_states)
decoder_outputs = Dense(num_decoder_tokens, activation='softmax')(x)

model = Model([encoder_inputs, decoder_inputs], decoder_outputs)


the model compiles and looks like this:

Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, None)         0                                            
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, None)         0                                            
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, None, 256)    914432      input_35[0][0]                   
__________________________________________________________________________________________________
embedding_33 (Embedding)        (None, None, 256)    914432      input_36[0][0]                   
__________________________________________________________________________________________________
lstm_32 (LSTM)                  [(None, 256), (None, 525312      embedding_32[0][0]               
__________________________________________________________________________________________________
lstm_33 (LSTM)                  (None, None, 256)    525312      embedding_33[0][0]               
                                                                 lstm_32[0][1]                    
                                                                 lstm_32[0][2]                    
__________________________________________________________________________________________________
dense_21 (Dense)                (None, None, 3572)   918004      lstm_33[0][0]                    


While compile works, training

model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=32, epochs=1, validation_split=0.2)


fails with the following error: ValueError: Error when checking target: expected dense_21 to have 3 dimensions, but got array with shape (90000, 16) with the latter being the shape of the decoder input/target. Why does the Dense layer an array of the shape of the decoder input data?

Things I've tried:


I find it a bit strange that the decoder LSTM has a return_sequences=True since I thought I cannot give a sequences to a Dense layer (and the decoder of the original character-level model does not state this). However, simply removing or setting return_sequences=False did not help. Of course, the Dense layer now has an output shape of (None, 3572).
I don' quite get the need for the Input layers. I've set them to shape=(max_input_seq_len, ) and shape=(max_target_seq_len, ) respectively so that the summary doesn't show (None, None) but the respective values, e.g., (None, 16). No change.
In the Keras Docs I've read that an Embedding layer should be used with input_length, otherwise a Dense layer upstream cannot compute its outputs. But again, still errors when I set input_length accordingly.


I'm a bit at a deadlock right? Am I even on the right track or do I missing something more fundamentally. Is the shape of my data wrong? Why does the last Dense layer get array with shape (90000, 16)? That seems rather off.

UPDATE: I figured out that the problem seems to be decoder_target_data which currently has the shape (#sample, max_seq_len), e.g., (90000, 16). But I assume I need to one-hot encode the target output with respect to the vocabulary: (#sample, max_seq_len, vocab_size), e.g., (90000, 16, 3572).

Unfortunately, this throws a Memory error. However, when I do for debugging purposes, i.e., assume a vocabulary size of 10:

decoder_target_data = np.zeros((len(input_sequences), max_target_seq_len, 10), dtype='float32')


and later in the decoder model:

x = Dense(10, activation='softmax')(x)


then the model trains without error. In case that's indeed my issue, I have to train the model with manually generate batches so I can keep the vocabulary size but reduce the #samples, e.g., to 90 batches each of shape (1000, 16, 3572). Am I on the right track here?
",7,2991,"Recently I was also facing this problem. There is no other solution then creating small batches say batch_size=64 in a generator and then instead of model.fit do model.fit_generator. I have attached my generate_batch code below:     

def generate_batch(X, y, batch_size=64):
    ''' Generate a batch of data '''
    while True:
        for j in range(0, len(X), batch_size):
            encoder_input_data = np.zeros((batch_size, max_encoder_seq_length),dtype='float32')
            decoder_input_data = np.zeros((batch_size, max_decoder_seq_length+2),dtype='float32')
            decoder_target_data = np.zeros((batch_size, max_decoder_seq_length+2, num_decoder_tokens),dtype='float32')

            for i, (input_text_seq, target_text_seq) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):
                for t, word_index in enumerate(input_text_seq):
                    encoder_input_data[i, t] = word_index # encoder input seq

                for t, word_index in enumerate(target_text_seq):
                    decoder_input_data[i, t] = word_index
                    if (t&gt;0)&amp;(word_index&lt;=num_decoder_tokens):
                        decoder_target_data[i, t-1, word_index-1] = 1.

            yield([encoder_input_data, decoder_input_data], decoder_target_data)


And then training like this: 

batch_size = 64
epochs = 2

# Run training
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit_generator(
    generator=generate_batch(X=X_train_sequences, y=y_train_sequences, batch_size=batch_size),
    steps_per_epoch=math.ceil(len(X_train_sequences)/batch_size),
    epochs=epochs,
    verbose=1,
    validation_data=generate_batch(X=X_val_sequences, y=y_val_sequences, batch_size=batch_size),
    validation_steps=math.ceil(len(X_val_sequences)/batch_size),
    workers=1,
    )


X_train_sequences is list of lists like [[23,34,56], [2, 33544, 6, 10]].
Similarly others.      

Also took help from this blog - word-level-english-to-marathi-nmt
",,
Keras strange result,https://stackoverflow.com/questions/42473052,TensorFlow 1.0 does not see GPU on Windows (but Theano does),,5,1457,,,
Keras strange result,https://stackoverflow.com/questions/63271509,Why do i get lagged results on my LSTM model,"I am new to machine learning and I am performing a Multivariate Time Series Forecast using LSTMs in Keras. I have a monthly timeseries dataset with 4 input variables (temperature, precipitation, Dew and wind_spreed) and 1 output variable (pollution). Using this data i framed a forecasting problem where, given the weather conditions and pollution for prior months, I forecast the pollution at the next month. Below is my code
X = df[['Temperature', 'Precipitation', 'Dew', 'Wind_speed' ,'Pollution (t_1)']].values
y = df['Pollution (t)'].values
y = y.reshape(-1,1)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(X)

#dataset has 359 samples in total
train_X, train_y = X[:278], y[:278]
test_X, test_y = X[278:], y[278:]
# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape) 


model = Sequential()
model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dropout(0.2))
#    model.add(LSTM(70))
#    model.add(Dropout(0.3))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')

history = model.fit(train_X, train_y, epochs=700, batch_size=70, validation_data=(test_X, test_y), verbose=2, shuffle=False)

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

To do predictions i use the following code
from sklearn.metrics import mean_squared_error,r2_score
yhat = model.predict(test_X)
mse  = mean_squared_error(test_y, yhat)
rmse = np.sqrt(mse)
r2   = r2_score(test_y, yhat)

print(""test set performance"")
print(""--------------------"")
print(""MSE:"",mse)
print(""RMSE:"",rmse)
print(""R^2: "",r2)

fig, ax = plt.subplots(figsize=(10,5))
ax.plot(range(len(test_y)), test_y, '-b',label='Actual')
ax.plot(range(len(yhat)), yhat, 'r', label='Predicted')
plt.legend()
plt.show()

Running this code i fell into the following issues:

For some reason am getting a lagged result for my test set which is not in my training data as shown on the below image. I do not understand why i have these lagged results (does it have something to do with including 'pollution (t_1)' as part of my inputs)?

Graph Results:


By adding ""pollution (t_1)"" which is a shift by 1 lag of the polution variable as part of my inputs this variable now seems to dominate the prediction as removing the other varibales seems to have no influence on my results (r-squared and rmse) which is strange since all these variables do assit in pollution prediction.

Is there something i am doing wrong in my code which is the reason for these issues? I am new to python so any help to answer the above 2 questions will be greatly appreaciated.
",5,2081,"First of all, I think it is not appropriate to input '1' as Timesteps value, because LSTM model is the one treating timeseries or sequence data.
I think the following script of data mining will work well
def lstm_data(df,timestamps):
    array_data=df.values
    sc=MinMaxScaler()
    array_data_=sc.fit_transform(array_data)

    array=np.empty((0,array_data_.shape[1]))
    range_=array_data_.shape[0]-(timestamps-1)
    for t in range(range_):
        array_data_p=array_data_[t:t+sequenth_length,:]
        array=np.vstack((array,array_data_p))

    array_=array.reshape(-1,timestamps, array.shape[1])
    return array_

#timestamps depend on your objection, but not '1'
x_data=lstm_data(x, timestamps=4) 
y_data=lstm_data(y, timestamps=4)
y_data=y_data.reshape(-1,1)

#Divide each data into train and test
#Input the divided data into your LSTM model

",,
Keras strange result,https://stackoverflow.com/questions/44716415,Approximating sine function with Neural Network and ReLU,"I am trying to approximate a sine function with a neural network (Keras).

Yes, I read the related posts :)


Link 1
Link 2
Link 3


Using four hidden neurons with sigmoid and an output layer with linear activation works fine.

But there are also settings that provide results that seem strange to me.

Since I am just started to work with I am interested in what and why things happen, but I could not figure that out so far.

# -*- coding: utf-8 -*-

import numpy as np
np.random.seed(7)

from keras.models import Sequential
from keras.layers import Dense
import pylab as pl
from sklearn.preprocessing import MinMaxScaler

X = np.linspace(0.0 , 2.0 * np.pi, 10000).reshape(-1, 1)
Y = np.sin(X)

x_scaler = MinMaxScaler()
#y_scaler = MinMaxScaler(feature_range=(-1.0, 1.0))
y_scaler = MinMaxScaler()

X = x_scaler.fit_transform(X)
Y = y_scaler.fit_transform(Y)

model = Sequential()
model.add(Dense(4, input_dim=X.shape[1], kernel_initializer='uniform', activation='relu'))
# model.add(Dense(4, input_dim=X.shape[1], kernel_initializer='uniform', activation='sigmoid'))
# model.add(Dense(4, input_dim=X.shape[1], kernel_initializer='uniform', activation='tanh'))
model.add(Dense(1, kernel_initializer='uniform', activation='linear'))

model.compile(loss='mse', optimizer='adam', metrics=['mae'])

model.fit(X, Y, epochs=500, batch_size=32, verbose=2)

res = model.predict(X, batch_size=32)

res_rscl = y_scaler.inverse_transform(res)

Y_rscl = y_scaler.inverse_transform(Y)

pl.subplot(211)
pl.plot(res_rscl, label='ann')
pl.plot(Y_rscl, label='train')
pl.xlabel('#')
pl.ylabel('value [arb.]')
pl.legend()
pl.subplot(212)
pl.plot(Y_rscl - res_rscl, label='diff')
pl.legend()
pl.show()


This is the result for four hidden neurons (ReLU) and linear output activation. 


Why does the result take the shape of the ReLU? 

Does this have something to do with the output normalization? 
",5,6966,"Try adding more hidden layers, each with more hidden units. I used this code:

model = Sequential()
model.add(Dense(50, input_dim=X.shape[1], activation='relu'))
model.add(Dense(50, input_dim=X.shape[1], activation='relu'))
model.add(Dense(1, activation='linear'))


and got these results:


",,
Keras strange result,https://stackoverflow.com/questions/60323897,Tensorflow/Keras Conv2D layers with padding=&#39;SAME&#39; behave strangely,,4,3673,"padding='Same' in Keras means padding is added as required to make up for overlaps when the input size and kernel size do not perfectly fit. 

Example of padding='Same':

# Importing dependency
import keras
from keras.models import Sequential
from keras.layers import Conv2D

# Create a sequential model
model = Sequential()

# Convolutional Layer
model.add(Conv2D(filters=24, input_shape=(5,5,1), kernel_size=(2,2), strides =(2,2) ,padding='Same'))

# Model Summary
model.summary()


Output of the code - 

Model: ""sequential_20""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_28 (Conv2D)           (None, 3, 3, 24)          120       
=================================================================
Total params: 120
Trainable params: 120
Non-trainable params: 0
_________________________________________________________________


Pictorial Representation:
Below image shows how the padding for the input (input_shape=(5,5,1), kernel_size=(2,2), strides =(2,2)) when padding='Same'.



------------------------------------------------------------------------------------------------------------------

padding='Valid' in Keras means no padding is added.

Example of padding='Valid': Have used same input for Conv2D that we used above for padding = 'Same' .i.e. (input_shape=(5,5,1), kernel_size=(2,2), strides =(2,2))

# Importing dependency
import keras
from keras.models import Sequential
from keras.layers import Conv2D

# Create a sequential model
model = Sequential()

# Convolutional Layer
model.add(Conv2D(filters=24, input_shape=(5,5,1), kernel_size=(2,2), strides =(2,2) ,padding='Valid'))

# Model Summary
model.summary()


Output of the code - 

Model: ""sequential_21""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_29 (Conv2D)           (None, 2, 2, 24)          120       
=================================================================
Total params: 120
Trainable params: 120
Non-trainable params: 0
_________________________________________________________________


Pictorial Representation:
Below image shows there is no padding added for the input (input_shape=(5,5,1), kernel_size=(2,2), strides =(2,2)) when padding='Valid'.


------------------------------------------------------------------------------------------------------------------

Now lets try same code that we used for padding='Valid' for the input (input_shape=(6,6,1), kernel_size=(2,2), strides =(2,2)). Here padding='Valid' should behave same as padding='Same'.

Code -

# Importing dependency
import keras
from keras.models import Sequential
from keras.layers import Conv2D

# Create a sequential model
model = Sequential()

# Convolutional Layer
model.add(Conv2D(filters=24, input_shape=(6,6,1), kernel_size=(2,2), strides =(2,2) ,padding='Valid'))

# Model Summary
model.summary()


Output of the code - 

Model: ""sequential_22""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_30 (Conv2D)           (None, 3, 3, 24)          120       
=================================================================
Total params: 120
Trainable params: 120
Non-trainable params: 0
_________________________________________________________________

",,
Keras strange result,https://stackoverflow.com/questions/64645579,Keras: custom data validation callback on training data always returns validation data results,,4,1136,"It seems that this is a bug in tensorflow versions 2.3.x (tested with 2.3.0 and 2.3.1). In versions 2.4.0-rc0 and 2.2.1, the loss outputs of loss and custom_loss differ, which is the expected behavior:
{'custom_loss': [0.7694963216781616, 0.541864812374115],
'loss': [0.9665887951850891, 0.6637843251228333],
'val_loss': [0.7624925374984741, 0.5331208109855652]}

",,
Keras strange result,https://stackoverflow.com/questions/46580112,R-squared results of test and validation differ by a huge margin,,3,4042,"Tensorflow's built-in evaluate method evaluates your test set batch by batch and hence calculates r2 at each batch. The metrics produced from model.evaluate() is then simple average of all r2 from each batch. While in model.fit(), r2 (and all metrics on validation set) are calculated per epoch (instead of per batch and then take avg.)
You may slice your output and output_pred into batches of the same batch size you used in model.evaluate() and calculate r2 on each batch. I guess the model produces high r2 on batches with high total sum of squares (SS_tot) and bad r2 on lower ones. So when taken average, result would be poor (however when calculate r2 on entire dataset, samples with higher ss_tot usually dominate the result).
",,
Keras strange result,https://stackoverflow.com/questions/60251715,Difference between Keras and TensorFlow Hub Version of MobileNetV2,,3,1538,"There are several documented differences:


Like Szymon said, the TF Hub version returns logits (before the softmax function that turns them into probabilities), which is a common practice, because the cross-entropy loss can be computed with greater numerical stability from the logits.
The TF Hub model assumes float32 inputs in the range of [0,1], which is what you get from tf.image.decode_jpeg(...) followed by tf.image.convert_image_dtype(..., tf.float32). The Keras code uses a model-specific range (likely [-1,+1]).
The TF Hub model reflects the original SLIM checkpoint more completely in returning all its 1001 output classes. As stated in the ImageNetLabels.txt linked from the documentation, the added class 0 is ""background"" (aka. ""stuff""). That is what object detection uses to indicate image background as opposed to an object of any known class.

",,
Keras strange result,https://stackoverflow.com/questions/49370940,One hot encoding characters,,3,3629,,,
Keras strange result,https://stackoverflow.com/questions/55772880,TF 2.0 - Method estimator.model_to_estimator( ) fails but model.fit works for tf.keras created model,,3,408,"As discussed here , this is indeed a bug that was caused by the combination of tf.function in keras layer and v1 tf.session which is used by model_to_estimator

Also you can refer to the following link, where the bug has been fixed. Thanks!

If you feel like the answer was helpful, please upvote it. Thanks!
",,
Keras strange result,https://stackoverflow.com/questions/39550258,keras errors with theano/tensowflow backend,,2,1037,,,
Keras strange result,https://stackoverflow.com/questions/56721498,How can I use the Keras.applications&#39; ResNeXt in TensorFlow&#39;s eager execution?,,2,4123,"As the error indicates, tf.placeholder() which is used as placeholder for feeding data to a tf Session using feed_dict, is incompatible with eager mode. 

This link nicely explains it with an example: https://github.com/tensorflow/tensorflow/issues/18165#issuecomment-377841925

You can use models from tf.keras.applications for this purpose. I've tried with TF2.0 Beta release.

https://www.tensorflow.org/beta/tutorials/images/transfer_learning#create_the_base_model_from_the_pre-trained_convnets

import tensorflow as tf
resnext = tf.keras.applications.ResNeXt50(weights=None)
print(tf.executing_eagerly())


True

ResNeXt models are not available(I had to make some changes like copying resnext.py from keras/applications to tensorflow/python/keras/applications and changes to __init__.py etc.) but you can try with the existing models like ResNet50, if they work then you can try porting ResNeXt.  
",,
Keras strange result,https://stackoverflow.com/questions/55974198,Strange results while training with keras,,2,459,"I believe your problem is your loss function/metrics. If most patients do not have any tumor, and the accuracy or jaccard distance equally regard both classes, your model will return high values of accuracy and low values of jaccard index by simpy saying everything is backgorund/healthy. You can check this by you implement a custom loss that always returns the class label for background and compare it to your current results. To solve your problem implement something like a jaccard distance that gives lower weight to the background. An overview of different metrics that may be more suitable than accuracy can be found here.

Also, maybe I didn't understand the dataset, but shouldn't you segment different kinds of tumors and therefore use categorical instead of binary classification?
",,
Keras strange result,https://stackoverflow.com/questions/59855961,Converting Keras model with TFLiteConverter to quantized tflite version results in NOTYPE error,"When converting and doing 8-Bit quantization of a keras model I ran into a strange error that did not happen for image data sets.   

import tensorflow.python.keras.backend as K
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import load_model
import numpy as np

x_train = np.array([[0.6171875  0.59791666],[0.6171875  0.59791666],[0.6171875  0.59791666]])
y_train = np.array([[0.6171875  0.59791666],[0.6171875  0.59791666],[0.6171875  0.59791666]])


def representative_dataset_gen():
    for i in range(1):
        # Get sample input data as a numpy array in a method of your choosing.
        sample = np.array([0.5,0.6])
        sample = np.expand_dims(sample, axis=0)
        yield [sample]



model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(2,)),
  tf.keras.layers.Dense(12, activation='relu'),
  tf.keras.layers.Dense(2, activation='softmax')
])


model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=1)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.float32
converter.inference_output_type = tf.float32
converter.representative_dataset = representative_dataset_gen

tflite_quant_model = converter.convert()



This results in the error

ValueError: Cannot set tensor: Got value of type NOTYPE but expected type FLOAT32 for input 1, name: dense_1_input   

This procedure used to work when using image data but now this happens. Tried different TF version including nightly TF2.1.
",2,1749,"Apparently the problem has to do with the data type of the input tensor which was by default Float64 and not the expected Float32. Since tflite does not know about Float64 it treads it as NOTYPE which is confusing.

Supported TF Lite Types

A cast to float32 solves the problem

sample = sample.astype(np.float32)
",,
Keras strange result,https://stackoverflow.com/questions/61076672,Resnet-50 adversarial training with cleverhans FGSM accuracy stuck at 5%,,1,561,"It is a side-effect of the way we estimate the moving averages on BatchNormalization.

The mean and variance of the training data that you used are different from the ones of the dataset used to train the ResNet50. Because the momentum on the BatchNormalization has a default value of 0.99, with only 10 iterations it does not converge quickly enough to the correct values for the moving mean and variance. This is not obvious during training when the learning_phase is 1 because BN uses the mean/variance of the batch. Nevertheless when we set learning_phase to 0, the incorrect mean/variance values which are learned during training significantly affect the accuracy. 

You can fix this problem by below approachs:


More iterations


Reduce the size of the batch from 32 to 16(to perform more updates per epoch) and increase the number of epochs from 10 to 250. This way the moving average and variance will converge to the correct values.


Change the momentum of BatchNormalization


Keep the number of iterations fixed but change the momentum of the BatchNormalization layer to update more aggressively the rolling mean and variance (not recommended for production models). 

On the original snippet, add the following code between reading the base_model and defining the new layers:

# ....
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)

# PATCH MOMENTUM - START
import json
conf = json.loads(base_model.to_json())
for l in conf['config']['layers']:
    if l['class_name'] == 'BatchNormalization':
        l['config']['momentum'] = 0.5


m = Model.from_config(conf['config'])
for l in base_model.layers:
    m.get_layer(l.name).set_weights(l.get_weights())

base_model = m
# PATCH MOMENTUM - END

x = base_model.output
# ....


Would also recommend you to try another hack provided bu us here.
",,
Keras strange result,https://stackoverflow.com/questions/46447882,Weights of CNN model go to really small values and after NaN,,1,689,,,
Keras strange result,https://stackoverflow.com/questions/68701979,Recall and precision not working correctly(keras),"I have to build a model in keras. I am really struggling with my actual dataset, hence I am just trying to figure out the basics on a simpler dataset.
model = Sequential([
    Dense(32, input_dim=X_train.shape[1], activation=""sigmoid""),
    Dense(2, activation=""softmax""),
])
metrics=[ 
    tf.keras.metrics.TruePositives(name=""tp""),
    tf.keras.metrics.TrueNegatives(name=""tn""),
    tf.keras.metrics.FalseNegatives(name=""fn""),
    tf.keras.metrics.FalsePositives(name=""fp""),
    tf.keras.metrics.Recall(name=""recall""),
    tf.keras.metrics.Precision(name=""precision"")
]

model.compile(loss=""categorical_crossentropy"", metrics=metrics, optimizer=""sgd"")
model.evaluate(X_test, y_test)
evaluation = model.evaluate(X_test, y_test)
for i, m in model.metrics_names:
    print(m, evaluation[i])

This gets printed out:
loss 0.4604386021425058
tp 2965.5
tn 2965.5
fn 531.25
fp 531.25
recall 0.8480753898620605
precision 0.8480753898620605

Something really strange about this results. I believe it is due to using the softmax with two nodes.
y_train looks something like this:
array([[1., 0.],
       [1., 0.],
       [1., 0.],

       [1., 0.]], dtype=float32)

I tried a sigmoid, but then the whole model breaks down, at least here the fitting works.
Is there a way to configure recall and precision so they consider one output notes as Positive?
",1,2243,"The only solution in your case is to transform the problem into a one-dimensional one, i.e.

Use Dense(1,activation='sigmoid') instead of Dense(2,activation='softmax'); change [0,1] to 0 and [1,0] to 1 as an example.
Use binary_crossentropy instead of categorical_crossentropy.

Otherwise, you can implement a special callback to retrieve those metrics (using scikit-learn, like in the example below):
How to get other metrics in Tensorflow 2.0 (not only accuracy)?
",,
Keras strange result,https://stackoverflow.com/questions/66245520,training on imdb dataset with tensorflow2/keras give strange result,"I'm new to tensorflow2/keras. I was following this tutorial on tensorflow website. Instead of downloading the text data to directory, I use tensorflow_datasets to load imdb dataset directly to tensors/numpy arrays. Below is my code.
import os
import re
import string
import pandas as pd
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds

import matplotlib.pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

print(""Version: "", tf.__version__)
print(""Eager mode: "", tf.executing_eagerly())
print(""Hub version: "", hub.__version__)
print(""GPU is"", ""available"" if tf.config.list_physical_devices('GPU') else ""NOT AVAILABLE"")

train_data, test_data = tfds.load(name=""imdb_reviews"", split=[""train"", ""test""], 
                                  batch_size=-1, as_supervised=True)

X_train, y_train = tfds.as_numpy(train_data)
X_test, y_test = tfds.as_numpy(test_data)

# process text
def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '&lt;br /&gt;', ' ')
  return tf.strings.regex_replace(stripped_html,
                                  '[%s]' % re.escape(string.punctuation),
                                  '')
max_features = 1000
sequence_length = 50

vectorize_layer = TextVectorization(
    standardize=custom_standardization,
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)
# Make a text-only dataset (without labels), then call adapt
vectorize_layer.adapt(X_train)

def vectorize_text(text):
  text = tf.expand_dims(text, -1)
  return vectorize_layer(text)

#check data
first_review, first_label = X_train[0], y_train[0]
print(""Review"", first_review)
print(""Vectorized review"", vectorize_text(first_review))
print(""11 ---&gt; "",vectorize_layer.get_vocabulary()[11])
print("" 44 ---&gt; "",vectorize_layer.get_vocabulary()[44])
print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))


# vectorize both train and test text data
X_train = vectorize_text(X_train)
X_test = vectorize_text(X_test)


embedding_dim = 16
#define and compile model
model = tf.keras.Sequential([
  layers.Embedding(max_features + 1, embedding_dim),
  layers.GlobalAveragePooling1D(),
  layers.Dropout(0.2),
  layers.Dense(256, activation='relu'),
  layers.Dropout(0.2),
  layers.Dense(1)])
model.summary()
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
# fit the model
history = model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=2, validation_data=(X_test, y_test))

The output I got is like below:
_________________________________________________________________
Epoch 1/5
782/782 - 4s - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000
Epoch 2/5
782/782 - 4s - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000
Epoch 3/5
782/782 - 4s - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000
Epoch 4/5
782/782 - 4s - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000
Epoch 5/5
782/782 - 4s - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000

Accuracy are all 50%! Something went wrong.  I'm confused why that happened? I was following the tutorial, train the embedding layer from scratch.  Spent hours trying to figuring out why.
Does anyone know why went wrong? thanks!
",1,366,"Your dense layer is linear which means you don't have any activation passed into it. There are several solutions, as you are doing binary_classification.
If you want to use Dense(1) as it is, then you should change loss function into:
model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

Or you can use Dense(1, activation = tf.nn.sigmoid), then your loss function should be:
model.compile(optimizer='adam', loss='binary_crossentropy',
              metrics=['accuracy'])

Or another solution if your labels are one hot encoded, you can set Dense(2, activation = tf.nn.softmax) then loss function can be:
model.compile(optimizer='adam', loss='categorical_crossentropy',
              metrics=['accuracy'])

I reproduced the exact code of yours with changings:
  layers.Dense(1, activation = tf.nn.sigmoid)])
  model.compile(optimizer='adam', loss='binary_crossentropy', metrics= 
                                   ['accuracy'])

Got:
782/782 [==============================] - 4s 5ms/step - loss: 0.4836 - accuracy: 0.7675 - val_loss: 0.5065 - val_accuracy: 0.7447

Edit 2: sigmoid squeezes your output into a range of [0,1]. For example to get predictions you need all x_test. X_test[0] will mess up the predictions as it only contains the first sentence. Get predictions by using:
y_hat = model.predict(X_test)
y_hat

array([[0.8105568 ],
       [0.6332975 ],
       [0.20526059],
       ...,
       [0.03132877],
       [0.5318063 ],
       [0.8626927 ]], dtype=float32)

Now set a threshold and convert them into 0 and 1s. This can be done with:
y_pred = [1 * (x[0]&gt;=0.5) for x in y_hat]

y_pred will be containing zeros and ones. Note that threshold is 0.5 here which means if output from sigmoid is bigger than 0.5, than it belongs to second class.
",,
Keras strange result,https://stackoverflow.com/questions/46576598,Strange keyword &#39;metirics&#39; in TypeError message - Keras,,1,275,,,
Keras strange result,https://stackoverflow.com/questions/38991327,Problems with initialisation of weight and biases?,"I use a CNN to do classification (6 classes) of 32x32bit images.

The network is behaving very strangely as it takes 900 steps to do anything different from these results pasted. Afterwards it starts to move to reach reasonable values after several 1000 steps. The ""same network"" done in Theano/Keras with the same data is showing results much faster. I say ""same network"" this because Theano/Keras let me define leakyRELU and some other initialisation methods on variables (init = ""orthogonal"") that let me believe my problem is with initialisation. But I could be far off ;-)

    step        -&gt;  900
    Minibatch loss at step 900: 1.440395
    Minibatch accuracy: 43.0%
    Validation accuracy: 35.5%
    Val F-score: 0.0872979214781    
    confusion matrix 
 [[  0   0   0   0   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0] 
 [ 21 292 224 398 567  96]
 [  0   0   0   0   0   0]]


my optimizer is as

tf.train.GradientDescentOptimizer(0.1).minimize(loss)


my weights and bias initialisation is (only layer 1 of 6 is shown, but identical)

wc1 = tf.Variable(tf.truncated_normal([2, 2, 1, 16], stddev=0.05), name=""weights_1"")
bc1 = tf.Variable(tf.zeros([16]), name=""bias_1"")


can anybody give me hint towards: 


why it takes 900 steps to get the network to ""do something""
why is the confusion matrix giving me this uniform prediction in the beginning


thanks a lot
Peter
",1,842,"If you use relu activation function, look at ""kaiming initialisation"" for your weight. The objective is to keep a mean of 0 and a standard deviation of 1 for your output after each layer during the forward pass. 

For relu activation fuction you have to initialize with random normal distribution multiplied by the square root of 2/(number of input for the given layer).

weight_initialisation = random_normal * sqrt(2/(number of input for the layer))


For CNN, I think the number of input will be number of filter * number of cell in the kernel (or 5 * 5 for a [5, 5] kernel)
",,
Keras strange result,https://stackoverflow.com/questions/49351711,Saved multi-GPU trained model loaded into single-GPU; inconsistent results,,0,1298,"write the code os.environ[""CUDA_VISIBLE_DEVICES""] = '0' before you load the model,then it will not use multiple gpu
",,
Keras strange result,https://stackoverflow.com/questions/73804090,Is it possible to convert a TensorFlow (Keras) model from BGR to RGB?,,0,367,"You can create a new model: first a lambda layer which will reverse the channel order, than your saved model:
input_shape = old_model.get_layer(index = 0).input_shape[0][1:]
inputs = Input(shape=input_shape)
lambda_layer = Lambda(lambda x: x[:,:,:,::-1])(inputs)
outputs = old_model(lambda_layer)
new_model = Model(inputs=inputs, outputs=outputs)

",,
Keras strange result,https://stackoverflow.com/questions/65512540,TensorFlow: &quot;Expected axis -1 of input shape...&quot;,,0,320,"Predict need batch dimension. Try this:
insert = np.array([[6.1, 3.0, 4.6, 1.4]]).astype('float') 

",,
Keras strange result,https://stackoverflow.com/questions/56323567,Siamese neural network with two pre-trained ResNet 50 - strange behavior while testing model,,0,3993,"What you are getting is expected. I'm not sure what you mean by


  Also I noticed strange behavior: the greater the number of epochs the result is worse.  


But the results you shown are valid and expected. Let's start with what the model is outputting. Your model output is (normalized)distance between the first and second inputs. If the inputs are similar, then the distance should be close to zero. As number of training step increases the model learns to identify the inputs, i.e if the inputs are similar the model learns to output values close to zero, and if the inputs are different the model learns to output values close to one. So, 


  ... trained model with 10 epoch gives prediction: ""8.jpg"": 0.5180479884147644 but the same model trained with 100 epoch gives ""8.jpg"": 5.579867080537926E-13 However for 100 epoch I have better train results.  


, confirms that the model has learned that the two inputs are similar and outputs 5.579867080537926E-13 ~ 0(approximately close to 0). 

Although the model is performing well, there is one issue I've observed in the model definition:- The output layer is dropout layer. Dropout is not valid output layer. What you are doing by this setting is, randomly with probability 0.2 you are setting the output of the model to be zero. 

Let's assume the target variable has 1(the two inputs are different), and model has learnt to identify the images correctly and outputs value close to 1 before the dropout layer. Let's further assume that the dropout layer has decided to set the output to be zero. So the model output will be zero. Even though the layers before dropout layer have performed well, because of the dropout layer, they will be penalized. If this is not what you are looking then remove the last dropout layer.

L1_prediction = Dense(1, use_bias=True,
                    activation='sigmoid',
                    kernel_initializer=RandomNormal(mean=0.0, stddev=0.001),
                    name='weighted-average')(L1_distance)


siamese_model = Model(inputs=[input_left, input_right], outputs=L1_prediction)


However, sometimes this behavior is needed if one want to add noise to the model. This has the same effect with randomly altering the target variable when the value is 1. 
",,
Keras strange result,https://stackoverflow.com/questions/52584378,How to execute inference of tensorflow model in Android,,0,459,,,
Keras strange result,https://stackoverflow.com/questions/69600915,Keras model.fit() IndexError: list index out of range,"I need some help, I keep getting this strange situation where my Keras model goes out of range
print(np.array(train_x).shape)
print(np.array(train_y).shape)

Returns:
(731, 42)
(731,)

Then:
normalizer = Normalization(input_shape=[42,], axis=None)
normalizer.adapt(train_x[0])

linear_model = Sequential([
    normalizer,
    Dense(units=1)
])
linear_model.summary()

Shows:
Model: ""sequential_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
normalization_5 (Normalizati (None, 42)                3         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 43        
=================================================================
Total params: 46
Trainable params: 43
Non-trainable params: 3
_________________________________________________________________

Then:
linear_model.compile(
    optimizer=tf.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')

linear_model.fit(
    train_x,
    train_y,
    epochs=100)

Which results in an IndexError: list index out of range. It looks like my inputs are in the right shape. Any idea what could be causing this?
",0,429,"train_x and train_y needed to be numpy arrays.
",,
Keras strange result,https://stackoverflow.com/questions/62944015,Tensorflow/Keras model output is constant,"I am trying to train a CNN in using keras. The input is a 128x128x3 rbg image and output is a single value between 0 and 1 (this is not a classifier model). I have normalised the input. Initially, my model was achieving some reasonable results, getting the mean absolute error to &lt; 0.1. As I tried to tweak the model slightly I found the loss would plateau very quickly to around 0.23. I investigated further and found that it was outputting the same value for every input.
So I reverted my code back to when it was working, but it was no longer working. I eventually found that about 90% of the time it will get stuck at this local minima, outputting a constant value (which I suspect is mean of the training reference values (0.39). The other 10% of the time it will behave nicely and regress down to an error of &lt; 0.1. So it is basically giving qualitatively different behaviour randomly and desired results rarely. The strange thing is, is that I swear it was consistently working before.
I have tried:

Changing the input size
Increasing/decreasing the learning rate by factor of 10
Removing a couple of dense layers
Changing 'relu' to 'leaky relu'
Increasing/removing dropout

def load_data(dir):
    csv_data = get_csv_data()
    xs = []
    ys = []
    for (name, y) in csv_data:
        path = DIR + dir + ""/"" + name
        img = tf.keras.preprocessing.image.load_img(path)
        xs.append(tf.keras.preprocessing.image.img_to_array(img) * (1 / 255.0))
        ys.append(normalize_output(float(y)))
    return np.array(xs).reshape(len(csv_data), IMAGE_DIM, IMAGE_DIM, 3), np.array(ys).reshape(len(csv_data), 1)

def gen_model():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size = (5, 5), activation='relu', input_shape=(IMAGE_DIM, IMAGE_DIM, CHAN_COUNT)))
    model.add(tf.keras.layers.MaxPool2D())
    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size = (5, 5), activation='relu'))
    model.add(tf.keras.layers.MaxPool2D())
    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size = (5, 5), activation='relu'))
    model.add(tf.keras.layers.MaxPool2D())
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(256, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.1))
    model.add(tf.keras.layers.Dense(128, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.1))
    model.add(tf.keras.layers.Dense(64, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.1))
    model.add(tf.keras.layers.LeakyReLU())
    model.add(tf.keras.layers.Dense(16, activation='sigmoid'))
    model.add(tf.keras.layers.LeakyReLU())
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    model.compile(loss=keras.losses.MeanSquaredError(),
                  optimizer=tf.keras.optimizers.Adam(),
                  metrics=[keras.metrics.MeanAbsoluteError()])
    return model

def run():
    model = gen_model()

    xs, ys = load_data(""output"")
   
    generator = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,
                                                                samplewise_center=False,
                                                                featurewise_std_normalization=False,
                                                                samplewise_std_normalization=False,
                                                                validation_split=0.1,
                                                                rotation_range=12,
                                                                horizontal_flip=True,
                                                                vertical_flip=True)

    model.fit(generator.flow(xs, ys, batch_size=32, shuffle=True),
              steps_per_epoch=len(xs) / 32,
              epochs = 10,
              use_multiprocessing=False)

",0,836,"I rearranged activation on the layers. Please give it a try :
def gen_model():
   model = tf.keras.Sequential()
   model.add(tf.keras.layers.Conv2D(filters=64, kernel_size = (5, 5), activation='relu', input_shape=(IMAGE_DIM, IMAGE_DIM, CHAN_COUNT)))
   model.add(tf.keras.layers.MaxPool2D())
   model.add(tf.keras.layers.Conv2D(filters=64, kernel_size = (5, 5), activation='relu'))
   model.add(tf.keras.layers.MaxPool2D())
   model.add(tf.keras.layers.Conv2D(filters=128, kernel_size = (5, 5), activation='relu'))
   model.add(tf.keras.layers.MaxPool2D())
   model.add(tf.keras.layers.Flatten())
   model.add(tf.keras.layers.Dense(256, activation='relu'))
   model.add(tf.keras.layers.Dropout(0.1))
   model.add(tf.keras.layers.Dense(128, activation='relu'))
   model.add(tf.keras.layers.Dropout(0.1))
   model.add(tf.keras.layers.Dense(64, activation='relu'))
   model.add(tf.keras.layers.Dropout(0.1))
   model.add(tf.keras.layers.Dense(16, activation='relu'))
   model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
   model.compile(loss=keras.losses.MeanSquaredError(),
              optimizer=tf.keras.optimizers.Adam(),
              metrics=[keras.metrics.MeanAbsoluteError()])
   return model

",,
Keras strange result,https://stackoverflow.com/questions/60931790,Big difference between val-acc and prediction accuracy in Keras Neural Network,"I have a dataset that I used for making NN model in Keras, i took 2000 rows from that dataset to have them as validation data, those 2000 rows should be added in .predict function.

I wrote a code for Keras NN and for now it works good, but I noticed something that is very strange for me. It gives me very good accuracy of more than 83%, loss is around 0.12, but when I want to make a prediction with unseen data (those 2000 rows), it only predicts correct in average of 65%.
When I add Dropout layer, it only decreases accuracy.

Then I have added EarlyStopping, and it gave me accuracy around 86%, loss is around 0.10, but still when I make prediction with unseen data, I get final prediction accuracy of 67%.

Does this mean that model made correct prediction in 87% of situations? Im going with a logic, if I add 100 samples in my .predict function, that program should make good prediction for 87/100 samples, or somewhere in that range (lets say more than 80)? I have tried to add 100, 500, 1000, 1500 and 2000 samples in my .predict function, and it always make correct prediction in 65-68% of the samples.

Why is that, am I doing something wrong?
I have tried to play with number of layers, number of nodes, with different activation functions and with different optimizers but it only changes the results by 1-2%.
My dataset looks like this:

DataFrame shape (59249, 33)
x_train shape (47399, 32)
y_train shape (47399,)
x_test shape (11850, 32)
y_test shape (11850,)
testing_features shape (1000, 32)


This is my NN model:

model = Sequential()
model.add(Dense(64, input_dim = x_train.shape[1], activation = 'relu')) # input layer requires input_dim param
model.add(Dropout(0.2))
model.add(Dense(32, activation = 'relu'))
model.add(Dropout(0.2))
model.add(Dense(16, activation = 'relu'))
model.add(Dense(1, activation='sigmoid')) # sigmoid instead of relu for final probability between 0 and 1

# compile the model, adam gradient descent (optimized)
model.compile(loss=""binary_crossentropy"", optimizer= ""adam"", metrics=['accuracy'])


# call the function to fit to the data training the network)
es = EarlyStopping(monitor='val_loss', min_delta=0.0, patience=1, verbose=0, mode='auto')
model.fit(x_train, y_train, epochs = 15, shuffle = True, batch_size=32, validation_data=(x_test, y_test), verbose=2, callbacks=[es])

scores = model.evaluate(x_test, y_test)
print(model.metrics_names[0], round(scores[0]*100,2), model.metrics_names[1], round(scores[1]*100,2))


These are the results:

Train on 47399 samples, validate on 11850 samples
Epoch 1/15
 - 25s - loss: 0.3648 - acc: 0.8451 - val_loss: 0.2825 - val_acc: 0.8756
Epoch 2/15
 - 9s - loss: 0.2949 - acc: 0.8689 - val_loss: 0.2566 - val_acc: 0.8797
Epoch 3/15
 - 9s - loss: 0.2741 - acc: 0.8773 - val_loss: 0.2468 - val_acc: 0.8849
Epoch 4/15
 - 9s - loss: 0.2626 - acc: 0.8816 - val_loss: 0.2416 - val_acc: 0.8845
Epoch 5/15
 - 10s - loss: 0.2566 - acc: 0.8827 - val_loss: 0.2401 - val_acc: 0.8867
Epoch 6/15
 - 8s - loss: 0.2503 - acc: 0.8858 - val_loss: 0.2364 - val_acc: 0.8893
Epoch 7/15
 - 9s - loss: 0.2480 - acc: 0.8873 - val_loss: 0.2321 - val_acc: 0.8895
Epoch 8/15
 - 9s - loss: 0.2450 - acc: 0.8886 - val_loss: 0.2357 - val_acc: 0.8888
11850/11850 [==============================] - 2s 173us/step
loss 23.57 acc 88.88


And this is for prediction:

#testing_features are 2000 rows that i extracted from dataset (these samples are not used in training, this is separate dataset thats imported)

prediction = model.predict(testing_features , batch_size=32)

res = []
for p in prediction:
    res.append(p[0].round(0))


# Accuracy with sklearn - also much lower 
acc_score = accuracy_score(testing_results, res)
print(""Sklearn acc"", acc_score)    

result_df = pd.DataFrame({""label"":testing_results,
                          ""prediction"":res})


result_df[""prediction""] = result_df[""prediction""].astype(int)

s = 0
for x,y in zip(result_df[""label""], result_df[""prediction""]):
    if x == y:
        s+=1

print(s,""/"",len(result_df))
acc = s*100/len(result_df)
print('TOTAL ACC:', round(acc,2))


The problem is...now I get accuracy with sklearn 52% and my_acc 52%.
Why do I get such low accuracy on validation, when it says that its much larger?
",0,3974,"The training data you posted gives high validation accuracy, so I'm a bit confused as to where you get that 65% from, but in general when your model performs much better on training data than on unseen data, that means you're over fitting. This is a big and recurring problem in machine learning, and there is no method guaranteed to prevent this, but there are a couple of things you can try:


regularizing the weights of your network, e.g. using l2 regularization
using stochastic regularization techniques such as drop-out during training
early stopping
reducing model complexity (but you say you've already tried this)

","I will list the problems/recommendations that I see on your model.


What are you trying to predict? You are using sigmoid activation function in the last layer which seems it is a binary classification but in your loss fuction you used mse which seems strange. You can try binary_crossentropy instead of mse loss function for your model.
Your model seems suffer from overfitting so you can increase the prob. of Dropout and also add new Dropout between other hidden layers or you can remove one of the hidden layers because it seem your model is too complex. 
You can change your neuron numbers in layers like a narrower =&gt; 64 -&gt; 32 -&gt; 16 -&gt; 1 or try different NN architectures. 
Try adam optimizer instead of sgd.
If you have 57849 sample you can use 47000 samples in training+validation and rest of will be your test set. 
Don't use the same sets for your evaluation and validation. First split your data into train and test set. Then when you are fitting your model give validation_split_ratio then it will automatically give validation set from your training set.

",
Keras strange result,https://stackoverflow.com/questions/59149885,How to calculate the trace of a matrix in Keras backend?,"I am implementing a custom loss function in Keras, and I need to perform a matrix multiplication and then get the trace of the resulting matrix. I could go one of two ways, both of which are giving me trouble in Keras:

Option 1: Iterate over the rows ( and columns ) of the 2 matrices. They both have the same shape, and sum the dot products of the corresponding rows and columns:

# Define custom loss
def custom_loss(y_true,y_pred):
    batch_size = y_true.shape[0]
    X = K.log( K.dot(y_true, K.transpose(y_pred) ) )
    y = 0
    for i in range(0,int(batch_size)):
        y -= X[i,i]
    return y


Error:   


  File ""neural_net.py"", line 71, in custom_loss
      for i in range(0,int(batch_size)): TypeError: int returned non-int (type NoneType)


Strange that the shape of the matrix is returning none.

Option 2: Just do the matrix multiplication and get the trace of the matrix:

# Define custom loss
def custom_loss(y_true,y_pred):
    X = K.log( K.dot(y_true, K.transpose(y_pred) ) )
    return -np.trace(X)


Error: 


  return asanyarray(a).trace(offset=offset, axis1=axis1, axis2=axis2,
  dtype=dtype, out=out) ValueError: diag requires an array of at least
  two dimensions


I am fairly certain that X is a 2-d matrix. My batch size is 32 and there are 47 nodes in the output layer. I've seen output showing that both the matrices are 32 x 47, so the dot product is definitely 2-D. But i was reading that there are some issues about accessing the size of the matrices and using numpy operations inside the keras backend, so I was hoping to find a trace function in the keras backend but was unable to find one.

Does anyone know how to either solve option 1 iteration issue or a way to calculate the trace in the Keras backend?

Thanks!
",0,356,"I would probably use - appropriate Layer
tf.keras.layers.Lambda(
    function, output_shape=None, mask=None, arguments=None, **kwargs
)

with function as callback (to use in it numpy.trace())... Can not test now...
I also can suppose, that you can try Keras Models With TensorFlow NumPy
","Just wondering, wouldn't this work? 

def custom_loss(y_true,y_pred):
    batch_size = y_true.shape[0]
    X = K.log( K.dot(y_true, K.transpose(y_pred) ) )
    y = - tf.trace(X)
    return y

",
Keras strange result,https://stackoverflow.com/questions/56451239,"When processing IMDB data prepared by myself with a Keras RNN, accuracy never exceeds 0.5","A very strange thing is happening. I fetched the IMDB corpus from Kaggle, kept only the 50,000 positive and negative texts, counted word frequencies, sorted words according to their decreasing frequency, replaced in the texts the 10,000 most frequent words by their rank (plus 3 units), inserted a 1 at all sentence begins and replaced all words outside the 10,000 most frequent ones by number 2. In this I followed exactly the instructions given in the documentation of Keras imdb class.

I then ran a RNN with an Embedding layer, a SimpleRNN layer, a Dense layer. The result I get is an accuracy always around 0.5, no matter how hard I try. I then replace my code by imdb.load_data(num_words=10000) and get an accuracy of 0.86 already at the third epoch. How is this possible? Why such an extreme difference? What have I done wrong?

Here is the code I used:

import re, os, time, pickle
word=re.compile(r'^[A-Za-z]+$')
spacyline=re.compile(r'^([0-9]+) ([^ ]+) ([^ ]+) ([^ ]+) ([0-9]+) ([A-Za-z]+)')

DICT=dict()

inp=open(""imdb_master_lemma.txt"")
for ligne in inp:
    if (ligne[0:9]==""DEBUT DOC""):
        if (ligne[-4:-1]==""neg""):
            classe=-1
        elif (ligne[-4:-1]==""pos""):
            classe=1
    elif (ligne[0:9]==""FIN DOCUM""):
        a=1
    else:
        res=spacyline.findall(ligne)
        if res:
            lemma=str(res[0][3])
            if (word.match(lemma)):
                if (lemma in DICT.keys()):
                    DICT[lemma] += 1
                else:
                    DICT[lemma]=1
inp.close()

SORTED_WORDS=sorted(DICT.keys(), key=lambda x:DICT[x], reverse=True)
THOUSAND=SORTED_WORDS[:9997]
ORDRE=dict()
c=0
for w in THOUSAND:
    ORDRE[w]=c
    c+=1
CORPUS=[]
CLASSES=[]

inp=open(""imdb_master_lemma.txt"")
for ligne in inp:
    if (ligne[0:9]==""DEBUT DOC""):
        if (ligne[-4:-1]==""neg""):
            classe=0
        elif (ligne[-4:-1]==""pos""):
            classe=1
        a=[]
    if (ligne[0:9]==""DEBUT PHR""):
        a.append(1)
    elif (ligne[0:9]==""FIN DOCUM""):
        CORPUS.append(a)
        CLASSES.append(classe)
    else:
        res=spacyline.findall(ligne)
        if res:
            lemma=str(res[0][3])
            if lemma in ORDRE:
                a.append(ORDRE[lemma]+3)
            elif (word.match(lemma)):
                a.append(2)
inp.close()

from sklearn.utils import shuffle
CORPUS, CLASSES=shuffle(CORPUS, CLASSES)

out=open(""keras1000.pickle"",""wb"")
pickle.dump((CORPUS,CLASSES,ORDRE),out)
out.close()


The file imdb_master_lemma.txt contains the IMDB texts processed by Spacy, and I keep only the lemma (which is already in lowercase, so this is more or less what is used in Keras imdb only it should work even better since there are no plurals and verbs are lemmatized). Once the pickle file stored, I recall it and use it as follows:

picklefile=open(""keras1000.pickle"",""rb"")
(CORPUS,CLASSES,ORDRE)=pickle.load(picklefile)
picklefile.close()

import numpy as np
def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results
x_train = np.array(vectorize_sequences(CORPUS[:25000]),dtype=object)
x_test = np.array(vectorize_sequences(CORPUS[25000:]),dtype=object)
train_labels = np.array(CLASSES[:25000])
test_labels = np.array(CLASSES[25000:])

from keras import models
from keras import layers
from keras.models import Sequential
from keras.layers import Flatten, Dense, Embedding, SimpleRNN, LSTM, Bidirectional
from keras.preprocessing import sequence

input_train = sequence.pad_sequences(x_train, maxlen=500)
input_test = sequence.pad_sequences(x_test, maxlen=500)
print('input_train shape:', input_train.shape)
print('input_test shape:', input_test.shape)

model = Sequential()
model.add(Embedding(10000, 32))
model.add(SimpleRNN(32))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])

history = model.fit(input_train,
                    train_labels,
                    epochs=10,
                    batch_size=128,
                    validation_split=0.2)
results = model.evaluate(input_test, test_labels)

print(results)


The result is utterly disappointing, an accuracy around 0.5. When I replace the 14 first lines by

from keras.datasets import imdb
(x_train, train_labels), (x_test, test_labels) = imdb.load_data(num_words=10000)


then everything works as described in Chollet's book and I get immediately a very high accuracy.

Can anyone tell me what I am doing wrong?

PS. Here is a small sample of the data, to illustrate the preparation process: The two first sentences of the first IMDB document, as processed by spaCy, are

DEBUT DOCUMENT neg
DEBUT PHRASE
0 Once RB once 1 advmod
1 again RB again 5 advmod
2 Mr. NNP mr. 3 compound
3 Costner NNP costner 5 nsubj
4 has VBZ have 5 aux
5 dragged VBN drag 5 ROOT
6 out RP out 5 prt
7 a DT a 8 det
8 movie NN movie 5 dobj
9 for IN for 5 prep
10 far RB far 11 advmod
11 longer JJR long 9 pcomp
12 than IN than 11 prep
13 necessary JJ necessary 12 amod
14 . . . 5 punct
FIN PHRASE
DEBUT PHRASE
15 Aside RB aside 16 advmod
16 from IN from 33 prep
17 the DT the 21 det
18 terrific JJ terrific 19 amod
19 sea NN sea 21 compound
20 rescue NN rescue 21 compound
21 sequences NNS sequence 16 pobj
22 , , , 21 punct
23 of IN of 26 prep
24 which WDT which 23 pobj
25 there EX there 26 expl
26 are VBP be 21 relcl
27 very RB very 28 advmod
28 few JJ few 26 acomp
29 I PRP -PRON- 33 nsubj
30 just RB just 33 advmod
31 did VBD do 33 aux
32 not RB not 33 neg
33 care VB care 33 ROOT
34 about IN about 33 prep
35 any DT any 34 pobj
36 of IN of 35 prep
37 the DT the 38 det
38 characters NNS character 36 pobj
39 . . . 33 punct
FIN PHRASE


This becomes:

[1, 258, 155, 5920, 13, 979, 38, 6, 14, 17, 207, 165, 68, 1526, 1, 1044, 33, 3, 1212, 1380, 1396, 382, 7, 58, 34, 4, 51, 150, 37, 19, 12, 338, 39, 91, 7, 3, 46,


etc. As you can see, the 1 shows the sentence begin, 258 is once, 155 is again, I have missed mr. because it contains a period (but this can hardly be the reason my system is failing), 5920 is costner (apparently Kevin Costner's name appears so often, that it is included in the 10,000 most frequent words), 13 is have, 979 is drag, 38 is out, 6 is the article a, 14 is the word movie, and so on. These ranks are all very reasonable, I think, so I don't see what may have went wrong.
",0,731,"I think the problem is that you are one-hot encoding your input data (x_trainand x_test) in your vectorize_sequences function. If you skip that step, your model should work as with the Keras example data.

The reason is that your input layer model.add(Embedding(10000, 32))expects the actual indices of each word in a sequence. So something as you show in your example:

In [1] : print(x_train[0])
Out[1] : [1, 258, 155, 5920, 13, 979, 38, 6, 14, 17, 207, ...]


The embedding layer will then map those indices to the corresponding word vectors and stack them together in the right order before feeding them to your RNN.

When you one-hot encode the sequences not only do you lose the ordering in your text, but you also get a vector with dimension 10000, that Keras probably chops out when you define your maxlen for padding.

input_train = sequence.pad_sequences(x_train, maxlen=500)


That is not to say that one-hot encoding is not a valid approach. It just does not fit your architecture and would be more suitable for a simple feedforward network.

All that said, I have not tested your code and cannot say yet whether that is all that needs fixing. Let me know if that helps or if you need other insights.

Update

I just downloaded your data and ran your code only changing the input vectors. As mentioned above, you just need to feed the network with the indices you got from your preprocessing step.

Simply replace

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results
x_train = np.array(vectorize_sequences(CORPUS[:25000]),dtype=object)
x_test = np.array(vectorize_sequences(CORPUS[25000:]),dtype=object)


with

x_train = CORPUS[:25000]
x_test = CORPUS[25000:]


and your code should work nicely. I quickly got to 95% accuracy in 5 epochs.
",,
Keras strange result,https://stackoverflow.com/questions/55555044,Keras Image Classification - Prediction accuracy on validation dataset does not match val_acc,"I am trying to classify a set of images within two categories: left and right.

I built a CNN using Keras, my classifier seems to work well:


I have 1,939 images used for training (50% left, 50% right)
I have 648 images used for validation (50% left, 50% right)
All images are 115x45, in greyscale
acc is increasing up to 99.53%
val_acc is increasing up to 98.38%
Both loss and val_loss are converging close to 0


Keras verbose looks normal to me:

60/60 [==============================] - 6s 98ms/step - loss: 0.6295 - acc: 0.6393 - val_loss: 0.4877 - val_acc: 0.7641
Epoch 2/32
60/60 [==============================] - 5s 78ms/step - loss: 0.4825 - acc: 0.7734 - val_loss: 0.3403 - val_acc: 0.8799
Epoch 3/32
60/60 [==============================] - 5s 77ms/step - loss: 0.3258 - acc: 0.8663 - val_loss: 0.2314 - val_acc: 0.9042
Epoch 4/32
60/60 [==============================] - 5s 83ms/step - loss: 0.2498 - acc: 0.8942 - val_loss: 0.2329 - val_acc: 0.9042
Epoch 5/32
60/60 [==============================] - 5s 76ms/step - loss: 0.2408 - acc: 0.9002 - val_loss: 0.1426 - val_acc: 0.9432
Epoch 6/32
60/60 [==============================] - 5s 80ms/step - loss: 0.1968 - acc: 0.9260 - val_loss: 0.1484 - val_acc: 0.9367
Epoch 7/32
60/60 [==============================] - 5s 77ms/step - loss: 0.1621 - acc: 0.9319 - val_loss: 0.1141 - val_acc: 0.9578
Epoch 8/32
60/60 [==============================] - 5s 81ms/step - loss: 0.1600 - acc: 0.9361 - val_loss: 0.1229 - val_acc: 0.9513
Epoch 9/32
60/60 [==============================] - 4s 70ms/step - loss: 0.1358 - acc: 0.9462 - val_loss: 0.0884 - val_acc: 0.9692
Epoch 10/32
60/60 [==============================] - 4s 74ms/step - loss: 0.1193 - acc: 0.9542 - val_loss: 0.1232 - val_acc: 0.9529
Epoch 11/32
60/60 [==============================] - 5s 79ms/step - loss: 0.1075 - acc: 0.9595 - val_loss: 0.0865 - val_acc: 0.9724
Epoch 12/32
60/60 [==============================] - 4s 73ms/step - loss: 0.1209 - acc: 0.9531 - val_loss: 0.1067 - val_acc: 0.9497
Epoch 13/32
60/60 [==============================] - 4s 73ms/step - loss: 0.1135 - acc: 0.9609 - val_loss: 0.0860 - val_acc: 0.9838
Epoch 14/32
60/60 [==============================] - 4s 70ms/step - loss: 0.0869 - acc: 0.9682 - val_loss: 0.0907 - val_acc: 0.9675
Epoch 15/32
60/60 [==============================] - 4s 71ms/step - loss: 0.0960 - acc: 0.9637 - val_loss: 0.0996 - val_acc: 0.9643
Epoch 16/32
60/60 [==============================] - 4s 73ms/step - loss: 0.0951 - acc: 0.9625 - val_loss: 0.1223 - val_acc: 0.9481
Epoch 17/32
60/60 [==============================] - 4s 70ms/step - loss: 0.0685 - acc: 0.9729 - val_loss: 0.1220 - val_acc: 0.9513
Epoch 18/32
60/60 [==============================] - 4s 73ms/step - loss: 0.0791 - acc: 0.9715 - val_loss: 0.0959 - val_acc: 0.9692
Epoch 19/32
60/60 [==============================] - 4s 71ms/step - loss: 0.0595 - acc: 0.9802 - val_loss: 0.0648 - val_acc: 0.9773
Epoch 20/32
60/60 [==============================] - 4s 71ms/step - loss: 0.0486 - acc: 0.9844 - val_loss: 0.0691 - val_acc: 0.9838
Epoch 21/32
60/60 [==============================] - 4s 70ms/step - loss: 0.0499 - acc: 0.9812 - val_loss: 0.1166 - val_acc: 0.9627
Epoch 22/32
60/60 [==============================] - 4s 71ms/step - loss: 0.0481 - acc: 0.9844 - val_loss: 0.0875 - val_acc: 0.9734
Epoch 23/32
60/60 [==============================] - 4s 70ms/step - loss: 0.0533 - acc: 0.9814 - val_loss: 0.1094 - val_acc: 0.9724
Epoch 24/32
60/60 [==============================] - 4s 70ms/step - loss: 0.0487 - acc: 0.9812 - val_loss: 0.0722 - val_acc: 0.9740
Epoch 25/32
60/60 [==============================] - 4s 72ms/step - loss: 0.0441 - acc: 0.9828 - val_loss: 0.0992 - val_acc: 0.9773
Epoch 26/32
60/60 [==============================] - 4s 71ms/step - loss: 0.0667 - acc: 0.9726 - val_loss: 0.0964 - val_acc: 0.9643
Epoch 27/32
60/60 [==============================] - 4s 73ms/step - loss: 0.0436 - acc: 0.9835 - val_loss: 0.0771 - val_acc: 0.9708
Epoch 28/32
60/60 [==============================] - 4s 71ms/step - loss: 0.0322 - acc: 0.9896 - val_loss: 0.0872 - val_acc: 0.9756
Epoch 29/32
60/60 [==============================] - 5s 80ms/step - loss: 0.0294 - acc: 0.9943 - val_loss: 0.1414 - val_acc: 0.9578
Epoch 30/32
60/60 [==============================] - 5s 76ms/step - loss: 0.0348 - acc: 0.9870 - val_loss: 0.1102 - val_acc: 0.9659
Epoch 31/32
60/60 [==============================] - 5s 76ms/step - loss: 0.0306 - acc: 0.9922 - val_loss: 0.0794 - val_acc: 0.9659
Epoch 32/32
60/60 [==============================] - 5s 76ms/step - loss: 0.0152 - acc: 0.9953 - val_loss: 0.1051 - val_acc: 0.9724
Model: ""sequential_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 113, 43, 32)       896
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 56, 21, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 54, 19, 32)        9248
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 27, 9, 32)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 7776)              0
_________________________________________________________________
dense_1 (Dense)              (None, 128)               995456
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 129
=================================================================
Total params: 1,005,729
Trainable params: 1,005,729
Non-trainable params: 0


So everything looks great, but when I tried to predict the category of 2,000 samples I got very strange results, with an accuracy &lt; 70%.

At first I thought this sample might be biased, so I tried, instead, to predict the images in the validation dataset.

I should have a 98.38% accuracy, and a perfect 50-50 split, but instead, once again I got:


170 images predicted right, instead of 324, with an accuracy of 98.8%
478 images predicted left, instead of 324, with an accuracy of 67.3%
Average accuracy: 75.69% and not 98.38%


I guess something is wrong either in my CNN or my prediction script.

CNN classifier code:

from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten
from keras.layers import Dense
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

# Init CNN
classifier = Sequential()

# Step 1 - Convolution
classifier.add(Conv2D(32, (3, 3), input_shape = (115, 45, 3), activation = 'relu'))

# Step 2 - Pooling
classifier.add(MaxPooling2D(pool_size = (2, 2)))

# Adding a second convolutional layer
classifier.add(Conv2D(32, (3, 3), activation = 'relu'))
classifier.add(MaxPooling2D(pool_size = (2, 2)))

# Step 3 - Flattening
classifier.add(Flatten())

# Step 4 - Full connection
classifier.add(Dense(units = 128, activation = 'relu'))
classifier.add(Dense(units = 1, activation = 'sigmoid'))

# Compiling the CNN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Part 2 - Fitting the CNN to the images
from keras.preprocessing.image import ImageDataGenerator
import numpy

train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = False)
test_datagen = ImageDataGenerator(rescale = 1./255)

training_set = train_datagen.flow_from_directory('./dataset/training_set',
                                                 target_size = (115, 45),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

test_set = test_datagen.flow_from_directory('./dataset/test_set',
                                            target_size = (115, 45),
                                            batch_size = 32,
                                            class_mode = 'binary')

classifier.fit_generator(training_set,
                         steps_per_epoch = 1939/32, # total samples / batch size
                         epochs = 32,
                         validation_data = test_set,
                         validation_steps = 648/32)

# Save the classifier
classifier.evaluate_generator(generator=test_set)
classifier.summary()
classifier.save('./classifier.h5')


Prediction code:

from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten
from keras.layers import Dense
from keras.models import load_model
from keras.preprocessing.image import ImageDataGenerator
import os
import numpy as np
from keras.preprocessing import image
from shutil import copyfile

classifier = load_model('./classifier.h5')
folder = './small/'
files = os.listdir(folder)
pleft = 0
pright = 0
for f in files:
    test_image = image.load_img(folder+f, target_size = (115, 45))
    test_image = image.img_to_array(test_image)
    test_image = np.expand_dims(test_image, axis = 0)
    result = classifier.predict(test_image)
        #print training_set.class_indices
    if result[0][0] == 1:
        pright=pright+1
        prediction = 'right'
        copyfile(folder+'../'+f, '/found_right/'+f)
    else:
        prediction = 'left'
        copyfile(folder+'../'+f, '/found_left/'+f)
        pleft=pleft+1

ptot = pleft + pright
print 'Left = '+str(pleft)+' ('+str(pleft / (ptot / 100))+'%)'
print 'Right = '+str(pright)
print 'Total = '+str(ptot)


Output:

Left = 478 (79%)
Right = 170
Total = 648


Your help will be much appreciated.
",0,734,"I resolved this issue by doing two things:


As @Matias Valdenegro suggested, I had to rescale the image values before predicting, I added test_image /= 255. before calling predict().
As my val_loss was still a bit high, I added an EarlyStopping callback as well as two Dropout() before my Dense layers.


My prediction results are now consistent with the ones obtained during training/validation.
",,
Keras strange result,https://stackoverflow.com/questions/53920322,Keras model evaluation accuracy on training data set is lower than fitting accuracy,"I get some strange result on evaluating my model with the training data set.
I want to develop a cnn with such structure:

Input --&gt; conv1d --&gt; MaxPool1D --&gt; Flatten --&gt; Dense --&gt; Dense


This is my model:

model = Model(inputs=[inputLayerU, inputLayerM], outputs=outputLayer)
model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])    
model.fit([inputU, inputM],outputY , epochs=100, steps_per_epoch=500)


and this is the result of training the model:

Epoch 95/100
500/500 [==============================] - 329s 659ms/step - loss: 0.5058 - acc: 0.8845
Epoch 96/100
500/500 [==============================] - 329s 659ms/step - loss: 0.4137 - acc: 0.9259
Epoch 97/100
500/500 [==============================] - 329s 659ms/step - loss: 0.3221 - acc: 0.9534
Epoch 98/100
500/500 [==============================] - 329s 659ms/step - loss: 0.2938 - acc: 0.9596
Epoch 99/100
500/500 [==============================] - 330s 659ms/step - loss: 0.4707 - acc: 0.9352
Epoch 100/100
500/500 [==============================] - 329s 659ms/step - loss: 0.4324 - acc: 0.9543


I save the model and weights and then load them and evaluate the model using the same training data set:

loaded_model = model_from_json(loaded_model_json)
loaded_model.load_weights(""GCN-conv1d-acc.h5"")
loaded_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])
score = loaded_model.evaluate(inputTrain,outTrain,steps=500, verbose=0)


However I get this result:

[7.320816993713379, 0.3042338788509369]


I expected to get some results close to the fitting result but it's too far away.
I checked these postes:

Strange behaviour of the loss function in keras model, with pretrained convolutional base

http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/

They say Keras has some problems with batch normalization and dropout layers, however I don't use neither of them.
",0,667,"As far as i know, Adam is the the variation of SGD, which does not take into account the entire data points for model fitting, and pick a batch(subset of rows) for the training,

So this could lead to inconsistency, when you first train model and save weights, 
The next time you run the algo using same weights, the subset on which the weights are being applied could have changed
",,
Keras strange result,https://stackoverflow.com/questions/53906309,Different between fit and evaluate in keras,"I have used 100000 samples to train a general model in Keras and achieve good performance. Then, for a particular sample, I want to use the trained weights as initialization and continue to optimize the weights to further optimize the loss of the particular sample.

However, the problem occurred. First, I load the trained weight by the keras API easily, then, I evaluate the loss of the one particular sample, and the loss is close to the loss of the validation loss during the training of the model. I think it is normal. However, when I use the trained weight as the inital and further optimize the weight over the one sample by model.fit(), the loss is really strange. It is much higher than the evaluate result and gradually became normal after several epochs. 

I think it is strange that, for the same one simple and loading the same model weight, why the model.fit() and model.evaluate() return different results. I used batch normalization layers in my model and I wonder that it may be the reason. The result of model.evaluate() seems normal, as it is close to what I seen in the validation set before. 

So what cause the different between fit and evaluation? How can I solve it? 
",0,908,,,
Keras strange result,https://stackoverflow.com/questions/50701786,Accuracy lower than AUC for identical Keras models &amp; data,"I have a strange Keras classification behaviour.

I got different accuracy when using cross-validation vs a holdout set.

2 Identical models but with different evaluation methods:


Model1 uses 10-Kfold cross validation (achieving 0.98 mean AUC and lowest AUC 0.89).
Model2 uses hold out set (Accuracy 0.82)


I was expecting to see the worst accuracy of model 2 to be the lowest fold accuracy (0.89 not 0.82).

Data of a small size ~10k x 13

Kfold: 10 folds

Model 1:

def create_baseline():
    # create model
    model = models.Sequential()
    model.add(layers.Dense(64, input_dim=set_1.iloc[:,0:-1].shape[1], activation='relu'))
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model


This the important part of my code (the rest is related to ploting the ROC):

Note: I have tried both with and without standardization

estimators = []
estimators.append(('standardize', MinMaxScaler()))
estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, nb_epoch=1000, batch_size=1000, verbose=0)))
pipeline = Pipeline(estimators)
cv = StratifiedKFold(n_splits=10)
classifier = pipeline
mean_tpr = 0.0
mean_fpr = np.linspace(0, 1, 100)

colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])
lw = 2
i = 0
for (train, test), color in zip(cv.split(X, y), colors):
    classifier.fit(X[train], y[train])
    probas_ = classifier.predict_proba(X[test])
    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])
    mean_tpr += interp(mean_fpr, fpr, tpr)
    mean_tpr[0] = 0.0
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=lw, color=color,
             label='ROC fold %d (area = %0.2f)' % (i, roc_auc))

    i += 1


Output:


As you can see, I have .98 averag ROC.

Issue:

Model 2:

std = MinMaxScaler()
X_norm = std.fit_transform(X)
X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(X_norm, y, test_size=0.1, random_state=5)


Keras model

model_2 = models.Sequential()
model_2.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model_2.add(layers.Dense(64, activation='relu'))
model_2.add(layers.Dense(1, activation='sigmoid'))
model_2.compile(optimizer='adam',
loss='binary_crossentropy',
metrics=['accuracy'])


Runing the model:

history = model_2.fit(X_train_norm,
y_train_norm,
epochs=1000,
batch_size=1000,
validation_data=(X_test_norm, y_test_norm))


Results (last iterations):

8988/8988 [==============================] - 0s - loss: 0.3517 - acc: 0.8249 - val_loss: 0.3701 - val_acc: 0.7954
Epoch 997/1000
8988/8988 [==============================] - 0s - loss: 0.3516 - acc: 0.8238 - val_loss: 0.3699 - val_acc: 0.8059
Epoch 998/1000
8988/8988 [==============================] - 0s - loss: 0.3516 - acc: 0.8250 - val_loss: 0.3694 - val_acc: 0.8038
Epoch 999/1000
8988/8988 [==============================] - 0s - loss: 0.3512 - acc: 0.8241 - val_loss: 0.3692 - val_acc: 0.7975
Epoch 1000/1000
8988/8988 [==============================] - 0s - loss: 0.3504 - acc: 0.8247 - val_loss: 0.3696 - val_acc: 0.7975


Why the performance of model2 is lower than model1?

Note: 
- same data, type of keras model, and seed but different results!
- I did multiple test with and without standarization and with same and differents seeds, and I still have the same issue.
- I understant that I can use simpler models but my issue is realted to using Keras classifier.

Please correct me if I am doing something wrong.
",0,872,,,
Keras strange result,https://stackoverflow.com/questions/43291488,Strange colour cast in Python plt.imshow() image,,0,795,,,
Keras strange result,https://stackoverflow.com/questions/46958739,Error trying to pass list to hyperas as a parameter,,-1,2112,,,
Keras strange issue,https://stackoverflow.com/questions/50063613,What is the purpose of the add_loss function in Keras?,"Currently I stumbled across variational autoencoders and tried to make them work on MNIST using keras. I found a tutorial on github.

My question concerns the following lines of code:



# Build model
vae = Model(x, x_decoded_mean)

# Calculate custom loss
xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)
kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
vae_loss = K.mean(xent_loss + kl_loss)

# Compile
vae.add_loss(vae_loss)
vae.compile(optimizer='rmsprop')


Why is add_loss used instead of specifying it as compile option? Something like  vae.compile(optimizer='rmsprop', loss=vae_loss) does not seem to work and throws the following error:

ValueError: The model cannot be compiled because it has no loss to optimize.


What is the difference between this function and a custom loss function, that I can add as an argument for Model.fit()?

Thanks in advance! 

P.S.: I know there are several issues concerning this on github, but most of them were open and uncommented. If this has been resolved already, please share the link!



Edit 1

I removed the line which adds the loss to the model and used the loss argument of the compile function. It looks like this now:



# Build model
vae = Model(x, x_decoded_mean)

# Calculate custom loss
xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)
kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
vae_loss = K.mean(xent_loss + kl_loss)

# Compile
vae.compile(optimizer='rmsprop', loss=vae_loss)


This throws an TypeError:

TypeError: Using a 'tf.Tensor' as a Python 'bool' is not allowed. Use 'if t is not None:' instead of 'if t:' to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.




Edit 2

Thanks to @MarioZ's efforts, I was able to figure out a workaround for this. 



# Build model
vae = Model(x, x_decoded_mean)

# Calculate custom loss in separate function
def vae_loss(x, x_decoded_mean):
    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)
    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
    vae_loss = K.mean(xent_loss + kl_loss)
    return vae_loss

# Compile
vae.compile(optimizer='rmsprop', loss=vae_loss)

...

vae.fit(x_train, 
    x_train,        # &lt;-- did not need this previously
    shuffle=True,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(x_test, x_test))     # &lt;-- worked with (x_test, None) before


For some strange reason, I had to explicitly specify y and y_test while fitting the model. Originally, I didn't need to do this. The produced samples seem reasonable to me. 

Although I could resolve this, I still don't know what the differences and disadvantages of these two methods are (other than needing a different syntax). Can someone give me more insight?
",51,37909,"I'll try to answer the original question of why model.add_loss() is being used instead of specifying a custom loss function to model.compile(loss=...).

All loss functions in Keras always take two parameters y_true and y_pred.  Have a look at the definition of the various standard loss functions available in Keras, they all have these two parameters.  They are the 'targets' (the Y variable in many textbooks) and the actual output of the model.  Most standard loss functions can be written as an expression of these two tensors.  But some more complex losses cannot be written in that way.  For your VAE example this is the case because the loss function also depends on additional tensors, namely z_log_var and z_mean, which are not available to the loss functions.  Using model.add_loss() has no such restriction and allows you to write much more complex losses that depend on many other tensors, but it has the inconvenience of being more dependent on the model, whereas the standard loss functions work with just any model.

(Note: The code proposed in other answers here are somewhat cheating in as much as they just use global variables to sneak in the additional required dependencies. This makes the loss function not a true function in the mathematical sense. I consider this to be much less clean code and I expect it to be more error-prone.)
","JIH's answer is right of course but maybe it is useful to add:

model.add_loss() has no restrictions, but it also removes the comfort of using for example targets in the model.fit().

If you have a loss that depends on additional parameters of the model, of other models or external variables, you can still use a Keras type encapsulated loss function by having an encapsulating function where you pass all the additional parameters:

def loss_carrier(extra_param1, extra_param2):
    def loss(y_true, y_pred):
        #x = complicated math involving extra_param1, extraparam2, y_true, y_pred
        #remember to use tensor objects, so for example keras.sum, keras.square, keras.mean
        #also remember that if extra_param1, extra_maram2 are variable tensors instead of simple floats,
        #you need to have them defined as inputs=(main,extra_param1, extraparam2) in your keras.model instantiation.
        #and have them defind as keras.Input or tf.placeholder with the right shape.
        return x
    return loss

model.compile(optimizer='adam', loss=loss_carrier)


The trick is the last row where you return a function as Keras expects them with just two parameters y_true and y_pred.

Possibly looks more complicated than the model.add_loss version, but the loss stays modular.
","I was also wondering about the same query and some related stuff like how to add loss function within the intermediate layers. Here I'm sharing some of the observed information, hope it may help others. It's true that standard keras loss functions only take two arguments, y_true and y_pred. But during the experiment, there can some cases where we need some external parameter or coefficient while computing with these two values (y_true, y_pred). This can be needed at the last layer as usual or somewhere in the middle of the model's layer.
model.add_loss()
The accepted answer correctly said about the model.add_loss() functions. It potentially depends on the layer inputs (tensor). According to the official doc, when writing the call method of a custom layer or a subclassed model, we may want to compute scalar quantities that we want to minimize during training (e.g. regularization losses). We can use the add_loss() layer method to keep track of such loss terms. For instance, activity regularization losses dependent on the inputs passed when calling a layer. Here's an example of a layer that adds a sparsity regularization loss based on the L2 norm of the inputs:
from tensorflow.keras.layers import Layer

class MyActivityRegularizer(Layer):
  """"""Layer that creates an activity sparsity regularization loss.""""""

  def __init__(self, rate=1e-2):
    super(MyActivityRegularizer, self).__init__()
    self.rate = rate

  def call(self, inputs):
    # We use `add_loss` to create a regularization loss
    # that depends on the inputs.
    self.add_loss(self.rate * tf.reduce_sum(tf.square(inputs)))
    return inputs

Loss values added via add_loss can be retrieved in the .losses list property of any Layer or Model (they are recursively retrieved from every underlying layer):
from tensorflow.keras import layers

class SparseMLP(Layer):
  """"""Stack of Linear layers with a sparsity regularization loss.""""""

  def __init__(self, output_dim):
      super(SparseMLP, self).__init__()
      self.dense_1 = layers.Dense(32, activation=tf.nn.relu)
      self.regularization = MyActivityRegularizer(1e-2)
      self.dense_2 = layers.Dense(output_dim)

  def call(self, inputs):
      x = self.dense_1(inputs)
      x = self.regularization(x)
      return self.dense_2(x)


mlp = SparseMLP(1)
y = mlp(tf.ones((10, 10)))

print(mlp.losses)  # List containing one float32 scalar

Also note, when using model.fit(), such loss terms are handled automatically. When writing a custom training loop, we should retrieve these terms by hand from model.losses, like this:
loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam()

# Iterate over the batches of a dataset.
for x, y in dataset:
    with tf.GradientTape() as tape:
        # Forward pass.
        logits = model(x)
        # Loss value for this batch.
        loss_value = loss_fn(y, logits)
        # Add extra loss terms to the loss value.
        loss_value += sum(model.losses) # &lt; ------------- HERE ---------

    # Update the weights of the model to minimize the loss value.
    gradients = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(gradients, model.trainable_weights))

Custom losses
With model.add_loss(), (AFAIK), we can use it somewhere in the middle of the network. Here we no longer bound with only two parameters i.e. y_true, y_pred. But what if we also want to impute external parameter or coefficient to the last layer loss functions of the network. Nric answer is correct. But it can also be implemented by subclassing the tf.keras.losses.Loss class by implementing the following two methods:

__init__(self): accept parameters to pass during the call of your loss function
call(self, y_true, y_pred): use the targets (y_true) and the model predictions (y_pred) to compute the model's loss

Here is an example of a custom MSE by subclassing the tf.keras.losses.Loss class. And here we also no longer bound only two parameters i.e. y_ture, y_pred.
class CustomMSE(keras.losses.Loss):
    def __init__(self, regularization_factor=0.1, name=""custom_mse""):
        super().__init__(name=name)
        self.regularization_factor = regularization_factor

    def call(self, y_true, y_pred):
        mse = tf.math.reduce_mean(tf.square(y_true - y_pred))
        reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))
        return mse + reg * self.regularization_factor

model.compile(optimizer=..., loss=CustomMSE())

"
Keras strange issue,https://stackoverflow.com/questions/52700621,Tensorflow server: I don&#39;t want to initialize global variables for every session,,15,1780,"Have you considered TensorFlow Serving? https://www.tensorflow.org/serving/

Generally you'd want to cache Sessions, which I believe is the strategy TF Serving uses. That will be by far the best experience for deploying a TF model into a datacenter.

You could also go the other direction and tf.enable_eager_execution(), which eliminates the need for Sessions. Variables still get initialized, although it happens as soon as the Python variable objects are created.

But if you really want to create and destroy Sessions, you could replace variables in the graph with constants (""freeze"" it). I'd also consider disabling graph optimizations in this case, as the first session.run call with a new set of feeds and fetches will by default spend some time optimizing the graph (configured through a RewriterConfig inside a GraphOptions proto).

(Expanded from a comment on the question)
","I am not sure if this could help you, but one needs to know that in tensorflow, variables are only initialized for a given Session. A variable needs to be initialized in each Session it is used -- even in the simpliest possible scenario:

import tensorflow as tf

x = tf.Variable(0.)

with tf.Session() as sess:
    tf.global_variables_initializer().run()
    # x is initialized -- no issue here
    x.eval()

with tf.Session() as sess:
    x.eval()
    # Error -- x was never initialized in this session, even though
    # it has been initialized before in another session

",
Keras strange issue,https://stackoverflow.com/questions/57122907,Tensorboard AttributeError: &#39;ModelCheckpoint&#39; object has no attribute &#39;on_train_batch_begin&#39;,,10,14410,"In your imports you are mixing keras and tf.keras, which are NOT compatible with each other, as you get weird errors like these.

So a simple solution is to choose keras or tf.keras, and make all imports from that package, and never mix it with the other.
","I replace this line 

from keras.callbacks import EarlyStopping, ModelCheckpoint


To this line 

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

","Yes imports are mixed from keras and tensorflow 

try sticking on to tensorflow.keras for example :

from tensorflow.keras.callbacks import EarlyStopping

"
Keras strange issue,https://stackoverflow.com/questions/55569181,Why is accuracy from fit_generator different to that from evaluate_generator in Keras?,"What I do:


I am training a pre-trained CNN with Keras fit_generator(). This produces evaluation metrics (loss, acc, val_loss, val_acc) after each epoch. After training the model, I produce evaluation metrics (loss, acc) with evaluate_generator().


What I expect:


If I train the model for one epoch, I would expect that the metrics obtained with fit_generator() and evaluate_generator() are the same. They both should derive the metrics based on the entire dataset.


What I observe: 


Both loss and acc are different from fit_generator() and  evaluate_generator():



What I don't understand:


Why the accuracy from fit_generator() is
different to that from evaluate_generator()


My code:

def generate_data(path, imagesize, nBatches):
    datagen = ImageDataGenerator(rescale=1./255)
    generator = datagen.flow_from_directory\
        (directory=path,                                        # path to the target directory
         target_size=(imagesize,imagesize),                     # dimensions to which all images found will be resize
         color_mode='rgb',                                      # whether the images will be converted to have 1, 3, or 4 channels
         classes=None,                                          # optional list of class subdirectories
         class_mode='categorical',                              # type of label arrays that are returned
         batch_size=nBatches,                                   # size of the batches of data
         shuffle=True)                                          # whether to shuffle the data
    return generator


[...]

def train_model(model, nBatches, nEpochs, trainGenerator, valGenerator, resultPath):
    history = model.fit_generator(generator=trainGenerator,
                                  steps_per_epoch=trainGenerator.samples//nBatches,     # total number of steps (batches of samples)
                                  epochs=nEpochs,                   # number of epochs to train the model
                                  verbose=2,                        # verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch
                                  callbacks=None,                   # keras.callbacks.Callback instances to apply during training
                                  validation_data=valGenerator,     # generator or tuple on which to evaluate the loss and any model metrics at the end of each epoch
                                  validation_steps=
                                  valGenerator.samples//nBatches,   # number of steps (batches of samples) to yield from validation_data generator before stopping at the end of every epoch
                                  class_weight=None,                # optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function
                                  max_queue_size=10,                # maximum size for the generator queue
                                  workers=32,                       # maximum number of processes to spin up when using process-based threading
                                  use_multiprocessing=True,         # whether to use process-based threading
                                  shuffle=False,                     # whether to shuffle the order of the batches at the beginning of each epoch
                                  initial_epoch=0)                  # epoch at which to start training
    print(""%s: Model trained."" % datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))

    # Save model
    modelPath = os.path.join(resultPath, datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + '_modelArchitecture.h5')
    weightsPath = os.path.join(resultPath, datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + '_modelWeights.h5')
    model.save(modelPath)
    model.save_weights(weightsPath)
    print(""%s: Model saved."" % datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))
    return history, model


[...]

def evaluate_model(model, generator):
    score = model.evaluate_generator(generator=generator,           # Generator yielding tuples
                                     steps=
                                     generator.samples//nBatches)   # number of steps (batches of samples) to yield from generator before stopping

    print(""%s: Model evaluated:""
          ""\n\t\t\t\t\t\t Loss: %.3f""
          ""\n\t\t\t\t\t\t Accuracy: %.3f"" %
          (datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),
           score[0], score[1]))


[...]

def main():
    # Create model
    modelUntrained = create_model(imagesize, nBands, nClasses)

    # Prepare training and validation data
    trainGenerator = generate_data(imagePathTraining, imagesize, nBatches)
    valGenerator = generate_data(imagePathValidation, imagesize, nBatches)

    # Train and save model
    history, modelTrained = train_model(modelUntrained, nBatches, nEpochs, trainGenerator, valGenerator, resultPath)

    # Evaluate on validation data
    print(""%s: Model evaluation (valX, valY):"" % datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))
    evaluate_model(modelTrained, valGenerator)

    # Evaluate on training data
    print(""%s: Model evaluation (trainX, trainY):"" % datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))
    evaluate_model(modelTrained, trainGenerator)




Update

I found some sites that report on this issue:


The Batch Normalization layer of Keras is broken 
Strange
behaviour of the loss function in keras model, with pretrained
convolutional base 
model.evaluate() gives a different loss on
training data from the one in training process 
Got different    accuracy between history and evaluate
ResNet: 100% accuracy    during training, but 33% prediction
accuracy with the same data


I tried following some of their suggested solutions without success so far. acc and loss are still different from fit_generator() and evaluate_generator(), even when using the exact same data generated with the same generator for training and validation. Here is what I tried:


statically setting the learning_phase for the entire script or before adding new layers to the pre-trained ones


    K.set_learning_phase(0) # testing  
    K.set_learning_phase(1) # training



unfreezing all batch normalization layers from the pre-trained model


    for i in range(len(model.layers)):
        if str.startswith(model.layers[i].name, 'bn'):
            model.layers[i].trainable=True



not adding dropout or batch normalization as untrained layers


    # Create pre-trained base model
    basemodel = ResNet50(include_top=False,                     # exclude final pooling and fully connected layer in the original model
                         weights='imagenet',                    # pre-training on ImageNet
                         input_tensor=None,                     # optional tensor to use as image input for the model
                         input_shape=(imagesize,                # shape tuple
                                      imagesize,
                                      nBands),
                         pooling=None,                          # output of the model will be the 4D tensor output of the last convolutional layer
                         classes=nClasses)                      # number of classes to classify images into

    # Create new untrained layers
    x = basemodel.output
    x = GlobalAveragePooling2D()(x)                             # global spatial average pooling layer
    x = Dense(1024, activation='relu')(x)                       # fully-connected layer
    y = Dense(nClasses, activation='softmax')(x)                # logistic layer making sure that probabilities sum up to 1

    # Create model combining pre-trained base model and new untrained layers
    model = Model(inputs=basemodel.input,
                  outputs=y)

    # Freeze weights on pre-trained layers
    for layer in basemodel.layers:
        layer.trainable = False

    # Define learning optimizer
    learningRate = 0.01
    optimizerSGD = optimizers.SGD(lr=learningRate,              # learning rate.
                                  momentum=0.9,                 # parameter that accelerates SGD in the relevant direction and dampens oscillations
                                  decay=learningRate/nEpochs,   # learning rate decay over each update
                                  nesterov=True)                # whether to apply Nesterov momentum
    # Compile model
    model.compile(optimizer=optimizerSGD,                       # stochastic gradient descent optimizer
                  loss='categorical_crossentropy',              # objective function
                  metrics=['accuracy'],                         # metrics to be evaluated by the model during training and testing
                  loss_weights=None,                            # scalar coefficients to weight the loss contributions of different model outputs
                  sample_weight_mode=None,                      # sample-wise weights
                  weighted_metrics=None,                        # metrics to be evaluated and weighted by sample_weight or class_weight during training and testing
                  target_tensors=None)                          # tensor model's target, which will be fed with the target data during training



using different pre-trained CNNs as base model (VGG19, InceptionV3, InceptionResNetV2, Xception)


    from keras.applications.vgg19 import VGG19

    basemodel = VGG19(include_top=False,                        # exclude final pooling and fully connected layer in the original model
                         weights='imagenet',                    # pre-training on ImageNet
                         input_tensor=None,                     # optional tensor to use as image input for the model
                         input_shape=(imagesize,                # shape tuple
                                      imagesize,
                                      nBands),
                         pooling=None,                          # output of the model will be the 4D tensor output of the last convolutional layer
                         classes=nClasses)                      # number of classes to classify images into


Please let me know if there are other solutions around that I am missing.
",10,7473,"I now managed having the same evaluation metrics. I changed the following:


I set seed in flow_from_directory() as suggested by @Anakin


def generate_data(path, imagesize, nBatches):
        datagen = ImageDataGenerator(rescale=1./255)
        generator = datagen.flow_from_directory(directory=path,     # path to the target directory
             target_size=(imagesize,imagesize),                     # dimensions to which all images found will be resize
             color_mode='rgb',                                      # whether the images will be converted to have 1, 3, or 4 channels
             classes=None,                                          # optional list of class subdirectories
             class_mode='categorical',                              # type of label arrays that are returned
             batch_size=nBatches,                                   # size of the batches of data
             shuffle=True,                                          # whether to shuffle the data
             seed=42)                                               # random seed for shuffling and transformations
        return generator





I set use_multiprocessing=False in fit_generator() according to the warning: use_multiprocessing=True and multiple workers may duplicate your data


history = model.fit_generator(generator=trainGenerator,
                                  steps_per_epoch=trainGenerator.samples//nBatches,     # total number of steps (batches of samples)
                                  epochs=nEpochs,                   # number of epochs to train the model
                                  verbose=2,                        # verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch
                                  callbacks=callback,               # keras.callbacks.Callback instances to apply during training
                                  validation_data=valGenerator,     # generator or tuple on which to evaluate the loss and any model metrics at the end of each epoch
                                  validation_steps=
                                  valGenerator.samples//nBatches,   # number of steps (batches of samples) to yield from validation_data generator before stopping at the end of every epoch
                                  class_weight=None,                # optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function
                                  max_queue_size=10,                # maximum size for the generator queue
                                  workers=1,                        # maximum number of processes to spin up when using process-based threading
                                  use_multiprocessing=False,        # whether to use process-based threading
                                  shuffle=False,                    # whether to shuffle the order of the batches at the beginning of each epoch
                                  initial_epoch=0)                  # epoch at which to start training





I unified my python setup as suggested in the keras documentation on how to  obtain reproducible results using Keras during development


import tensorflow as tf
import random as rn
from keras import backend as K

np.random.seed(42)
rn.seed(12345)
session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,
                              inter_op_parallelism_threads=1)
tf.set_random_seed(1234)
sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)





Instead of rescaling input images with datagen = ImageDataGenerator(rescale=1./255), I now generate my data with:


from keras.applications.resnet50 import preprocess_input
datagen = ImageDataGenerator(preprocessing_function=preprocess_input)




With this, I managed to have a similar accuracy and loss from fit_generator() and evaluate_generator(). Also, using the same data for training and testing now results in a similar metrics. Reasons for remaining differences are provided in the keras documentation.
","Set use_multiprocessing=False at fit_generator level fixes the problem BUT at the cost of slowing down training significantly. A better but still imperfect workround would be to set use_multiprocessing=False for only the validation generator as the code below modified from keras' fit_generator function. 

...
        try:
            if do_validation:
                if val_gen and workers &gt; 0:
                    # Create an Enqueuer that can be reused
                    val_data = validation_data
                    if isinstance(val_data, Sequence):
                        val_enqueuer = OrderedEnqueuer(val_data,
                                                       **use_multiprocessing=False**)
                        validation_steps = len(val_data)
                    else:
                        val_enqueuer = GeneratorEnqueuer(val_data,
                                                         **use_multiprocessing=False**)
                    val_enqueuer.start(workers=workers,
                                       max_queue_size=max_queue_size)
                    val_enqueuer_gen = val_enqueuer.get()
...

","Training for one epoch might not be informative enough in this case. Also your train and test data may not be exactly same, since you are not setting a random seed to the flow_from_directory method. Have a look here.

Maybe, you can set a seed, remove augmentations (if any) and save trained model weights to load them later to check.
"
Keras strange issue,https://stackoverflow.com/questions/43322683,Python loop taking more time at each iteration,"I made a for loop which strangely increases in duration at each iteration although the amount of variables manipulated remains constant. The code is below with:


X [N*F]: a numpy array with N samples containing F variables
(features);
parts [N]: a numpy array containing the number of the
participant of each sample in X;
model_filename: the model template file
name for each participant (i.e. I have a model per participant)


My goal is to apply the model of participant p to the data of participant p and and to save its output (i.e. N outputs).

outputs = np.full((X.shape[0],), np.nan)
for curr_part in np.unique(parts):
    print(""processing participant {0}"".format(curr_part))
    model = load_model(model_filename.format(curr_part)) # I measured the duration of this call (d0)
    idx = (parts == curr_part)
    outputs[idx] = np.squeeze(model.predict(X[idx,:])); # I measured the duration of this call (d1)


Both d1 and d0 increase at each iteration of the loop (the whole loop take 1.5 seconds at iteration 0 and around 8 seconds at iteration 20). I completely fail to understand why. Also interestingly, if I run the code several times in ipython the duration accumulate as long as I do not restart the kernel (i.e. on the second run iteration 0 takes around 8 seconds). Of course I want to run the code several times so this issue is critical on the long run.

I also tried with the following code which takes approx. the same total duration although I cannot measure the time of each call:

unik_parts = np.unique(parts);
models = [(p, load_model(model_filename.format(p))) for p in unik_parts]
outputs = [np.squeeze(m.predict(X[parts == p,:])) for p,m in models]


Python version 2.7

Models are models from keras
",9,1337,"I've seen this quite a few times when preprocessing data; Typically, in my experience, the memory usage creeps up after each iteration with each following iteration slowing down slightly.
I find that the easiest way to solve this is to separate the tasks into different processes and then use an orchestration process to manage the program flow.
When each task is completed, the associated process is culled and your resources can continue to be allocated to the next task in the flow. This is most helpful for keeping long-running processes crisp.
You could structure the process in this way:
Parent Process
     |_ Pickle Input to Child Proc
     |_ Trigger Child Proc
            |_ Collect Input
            |_ Complete Task
            |_ Pickle Output
     |_ Collect Output



Parent Process -&gt; pickle input -&gt; Child Process
      ^                              |
      |                              |
      ----------------pickle output &lt;-

One of the things you can do to manage the task flow is to create an id and use it to create an empty file, then pass that id to the child process and, once the work is complete, delete it with the child process. This is a simple and convenient way for the parent process to know the child process is complete.
",,
Keras strange issue,https://stackoverflow.com/questions/46355068,keras loss function for 360 degree prediction,,8,2817,"Converting my comment into an answer. Given two angles a (gt), b (prediction) as radians you get the angle difference by 

tf.atan2(tf.sin(a - b), tf.cos(a - b))


By definition tf.atan2 gives the difference automatically in the closed interval [-pi, +pi] (that is, [-180 degrees, +180 degrees]).

Hence, you can use 

tf.reduce_mean(tf.abs(tf.atan2(tf.sin(a - b), tf.cos(a - b))))


I think Keras understand this TensorFlow code.
",,
Keras strange issue,https://stackoverflow.com/questions/48728099,Word-level Seq2Seq with Keras,,7,2991,"Recently I was also facing this problem. There is no other solution then creating small batches say batch_size=64 in a generator and then instead of model.fit do model.fit_generator. I have attached my generate_batch code below:     

def generate_batch(X, y, batch_size=64):
    ''' Generate a batch of data '''
    while True:
        for j in range(0, len(X), batch_size):
            encoder_input_data = np.zeros((batch_size, max_encoder_seq_length),dtype='float32')
            decoder_input_data = np.zeros((batch_size, max_decoder_seq_length+2),dtype='float32')
            decoder_target_data = np.zeros((batch_size, max_decoder_seq_length+2, num_decoder_tokens),dtype='float32')

            for i, (input_text_seq, target_text_seq) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):
                for t, word_index in enumerate(input_text_seq):
                    encoder_input_data[i, t] = word_index # encoder input seq

                for t, word_index in enumerate(target_text_seq):
                    decoder_input_data[i, t] = word_index
                    if (t&gt;0)&amp;(word_index&lt;=num_decoder_tokens):
                        decoder_target_data[i, t-1, word_index-1] = 1.

            yield([encoder_input_data, decoder_input_data], decoder_target_data)


And then training like this: 

batch_size = 64
epochs = 2

# Run training
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit_generator(
    generator=generate_batch(X=X_train_sequences, y=y_train_sequences, batch_size=batch_size),
    steps_per_epoch=math.ceil(len(X_train_sequences)/batch_size),
    epochs=epochs,
    verbose=1,
    validation_data=generate_batch(X=X_val_sequences, y=y_val_sequences, batch_size=batch_size),
    validation_steps=math.ceil(len(X_val_sequences)/batch_size),
    workers=1,
    )


X_train_sequences is list of lists like [[23,34,56], [2, 33544, 6, 10]].
Similarly others.      

Also took help from this blog - word-level-english-to-marathi-nmt
",,
Keras strange issue,https://stackoverflow.com/questions/63464944,Keras loss and metrics values do not match with same function in each,"I am using keras with a custom loss function like below:
def custom_fn(y_true, y_pred):
   # changing y_true, y_pred values systematically
   return mean_absolute_percentage_error(y_true, y_pred)
   

Then I am calling model.compile(loss=custom_fn) and model.fit(X, y,..validation_data=(X_val, y_val)..)
Keras is then saving loss and val_loss in model history. As a sanity check, when the model finishes training, I am using model.predict(X_val) so I can calculate validation loss manually with my custom_fn using the trained model.
I am saving the model with the best epoch using this callback:
callbacks.append(ModelCheckpoint(path, save_best_only=True, monitor='val_loss', mode='min'))

so after calculating this, the validation loss should match keras' val_loss value of the best epoch. But this is not happening.
As another attempt to figure this issue out, I am also doing this:
    model.compile(loss=custom_fn, metrics=[custom_fn])

And to my surprise, val_loss and val_custom_fn do not match (neither loss or loss_custom_fn for that matter).
This is really strange, my custom_fn is essentially keras' built in mape with the y_true and y_pred slightly manipulated. what is going on here?
PS: the layers I am using are LSTM layers and a final Dense layer. But I think this information is not relevant to the problem. I am also using regularisation as hyperparameter but not dropout.
Update
Even removing custom_fn and using keras' built in mape as a loss function and metric like so:
model.compile(loss='mape', metrics=['mape'])

and for simplicity, removing ModelCheckpoint callback is having the same effect; val_loss and val_mape for each epoch are not equivalent. This is extremely strange to me. I am either missing something or there is a bug in Keras code..the former might be more realistic.
",7,1702,"This blog post suggests that keras adds any regularisation used in the training when calculating the validation loss. And obviously, when calculating the metric of choice no regularisation is applied. This is why it occurs with any loss function of choice as stated in the question.
This is something I could not find any documentation on from Keras. However, it seems to hold up since when I remove all regularisation hyperparameters, the val_loss and val_custom_fn match exactly in each epoch.
An easy workaround is to either use the custom_fn as a metric and save the best model based on the metric (val_custom_fn) than on the val_loss. Or else Loop through each epoch manually and calculate the correct val_loss manually after training each epoch. The latter seems to make more sense since there is no reason to include custom_fn both as a metric and as a loss function.
If anyone can find any evidence of this in the Keras documentation that would be helpful.
",,
Keras strange issue,https://stackoverflow.com/questions/47947629,"Tensorflow: Keras, Estimators and custom input function",,5,1544,,,
Keras strange issue,https://stackoverflow.com/questions/63271509,Why do i get lagged results on my LSTM model,,5,2081,"First of all, I think it is not appropriate to input '1' as Timesteps value, because LSTM model is the one treating timeseries or sequence data.
I think the following script of data mining will work well
def lstm_data(df,timestamps):
    array_data=df.values
    sc=MinMaxScaler()
    array_data_=sc.fit_transform(array_data)

    array=np.empty((0,array_data_.shape[1]))
    range_=array_data_.shape[0]-(timestamps-1)
    for t in range(range_):
        array_data_p=array_data_[t:t+sequenth_length,:]
        array=np.vstack((array,array_data_p))

    array_=array.reshape(-1,timestamps, array.shape[1])
    return array_

#timestamps depend on your objection, but not '1'
x_data=lstm_data(x, timestamps=4) 
y_data=lstm_data(y, timestamps=4)
y_data=y_data.reshape(-1,1)

#Divide each data into train and test
#Input the divided data into your LSTM model

",,
Keras strange issue,https://stackoverflow.com/questions/42473052,TensorFlow 1.0 does not see GPU on Windows (but Theano does),,5,1457,,,
Keras strange issue,https://stackoverflow.com/questions/64645579,Keras: custom data validation callback on training data always returns validation data results,,4,1136,"It seems that this is a bug in tensorflow versions 2.3.x (tested with 2.3.0 and 2.3.1). In versions 2.4.0-rc0 and 2.2.1, the loss outputs of loss and custom_loss differ, which is the expected behavior:
{'custom_loss': [0.7694963216781616, 0.541864812374115],
'loss': [0.9665887951850891, 0.6637843251228333],
'val_loss': [0.7624925374984741, 0.5331208109855652]}

",,
Keras strange issue,https://stackoverflow.com/questions/55935277,simpleRNN input/output shape,"I have defined a simpleRNN in keras with the following code :

# define RNN architecture
from keras.layers import Input
from keras.models import Model
from keras.layers import SimpleRNN
from keras.models import Sequential

model = Sequential()
model.add(SimpleRNN(units = 10,
                    return_sequences=False, 
                    unroll=True,
                    input_shape=(6, 2)))

model.compile(loss='mse',
              optimizer='rmsprop',
              metrics=['accuracy'])
model.summary()


then I feed it with input data having shape (batch_size, 6, 2) i.e. 6 timesteps each having two features. I therefore expect 6 simpleRNN cells.

When launching the training, I get the following error message : 

Error when checking target: expected simple_rnn_2 to have shape (10,) but got array with shape (1,)


and I don't understand why. 

The point of the RNN (my understanding) is to have its input fed by the previous RNN cell in case it is not the first RNN cell and the new timestep input.

So in this case, I expect the second RNN cell to be fed by the first RNN cell a vector of shape  (10,) since units = 10. How come that it gets a (1,) sized vector ?

What is strange is that as soon as I add a Dense layer in the model, this solves the issue. So the following architecture : 

# define RNN architecture
from keras.layers import Input
from keras.models import Model
from keras.layers import SimpleRNN, Dense
from keras.models import Sequential

model = Sequential()
model.add(SimpleRNN(units = 10,
                    return_sequences=False, 
                    unroll=False,
                    input_shape=(6, 2)))
model.add(Dense(1, activation='relu'))
model.compile(loss='mse',
              optimizer='rmsprop',
              metrics=['accuracy'])
model.summary()


does not throw an error. Any idea why ?
",4,2143,"Assuming you are actually training the model (you did not include that code), the problem is that you are feeding it target outputs of shape (1,) while the SimpleRNN expects input of shape (10,). You can look up the docs here: https://keras.io/layers/recurrent/

The docs clearly state that the output of the SimpleRNN is equal to units, which is 10. Each unit produces one output.

The second sample does work because you have added a Dense layer that reduces the output size to (1,). Now the model can accept your training target outputs and they are backpropped through the network.
",,
Keras strange issue,https://stackoverflow.com/questions/51343169,Is Keras fit_generator the best thing to use when handling data that does not fit in RAM?,"I am working to build a classifier that can classify Knots. Currently I have a dataset that contains 100,000 images of ""unknots"", 100,000 ""plus trefoil"", and 100,000 ""minus trefoil"".

I have been trying to get a classifier working on this large data set for the last 4ish days. The list of problems I had so far were:

1) Data set does not fit in CPU main memory: fixed this by making a number of EArrays with PyTables and Hdf5 and appending them on disk. So now I have one 1.2Gb file which is the dataset.

2) Even a very simple neural network in Keras was hitting 100% GPU (nvidia k80) memory usage after the model compiled... hadn't even fit the model yet. I read this was due to Keras backend automatically allocating near 100% of available resources upon compile: I fixed this as well.

3) Once errors 1 and 2 were fixed, I was still getting strange accuracy from Keras fit_generator().  

Questions: 

1) Is the method I have described of merging small numpy arrays into one large EArray using PyTables a good way to make a very large (300,000 images 128x128, total size = 1.2Gb) dataset?

2) Should fit generator be used over train_on_batch in Keras? Will they return significantly different final loss/accuracy scores?

3) What is wrong with my generator method if I want to train the neural network on batches of 50 images from the hdf5 file, and after each training epoch, delete the images the network just trained on out of main memory?

import tables
hdf5_path = ""300K_Knot_data.hdf5""
extendable_hdf5_file = tables.open_file(hdf5_path, mode='r')

def imageLoader(files, batch_size):

    L = len(files.root.train_data)

    #this line is just to make the generator infinite, keras needs that    
    while True:

        batch_start = 0
        batch_end = batch_size

        while batch_start &lt; L:
            limit = min(batch_end, L)
            X = files.root.train_data[batch_start:limit]
            X = np.reshape(X, (X.shape[0],128,128,3))
            X = X/255
            Y = files.root.train_label[batch_start:limit]

            yield (X,Y) #a tuple with two numpy arrays with batch_size samples     

            batch_start += batch_size   
            batch_end += batch_size

img_rows, img_cols = 128,128
###################################
# TensorFlow wizardry
config = tf.ConfigProto()

# Don't pre-allocate memory; allocate as-needed
config.gpu_options.allow_growth = True

# Only allow a total of half the GPU memory to be allocated
config.gpu_options.per_process_gpu_memory_fraction = 0.5

# Create a session with the above options specified.
K.tensorflow_backend.set_session(tf.Session(config=config))
###################################

model = Sequential()
model.add(Conv2D(64, (3,3), input_shape=(img_rows,img_cols,3)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(96, (3,3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(128, (3,3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))


model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(3))
model.add(Activation('softmax', name='preds'))


#lr was originally 0.01

model.compile(loss='categorical_crossentropy',
optimizer=keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0),metrics= 
['accuracy'])

# fits the model on batches with real-time data augmentation:

model.fit_generator(imageLoader(extendable_hdf5_file, 50),
                steps_per_epoch=240000 / 50, epochs=50)


Here is the output of fitting the model:



Sorry if this is not the correct place for the post.
My research adviser is out of town, and I have spent a considerable amount of time trying to fix this issue and I just need some input, because I cannot find an adequate solution on the internet, or cannot seem to wrap my head around what I do find well enough to implement it in a nice manner.  I am not a new programmer, but am relatively inexperienced  in python.
",4,3366,"I think the way you have defined your generator is fine. Nothing seems to be wrong with it. Although, currently using Sequence class is the recommended way especially because it is much safer when doing multi-processing. However, using generator is fine and it is still heavily used.

As for the weird accuracy numbers, you mentioned that it is 100% for 3000 steps and then it drops to 33%. I have the following recommendations to diagnose this:

1) Decrease the learning rate to for example 3e-3 or 1e-3 or 1-e4. I recommend to use an adaptive optimizer like Adagrad or RMSprop or Adam. I recommend RMSprop with the default parameters; don't change any parameters first, instead experiment with it and make changes according to the feedback you get (if the loss decreases very slowly then increase the learning rate a bit; if it increases or is stable then decrease the learning rate, though these are not definite rules. You must experiment and take into account the validation loss as well). Using an adaptive optimizer reduce the need of using a callback like ReduceLearningRateOnPlateu (at least not until you have solid reasons to use it).

2) If it is possible for you, split your whole data into train/validation/test sets (or at least train/validation). The ratios of 60/20/20 or 70/15/15 are the most commonly used ones. But make sure that the classes in each of those three sets are equally represented (i.e. you have more or less the same number of ""unknots"", ""plus trefoil"", ""minus trefoil"" in each set). Note that their distribution should be more or less the same as well. For example you should not handpick the easy ones for the validation and test set. Usually selecting after shuffling the whole data (i.e. to make sure they are not in any particular order) would work. Creating a validation set helps you to make sure that the progress you see during training process is real and the model is not over-fitting on the training data. 

You need to define a new generator for validation and pass the validation data (i.e. load the file which consists of validation data) the same way you have done with training data. Then set the validation_data and validation_steps arguments of fit_generator method with appropriate values.

3) Using batch normalization before or after the activation is still debatable. Though, some people claim that it works better if you put it after the activation or exactly right before the next layer. You can experiment with this as well.

4) If you are not using your GPU for anything else, then use its full capacity (i.e. don't limit its RAM usage). And use a batch size which is a power of two (i.e. 64, 128, 256, etc.) since it helps with GPU memory allocation and may speed it up. 128 or 256 seems good choices considering the number of samples you have.

5) If training for one epoch takes a lot of time then consider using ModelCheckpoint and EarlyStopping callbacks.
",,
Keras strange issue,https://stackoverflow.com/questions/46580112,R-squared results of test and validation differ by a huge margin,,3,4042,"Tensorflow's built-in evaluate method evaluates your test set batch by batch and hence calculates r2 at each batch. The metrics produced from model.evaluate() is then simple average of all r2 from each batch. While in model.fit(), r2 (and all metrics on validation set) are calculated per epoch (instead of per batch and then take avg.)
You may slice your output and output_pred into batches of the same batch size you used in model.evaluate() and calculate r2 on each batch. I guess the model produces high r2 on batches with high total sum of squares (SS_tot) and bad r2 on lower ones. So when taken average, result would be poor (however when calculate r2 on entire dataset, samples with higher ss_tot usually dominate the result).
",,
Keras strange issue,https://stackoverflow.com/questions/55772880,TF 2.0 - Method estimator.model_to_estimator( ) fails but model.fit works for tf.keras created model,,3,408,"As discussed here , this is indeed a bug that was caused by the combination of tf.function in keras layer and v1 tf.session which is used by model_to_estimator

Also you can refer to the following link, where the bug has been fixed. Thanks!

If you feel like the answer was helpful, please upvote it. Thanks!
",,
Keras strange issue,https://stackoverflow.com/questions/55235620,After installing Jupyter Notebook NumPy and TensorFlow are not working,,2,896,"Go to your terminal and locate the environment where you have Numpy and the other modules installed. If your'e using virtualenvwrapper you can run:
lsvirtualenv a list of your virtual envs you have will be displayed.

Then type workon virtualenv1 (replace with name of displayed virtual env) and check for your installed modules e.g after selecting the virtual env you can then type
python and get into the shell. Then you can do import numpy etc  to check if the modules youre looking for exist in that virtualenv. When you find them you can then exit out of the shell and do a pip install jupyter to install jupyter notebooks on the virtual env with the modules you need. To avoid meeting such problems in future you should make sure you maintain virtual environments in your machine with the modules that your're interested  to work with as per virtual env. Virtualenvwrapper is a great tool that abstracts away most of the common pitfalls found with working with virtual envs. More info in managing and creating virtual envs can be found here.
","In my case I just spent around an hour uninstalling everything Python-related (Python, Anaconda, etc.) and completely installing everything from scratch again. It was a bit of a pain but I didn't know what else to do without having to dive too deeply into the issue.
",
Keras strange issue,https://stackoverflow.com/questions/42422646,Keras train partial model issue (about GAN model),,2,6004,,,
Keras strange issue,https://stackoverflow.com/questions/40381101,Keras writing a Recurrent Layer which accepts images,,1,915,,,
Keras strange issue,https://stackoverflow.com/questions/50418332,Got OOM issue with google colab,"I am building a keras model to run some simple image recognition task. If i do everything in raw Keras, I don't hit OOM. However, strangely, when I do it through a mini framework I wrote, which is fairly simple and mainly so that I can keep track of the hyperparameters and setup I used, I hit OOM. Most of the executions should be the same as running the raw Keras. I am guessing somewhere I made some mistakes in my code. Note that this same mini framework had no issue running with CPU on my local laptop. I think I will need to debug. But before that, anyone has any general advice? 

Here's a few lines of the errors I got:

Epoch 1/50
2018-05-18 17:40:27.435366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-05-18 17:40:27.435906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235 pciBusID: 0000:00:04.0 totalMemory: 11.17GiB freeMemory: 504.38MiB
2018-05-18 17:40:27.435992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2018-05-18 17:40:27.784517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-18 17:40:27.784675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 
2018-05-18 17:40:27.784724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N 
2018-05-18 17:40:27.785072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 243 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2018-05-18 17:40:38.569609: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 36.00MiB.  Current allocation summary follows.
2018-05-18 17:40:38.569702: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 66, Chunks in use: 66. 16.5KiB allocated for chunks. 16.5KiB in use in bin. 2.3KiB client-requested in use in bin.
2018-05-18 17:40:38.569768: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 10, Chunks in use: 10. 5.0KiB allocated for chunks. 5.0KiB in use in bin. 5.0KiB client- etc. etc

2018-05-18 17:40:38.573706: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at cwise_ops_common.cc:70 : Resource exhausted: OOM when allocating tensor with shape[18432,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc

",1,4178,,,
Keras strange issue,https://stackoverflow.com/questions/46222755,LSTM results seems to be shifted by one time period back,"With LSTM and Keras I built a simple time series prediction model which works like this:
Providing that I have the data (normalized and prepared for training) for some past 10 time periods, the model predicts the values of the proceeding two time periods.

My Data is shaped similar to this:

X = [[[1,2,3,4,5,6,7,8,9,10],[2,3,4,5,6,7,8,9,10,11],[3,4,5,6,7,8,9,10,11,12]]]

Y = [[11,12],[12,13],[14,15]]

And the model is composed of one LSTM layer with rnn_size number of nodes
as well as one dropout layer.

    model = Sequential()

    model.add(LSTM(rnn_size,
    batch_input_shape=(batch_size, X.shape[1], 
    X.shape[2]),stateful=True,dropout=dropout))

    model.add(Dropout(dropout)) 
    model.add(Dense(y.shape[1]))
    adam_optimizer = keras.optimizers.Adam(clipvalue=5)
    model.compile(loss='mean_squared_error', optimizer=adam_optimizer)
    history = model.fit(X, y, batch_size=batch_size, epochs=nb_epoch, 
                    verbose=2, validation_split=0.1,shuffle=False)
    #################################    
    # Predict
    result = scaler.inverse_transform(
              model.predict_on_batch(test_values[start_date:end_date]
              .values.reshape(1, 1, -1)))


The problem is that the predicted value for time period t+1 - as can be seen in the graph - seems to be somehow just a slightly different value from the one at the time period t.

Is this a (normal) behaviour?
If not, how can I overcome it?

I think I have a relatively good understanding of how LSTM work, however I could not wrap my head around this particular issue.

Edit 1:

I edited the code to use stateless network configurations and edited the shape as suggested by Daniel. However the problem persist.
Strangely also, the validation loss (MSE) is always lower than the training loss.

Edit 2:

Rest of the code added as per Marcin's request

actual value vs. predicted value for the test data set that was not shown to the model before

Validation Loss is less than Training Loss! ( small fluctuation is because of Dropout )
",1,1257,,,
Keras strange issue,https://stackoverflow.com/questions/42833585,keras 2 - fit_generator on Jupyter notebook broken?,"

https://github.com/fchollet/keras/issues/5818



I've updated to keras v2 yesterday. 

I adapted all my code from version 1 to the new API, following all the warnings I encountered.

However I'm having some very strange problems with fit_generator method of Model.

Using this toy example, wich worked totally fine in version 1:

from keras.models import Model
from keras.layers import Input, Dense, Flatten
from keras.optimizers import SGD
from keras.losses import categorical_crossentropy
from keras.preprocessing.image import ImageDataGenerator

gen = ImageDataGenerator()
train_batches = gen.flow_from_directory(""D:/GitHub/Kaggle/redux/train/"")

inp = Input(shape=(256,256,3))
l1 = Flatten()(inp)
out = Dense(2, activation=""softmax"")(l1)

model = Model(inp, out)

model.compile(loss=categorical_crossentropy, optimizer=SGD(lr=0.01))

model.fit_generator(train_batches, train_batches.samples // train_batches.batch_size)


The output in jupyter notebook is quite strange, printing a unknown symbol until the notebook crashes:

Epoch 1/1
 23/718 [..............................] - ETA: 522s - loss: 8.4146 


Running the code from the terminal don't print those strange symbols.
",1,649,,,
Keras strange issue,https://stackoverflow.com/questions/40510703,Implement Siamese Network in Keras Issue,"I was trying to implement a Siamese Network with Keras, I have two input data,

X1 shape: (10000, 52)
X2 shape: (10000, 600)

Each sample in X1 is assumed similar to sample in X2. For example: X1[0] is similar to X2[0]. Thus, I used the ""cosine similarity"" when two inputs are merged. I was trying to use the following code:

def contrastive_loss(y_true, y_pred):
'''Contrastive loss from Hadsell-et-al.'06
http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
'''

margin = 1
return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))

left_branch = Sequential()
left_branch.add(Embedding(1000, 32, input_length=52))
left_branch.add(LSTM(64))
left_branch.add(Dense(256))

right_branch = Sequential()
right_branch.add(Embedding(1000, 32, input_length=600))
right_branch.add(LSTM(64))
right_branch.add(Dense(256))

merged = Merge([left_branch, right_branch], mode='cos',dot_axes=1)

final_model = Sequential()
final_model.add(merged)
final_model.summary()

x1 = np.random.randint(1000, size=(10000, 52))
x2 = np.random.randint(1000, size=(10000, 600))
y = np.ones((10000,),dtype=np.int)

final_model.compile(optimizer='adam', loss=contrastive_loss)

final_model.fit([x1, x2], y, nb_epoch=2, batch_size=32)

getoutput_t = K.function([merged.layers[0].layers[0].input], [merged.layers[0].layers[2].output])

getoutput_d = K.function([merged.layers[1].layers[0].input], [merged.layers[1].layers[2].output])

t1 = getoutput_t([x1])[0]

t2 = getoutput_d([x2])[0]


My issue is: after training, t1[0] was very similar to t1[1], but very different from t2[0]. I checked the output of t1 and t2, the generated output were strange:

t1:

array([[ -3.48182112e-01, 6.57344190e-03, -6.75882818e-03,
4.11706511e-03, -2.41146213e-03, -1.14028137e-02,
2.49683809e+00, -9.19755269e-03, -2.37327255e-02....]

[ -3.48182231e-01, 6.57369522e-03, -6.75880583e-03,
4.11719829e-03, -2.41167075e-03, -1.14026833e-02,
2.49683785e+00, -9.19752941e-03, -2.37324722e-02,...]
.......]])

The t1[0] was almost same to t1[1], same issue also was found in t2.

Thus, I have to concerns:

Is there any issue in my Siamese model? (e.g. loss function or layer design)

I don't have negative sampling so far, thus, the target values are ""1"", is this the issue?
",1,3051,,,
Keras strange issue,https://stackoverflow.com/questions/29985453,Linux error when installing Keras,,1,2473,,,
Keras strange issue,https://stackoverflow.com/questions/49351711,Saved multi-GPU trained model loaded into single-GPU; inconsistent results,,0,1298,"write the code os.environ[""CUDA_VISIBLE_DEVICES""] = '0' before you load the model,then it will not use multiple gpu
",,
Keras strange issue,https://stackoverflow.com/questions/50701786,Accuracy lower than AUC for identical Keras models &amp; data,"I have a strange Keras classification behaviour.

I got different accuracy when using cross-validation vs a holdout set.

2 Identical models but with different evaluation methods:


Model1 uses 10-Kfold cross validation (achieving 0.98 mean AUC and lowest AUC 0.89).
Model2 uses hold out set (Accuracy 0.82)


I was expecting to see the worst accuracy of model 2 to be the lowest fold accuracy (0.89 not 0.82).

Data of a small size ~10k x 13

Kfold: 10 folds

Model 1:

def create_baseline():
    # create model
    model = models.Sequential()
    model.add(layers.Dense(64, input_dim=set_1.iloc[:,0:-1].shape[1], activation='relu'))
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model


This the important part of my code (the rest is related to ploting the ROC):

Note: I have tried both with and without standardization

estimators = []
estimators.append(('standardize', MinMaxScaler()))
estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, nb_epoch=1000, batch_size=1000, verbose=0)))
pipeline = Pipeline(estimators)
cv = StratifiedKFold(n_splits=10)
classifier = pipeline
mean_tpr = 0.0
mean_fpr = np.linspace(0, 1, 100)

colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])
lw = 2
i = 0
for (train, test), color in zip(cv.split(X, y), colors):
    classifier.fit(X[train], y[train])
    probas_ = classifier.predict_proba(X[test])
    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])
    mean_tpr += interp(mean_fpr, fpr, tpr)
    mean_tpr[0] = 0.0
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=lw, color=color,
             label='ROC fold %d (area = %0.2f)' % (i, roc_auc))

    i += 1


Output:


As you can see, I have .98 averag ROC.

Issue:

Model 2:

std = MinMaxScaler()
X_norm = std.fit_transform(X)
X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(X_norm, y, test_size=0.1, random_state=5)


Keras model

model_2 = models.Sequential()
model_2.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model_2.add(layers.Dense(64, activation='relu'))
model_2.add(layers.Dense(1, activation='sigmoid'))
model_2.compile(optimizer='adam',
loss='binary_crossentropy',
metrics=['accuracy'])


Runing the model:

history = model_2.fit(X_train_norm,
y_train_norm,
epochs=1000,
batch_size=1000,
validation_data=(X_test_norm, y_test_norm))


Results (last iterations):

8988/8988 [==============================] - 0s - loss: 0.3517 - acc: 0.8249 - val_loss: 0.3701 - val_acc: 0.7954
Epoch 997/1000
8988/8988 [==============================] - 0s - loss: 0.3516 - acc: 0.8238 - val_loss: 0.3699 - val_acc: 0.8059
Epoch 998/1000
8988/8988 [==============================] - 0s - loss: 0.3516 - acc: 0.8250 - val_loss: 0.3694 - val_acc: 0.8038
Epoch 999/1000
8988/8988 [==============================] - 0s - loss: 0.3512 - acc: 0.8241 - val_loss: 0.3692 - val_acc: 0.7975
Epoch 1000/1000
8988/8988 [==============================] - 0s - loss: 0.3504 - acc: 0.8247 - val_loss: 0.3696 - val_acc: 0.7975


Why the performance of model2 is lower than model1?

Note: 
- same data, type of keras model, and seed but different results!
- I did multiple test with and without standarization and with same and differents seeds, and I still have the same issue.
- I understant that I can use simpler models but my issue is realted to using Keras classifier.

Please correct me if I am doing something wrong.
",0,872,,,
Keras strange issue,https://stackoverflow.com/questions/47798854,Keras CNN accuracy and loss are constant,,0,1226,,,
Keras strange issue,https://stackoverflow.com/questions/65453002,I keep receiving a weird &quot;invalid syntax&quot; error when trying to implement a convolution neural network program using tensorflow in python,,0,1637,"For anyone else that runs into any similar problems, this is what I ended up doing:

My first mistake was forgetting a parenthesis, so I added that


loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels =
y_conv, logits=y_loss))


This then turned into an args error - so I had to switch the arguments in the parentheses


loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels =
y_loss, logits=y_conv))

Now it works fine and the whole section now looks like this:
    y_loss = tf.placeholder(tf.float32, [None, 10])
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_loss, logits=y_conv))
    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)

",,
Keras strange issue,https://stackoverflow.com/questions/46559655,Queue manager for experiments in neural network training,,0,291,,,
Keras strange issue,https://stackoverflow.com/questions/59149885,How to calculate the trace of a matrix in Keras backend?,,0,356,"I would probably use - appropriate Layer
tf.keras.layers.Lambda(
    function, output_shape=None, mask=None, arguments=None, **kwargs
)

with function as callback (to use in it numpy.trace())... Can not test now...
I also can suppose, that you can try Keras Models With TensorFlow NumPy
","Just wondering, wouldn't this work? 

def custom_loss(y_true,y_pred):
    batch_size = y_true.shape[0]
    X = K.log( K.dot(y_true, K.transpose(y_pred) ) )
    y = - tf.trace(X)
    return y

",
Keras strange issue,https://stackoverflow.com/questions/41547497,Keras input issue,,0,765,,,
Keras strange issue,https://stackoverflow.com/questions/46958739,Error trying to pass list to hyperas as a parameter,,-1,2112,,,
Keras strange issue,https://stackoverflow.com/questions/64119338,Why Keras MAPE metric is exploding during training but MSE loss is not?,"I implemented an LSTM with attention in Keras to reproduce this paper. The strange behavior is simple: I have an MSE loss function and an MAPE and MAE as metrics. During training the MAPE is exploding but the MSE and MAE seem to  train normally:
Epoch 1/20
275/275 [==============================] - 191s 693ms/step - loss: 0.1005 - mape: 15794.8682 - mae: 0.2382 - val_loss: 0.0334 - val_mape: 24.9470 - val_mae: 0.1607
Epoch 2/20
275/275 [==============================] - 184s 669ms/step - loss: 0.0099 - mape: 6385.5464 - mae: 0.0725 - val_loss: 0.0078 - val_mape: 11.3268 - val_mae: 0.0803
Epoch 3/20
275/275 [==============================] - 186s 676ms/step - loss: 0.0025 - mape: 5909.3735 - mae: 0.0369 - val_loss: 0.0131 - val_mape: 14.9827 - val_mae: 0.1061
Epoch 4/20
275/275 [==============================] - 187s 678ms/step - loss: 0.0015 - mape: 4746.2788 - mae: 0.0278 - val_loss: 0.0142 - val_mape: 16.1894 - val_mae: 0.1122
Epoch 5/20
 30/275 [==&gt;...........................] - ETA: 2:38 - loss: 0.0012 - mape: 9.3647 - mae: 0.0246

The MAPE is exploding at the end of each epoch. What could be the cause of this specific behavior?
The MAPE is still decreasing with each epoch so is this not really an issue since it is not hindering the training process?
",-1,3992,"Your loss and MAPE are decreasing so it sounds good. But if you fear the high values in MAPE you can tell if there is a Y value near zero. Because MAPE is a percentage error.
MAPE results can be misleading. From Wikipedia:

Although the concept of MAPE sounds very simple and convincing, it has
major drawbacks in practical application, and there are many studies
on shortcomings and misleading results from MAPE.

It cannot be used if there are zero values (which sometimes happens for example in demand data) because there would be a division
by zero.
For forecasts which are too low the percentage error cannot exceed 100%, but for forecasts which are too high there is no upper limit to
the percentage error.
MAPE puts a heavier penalty on negative errors, than on positive errors.

To overcome these issues with MAPE, there are some other measures
proposed in literature:

Mean Absolute Scaled Error (MASE)
Symmetric Mean Absolute Percentage Error (sMAPE)
Mean Directional Accuracy (MDA)
Mean Arctangent Absolute Percentage Error (MAAPE)


",,
Altair unexpected behavior,https://stackoverflow.com/questions/58919888,Multiple selections in altair,"I would like to have two interactive legends, similar to the second example in 
this section
of the Altair docs, but without the rectangular legend.
One legend should control the color via one categorical variable (""First name""), while the second should control the marker via another categorical variable (""Last name""). While I'm able to get the aesthetics right (see below plot), the interactivity of the second legend leads to unexpected behavior.



The code I used to generate the above figure is below:

import pandas as pd
import altair as alt

# Create dataframe
data = [[7, 10, 'Alex', 'Smith'],
        [12, 20, 'Bob', 'Jones'],
        [10, 30, 'Clive', 'Smith'],
        [42, 40,  'Alex', 'Johnson']]
df = pd.DataFrame(data,columns=['Favorite number', 'Age', 'First name', 'Last name'])

# Create selections
selection_first_name = alt.selection_multi(fields=['First name'])
selection_last_name = alt.selection_multi(fields=['Last name'])

# Create first name conditions
color_first_name = alt.condition(selection_first_name,
                                 alt.Color('First name:N', legend=None),
                                 alt.value('lightgray'))

# Create last name conditions
shape_last_name = alt.condition(selection_last_name,
                                    alt.Shape('Last name:N', legend=None),
                                    alt.value('lightgray'))

# Create interactive scatter plot
scatter = alt.Chart(df).mark_point(size=100).encode(
    x='Favorite number:Q',
    y='Age:Q',
    color=color_first_name,
    shape=shape_last_name,
    tooltip=['First name', 'Last name']
).add_selection(
    selection_first_name
).add_selection(
    selection_last_name
).add_selection(
    alt.selection_interval(bind='scales')
)

# Create interactive model name legend
legend_first_name = alt.Chart(df).mark_point(size=100).encode(
    y=alt.Y('First name:N', axis=alt.Axis(orient='right')),
    color=color_first_name
).add_selection(
    selection_first_name
)

# Create interactive model name legend
legend_last_name = alt.Chart(df).mark_point(size=100).encode(
    y=alt.Y('Last name:N', axis=alt.Axis(orient='right')),
    shape=shape_last_name
).add_selection(
    selection_last_name
)

# Combine plotting elements
chart = scatter | legend_first_name | legend_last_name


The resulting HTML chart is here:



&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;style&gt;
    .vega-actions a {
        margin-right: 12px;
        color: #757575;
        font-weight: normal;
        font-size: 13px;
    }
    .error {
        color: red;
    }
  &lt;/style&gt;
  &lt;script type=""text/javascript"" src=""https://cdn.jsdelivr.net/npm//vega@5""&gt;&lt;/script&gt;
  &lt;script type=""text/javascript"" src=""https://cdn.jsdelivr.net/npm//vega-lite@3.4.0""&gt;&lt;/script&gt;
  &lt;script type=""text/javascript"" src=""https://cdn.jsdelivr.net/npm//vega-embed@4""&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;div id=""vis""&gt;&lt;/div&gt;
  &lt;script&gt;
    (function(vegaEmbed) {
      var spec = {""config"": {""view"": {""width"": 400, ""height"": 300}, ""mark"": {""tooltip"": null}}, ""hconcat"": [{""mark"": {""type"": ""point"", ""size"": 100}, ""encoding"": {""color"": {""condition"": {""type"": ""nominal"", ""field"": ""First name"", ""legend"": null, ""selection"": ""selector001""}, ""value"": ""lightgray""}, ""shape"": {""condition"": {""type"": ""nominal"", ""field"": ""Last name"", ""legend"": null, ""selection"": ""selector002""}, ""value"": ""lightgray""}, ""tooltip"": [{""type"": ""nominal"", ""field"": ""First name""}, {""type"": ""nominal"", ""field"": ""Last name""}], ""x"": {""type"": ""quantitative"", ""field"": ""Favorite number""}, ""y"": {""type"": ""quantitative"", ""field"": ""Age""}}, ""selection"": {""selector001"": {""type"": ""multi"", ""fields"": [""First name""]}, ""selector002"": {""type"": ""multi"", ""fields"": [""Last name""]}, ""selector003"": {""type"": ""interval"", ""bind"": ""scales""}}}, {""mark"": {""type"": ""point"", ""size"": 100}, ""encoding"": {""color"": {""condition"": {""type"": ""nominal"", ""field"": ""First name"", ""legend"": null, ""selection"": ""selector001""}, ""value"": ""lightgray""}, ""y"": {""type"": ""nominal"", ""axis"": {""orient"": ""right""}, ""field"": ""First name""}}, ""selection"": {""selector001"": {""type"": ""multi"", ""fields"": [""First name""]}}}, {""mark"": {""type"": ""point"", ""size"": 100}, ""encoding"": {""shape"": {""condition"": {""type"": ""nominal"", ""field"": ""Last name"", ""legend"": null, ""selection"": ""selector002""}, ""value"": ""lightgray""}, ""y"": {""type"": ""nominal"", ""axis"": {""orient"": ""right""}, ""field"": ""Last name""}}, ""selection"": {""selector002"": {""type"": ""multi"", ""fields"": [""Last name""]}}}], ""data"": {""name"": ""data-ec767a72044047bc9825da631c9465fc""}, ""$schema"": ""https://vega.github.io/schema/vega-lite/v3.4.0.json"", ""datasets"": {""data-ec767a72044047bc9825da631c9465fc"": [{""Favorite number"": 7, ""Age"": 10, ""First name"": ""Alex"", ""Last name"": ""Smith""}, {""Favorite number"": 12, ""Age"": 20, ""First name"": ""Bob"", ""Last name"": ""Jones""}, {""Favorite number"": 10, ""Age"": 30, ""First name"": ""Clive"", ""Last name"": ""Smith""}, {""Favorite number"": 42, ""Age"": 40, ""First name"": ""Alex"", ""Last name"": ""Johnson""}]}};
      var embedOpt = {""mode"": ""vega-lite""};

      function showError(el, error){
          el.innerHTML = ('&lt;div class=""error"" style=""color:red;""&gt;'
                          + '&lt;p&gt;JavaScript Error: ' + error.message + '&lt;/p&gt;'
                          + ""&lt;p&gt;This usually means there's a typo in your chart specification. ""
                          + ""See the javascript console for the full traceback.&lt;/p&gt;""
                          + '&lt;/div&gt;');
          throw error;
      }
      const el = document.getElementById('vis');
      vegaEmbed(""#vis"", spec, embedOpt)
        .catch(error =&gt; showError(el, error));
    })(vegaEmbed);

  &lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;



",1,6984,"Your shape encoding has a condition that looks like this:

shape_last_name = alt.condition(selection_last_name,
                                    alt.Shape('Last name:N', legend=None),
                                    alt.value('lightgray'))


it specifies that when a point is not selected, it should have the shape named lightgray. lightgray is not a valid shape, so no points are drawn.

If you want to change the color based on the shape selection, you should put the condition in the color encoding. If you want to change the shape based on the shape legend selection, you should use a valid shape value within the condition.

I suspect what you were after was something like this:

import pandas as pd
import altair as alt

# Create dataframe
data = [[7, 10, 'Alex', 'Smith'],
        [12, 20, 'Bob', 'Jones'],
        [10, 30, 'Clive', 'Smith'],
        [42, 40,  'Alex', 'Johnson']]
df = pd.DataFrame(data,columns=['Favorite number', 'Age', 'First name', 'Last name'])

# Create selections
selection_first_name = alt.selection_multi(fields=['First name'], empty='none')
selection_last_name = alt.selection_multi(fields=['Last name'], empty='none')

# Create interactive scatter plot
scatter = alt.Chart(df).mark_point(size=100).encode(
    x='Favorite number:Q',
    y='Age:Q',
    color=alt.condition(selection_first_name &amp; selection_last_name,
                        alt.Color('First name:N', legend=None),
                        alt.value('lightgray') ),
    shape=alt.Shape('Last name:N', legend=None),
    tooltip=['First name', 'Last name']
).add_selection(
    alt.selection_interval(bind='scales')
)

# Create interactive model name legend
legend_first_name = alt.Chart(df).mark_point(size=100).encode(
    y=alt.Y('First name:N', axis=alt.Axis(orient='right')),
    color=alt.condition(selection_first_name,
                        alt.Color('First name:N', legend=None),
                        alt.value('lightgray') ),
).add_selection(
    selection_first_name

)

# Create interactive model name legend
legend_last_name = alt.Chart(df).mark_point(size=100).encode(
    y=alt.Y('Last name:N', axis=alt.Axis(orient='right')),
    shape=alt.Shape('Last name:N', legend=None),
    color=alt.condition(selection_last_name,
                        alt.value('black'),
                        alt.value('lightgray') ),
).add_selection(
    selection_last_name
)

# Combine plotting elements
chart = scatter | legend_first_name | legend_last_name

",,
Altair unexpected result,https://stackoverflow.com/questions/58919888,Multiple selections in altair,,1,6984,"Your shape encoding has a condition that looks like this:

shape_last_name = alt.condition(selection_last_name,
                                    alt.Shape('Last name:N', legend=None),
                                    alt.value('lightgray'))


it specifies that when a point is not selected, it should have the shape named lightgray. lightgray is not a valid shape, so no points are drawn.

If you want to change the color based on the shape selection, you should put the condition in the color encoding. If you want to change the shape based on the shape legend selection, you should use a valid shape value within the condition.

I suspect what you were after was something like this:

import pandas as pd
import altair as alt

# Create dataframe
data = [[7, 10, 'Alex', 'Smith'],
        [12, 20, 'Bob', 'Jones'],
        [10, 30, 'Clive', 'Smith'],
        [42, 40,  'Alex', 'Johnson']]
df = pd.DataFrame(data,columns=['Favorite number', 'Age', 'First name', 'Last name'])

# Create selections
selection_first_name = alt.selection_multi(fields=['First name'], empty='none')
selection_last_name = alt.selection_multi(fields=['Last name'], empty='none')

# Create interactive scatter plot
scatter = alt.Chart(df).mark_point(size=100).encode(
    x='Favorite number:Q',
    y='Age:Q',
    color=alt.condition(selection_first_name &amp; selection_last_name,
                        alt.Color('First name:N', legend=None),
                        alt.value('lightgray') ),
    shape=alt.Shape('Last name:N', legend=None),
    tooltip=['First name', 'Last name']
).add_selection(
    alt.selection_interval(bind='scales')
)

# Create interactive model name legend
legend_first_name = alt.Chart(df).mark_point(size=100).encode(
    y=alt.Y('First name:N', axis=alt.Axis(orient='right')),
    color=alt.condition(selection_first_name,
                        alt.Color('First name:N', legend=None),
                        alt.value('lightgray') ),
).add_selection(
    selection_first_name

)

# Create interactive model name legend
legend_last_name = alt.Chart(df).mark_point(size=100).encode(
    y=alt.Y('Last name:N', axis=alt.Axis(orient='right')),
    shape=alt.Shape('Last name:N', legend=None),
    color=alt.condition(selection_last_name,
                        alt.value('black'),
                        alt.value('lightgray') ),
).add_selection(
    selection_last_name
)

# Combine plotting elements
chart = scatter | legend_first_name | legend_last_name

",,
Altair unexpected result,https://stackoverflow.com/questions/53709994,GraphQL/ React throws Network Error JSON.parse: unexpected character at line 1 column 1 of the JSON data,"I have a Java GraphQL Endpoint running at localhost:8080/someApplication/graphql (CORS activated) and when I write a query in Altair (Firefox-plugin), I do get a valid response:

Altair

POST http://localhost:8080/someApplication/graphql

query: 

{
    someInformation
    {
        nameOfInformation
    }
}


returns:

{
    ""data"": {
        ""someInformation"": 
        [
          {
            ""nameOfInformation"": ""hi""
          }
        ]
      },
      ""errors"": [],
      ""dataPresent"": true,
      ""extensions"": null
}


So the query seems to be working just fine.

React

I configured my GraphQL client (localhost:3000) as follows:

const httpLink = createHttpLink({
    uri: ""http://localhost:8080/someApplication/graphql"",
});

const client = new ApolloClient({
    link: httpLink,
    cache: new InMemoryCache()
});

....
&lt;ApolloProvider client={client}&gt;
&lt;Query query={gql`
    {
        someInformation{
            nameOfInformation
        }
    }
`}
&gt;
    {({loading, error, data}) =&gt; {
        if (loading) return &lt;p&gt;Loading...&lt;/p&gt;;

        if (error) return &lt;p&gt;{error.networkError.message}&lt;/p&gt;;

        return data.someInformation.map(({nameOfInformation})=&gt; (
            &lt;p&gt;{`${nameOfInformation}`}&lt;/p&gt;
        ));
    }}
&lt;/Query&gt;
&lt;/ApolloProvider&gt;


Result

I always get following error: JSON.parse: unexpected character at line 1 column 1 of the JSON data.

When I change the code so that the endpoint is the endpoint in Apollo Launchpad, the query returns the correct result and no error is thrown.

Is there any way I could see what the data that is returned and causes the error looks like? Or is there anything I am doing wrong in receiving the data?
I appreciate any help with this! Thanks for looking into this!
",1,3489,"The Problem was on the Server: If you configured your GraphQL Endpint over HTTP, your class must have the following Annotations:

@Path(""/your-path-here"")
@Consumes(MediaType.APPLICATION_JSON)
@Produces(MediaType.APPLICATION_JSON)
public class yourClass {... }


In my case the incomming Requests weren't recognized as JSON, which was the reason for the error.
",,
Altair unexpected result,https://stackoverflow.com/questions/56633393,Trying to get altair to save .png or .svg in the PythonAnywhere platform,"foochart.save(""foo.svg"") requires Selenium and Chrome- or Gecko-driver.

Was trying to set this up on my PythonAnywhere instance.  The guys there pointed me at this link showing how to use Selenium inside PythonAnywhere. 

Of note,  they say only install v2 Selenium.  I installed selenium-2.53.6

But looks like it does not work, and I'm suspicious that altair needs a newer version, as the failure reads like the __init__ signature maybe changed:  TypeError: __init__() got an unexpected keyword argument 'options'

Can anyone advise on this?  I really am bummed that I cannot get altair and pythonAnywhere to cooperate -- the charts are so much nicer to generate when I don't have to use matplotlib,  but my server stuff runs inside PythonAnywhere.

the full code and stacktrace:

iris = data.iris()

foo = alt.Chart(iris).mark_point().encode(
    x='petalLength',
    y='petalWidth',
    color='species'
)
foo.save(""foo.png"")


stacktrace:

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-3-6aeea1cba6a3&gt; in &lt;module&gt;
----&gt; 1 foo.save(""foo.png"")

~/.virtualenvs/modern2019/lib/python3.6/site-packages/altair/vegalite/v2/api.py in save(self, fp, format, override_data_transformer, scale_factor, vegalite_version, vega_version, vegaembed_version, **kwargs)
    515         if override_data_transformer:
    516             with data_transformers.enable('default', max_rows=None):
--&gt; 517                 result = save(**kwds)
    518         else:
    519             result = save(**kwds)

~/.virtualenvs/modern2019/lib/python3.6/site-packages/altair/utils/save.py in save(chart, fp, vega_version, vegaembed_version, format, mode, vegalite_version, embed_options, json_kwds, webdriver, scale_factor)
    100                                         vegaembed_version=vegaembed_version,
    101                                         webdriver=webdriver,
--&gt; 102                                         scale_factor=scale_factor)
    103         if format == 'png':
    104             write_file_or_filename(fp, mimebundle['image/png'], mode='wb')

~/.virtualenvs/modern2019/lib/python3.6/site-packages/altair/utils/mimebundle.py in spec_to_mimebundle(spec, format, mode, vega_version, vegaembed_version, vegalite_version, **kwargs)
     54                               vega_version=vega_version,
     55                               vegaembed_version=vegaembed_version,
---&gt; 56                               vegalite_version=vegalite_version, **kwargs)
     57         if format == 'png':
     58             render = base64.b64decode(render.split(',', 1)[1].encode())

~/.virtualenvs/modern2019/lib/python3.6/site-packages/altair/utils/headless.py in compile_spec(spec, format, mode, vega_version, vegaembed_version, vegalite_version, scale_factor, driver_timeout, webdriver)
    152             webdriver_options.add_argument('--no-sandbox')
    153 
--&gt; 154     driver = webdriver_class(options=webdriver_options)
    155 
    156     try:

TypeError: __init__() got an unexpected keyword argument 'options'

",0,1676,"Altair assumes you are using selenium version 3 or later. Update your selenium installation and the error should go away.
",,
Altair unexpected issue,https://stackoverflow.com/questions/68271655,TypeError: to_list_if_array() got an unexpected keyword argument &#39;convert_dtype&#39;,"I am trying to load some data from a google sheet onto colab, transform it into a Pandas dataframe then plot it using Altair. But I am getting an unexpected error TypeError: to_list_if_array() got an unexpected keyword argument 'convert_dtype'.
What is going on? Is this because I am converting the columns to numeric data? I tried removing the conversion and it didn't work. When I draw a scatter plot using one of the altair premade datasets it works, so it must be an issue with the dataset. But all the dtypes in the dataset seem correct, as well as their entries.
# Import data from google sheet
from google.colab import auth
auth.authenticate_user()

import gspread
from oauth2client.client import GoogleCredentials

gc = gspread.authorize(GoogleCredentials.get_application_default())
worksheet = gc.open('My cool spreadsheet').sheet1

# get_all_values gives a list of rows.
rows = worksheet.get_all_values()

# Convert to a DataFrame
import pandas as pd
df = pd.DataFrame.from_records(rows)

# Use first row as column names
df = df.rename(columns=df.iloc[0])
df = df.drop(0)

# Encode some columns as numeric types
df['Victims'] = pd.to_numeric(df['Victims'], errors='coerce')
df['Starting year'] = pd.to_numeric(df['Starting year'], errors='coerce')

# Plot
import altair as alt

alt.Chart(df).mark_point().encode(
  x=alt.X('Starting year'),
  y=alt.Y('Victims'),
)

Error trace:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/altair/vegalite/v4/api.py in to_dict(self, *args, **kwargs)
    361         copy = self.copy(deep=False)
    362         original_data = getattr(copy, ""data"", Undefined)
--&gt; 363         copy.data = _prepare_data(original_data, context)
    364 
    365         if original_data is not Undefined:

12 frames
/usr/local/lib/python3.7/dist-packages/altair/vegalite/v4/api.py in _prepare_data(data, context)
     82     # convert dataframes  or objects with __geo_interface__ to dict
     83     if isinstance(data, pd.DataFrame) or hasattr(data, ""__geo_interface__""):
---&gt; 84         data = _pipe(data, data_transformers.get())
     85 
     86     # convert string input to a URLData

/usr/local/lib/python3.7/dist-packages/toolz/functoolz.py in pipe(data, *funcs)
    625     """"""
    626     for func in funcs:
--&gt; 627         data = func(data)
    628     return data
    629 

/usr/local/lib/python3.7/dist-packages/toolz/functoolz.py in __call__(self, *args, **kwargs)
    301     def __call__(self, *args, **kwargs):
    302         try:
--&gt; 303             return self._partial(*args, **kwargs)
    304         except TypeError as exc:
    305             if self._should_curry(args, kwargs, exc):

/usr/local/lib/python3.7/dist-packages/altair/vegalite/data.py in default_data_transformer(data, max_rows)
     17 @curried.curry
     18 def default_data_transformer(data, max_rows=5000):
---&gt; 19     return curried.pipe(data, limit_rows(max_rows=max_rows), to_values)
     20 
     21 

/usr/local/lib/python3.7/dist-packages/toolz/functoolz.py in pipe(data, *funcs)
    625     """"""
    626     for func in funcs:
--&gt; 627         data = func(data)
    628     return data
    629 

/usr/local/lib/python3.7/dist-packages/toolz/functoolz.py in __call__(self, *args, **kwargs)
    301     def __call__(self, *args, **kwargs):
    302         try:
--&gt; 303             return self._partial(*args, **kwargs)
    304         except TypeError as exc:
    305             if self._should_curry(args, kwargs, exc):

/usr/local/lib/python3.7/dist-packages/altair/utils/data.py in to_values(data)
    147         return {""values"": data}
    148     elif isinstance(data, pd.DataFrame):
--&gt; 149         data = sanitize_dataframe(data)
    150         return {""values"": data.to_dict(orient=""records"")}
    151     elif isinstance(data, dict):

/usr/local/lib/python3.7/dist-packages/altair/utils/core.py in sanitize_dataframe(df)
    346             # Convert numpy arrays saved as objects to lists
    347             # Arrays are not JSON serializable
--&gt; 348             col = df[col_name].apply(to_list_if_array, convert_dtype=False)
    349             df[col_name] = col.where(col.notnull(), None)
    350     return df

/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py in apply(self, func, axis, raw, result_type, args, **kwds)
   7550             kwds=kwds,
   7551         )
-&gt; 7552         return op.get_result()
   7553 
   7554     def applymap(self, func) -&gt; ""DataFrame"":

/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py in get_result(self)
    183             return self.apply_raw()
    184 
--&gt; 185         return self.apply_standard()
    186 
    187     def apply_empty_result(self):

/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py in apply_standard(self)
    274 
    275     def apply_standard(self):
--&gt; 276         results, res_index = self.apply_series_generator()
    277 
    278         # wrap results

/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py in apply_series_generator(self)
    303                 for i, v in enumerate(series_gen):
    304                     # ignore SettingWithCopy here in case the user mutates
--&gt; 305                     results[i] = self.f(v)
    306                     if isinstance(results[i], ABCSeries):
    307                         # If we have a view on v, we need to make a copy because

/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py in f(x)
    112 
    113             def f(x):
--&gt; 114                 return func(x, *args, **kwds)
    115 
    116         else:

TypeError: to_list_if_array() got an unexpected keyword argument 'convert_dtype'

The data sheet being loaded is this one
",3,2077,"The following works fine for me:
import altair as alt
import pandas as pd


df = pd.read_csv('https://docs.google.com/spreadsheets/d/1ZSZF7iX8jDcgP_uMpgy6vaQagLXzXmg4dqZCG4h-sOc/export?format=csv#gid=0')

# Encode some columns as numeric types
df['Victims'] = pd.to_numeric(df['Victims'], errors='coerce')
df['Starting year'] = pd.to_numeric(df['Starting year'], errors='coerce')

# Plot

alt.Chart(df).mark_point().encode(
  x=alt.X('Starting year'),
  y=alt.Y('Victims'),
)


",,
Altair unexpected issue,https://stackoverflow.com/questions/72084295,Grouped bar chart in newer versions of altair (&gt;= 4.2.0),"I am trying to create a grouped bar chart in altair like in the answer to this question here.
The particular interesting part is the ""beautification:
chart = Chart(df).mark_bar().encode(
   column=Column('Genre', 
                 axis=Axis(axisWidth=1.0, offset=-8.0, orient='bottom'),
                 scale=Scale(padding=4.0)),
   x=X('Gender', axis=False),
   y=Y('Rating', axis=Axis(grid=False)),
   color=Color('Gender', scale=Scale(range=['#EA98D2', '#659CCA']))
).configure_facet_cell(
    strokeWidth=0.0,
)

chart.display()

The issue is, however, that none of the stuff in the columns (alt.Column) works in the current version of Altair (I am using 4.2).
In particular, I am getting:

SchemaValidationError: Invalid specification
altair.vegalite.v4.schema.channels.Column, validating
'additionalProperties' Additional properties are not allowed ('axis'
was unexpected)

Can something similar still be done?
",1,1073,"In Altair 4.2.0 you achieve a similar results like this (not sure if you can connect the facets with the x-axis line):
import altair as alt
import pandas as pd

# create dataframe
df = pd.DataFrame([['Action', 5, 'F'], 
                   ['Crime', 10, 'F'], 
                   ['Action', 3, 'M'], 
                   ['Crime', 9, 'M']], 
                  columns=['Genre', 'Rating', 'Gender'])

chart = alt.Chart(df).mark_bar().encode(
   column=alt.Column(
       'Genre', 
       header=alt.Header(orient='bottom')
    ),
   x=alt.X('Gender', axis=alt.Axis(ticks=False, labels=False, title='')),
   y=alt.Y('Rating', axis=alt.Axis(grid=False)),
   color='Gender'
).configure_view(
    stroke=None,
)

chart


In the current development version of Altair (will probably be released as 5.0), you can use the new offset channels to achieve the same result without faceting:
chart = alt.Chart(df).mark_bar().encode(
   x=alt.X('Genre', axis=alt.Axis(labelAngle=0)),
   xOffset='Gender',
   y=alt.Y('Rating', axis=alt.Axis(grid=False)),
   color='Gender'
).configure_view(
    stroke=None,
)

chart


",,



